{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eisbetterthanpi/vision/blob/main/Meta_Pseudo_Labels_try.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v9XrV9ZACgjm"
      },
      "outputs": [],
      "source": [
        "# Meta Pseudo Labels mar 2021 https://arxiv.org/pdf/2003.10580v4.pdf\n",
        "# https://github.com/kekmodel/MPL-pytorch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title download\n",
        "# # # google images unlabeled\n",
        "# !gdown 1ncx2DJ-GXqrQd6nL5UEmj6GLT4w-9qYs -O house.zip\n",
        "# !unzip /content/house.zip -d /\n",
        "# !rm -R /content/house/.ipynb_checkpoints\n",
        "\n",
        "# # # 70k+gmap\n",
        "# !gdown 1-CZp7TbhJLeRQpbKQCyT8ofGg89Yt137 -O gsv.zip\n",
        "# !unzip /content/gsv.zip -d /\n",
        "# !rm -R /content/gsv70kg/.ipynb_checkpoints\n",
        "# !rm -R /content/gsv70kg/01/.ipynb_checkpoints\n",
        "# !rm -R /content/gsv70kg/02/.ipynb_checkpoints\n",
        "# !rm -R /content/gsv70kg/03/.ipynb_checkpoints\n",
        "# !rm -R /content/gsv70kg/04/.ipynb_checkpoints\n",
        "# !rm -R /content/gsv70kg/05/.ipynb_checkpoints\n",
        "# !rm -R /content/gsv70kg/06/.ipynb_checkpoints\n"
      ],
      "metadata": {
        "id": "z9AEoUWZekls",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title data\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "from torchvision import datasets, transforms\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
        "\n",
        "labeled_dir='/content/gsv70kg'\n",
        "\n",
        "labeled_data = datasets.ImageFolder(labeled_dir, transform=transform)\n",
        "torch.manual_seed(0)\n",
        "train_data, test_data = torch.utils.data.random_split(labeled_data, [.9,.1])\n",
        "# train_data, _ = torch.utils.data.random_split(train_data, [.01,.99])\n",
        "# test_data, _ = torch.utils.data.random_split(test_data, [.01,.99])\n",
        "finetune_dataset = train_data\n",
        "\n",
        "unlabel_dir='/content/house'\n",
        "# unlabel_data = datasets.ImageFolder(unlabel_dir, transform=transform)\n",
        "\n",
        "import os\n",
        "from PIL import Image\n",
        "class Datasetme(torch.utils.data.Dataset):\n",
        "    def __init__(self, dir, transform=None):\n",
        "        self.dir = dir\n",
        "        self.data = os.listdir(dir)\n",
        "        self.transform = transform\n",
        "    def __getitem__(self, index):\n",
        "        img_file = self.data[index]\n",
        "        img_file = os.path.join(self.dir, img_file)\n",
        "        image = Image.open(img_file).convert(\"RGB\")\n",
        "        if self.transform: image = self.transform(image)\n",
        "        return image\n",
        "    def __len__(self): return len(self.data)\n",
        "\n",
        "unlabel_data = Datasetme(unlabel_dir, transform=transform)\n",
        "# unlabel_data, _ = torch.utils.data.random_split(unlabel_data, [.01,.99])\n",
        "\n",
        "\n",
        "batch_size = 64 # 16 is max for res152; default 64/ mainargs128\n",
        "grad_acc = 1\n",
        "\n",
        "# res152 batch16 gradacc4\n",
        "\n",
        "num_batches=int(np.ceil(len(train_data)/batch_size))\n",
        "\n",
        "# train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, num_workers=4, pin_memory=True)\n",
        "# test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, num_workers=4, pin_memory=True)\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, pin_memory=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, pin_memory=True)\n",
        "\n",
        "train_sampler = RandomSampler\n",
        "# labeled_loader = DataLoader(labeled_data, sampler=train_sampler(labeled_data), batch_size=batch_size, num_workers=4, drop_last=True)\n",
        "# unlabeled_loader = DataLoader(unlabel_data, sampler=train_sampler(unlabel_data), batch_size=batch_size * 7, num_workers=4, drop_last=True) # mu=7 ,coefficient of unlabeled batch size\n",
        "# test_loader = DataLoader(test_data, sampler=SequentialSampler(test_data), batch_size=batch_size, num_workers=4)\n",
        "\n",
        "labeled_loader = DataLoader(labeled_data, sampler=train_sampler(labeled_data), batch_size=batch_size*grad_acc, drop_last=True)\n",
        "mu=7 # coefficient of unlabeled batch size\n",
        "unlabeled_loader = DataLoader(unlabel_data, sampler=train_sampler(unlabel_data), batch_size=batch_size*grad_acc * mu, drop_last=True)\n",
        "test_loader = DataLoader(test_data, sampler=SequentialSampler(test_data), batch_size=batch_size*grad_acc)\n",
        "\n",
        "del labeled_data, train_data, test_data, unlabel_data\n"
      ],
      "metadata": {
        "id": "1MAfJG1xTW3b",
        "cellView": "form"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title torch augment\n",
        "# https://github.com/facebookresearch/vicreg/blob/main/augmentations.py\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "class TrainTransform(object):\n",
        "    def __init__(self):\n",
        "        # self.transform = transforms.RandomApply([transforms.Compose([\n",
        "        self.transform = transforms.Compose([\n",
        "                transforms.RandomPerspective(distortion_scale=0.3, p=0.5), # me\n",
        "                transforms.RandomResizedCrop((400,640), scale=(0.7, 1.0), ratio=(0.8, 1.25), interpolation=transforms.InterpolationMode.BICUBIC),\n",
        "                # transforms.RandomResizedCrop((32,32), scale=(0.7, 1.0), ratio=(0.8, 1.25), interpolation=transforms.InterpolationMode.BICUBIC),\n",
        "                transforms.RandomHorizontalFlip(p=0.5), # 0.5\n",
        "                transforms.Lambda(lambda x : torch.clamp(x, 0., 1.)), # clamp else ColorJitter will return nan https://discuss.pytorch.org/t/input-is-nan-after-transformation/125455/6\n",
        "                transforms.RandomApply([transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1)], p=0.8,), # brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1)], p=0.8\n",
        "                transforms.RandomGrayscale(p=0.2), # 0.2\n",
        "                # # transforms.RandomChoice(transforms.ColorJitter , transforms.RandomGrayscale(p=1.)\n",
        "                transforms.RandomApply([transforms.GaussianBlur(kernel_size=5, sigma=(0.1, 2.0)),], p=1.0),\n",
        "                # # transforms.RandomSolarize(threshold=130, p=0.5)\n",
        "                transforms.RandomErasing(p=1., scale=(0.1, 0.11), ratio=(1,1), value=(0.485, 0.456, 0.406)),\n",
        "                # transforms.ToTensor(), # ToTensored at dataset level, no need to ToTensor again\n",
        "                # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), # normalised at dataset level. default 0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225\n",
        "                ])\n",
        "            # ], p=1.)\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        dims = len(sample.shape)\n",
        "        if dims==3: x1 = self.transform(sample) # same transforms per minibatch\n",
        "        elif dims==4: x1 = transforms.Lambda(lambda x: torch.stack([self.transform(x_) for x_ in x]))(sample) # diff transforms per img in minibatch\n",
        "        # x1 = self.transform(sample)\n",
        "        return x1\n",
        "\n",
        "trs=TrainTransform()\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "6iYyVffJcYB5"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title utils\n",
        "# https://github.com/kekmodel/MPL-pytorch/blob/main/utils.py\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "def create_loss_fn():\n",
        "    label_smoothing = 0 # default 0 / mainargs 0.15\n",
        "    # if label_smoothing > 0:\n",
        "    #     criterion = SmoothCrossEntropyV2(alpha=label_smoothing)\n",
        "    # else:\n",
        "    criterion = nn.CrossEntropyLoss(label_smoothing=label_smoothing)\n",
        "    return criterion.to(device)\n",
        "\n",
        "from collections import OrderedDict\n",
        "def module_load_state_dict(model, state_dict):\n",
        "    try:\n",
        "        new_state_dict = OrderedDict()\n",
        "        for k, v in state_dict.items():\n",
        "            name = k[7:]  # remove `module.`\n",
        "            new_state_dict[name] = v\n",
        "        model.load_state_dict(new_state_dict)\n",
        "    except:\n",
        "        new_state_dict = OrderedDict()\n",
        "        for k, v in state_dict.items():\n",
        "            name = f'module.{k}'  # add `module.`\n",
        "            new_state_dict[name] = v\n",
        "        model.load_state_dict(new_state_dict)\n",
        "\n",
        "def model_load_state_dict(model, state_dict):\n",
        "    try: model.load_state_dict(state_dict)\n",
        "    except: module_load_state_dict(model, state_dict)\n",
        "\n",
        "\n",
        "def accuracy(output, target, topk=(1,)):\n",
        "    output = output.to(torch.device('cpu'))\n",
        "    target = target.to(torch.device('cpu'))\n",
        "    maxk = max(topk)\n",
        "    batch_size = target.shape[0]\n",
        "    _, idx = output.sort(dim=1, descending=True)\n",
        "    pred = idx.narrow(1, 0, maxk).t()\n",
        "    correct = pred.eq(target.reshape(1, -1).expand_as(pred))\n",
        "    res = []\n",
        "    for k in topk:\n",
        "        correct_k = correct[:k].reshape(-1).float().sum(dim=0, keepdim=True)\n",
        "        res.append(correct_k.mul_(100.0 / batch_size))\n",
        "    return res\n",
        "\n",
        "\n",
        "class SmoothCrossEntropy(nn.Module):\n",
        "    def __init__(self, alpha=0.1):\n",
        "        super(SmoothCrossEntropy, self).__init__()\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def forward(self, logits, labels):\n",
        "        if self.alpha == 0:\n",
        "            loss = F.cross_entropy(logits, labels)\n",
        "        else:\n",
        "            num_classes = logits.shape[-1]\n",
        "            alpha_div_k = self.alpha / num_classes\n",
        "            target_probs = F.one_hot(labels, num_classes=num_classes).float() * (1. - self.alpha) + alpha_div_k\n",
        "            loss = (-(target_probs * torch.log_softmax(logits, dim=-1)).sum(dim=-1)).mean()\n",
        "        return loss\n",
        "\n",
        "class SmoothCrossEntropyV2(nn.Module):\n",
        "    \"\"\"NLL loss with label smoothing.\"\"\"\n",
        "    def __init__(self, label_smoothing=0.1):\n",
        "        super().__init__()\n",
        "        assert label_smoothing < 1.0\n",
        "        self.smoothing = label_smoothing\n",
        "        self.confidence = 1. - label_smoothing\n",
        "\n",
        "    def forward(self, x, target):\n",
        "        if self.smoothing == 0:\n",
        "            loss = F.cross_entropy(x, target)\n",
        "        else:\n",
        "            logprobs = F.log_softmax(x, dim=-1)\n",
        "            nll_loss = -logprobs.gather(dim=-1, index=target.unsqueeze(1))\n",
        "            nll_loss = nll_loss.squeeze(1)\n",
        "            smooth_loss = -logprobs.mean(dim=-1)\n",
        "            loss = (self.confidence * nll_loss + self.smoothing * smooth_loss).mean()\n",
        "        return loss\n",
        "\n",
        "\n",
        "# from main\n",
        "import math\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "def get_cosine_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps, num_wait_steps=0, num_cycles=0.5, last_epoch=-1):\n",
        "    def lr_lambda(current_step):\n",
        "        if current_step < num_wait_steps:\n",
        "            return 0.0\n",
        "        if current_step < num_warmup_steps + num_wait_steps:\n",
        "            return float(current_step) / float(max(1, num_warmup_steps + num_wait_steps))\n",
        "        progress = float(current_step - num_warmup_steps - num_wait_steps) / float(max(1, num_training_steps - num_warmup_steps - num_wait_steps))\n",
        "        return max(0.0, 0.5 * (1.0 + math.cos(math.pi * float(num_cycles) * 2.0 * progress)))\n",
        "    return LambdaLR(optimizer, lr_lambda, last_epoch)\n",
        "\n"
      ],
      "metadata": {
        "id": "q_bJYwwSDNNy",
        "cellView": "form"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ModelEMA\n",
        "# expopnential moving average, smoothen model parameters\n",
        "# https://github.com/kekmodel/MPL-pytorch/blob/main/models.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from copy import deepcopy\n",
        "\n",
        "class ModelEMA(nn.Module):\n",
        "    def __init__(self, model, decay=0.9999, device=None):\n",
        "        super().__init__()\n",
        "        self.module = deepcopy(model)\n",
        "        self.module.eval()\n",
        "        self.decay = decay\n",
        "        self.device = device\n",
        "        if self.device is not None:\n",
        "            self.module.to(device=device)\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.module(input)\n",
        "\n",
        "    def _update(self, model, update_fn):\n",
        "        with torch.no_grad():\n",
        "            for ema_v, model_v in zip(self.module.parameters(), model.parameters()):\n",
        "                if self.device is not None:\n",
        "                    model_v = model_v.to(device=self.device)\n",
        "                ema_v.copy_(update_fn(ema_v, model_v))\n",
        "            for ema_v, model_v in zip(self.module.buffers(), model.buffers()):\n",
        "                if self.device is not None:\n",
        "                    model_v = model_v.to(device=self.device)\n",
        "                ema_v.copy_(model_v)\n",
        "\n",
        "    def update_parameters(self, model):\n",
        "        self._update(model, update_fn=lambda e, m: self.decay * e + (1. - self.decay) * m)\n",
        "\n",
        "    def state_dict(self):\n",
        "        return self.module.state_dict()\n",
        "\n",
        "    def load_state_dict(self, state_dict):\n",
        "        self.module.load_state_dict(state_dict)\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "4J3GeedWa6QO"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title big teacher\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "# !gdown 1ysJfdsvwMiWbCdkvFHwNqAUnJTtm6KbT -O res152adamw71.pth # ty\n",
        "# !gdown 1VaPxGoaLjmt7K9VHi0FWbJ5efEZTLhwd -O res18teacher.pth # A\n",
        "# !pip install bitsandbytes\n",
        "\n",
        "model = models.resnet18(weights='DEFAULT') # 18 34 50 101 152\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Sequential( # og (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
        "    # nn.Linear(num_ftrs, num_classes, bias=False),\n",
        "    # nn.Softmax(dim=1),\n",
        "    )\n",
        "\n",
        "# model.load_state_dict(torch.load('/content/bigTeacher.pth'))\n",
        "# _, modelsd, _,_ = torch.load('/content/bigTeacher.pth').values()\n",
        "# _, modelsd, _,_ = torch.load('/content/res152adamw71.pth').values()\n",
        "_, modelsd, _,_ = torch.load('/content/res18teacher.pth').values()\n",
        "model.load_state_dict(modelsd, strict=False)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "# model = torch.compile(model.to(device)) # compiling teacher leads to significantly higher vram usage\n",
        "\n",
        "\n",
        "model.eval()\n",
        "print('uh')\n",
        "\n",
        "\n",
        "# torch.save(model.state_dict(), '/content/model.pth')\n",
        "# modelsd = torch.load('/content/model.pth')\n",
        "# model.load_state_dict(modelsd, strict=False)\n",
        "# # model = torch.compile(model.to(device))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2n6C1jpb-WOk",
        "outputId": "d5212d64-cda3-45a5-fc9e-da86da5d7e99",
        "cellView": "form"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "uh\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title model\n",
        "\n",
        "num_classes = 6\n",
        "# if dataset == \"cifar10\": depth, widen_factor = 28, 2\n",
        "# elif dataset == 'cifar100': depth, widen_factor = 28, 8\n",
        "# teacher_model = WideResNet(num_classes=num_classes, depth=depth, widen_factor=widen_factor, dropout=0, dense_dropout=0.2)\n",
        "# student_model = WideResNet(num_classes=num_classes, depth=depth, widen_factor=widen_factor, dropout=0, dense_dropout=0.2)\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "def get_resnet():\n",
        "    model = models.resnet18(weights='DEFAULT') # 18 34 50 101 152\n",
        "    num_ftrs = model.fc.in_features\n",
        "    model.fc = nn.Sequential( # og (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
        "        nn.Linear(num_ftrs, num_classes, bias=False),\n",
        "        nn.Softmax(dim=1),\n",
        "        )\n",
        "    # model = model.to(device)\n",
        "    model = torch.compile(model.to(device))\n",
        "    return model\n",
        "\n",
        "class Small(nn.Module):\n",
        "    def __init__(self, embed_dim, output_dim):\n",
        "        super(Small, self).__init__()\n",
        "        hidden_size=512\n",
        "        self.lin = nn.Sequential(\n",
        "            nn.Linear(embed_dim, hidden_size), nn.ReLU(),\n",
        "            nn.Linear(hidden_size, hidden_size), nn.ReLU(),\n",
        "            nn.Linear(hidden_size, hidden_size), nn.ReLU(),\n",
        "            nn.Linear(hidden_size, hidden_size), nn.ReLU(),\n",
        "            nn.Linear(hidden_size, output_dim, bias=False),\n",
        "            nn.Softmax(dim=1),\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        logits = self.lin(x)\n",
        "        return logits\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# @title ensemble\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Ensemble(nn.Module):\n",
        "    def __init__(self, embed_dim, output_dim):\n",
        "        super().__init__()\n",
        "        self.output_dim = output_dim # 6\n",
        "        self.embed_dim = embed_dim\n",
        "        h_dim = 512\n",
        "        self.fwd = nn.Sequential(\n",
        "            nn.Linear(self.embed_dim, h_dim), nn.ReLU(),\n",
        "            # nn.Linear(h_dim, h_dim), nn.ReLU(),\n",
        "            # Block(h_dim, h_dim, 0.5),\n",
        "            Block(h_dim, h_dim),\n",
        "            Block(h_dim, h_dim),\n",
        "            Block(h_dim, h_dim),\n",
        "            nn.Linear(h_dim, self.output_dim, bias=False),\n",
        "            nn.Softmax(dim=1),\n",
        "            )\n",
        "    def forward(self, x):\n",
        "        out = self.fwd(x)\n",
        "        return out\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim, drop=None):\n",
        "        super().__init__()\n",
        "        if drop: self.fwd = nn.Sequential(nn.BatchNorm1d(in_dim), nn.Dropout(drop), nn.Linear(in_dim, out_dim), nn.ReLU(),)\n",
        "        else: self.fwd = nn.Sequential(nn.Linear(in_dim, out_dim), nn.ReLU(),)\n",
        "    def forward(self, x):\n",
        "        return x + self.fwd(x)\n",
        "\n",
        "# teacher_model = Ensemble(2048, 6).to(device)\n",
        "# teacher_model = torch.compile(Ensemble(2048, 6).to(device))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# teacher_model = Small(num_ftrs,6).to(device)\n",
        "teacher_model = torch.compile(Small(num_ftrs,6).to(device))\n",
        "# teacher_model = get_resnet()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "student_model = get_resnet()\n",
        "_, modelsd, _,_ = torch.load('/content/res18teacher.pth').values()\n",
        "student_model.load_state_dict(modelsd, strict=False)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "student_model = student_model.to(device)\n",
        "# student_model = torch.compile(student_model.to(device)) #\n",
        "\n",
        "\n",
        "avg_student_model = None\n",
        "ema = 0.995 # default 0 / mainargs 0.995\n",
        "\n",
        "if ema > 0: avg_student_model = ModelEMA(student_model, ema)\n",
        "\n",
        "no_decay = ['bn']\n",
        "weight_decay = 5e-4 # default 0 / mainargs 5e-4\n",
        "teacher_parameters = [{'params': [p for n, p in teacher_model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': weight_decay},\n",
        "    {'params': [p for n, p in teacher_model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}]\n",
        "student_parameters = [{'params': [p for n, p in student_model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': weight_decay},\n",
        "    {'params': [p for n, p in student_model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}]\n",
        "\n",
        "# teacher_model.zero_grad()\n",
        "# student_model.zero_grad()\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PVFBeYZgUya1",
        "cellView": "form"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title try\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# from torchvision import models\n",
        "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# # # 1.9, 1.7, 1.2\n",
        "# im = torch.rand(32,3,400,680,device=device) #\n",
        "# # print(32*3*400*680*32/8)\n",
        "# # print(im.element_size() * im.nelement())\n",
        "\n",
        "# def count_parameters(model): return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "# # print(count_parameters(model))\n",
        "\n",
        "\n",
        "# model = models.resnet152(weights='DEFAULT') # 18 34 50 101 152\n",
        "# model.fc = nn.Sequential()\n",
        "# model = model.to(device)\n",
        "# # model = torch.compile(model.to(device))\n",
        "# # 32, 50:3.3-5=1.7\n",
        "# # print(count_parameters(model))\n",
        "\n",
        "# # amp res152 student cant train on batch 32, compile or not\n",
        "\n",
        "# torch.backends.cuda.matmul.allow_tf32 = True\n",
        "# torch.backends.cudnn.allow_tf32 = True\n",
        "# model.eval()\n",
        "# student_model.eval()\n",
        "# with torch.cuda.amp.autocast():\n",
        "#     with torch.no_grad(): #\n",
        "#         # out = model(im) # 1:2.7, 4:6.3, 8:12.8\n",
        "#         out = student_model(im) #\n",
        "# # 16*15: 11.6\n",
        "# 32:1.3-8.7=6.4\n",
        "\n",
        "# student_model.train()\n",
        "# out = student_model(im) #\n",
        "\n",
        "# model.train()\n",
        "# with torch.cuda.amp.autocast():\n",
        "#     out = model(im) #\n",
        "\n",
        "\n",
        "# # 152:3.2\n",
        "# # im = torch.rand(4,3,400,680,device=device) #\n",
        "# # out = model(im) # 5.9\n",
        "\n",
        "# im = torch.rand(16,2048,device=device) #\n",
        "# out = teacher_model(im) # 5.9\n",
        "\n",
        "# torch.cuda.empty_cache()\n"
      ],
      "metadata": {
        "id": "ABqGijQ6H27_",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title wandb\n",
        "# https://docs.wandb.ai/quickstart\n",
        "!pip install wandb\n",
        "import wandb\n",
        "wandb.login() # 487a2109e55dce4e13fc70681781de9f50f27be7\n",
        "run = wandb.init(\n",
        "    project=\"mpl\",\n",
        "    config={\n",
        "        \"optim\": \"adamw\",\n",
        "        # \"lr\": lr,\n",
        "        # \"epochs\": epochs,\n",
        "    })\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "cellView": "form",
        "id": "2UGs03MKAHxs",
        "outputId": "24065b81-8eb4-4cad-dbb8-a247174fb86a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.15.5)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.6)\n",
            "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.32)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.27.1)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.28.1)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.10/dist-packages (from wandb) (0.1.2)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbobdole\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.15.5"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230724_043849-b9vwruqi</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/bobdole/mpl/runs/b9vwruqi' target=\"_blank\">swift-snowflake-10</a></strong> to <a href='https://wandb.ai/bobdole/mpl' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/bobdole/mpl' target=\"_blank\">https://wandb.ai/bobdole/mpl</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/bobdole/mpl/runs/b9vwruqi' target=\"_blank\">https://wandb.ai/bobdole/mpl/runs/b9vwruqi</a>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title main mpl train\n",
        "# ########## https://github.com/kekmodel/MPL-pytorch/blob/main/main.py\n",
        "# import random\n",
        "# import numpy as np\n",
        "# import torch\n",
        "# from torch.cuda import amp\n",
        "# from torch import nn\n",
        "# from torch.nn import functional as F\n",
        "\n",
        "# ema = 0.995 # default 0 / mainargs 0.995\n",
        "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# random.seed(0)\n",
        "# np.random.seed(0)\n",
        "# torch.manual_seed(0)\n",
        "# torch.cuda.manual_seed_all(0)\n",
        "\n",
        "# torch.backends.cuda.matmul.allow_tf32 = True\n",
        "# torch.backends.cudnn.allow_tf32 = True\n",
        "\n",
        "# t_scaler = amp.GradScaler()\n",
        "# s_scaler = amp.GradScaler()\n",
        "# def train(labeled_loader, unlabeled_loader, teacher_model, student_model,\n",
        "#         avg_student_model, criterion, t_optimizer, s_optimizer, t_scheduler, s_scheduler):\n",
        "#     labeled_iter = iter(labeled_loader)\n",
        "#     unlabeled_iter = iter(unlabeled_loader)\n",
        "\n",
        "#     # for author's code formula\n",
        "#     # moving_dot_product = torch.empty(1).to(device)\n",
        "#     # limit = 3.0**(0.5)  # 3 = 6 / (f_in + f_out)\n",
        "#     # nn.init.uniform_(moving_dot_product, -limit, limit)\n",
        "\n",
        "#     # eval_step = 10#1000\n",
        "#     # start_step=0\n",
        "#     # for step in range(start_step, total_steps):\n",
        "#     for step in range(len(unlabeled_loader)):\n",
        "#         teacher_model.train()\n",
        "#         student_model.train()\n",
        "\n",
        "#         try: images_l, targets = next(labeled_iter)\n",
        "#         except:\n",
        "#             labeled_iter = iter(labeled_loader)\n",
        "#             images_l, targets = next(labeled_iter)\n",
        "#         try: images_uw = next(unlabeled_iter) # images_uw, _ = next(unlabeled_iter)\n",
        "#         except:\n",
        "#             unlabeled_iter = iter(unlabeled_loader)\n",
        "#             images_uw = next(unlabeled_iter) # me\n",
        "#         images_l, targets = images_l.to(device), targets.to(device)\n",
        "#         # images_uw, images_us = images_uw.to(device), images_us.to(device)\n",
        "#         images_uw = images_uw.to(device)\n",
        "#         images_us = trs(images_uw)\n",
        "\n",
        "#         with amp.autocast():\n",
        "#             batch_size = images_l.shape[0]\n",
        "#             # print(images_l.shape, images_uw.shape, images_us.shape) # [16, 3, 400, 640], [112, 3, 400, 640], [112, 3, 400, 640] mu*batch_size\n",
        "#             t_images = torch.cat((images_l, images_uw, images_us))\n",
        "#             t_logits = teacher_model(t_images)\n",
        "#             t_logits_l = t_logits[:batch_size]\n",
        "#             t_logits_uw, t_logits_us = t_logits[batch_size:].chunk(2)\n",
        "#             del t_logits\n",
        "\n",
        "#             t_loss_l = criterion(t_logits_l, targets)\n",
        "\n",
        "#             temperature = 1 # default 1 / mainargs 0.7\n",
        "#             soft_pseudo_label = torch.softmax(t_logits_uw.detach() / temperature, dim=-1)\n",
        "#             max_probs, hard_pseudo_label = torch.max(soft_pseudo_label, dim=-1)\n",
        "\n",
        "#             threshold = 0.95 # default 0.95 / mainargs 0.6\n",
        "#             mask = max_probs.ge(threshold).float()\n",
        "#             t_loss_u = torch.mean(-(soft_pseudo_label * torch.log_softmax(t_logits_us, dim=-1)).sum(dim=-1) * mask)\n",
        "#             lambda_u = 8 # default 1 / mainargs 8 coefficient of unlabeled loss\n",
        "#             uda_steps = 10 # default 1 / mainargs 5000 warmup steps of lambda-u\n",
        "#             weight_u = lambda_u * min(1., (step + 1) / uda_steps)\n",
        "#             t_loss_uda = t_loss_l + weight_u * t_loss_u\n",
        "\n",
        "#             s_images = torch.cat((images_l, images_us))\n",
        "#             s_logits = student_model(s_images)\n",
        "#             s_logits_l = s_logits[:batch_size]\n",
        "#             s_logits_us = s_logits[batch_size:]\n",
        "#             del s_logits\n",
        "\n",
        "#             s_loss_l_old = F.cross_entropy(s_logits_l.detach(), targets)\n",
        "\n",
        "#             # print(\"s_logits_us, hard_pseudo_label: \", s_logits_us.shape, hard_pseudo_label.shape) # [448, 10] [224]\n",
        "#             s_loss = criterion(s_logits_us, hard_pseudo_label)\n",
        "\n",
        "#         s_scaler.scale(s_loss).backward()\n",
        "\n",
        "#         # if grad_clip > 0:\n",
        "#         s_scaler.unscale_(s_optimizer)\n",
        "#         nn.utils.clip_grad_norm_(student_model.parameters(), 1e9)\n",
        "\n",
        "#         s_scaler.step(s_optimizer)\n",
        "#         s_scaler.update()\n",
        "#         s_scheduler.step()\n",
        "\n",
        "#         if ema > 0: avg_student_model.update_parameters(student_model)\n",
        "\n",
        "#         with amp.autocast():\n",
        "#             with torch.no_grad():\n",
        "#                 s_logits_l = student_model(images_l)\n",
        "#             s_loss_l_new = F.cross_entropy(s_logits_l.detach(), targets)\n",
        "\n",
        "#             dot_product = s_loss_l_old - s_loss_l_new # theoretically correct formula (https://github.com/kekmodel/MPL-pytorch/issues/6)\n",
        "#             # dot_product = s_loss_l_new - s_loss_l_old # author's code formula\n",
        "#             # # moving_dot_product = moving_dot_product * 0.99 + dot_product * 0.01\n",
        "#             # # dot_product = dot_product - moving_dot_product\n",
        "\n",
        "#             _, hard_pseudo_label = torch.max(t_logits_us.detach(), dim=-1)\n",
        "#             t_loss_mpl = dot_product * F.cross_entropy(t_logits_us, hard_pseudo_label)\n",
        "\n",
        "#             # t_loss_mpl = torch.tensor(0.).to(device) # test\n",
        "#             t_loss = t_loss_uda + t_loss_mpl\n",
        "\n",
        "#         t_scaler.scale(t_loss).backward()\n",
        "\n",
        "#         # if grad_clip > 0:\n",
        "#         t_scaler.unscale_(t_optimizer)\n",
        "#         nn.utils.clip_grad_norm_(teacher_model.parameters(), 1e9)\n",
        "\n",
        "#         t_scaler.step(t_optimizer)\n",
        "#         t_scaler.update()\n",
        "#         t_scheduler.step()\n",
        "\n",
        "#         teacher_model.zero_grad()\n",
        "#         student_model.zero_grad()\n",
        "\n",
        "#         # if (step + 1) % eval_step == 0:\n",
        "#         #     # print(s_losses, t_losses, t_losses_l, t_losses_u, t_losses_mpl, mean_mask)\n",
        "#         #     test_model = avg_student_model if avg_student_model is not None else student_model\n",
        "#         #     test_loss, top1, top5 = evaluate(test_loader, test_model, criterion)\n",
        "#     return\n",
        "\n"
      ],
      "metadata": {
        "id": "wWBABdFBjZ9u",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title mpl try 1\n",
        "# # https://github.com/kekmodel/MPL-pytorch/blob/main/main.py\n",
        "# import random\n",
        "# import numpy as np\n",
        "# import torch\n",
        "# from torch.cuda import amp\n",
        "# from torch import nn\n",
        "# from torch.nn import functional as F\n",
        "\n",
        "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# random.seed(0)\n",
        "# np.random.seed(0)\n",
        "# torch.manual_seed(0)\n",
        "# torch.cuda.manual_seed_all(0)\n",
        "\n",
        "# torch.backends.cuda.matmul.allow_tf32 = True\n",
        "# torch.backends.cudnn.allow_tf32 = True\n",
        "\n",
        "# t_scaler = amp.GradScaler()\n",
        "# s_scaler = amp.GradScaler()\n",
        "# def train(labeled_loader, unlabeled_loader, teacher_model, student_model,\n",
        "#         avg_student_model, criterion, t_optimizer, s_optimizer, t_scheduler, s_scheduler):\n",
        "#     labeled_iter = iter(labeled_loader)\n",
        "#     unlabeled_iter = iter(unlabeled_loader)\n",
        "#     size = len(unlabeled_loader)\n",
        "\n",
        "#     # for author's code formula\n",
        "#     # moving_dot_product = torch.empty(1).to(device)\n",
        "#     # limit = 3.0**(0.5)  # 3 = 6 / (f_in + f_out)\n",
        "#     # nn.init.uniform_(moving_dot_product, -limit, limit)\n",
        "\n",
        "#     teacher_model.train()\n",
        "#     student_model.train()\n",
        "#     # eval_step = 10#1000\n",
        "#     # start_step=0\n",
        "#     # for step in range(start_step, total_steps):\n",
        "#     for step in range(len(unlabeled_loader)):\n",
        "\n",
        "#         try: images_l, targets = next(labeled_iter)\n",
        "#         except:\n",
        "#             labeled_iter = iter(labeled_loader)\n",
        "#             images_l, targets = next(labeled_iter)\n",
        "#         try: images_uw = next(unlabeled_iter) # images_uw, _ = next(unlabeled_iter)\n",
        "#         except:\n",
        "#             unlabeled_iter = iter(unlabeled_loader)\n",
        "#             images_uw = next(unlabeled_iter) # me\n",
        "#         images_l, targets = images_l.to(device), targets.to(device)\n",
        "#         images_uw = images_uw.to(device)\n",
        "#         images_us = trs(images_uw)\n",
        "\n",
        "#         with amp.autocast():\n",
        "#             batch_size = images_l.shape[0]\n",
        "\n",
        "#             t_images = torch.cat((images_l, images_us))\n",
        "#             with torch.no_grad(): t_images = model(t_images) # reduced pml # big teacher\n",
        "#             t_logits = teacher_model(t_images)\n",
        "#             t_logits_l, t_logits_us = t_logits[:batch_size], t_logits[batch_size:]\n",
        "#             del t_logits\n",
        "#             torch.cuda.empty_cache()\n",
        "#             with torch.no_grad(): # t_logits_uw no need grad\n",
        "#                 images_uw = model(images_uw) # reduced pml # big teacher\n",
        "#                 t_logits_uw = teacher_model(images_uw)\n",
        "#             torch.cuda.empty_cache()\n",
        "#             temperature = 1 # default 1 / mainargs 0.7\n",
        "#             # soft_pseudo_label = torch.softmax(t_logits_uw.detach() / temperature, dim=-1)\n",
        "#             soft_pseudo_label = torch.softmax(t_logits_uw / temperature, dim=-1)\n",
        "#             max_probs, hard_pseudo_label = torch.max(soft_pseudo_label, dim=-1)\n",
        "\n",
        "\n",
        "#         with amp.autocast():\n",
        "#             t_loss_l = criterion(t_logits_l, targets)\n",
        "#         # t_scaler.scale(t_loss_l).backward() # me backward first\n",
        "\n",
        "#         with amp.autocast():\n",
        "#             threshold = 0.95 # default 0.95 / mainargs 0.6\n",
        "#             mask = max_probs.ge(threshold).float()\n",
        "#             t_loss_u = torch.mean(-(soft_pseudo_label * torch.log_softmax(t_logits_us, dim=-1)).sum(dim=-1) * mask)\n",
        "#             lambda_u = 8 # default 1 / mainargs 8 coefficient of unlabeled loss\n",
        "#             uda_steps = 10 # default 1 / mainargs 5000 warmup steps of lambda-u\n",
        "#             weight_u = lambda_u * min(1., (step + 1) / uda_steps)\n",
        "#             t_loss_uda = t_loss_l + weight_u * t_loss_u\n",
        "#         # t_scaler.scale(weight_u * t_loss_u).backward()\n",
        "#         # del soft_pseudo_label, t_loss_u\n",
        "\n",
        "\n",
        "\n",
        "#         with amp.autocast():\n",
        "#             s_logits_us = student_model(images_us)\n",
        "#             with torch.no_grad():\n",
        "#                 s_logits_l = student_model(images_l)\n",
        "#             # s_loss_l_old = F.cross_entropy(s_logits_l.detach(), targets)\n",
        "#             s_loss_l_old = F.cross_entropy(s_logits_l, targets)\n",
        "#             s_loss = criterion(s_logits_us, hard_pseudo_label)\n",
        "#         s_scaler.scale(s_loss).backward()\n",
        "\n",
        "\n",
        "#         # if grad_clip > 0:\n",
        "#         s_scaler.unscale_(s_optimizer)\n",
        "#         nn.utils.clip_grad_norm_(student_model.parameters(), 1e9)\n",
        "#         s_scaler.step(s_optimizer)\n",
        "#         s_scaler.update()\n",
        "#         s_scheduler.step()\n",
        "#         student_model.zero_grad()\n",
        "#         if ema > 0: avg_student_model.update_parameters(student_model)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#         with amp.autocast():\n",
        "#             with torch.no_grad():\n",
        "#                 s_logits_l = student_model(images_l)\n",
        "#             # s_loss_l_new = F.cross_entropy(s_logits_l.detach(), targets)\n",
        "#             s_loss_l_new = F.cross_entropy(s_logits_l, targets)\n",
        "\n",
        "#             dot_product = s_loss_l_old - s_loss_l_new # theoretically correct formula (https://github.com/kekmodel/MPL-pytorch/issues/6)\n",
        "#             # dot_product = s_loss_l_new - s_loss_l_old # author's code formula\n",
        "#             # # moving_dot_product = moving_dot_product * 0.99 + dot_product * 0.01\n",
        "#             # # dot_product = dot_product - moving_dot_product\n",
        "\n",
        "#             _, hard_pseudo_label = torch.max(t_logits_us.detach(), dim=-1)\n",
        "#             t_loss_mpl = dot_product * F.cross_entropy(t_logits_us, hard_pseudo_label) # dot_product no grad\n",
        "\n",
        "#             # t_loss_mpl = torch.tensor(0.).to(device) # test\n",
        "#             t_loss = t_loss_uda + t_loss_mpl\n",
        "#         t_scaler.scale(t_loss).backward()\n",
        "#         print(\"t_loss, s_loss\", t_loss.item(), s_loss.item())\n",
        "#         # t_scaler.scale(t_loss_mpl).backward()\n",
        "\n",
        "\n",
        "#         t_scaler.unscale_(t_optimizer) # if grad_clip > 0:\n",
        "#         nn.utils.clip_grad_norm_(teacher_model.parameters(), 1e9)\n",
        "#         t_scaler.step(t_optimizer)\n",
        "#         t_scaler.update()\n",
        "#         t_scheduler.step()\n",
        "#         teacher_model.zero_grad()\n",
        "\n",
        "#     return\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "s9rTVKxUZhmn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title mpl try\n",
        "# # https://github.com/kekmodel/MPL-pytorch/blob/main/main.py\n",
        "# import random\n",
        "# import numpy as np\n",
        "# import torch\n",
        "# from torch.cuda import amp\n",
        "# from torch import nn\n",
        "# from torch.nn import functional as F\n",
        "\n",
        "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# random.seed(0)\n",
        "# np.random.seed(0)\n",
        "# torch.manual_seed(0)\n",
        "# torch.cuda.manual_seed_all(0)\n",
        "\n",
        "# torch.backends.cuda.matmul.allow_tf32 = True\n",
        "# torch.backends.cudnn.allow_tf32 = True\n",
        "\n",
        "# t_scaler = amp.GradScaler()\n",
        "# s_scaler = amp.GradScaler()\n",
        "# def train(labeled_loader, unlabeled_loader, teacher_model, student_model,\n",
        "#         avg_student_model, criterion, t_optimizer, s_optimizer, t_scheduler, s_scheduler):\n",
        "#     labeled_iter = iter(labeled_loader)\n",
        "#     unlabeled_iter = iter(unlabeled_loader)\n",
        "#     size = len(unlabeled_loader)\n",
        "\n",
        "#     # for author's code formula\n",
        "#     # moving_dot_product = torch.empty(1).to(device)\n",
        "#     # limit = 3.0**(0.5)  # 3 = 6 / (f_in + f_out)\n",
        "#     # nn.init.uniform_(moving_dot_product, -limit, limit)\n",
        "\n",
        "#     teacher_model.train()\n",
        "#     student_model.train()\n",
        "#     # eval_step = 10#1000\n",
        "#     # start_step=0\n",
        "#     # for step in range(start_step, total_steps):\n",
        "#     for step in range(len(unlabeled_loader)):\n",
        "\n",
        "#         try: images_l, targets = next(labeled_iter)\n",
        "#         except:\n",
        "#             labeled_iter = iter(labeled_loader)\n",
        "#             images_l, targets = next(labeled_iter)\n",
        "#         try: images_uw = next(unlabeled_iter) # images_uw, _ = next(unlabeled_iter)\n",
        "#         except:\n",
        "#             unlabeled_iter = iter(unlabeled_loader)\n",
        "#             images_uw = next(unlabeled_iter) # me\n",
        "#         images_l, targets = images_l.to(device), targets.to(device)\n",
        "#         images_uw = images_uw.to(device)\n",
        "#         images_us = trs(images_uw)\n",
        "\n",
        "#         with amp.autocast():\n",
        "#             batch_size = images_l.shape[0]\n",
        "\n",
        "#             t_images = torch.cat((images_l, images_us))\n",
        "#             with torch.no_grad(): t_images = model(t_images) # reduced pml # big teacher\n",
        "#             t_logits = teacher_model(t_images)\n",
        "#             t_logits_l, t_logits_us = t_logits[:batch_size], t_logits[batch_size:]\n",
        "#             del t_logits, t_images\n",
        "#             torch.cuda.empty_cache()\n",
        "#             with torch.no_grad(): # t_logits_uw no need grad\n",
        "#                 images_uw = model(images_uw) # reduced pml # big teacher\n",
        "#                 t_logits_uw = teacher_model(images_uw)\n",
        "#             torch.cuda.empty_cache()\n",
        "#             temperature = 1 # default 1 / mainargs 0.7\n",
        "#             # soft_pseudo_label = torch.softmax(t_logits_uw.detach() / temperature, dim=-1)\n",
        "#             soft_pseudo_label = torch.softmax(t_logits_uw / temperature, dim=-1)\n",
        "#             max_probs, hard_pseudo_label = torch.max(soft_pseudo_label, dim=-1) # all no grads\n",
        "\n",
        "\n",
        "#         with amp.autocast():\n",
        "#             t_loss_l = criterion(t_logits_l, targets)\n",
        "#         t_scaler.scale(t_loss_l).backward(retain_graph=True) # me backward first\n",
        "#         del t_loss_l\n",
        "\n",
        "#         with amp.autocast():\n",
        "#             threshold = 0.95 # default 0.95 / mainargs 0.6\n",
        "#             mask = max_probs.ge(threshold).float()\n",
        "#             t_loss_u = torch.mean(-(soft_pseudo_label * torch.log_softmax(t_logits_us, dim=-1)).sum(dim=-1) * mask)\n",
        "#             lambda_u = 8 # default 1 / mainargs 8 coefficient of unlabeled loss\n",
        "#             uda_steps = 10 # default 1 / mainargs 5000 warmup steps of lambda-u\n",
        "#             weight_u = lambda_u * min(1., (step + 1) / uda_steps)\n",
        "#             # t_loss_uda = t_loss_l + weight_u * t_loss_u\n",
        "#         t_scaler.scale(weight_u * t_loss_u).backward(retain_graph=True)\n",
        "#         del t_loss_u, soft_pseudo_label# t_loss_l\n",
        "#         torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "\n",
        "#         with amp.autocast():\n",
        "#             s_logits_us = student_model(images_us)\n",
        "#             with torch.no_grad():\n",
        "#                 s_logits_l = student_model(images_l)\n",
        "#             # s_loss_l_old = F.cross_entropy(s_logits_l.detach(), targets)\n",
        "#             s_loss_l_old = F.cross_entropy(s_logits_l, targets)\n",
        "#             s_loss = criterion(s_logits_us, hard_pseudo_label)\n",
        "#         s_scaler.scale(s_loss).backward()\n",
        "#         del s_logits_l, s_logits_us, hard_pseudo_label#, s_loss\n",
        "#         torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "#         # if grad_clip > 0:\n",
        "#         s_scaler.unscale_(s_optimizer)\n",
        "#         nn.utils.clip_grad_norm_(student_model.parameters(), 1e9)\n",
        "#         s_scaler.step(s_optimizer)\n",
        "#         s_scaler.update()\n",
        "#         s_scheduler.step()\n",
        "#         student_model.zero_grad()\n",
        "#         if ema > 0: avg_student_model.update_parameters(student_model)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#         with amp.autocast():\n",
        "#             with torch.no_grad():\n",
        "#                 s_logits_l = student_model(images_l)\n",
        "#             # s_loss_l_new = F.cross_entropy(s_logits_l.detach(), targets)\n",
        "#             s_loss_l_new = F.cross_entropy(s_logits_l, targets)\n",
        "\n",
        "#             dot_product = s_loss_l_old - s_loss_l_new # theoretically correct formula (https://github.com/kekmodel/MPL-pytorch/issues/6)\n",
        "#             # dot_product = s_loss_l_new - s_loss_l_old # author's code formula\n",
        "#             # # moving_dot_product = moving_dot_product * 0.99 + dot_product * 0.01\n",
        "#             # # dot_product = dot_product - moving_dot_product\n",
        "\n",
        "#             _, hard_pseudo_label = torch.max(t_logits_us.detach(), dim=-1)\n",
        "#             t_loss_mpl = dot_product * F.cross_entropy(t_logits_us, hard_pseudo_label) # dot_product no grad\n",
        "\n",
        "#             # t_loss_mpl = torch.tensor(0.).to(device) # test\n",
        "#             # t_loss = t_loss_uda + t_loss_mpl\n",
        "#         # t_scaler.scale(t_loss).backward()\n",
        "#         t_scaler.scale(t_loss_mpl).backward()\n",
        "#         # print(\"t_loss, s_loss\", t_loss.item(), s_loss.item())\n",
        "#         # t_scaler.scale(t_loss_mpl).backward()\n",
        "#         del s_logits_l, dot_product, s_loss_l_old, s_loss_l_new, t_logits_us, hard_pseudo_label, #t_loss, t_loss_uda, t_loss_mpl\n",
        "#         del s_loss\n",
        "#         torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "#         t_scaler.unscale_(t_optimizer) # if grad_clip > 0:\n",
        "#         nn.utils.clip_grad_norm_(teacher_model.parameters(), 1e9)\n",
        "#         t_scaler.step(t_optimizer)\n",
        "#         t_scaler.update()\n",
        "#         t_scheduler.step()\n",
        "#         teacher_model.zero_grad()\n",
        "\n",
        "#     return\n",
        "\n"
      ],
      "metadata": {
        "id": "0V8rI3DFJK3Y",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title mpl looop mu\n",
        "# # https://github.com/kekmodel/MPL-pytorch/blob/main/main.py\n",
        "# import random\n",
        "# import numpy as np\n",
        "# import torch\n",
        "# from torch.cuda import amp\n",
        "# from torch import nn\n",
        "# from torch.nn import functional as F\n",
        "\n",
        "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# random.seed(0)\n",
        "# np.random.seed(0)\n",
        "# torch.manual_seed(0)\n",
        "# torch.cuda.manual_seed_all(0)\n",
        "\n",
        "# torch.backends.cuda.matmul.allow_tf32 = True\n",
        "# torch.backends.cudnn.allow_tf32 = True\n",
        "\n",
        "# t_scaler = amp.GradScaler()\n",
        "# s_scaler = amp.GradScaler()\n",
        "# def train(labeled_loader, unlabeled_loader, teacher_model, student_model,\n",
        "#         avg_student_model, criterion, t_optimizer, s_optimizer, t_scheduler, s_scheduler):\n",
        "#     labeled_iter = iter(labeled_loader)\n",
        "#     unlabeled_iter = iter(unlabeled_loader)\n",
        "#     size = len(unlabeled_loader)\n",
        "\n",
        "#     # for author's code formula\n",
        "#     # moving_dot_product = torch.empty(1).to(device)\n",
        "#     # limit = 3.0**(0.5)  # 3 = 6 / (f_in + f_out)\n",
        "#     # nn.init.uniform_(moving_dot_product, -limit, limit)\n",
        "\n",
        "#     teacher_model.train()\n",
        "#     student_model.train()\n",
        "#     for step in range(len(unlabeled_loader)):\n",
        "\n",
        "#         try: images_l, targets = next(labeled_iter)\n",
        "#         except:\n",
        "#             labeled_iter = iter(labeled_loader)\n",
        "#             images_l, targets = next(labeled_iter)\n",
        "#         try: images_uw = next(unlabeled_iter) # images_uw, _ = next(unlabeled_iter)\n",
        "#         except:\n",
        "#             unlabeled_iter = iter(unlabeled_loader)\n",
        "#             images_uw = next(unlabeled_iter) # me\n",
        "#         images_l, targets = images_l.to(device), targets.to(device)\n",
        "#         images_uw = images_uw.to(device)\n",
        "#         images_us = trs(images_uw)\n",
        "\n",
        "#         # torch.cuda.empty_cache()\n",
        "#         with amp.autocast():\n",
        "#             batch_size = images_l.shape[0]\n",
        "\n",
        "#             with torch.no_grad(): m_logits_l = model(images_l) # reduced pml # big teacher\n",
        "#             t_logits_l = teacher_model(m_logits_l)\n",
        "#             t_loss_l = criterion(t_logits_l, targets)\n",
        "#         # t_scaler.scale(t_loss_l).backward(retain_graph=True) # me backward first\n",
        "#         t_scaler.scale(t_loss_l).backward() # me backward first\n",
        "#         # del t_loss_l\n",
        "\n",
        "\n",
        "\n",
        "#         # once, grad:s_loss_l_old\n",
        "#         with amp.autocast():\n",
        "#             with torch.no_grad(): s_logits_l = student_model(images_l)\n",
        "#             s_loss_l_old = F.cross_entropy(s_logits_l, targets)\n",
        "\n",
        "#         s_loss = 0\n",
        "#         t_loss_uda = t_loss_l\n",
        "#         t_logits_us = torch.empty(0, device=device)\n",
        "#         for i_us, i_uw in zip(images_us.chunk(mu), images_uw.chunk(mu)):\n",
        "#             with amp.autocast():\n",
        "#                 with torch.no_grad():\n",
        "#                     m_i_us = model(i_us) # reduced pml # big teacher\n",
        "#                     m_i_uw = model(i_uw) # reduced pml # big teacher\n",
        "#                     t_l_uw = teacher_model(m_i_uw) # t_logits_uw no need grad\n",
        "#                 t_l_us = teacher_model(m_i_us)\n",
        "#                 t_logits_us = torch.cat((t_logits_us, t_l_us))\n",
        "\n",
        "#                 temperature = 1 # default 1 / mainargs 0.7\n",
        "#                 # soft_pseudo_label = torch.softmax(t_logits_uw.detach() / temperature, dim=-1)\n",
        "#                 soft_pseudo_label = torch.softmax(t_l_uw / temperature, dim=-1)\n",
        "#                 max_probs, hard_pseudo_label = torch.max(soft_pseudo_label, dim=-1) # all no grads\n",
        "\n",
        "#                 threshold = 0.95 # default 0.95 / mainargs 0.6\n",
        "#                 mask = max_probs.ge(threshold).float()\n",
        "#                 t_loss_u = torch.mean(-(soft_pseudo_label * torch.log_softmax(t_l_us, dim=-1)).sum(dim=-1) * mask)\n",
        "#                 lambda_u = 8 # default 1 / mainargs 8 coefficient of unlabeled loss\n",
        "#                 uda_steps = 10 # default 1 / mainargs 5000 warmup steps of lambda-u\n",
        "#                 weight_u = lambda_u * min(1., (step + 1) / uda_steps)\n",
        "#                 # t_loss_uda = t_loss_l + weight_u * t_loss_u\n",
        "\n",
        "#                 # i_us.retain_grad()\n",
        "#                 i_us.requires_grad=True\n",
        "#                 s_l_us = student_model(i_us)\n",
        "#                 s_l = criterion(s_l_us, hard_pseudo_label)\n",
        "#             s_scaler.scale(s_l).backward()\n",
        "#             s_loss += s_l\n",
        "#             t_scaler.scale(weight_u * t_loss_u).backward(retain_graph=True)\n",
        "#             # t_scaler.scale(weight_u * t_loss_u).backward()\n",
        "#             t_loss_uda += weight_u * t_loss_u\n",
        "#         # del t_loss_u, soft_pseudo_label# t_loss_l\n",
        "#         # del s_logits_l, s_logits_us, hard_pseudo_label#, s_loss\n",
        "\n",
        "\n",
        "\n",
        "#         # if grad_clip > 0:\n",
        "#         s_scaler.unscale_(s_optimizer)\n",
        "#         nn.utils.clip_grad_norm_(student_model.parameters(), 1e9)\n",
        "#         s_scaler.step(s_optimizer)\n",
        "#         s_scaler.update()\n",
        "#         s_scheduler.step()\n",
        "#         student_model.zero_grad()\n",
        "#         if ema > 0: avg_student_model.update_parameters(student_model)\n",
        "\n",
        "#         # once, grad:t_logits_us, s_loss_l_old\n",
        "#         with amp.autocast():\n",
        "#             with torch.no_grad(): s_logits_l = student_model(images_l)\n",
        "#             s_loss_l_new = F.cross_entropy(s_logits_l, targets)\n",
        "#             dot_product = s_loss_l_old - s_loss_l_new\n",
        "#             # # moving_dot_product = moving_dot_product * 0.99 + dot_product * 0.01\n",
        "#             # # dot_product = dot_product - moving_dot_product\n",
        "#         t_loss_mpl = 0\n",
        "#         for t_l_us in t_logits_us.chunk(mu):\n",
        "#             with amp.autocast():\n",
        "#                 _, hard_pseudo_label = torch.max(t_l_us.detach(), dim=-1)\n",
        "#                 t_l_mpl = dot_product * F.cross_entropy(t_l_us, hard_pseudo_label) # dot_product no grad\n",
        "#             t_scaler.scale(t_l_mpl).backward(retain_graph=True)\n",
        "#             t_loss_mpl += t_l_mpl\n",
        "\n",
        "#         t_loss = t_loss_uda + t_loss_mpl\n",
        "#         # print(\"t_loss, s_loss\", t_loss.item(), s_loss.item())\n",
        "#         if step%10==0: print(step,\"/\",size,\" \", \"t_loss: \", t_loss.item(), \"s_loss: \", s_loss.item())\n",
        "#         try: wandb.log({\"t_loss\": t_loss.item(), \"s_loss\": s_loss.item()})\n",
        "#         except: pass\n",
        "#         # del s_logits_l, dot_product, s_loss_l_old, s_loss_l_new, t_logits_us, hard_pseudo_label, #t_loss, t_loss_uda, t_loss_mpl\n",
        "#         # del s_loss\n",
        "\n",
        "\n",
        "#         t_scaler.unscale_(t_optimizer) # if grad_clip > 0:\n",
        "#         nn.utils.clip_grad_norm_(teacher_model.parameters(), 1e9)\n",
        "#         t_scaler.step(t_optimizer)\n",
        "#         t_scaler.update()\n",
        "#         t_scheduler.step()\n",
        "#         teacher_model.zero_grad()\n",
        "\n",
        "#     return\n",
        "\n"
      ],
      "metadata": {
        "id": "XHoq81Wqy1kJ",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title mpl grad acc\n",
        "# https://github.com/kekmodel/MPL-pytorch/blob/main/main.py\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.cuda import amp\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "random.seed(0)\n",
        "np.random.seed(0)\n",
        "torch.manual_seed(0)\n",
        "torch.cuda.manual_seed_all(0)\n",
        "\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "\n",
        "t_scaler = amp.GradScaler()\n",
        "s_scaler = amp.GradScaler()\n",
        "def train(labeled_loader, unlabeled_loader, teacher_model, student_model,\n",
        "        avg_student_model, criterion, t_optimizer, s_optimizer, t_scheduler, s_scheduler):\n",
        "    labeled_iter = iter(labeled_loader)\n",
        "    unlabeled_iter = iter(unlabeled_loader)\n",
        "    size = len(unlabeled_loader)\n",
        "\n",
        "    # for author's code formula\n",
        "    # moving_dot_product = torch.empty(1).to(device)\n",
        "    # limit = 3.0**(0.5)  # 3 = 6 / (f_in + f_out)\n",
        "    # nn.init.uniform_(moving_dot_product, -limit, limit)\n",
        "\n",
        "    teacher_model.train()\n",
        "    student_model.train()\n",
        "    for step in range(len(unlabeled_loader)):\n",
        "\n",
        "        try: cimages_l, ctargets = next(labeled_iter)\n",
        "        except:\n",
        "            labeled_iter = iter(labeled_loader)\n",
        "            cimages_l, ctargets = next(labeled_iter)\n",
        "        try: cimages_uw = next(unlabeled_iter) # images_uw, _ = next(unlabeled_iter)\n",
        "        except:\n",
        "            unlabeled_iter = iter(unlabeled_loader)\n",
        "            cimages_uw = next(unlabeled_iter) # me\n",
        "        cimages_l, ctargets = cimages_l.to(device), ctargets.to(device)\n",
        "        cimages_uw = cimages_uw.to(device)\n",
        "        cimages_us = trs(cimages_uw)\n",
        "\n",
        "        for images_l, targets, images_uw, images_us in zip(cimages_l.chunk(grad_acc), ctargets.chunk(grad_acc), cimages_uw.chunk(grad_acc), cimages_us.chunk(grad_acc)): # for grad acc 1/2\n",
        "\n",
        "            with amp.autocast():\n",
        "                batch_size = images_l.shape[0]\n",
        "\n",
        "                with torch.no_grad(): m_logits_l = model(images_l) # reduced pml # big teacher\n",
        "                t_logits_l = teacher_model(m_logits_l)\n",
        "                t_loss_l = criterion(t_logits_l, targets)\n",
        "            # t_scaler.scale(t_loss_l).backward(retain_graph=True) # me backward first\n",
        "            t_scaler.scale(t_loss_l).backward() # me backward first\n",
        "            # del t_loss_l\n",
        "\n",
        "\n",
        "            with amp.autocast():\n",
        "                with torch.no_grad(): s_logits_l = student_model(images_l)\n",
        "                s_loss_l_old = F.cross_entropy(s_logits_l, targets)\n",
        "\n",
        "            s_loss = 0\n",
        "            t_loss_uda = t_loss_l\n",
        "            t_logits_us = torch.empty(0, device=device)\n",
        "            for i_us, i_uw in zip(images_us.chunk(mu), images_uw.chunk(mu)):\n",
        "                with amp.autocast():\n",
        "                    with torch.no_grad():\n",
        "                        m_i_us = model(i_us) # reduced pml # big teacher\n",
        "                        m_i_uw = model(i_uw) # reduced pml # big teacher\n",
        "                        t_l_uw = teacher_model(m_i_uw) # t_logits_uw no need grad\n",
        "                    t_l_us = teacher_model(m_i_us)\n",
        "                    t_logits_us = torch.cat((t_logits_us, t_l_us))\n",
        "\n",
        "                    temperature = 1 # default 1 / mainargs 0.7\n",
        "                    # soft_pseudo_label = torch.softmax(t_logits_uw.detach() / temperature, dim=-1)\n",
        "                    soft_pseudo_label = torch.softmax(t_l_uw / temperature, dim=-1)\n",
        "                    max_probs, hard_pseudo_label = torch.max(soft_pseudo_label, dim=-1) # all no grads\n",
        "\n",
        "                    threshold = 0.95 # default 0.95 / mainargs 0.6\n",
        "                    mask = max_probs.ge(threshold).float()\n",
        "                    t_loss_u = torch.mean(-(soft_pseudo_label * torch.log_softmax(t_l_us, dim=-1)).sum(dim=-1) * mask)\n",
        "                    lambda_u = 8 # default 1 / mainargs 8 coefficient of unlabeled loss\n",
        "                    uda_steps = 10 # default 1 / mainargs 5000 warmup steps of lambda-u\n",
        "                    weight_u = lambda_u * min(1., (step + 1) / uda_steps)\n",
        "                    # t_loss_uda = t_loss_l + weight_u * t_loss_u\n",
        "\n",
        "                    # i_us.retain_grad()\n",
        "                    i_us.requires_grad=True\n",
        "                    s_l_us = student_model(i_us)\n",
        "                    s_l = criterion(s_l_us, hard_pseudo_label)\n",
        "                s_scaler.scale(s_l).backward()\n",
        "                s_loss += s_l\n",
        "                t_scaler.scale(weight_u * t_loss_u).backward(retain_graph=True)\n",
        "                # t_scaler.scale(weight_u * t_loss_u).backward()\n",
        "                t_loss_uda += weight_u * t_loss_u\n",
        "            # del t_loss_u, soft_pseudo_label# t_loss_l\n",
        "            # del s_logits_l, s_logits_us, hard_pseudo_label#, s_loss\n",
        "\n",
        "\n",
        "\n",
        "        # if grad_clip > 0:\n",
        "        s_scaler.unscale_(s_optimizer)\n",
        "        nn.utils.clip_grad_norm_(student_model.parameters(), 1e9)\n",
        "        s_scaler.step(s_optimizer)\n",
        "        s_scaler.update()\n",
        "        s_scheduler.step()\n",
        "        student_model.zero_grad()\n",
        "        if ema > 0: avg_student_model.update_parameters(student_model)\n",
        "\n",
        "\n",
        "        for images_l, targets, images_uw, images_us in zip(cimages_l.chunk(grad_acc), ctargets.chunk(grad_acc), cimages_uw.chunk(grad_acc), cimages_us.chunk(grad_acc)): # for grad acc 2/2\n",
        "\n",
        "            with amp.autocast():\n",
        "                with torch.no_grad(): s_logits_l = student_model(images_l)\n",
        "                s_loss_l_new = F.cross_entropy(s_logits_l, targets)\n",
        "                dot_product = s_loss_l_old - s_loss_l_new\n",
        "                # # moving_dot_product = moving_dot_product * 0.99 + dot_product * 0.01\n",
        "                # # dot_product = dot_product - moving_dot_product\n",
        "            t_loss_mpl = 0\n",
        "            for t_l_us in t_logits_us.chunk(mu):\n",
        "                with amp.autocast():\n",
        "                    _, hard_pseudo_label = torch.max(t_l_us.detach(), dim=-1)\n",
        "                    t_l_mpl = dot_product * F.cross_entropy(t_l_us, hard_pseudo_label) # dot_product no grad\n",
        "                t_scaler.scale(t_l_mpl).backward(retain_graph=True)\n",
        "                t_loss_mpl += t_l_mpl\n",
        "\n",
        "            t_loss = t_loss_uda + t_loss_mpl\n",
        "            # print(\"t_loss, s_loss\", t_loss.item(), s_loss.item())\n",
        "            if step%10==0: print(step,\"/\",size,\" \", \"t_loss: \", t_loss.item(), \"s_loss: \", s_loss.item())\n",
        "            try: wandb.log({\"t_loss\": t_loss.item(), \"s_loss\": s_loss.item()})\n",
        "            except: pass\n",
        "            # del s_logits_l, dot_product, s_loss_l_old, s_loss_l_new, t_logits_us, hard_pseudo_label, #t_loss, t_loss_uda, t_loss_mpl\n",
        "            # del s_loss\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        t_scaler.unscale_(t_optimizer) # if grad_clip > 0:\n",
        "        nn.utils.clip_grad_norm_(teacher_model.parameters(), 1e9)\n",
        "        t_scaler.step(t_optimizer)\n",
        "        t_scaler.update()\n",
        "        t_scheduler.step()\n",
        "        teacher_model.zero_grad()\n",
        "\n",
        "    return\n",
        "\n"
      ],
      "metadata": {
        "id": "om2hOCE1BgrE"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title strain eval\n",
        "\n",
        "def evaluate(test_loader, model, criterion, verbose=True):\n",
        "    size = len(test_loader.dataset)\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for step, (images, targets) in enumerate(test_loader):\n",
        "            batch_size = images.shape[0]\n",
        "            images = images.to(device)\n",
        "            targets = targets.to(device)\n",
        "            with amp.autocast():\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, targets)\n",
        "            test_loss += loss.item()\n",
        "        #     acc1, acc5 = accuracy(outputs, targets, (1, 5))\n",
        "        #     losses, top1, top5 = loss.item(), acc1[0], acc5[0]\n",
        "        # # return losses, top1, top5\n",
        "            correct += (outputs.argmax(1) == targets).type(torch.float).sum().item()\n",
        "    test_loss /= size\n",
        "    correct /= size\n",
        "    try: wandb.log({\"test loss\": test_loss})\n",
        "    except: pass\n",
        "    if verbose: print(f\"Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f}\")\n",
        "    return correct, test_loss\n",
        "\n",
        "\n",
        "# @title train test function\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "# https://github.com/prigoyal/pytorch_memonger/blob/master/models/optimized/resnet_new.py\n",
        "from torch.utils.checkpoint import checkpoint, checkpoint_sequential\n",
        "\n",
        "trs=TrainTransform() # for image augmentation during train time\n",
        "# train function with automatic mixed precision\n",
        "def strain(dataloader, model, loss_fn, optimizer, scheduler=None, verbose=True):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    loss_list = []\n",
        "    for batch, (x, y) in enumerate(dataloader):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        with torch.cuda.amp.autocast(): # automatic mixed percision\n",
        "            x = trs(x) # image augmentation during train time to use gpu\n",
        "            pred = model(x) # default\n",
        "            loss = loss_fn(pred, y)\n",
        "        scaler.scale(loss).backward()\n",
        "        if ((batch + 1) % 4 == 0) or (batch + 1 == len(dataloader)): # gradient accumulation\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            optimizer.zero_grad()\n",
        "            if scheduler is not None:\n",
        "                scheduler.step()\n",
        "                # print(\"### lr: \", optimizer.param_groups[0][\"lr\"])\n",
        "\n",
        "        # print(model.state_dict()['_orig_mod.bn1.running_mean'][0])\n",
        "        train_loss = loss.item()/len(y)\n",
        "        loss_list.append(train_loss)\n",
        "        try: wandb.log({\"train loss\": train_loss})\n",
        "        except: pass\n",
        "        # if batch % (size//(10* len(y))) == 0:\n",
        "        current = batch * len(x)\n",
        "        if verbose: print(f\"loss: {train_loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "    return loss_list\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "W9URMRghTLc_"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title wwwwwwwwwww\n",
        "\n",
        "criterion = create_loss_fn()\n",
        "\n",
        "from torch import optim\n",
        "# lr default 0.01/ mainargs 0.05\n",
        "# og:t0.05s0.05 , psl:t1e-3s3e-4, rpsl:t\n",
        "# t_optimizer = optim.SGD(teacher_parameters, lr=0.05, momentum=0.9, nesterov=True)\n",
        "# s_optimizer = optim.SGD(student_parameters, lr=0.05, momentum=0.9, nesterov=True)\n",
        "t_optimizer = optim.SGD(teacher_parameters, lr=1e-6, momentum=0.9, nesterov=True)\n",
        "s_optimizer = optim.SGD(student_parameters, lr=1e-5, momentum=0.9, nesterov=True)\n",
        "# optimizer = bnb.optim.AdamW(model.parameters(), lr=1e-5, betas=(0.9, 0.999), optim_bits=8)\n",
        "# optimizer = Lamb(model.parameters(), lr=1e-5, betas=(0.9, 0.999), eps=1e-08, weight_decay=3e-6)\n",
        "\n",
        "# 3e-5 27.9->25.0\n",
        "# 3e-3 27.9->26.5\n",
        "# 0.05 29.4 -> 22.1\n",
        "\n",
        "epochs = 5\n",
        "num_batches=len(train_loader)\n",
        "total_steps=int(np.ceil(num_batches/grad_acc)*epochs +1) # +1 to excluse uptick at the end of onecycle\n",
        "# total_steps=30 # 300000\n",
        "warmup_steps = 10 # default 0 / mainargs 5000\n",
        "# t_scheduler = get_cosine_schedule_with_warmup(t_optimizer, warmup_steps, total_steps)\n",
        "t_scheduler = get_cosine_schedule_with_warmup(t_optimizer, warmup_steps, total_steps,10)\n",
        "student_wait_steps = 6 # default 0 / mainargs 3000\n",
        "s_scheduler = get_cosine_schedule_with_warmup(s_optimizer, warmup_steps, total_steps, student_wait_steps)\n",
        "\n"
      ],
      "metadata": {
        "id": "Aw0nwnUAh2Of"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# scheduler = torch.optim.lr_scheduler.PolynomialLR(optimizer, total_iters=int(np.ceil(num_batches/4)*3), power=1.0)\n",
        "# scheduler = PolynomialLR(optimizer, total_iters=4, power=1.0)\n",
        "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=10**(-1/2))\n",
        "\n",
        "\n",
        "# t_optimizer.param_groups[0][\"lr\"]\n",
        "# s_optimizer.param_groups[0][\"lr\"]\n",
        "\n",
        "# import time\n",
        "# start = time.time()\n",
        "\n",
        "# pth='/content/mpl.pth' # ty\n",
        "pth='/content/drive/MyDrive/frame/mpl18.pth' # M\n",
        "\n",
        "for t in range(0,epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    t_lr=t_optimizer.param_groups[0][\"lr\"]\n",
        "    s_lr=s_optimizer.param_groups[0][\"lr\"]\n",
        "    print('t_lr,s_lr',t_lr,s_lr)\n",
        "    train(labeled_loader, unlabeled_loader, teacher_model, student_model,\n",
        "        avg_student_model, criterion, t_optimizer, s_optimizer, t_scheduler, s_scheduler)\n",
        "\n",
        "    # evaluate(test_loader, student_model, criterion)\n",
        "    evaluate(test_loader, avg_student_model, criterion)\n",
        "\n",
        "    checkpoint = {\n",
        "    'epoch': t+1,\n",
        "    'teacher_model': teacher_model.state_dict(),\n",
        "    'student_model': student_model.state_dict(),\n",
        "    'avg_student_model': avg_student_model.state_dict(),\n",
        "    't_optimizer': t_optimizer.state_dict(),\n",
        "    's_optimizer': s_optimizer.state_dict(),\n",
        "    't_scheduler': t_scheduler.state_dict(),\n",
        "    's_scheduler': s_scheduler.state_dict(),}\n",
        "    torch.save(checkpoint, pth)\n",
        "\n",
        "\n",
        "# res34, batch4 8.8\n",
        "# res18, batch16 11.6 nocompilemodel\n",
        "\n",
        "# 16m28s\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vfpCXUfEzHZV",
        "outputId": "3d4a3320-329f-4f26-e276-8b986617ac4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "t_lr,s_lr 0.0 0.0\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 / 157   t_loss:  1.7915688753128052 s_loss:  12.792863845825195\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10 / 157   t_loss:  1.790928840637207 s_loss:  12.847481727600098\n",
            "20 / 157   t_loss:  1.7904629707336426 s_loss:  12.796439170837402\n",
            "30 / 157   t_loss:  1.7913095951080322 s_loss:  12.93379020690918\n",
            "40 / 157   t_loss:  1.7897231578826904 s_loss:  12.644171714782715\n",
            "50 / 157   t_loss:  1.7884248495101929 s_loss:  12.857980728149414\n",
            "60 / 157   t_loss:  1.7886351346969604 s_loss:  12.7106351852417\n",
            "70 / 157   t_loss:  1.789746642112732 s_loss:  12.616300582885742\n",
            "80 / 157   t_loss:  1.788618803024292 s_loss:  12.452911376953125\n",
            "90 / 157   t_loss:  1.7908527851104736 s_loss:  12.49996566772461\n",
            "100 / 157   t_loss:  1.789739966392517 s_loss:  12.630684852600098\n",
            "110 / 157   t_loss:  1.787540078163147 s_loss:  12.507417678833008\n",
            "120 / 157   t_loss:  1.7870131731033325 s_loss:  12.475946426391602\n",
            "130 / 157   t_loss:  1.7873234748840332 s_loss:  12.391185760498047\n",
            "140 / 157   t_loss:  1.788748025894165 s_loss:  12.291733741760254\n",
            "150 / 157   t_loss:  1.787776231765747 s_loss:  12.426130294799805\n",
            "Accuracy: 51.1%, Avg loss: 0.023926\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "t_lr,s_lr 9.979583149922618e-07 9.978410659099065e-06\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 / 157   t_loss:  1.7903753519058228 s_loss:  12.253273963928223\n",
            "10 / 157   t_loss:  1.7899593114852905 s_loss:  12.10791015625\n",
            "20 / 157   t_loss:  1.7883033752441406 s_loss:  12.418424606323242\n",
            "30 / 157   t_loss:  1.7867603302001953 s_loss:  12.069341659545898\n",
            "40 / 157   t_loss:  1.7882957458496094 s_loss:  11.937756538391113\n",
            "50 / 157   t_loss:  1.7863802909851074 s_loss:  12.063082695007324\n",
            "60 / 157   t_loss:  1.7878957986831665 s_loss:  12.109430313110352\n",
            "70 / 157   t_loss:  1.7880655527114868 s_loss:  12.139351844787598\n",
            "80 / 157   t_loss:  1.7887767553329468 s_loss:  11.826499938964844\n",
            "90 / 157   t_loss:  1.7870361804962158 s_loss:  11.899945259094238\n",
            "100 / 157   t_loss:  1.7845218181610107 s_loss:  11.708991050720215\n",
            "110 / 157   t_loss:  1.789860486984253 s_loss:  11.788382530212402\n",
            "120 / 157   t_loss:  1.7882481813430786 s_loss:  11.626124382019043\n",
            "130 / 157   t_loss:  1.7890647649765015 s_loss:  11.756231307983398\n",
            "140 / 157   t_loss:  1.7867783308029175 s_loss:  11.451495170593262\n",
            "150 / 157   t_loss:  1.7864677906036377 s_loss:  11.464086532592773\n",
            "Accuracy: 48.5%, Avg loss: 0.024301\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "t_lr,s_lr 9.906205893794821e-07 9.903805779356523e-06\n",
            "0 / 157   t_loss:  1.7863878011703491 s_loss:  11.564458847045898\n",
            "10 / 157   t_loss:  1.787749171257019 s_loss:  11.575650215148926\n",
            "20 / 157   t_loss:  1.7887860536575317 s_loss:  11.516383171081543\n",
            "30 / 157   t_loss:  1.7832157611846924 s_loss:  11.52294635772705\n",
            "40 / 157   t_loss:  1.7835718393325806 s_loss:  11.274771690368652\n",
            "50 / 157   t_loss:  1.7888367176055908 s_loss:  11.258813858032227\n",
            "60 / 157   t_loss:  1.7891255617141724 s_loss:  11.066218376159668\n",
            "70 / 157   t_loss:  1.7861677408218384 s_loss:  11.32955551147461\n",
            "80 / 157   t_loss:  1.7861742973327637 s_loss:  11.140281677246094\n",
            "90 / 157   t_loss:  1.7861899137496948 s_loss:  11.216531753540039\n",
            "100 / 157   t_loss:  1.7874857187271118 s_loss:  11.414390563964844\n",
            "110 / 157   t_loss:  1.7888360023498535 s_loss:  11.00354290008545\n",
            "120 / 157   t_loss:  1.7896291017532349 s_loss:  11.34243392944336\n",
            "130 / 157   t_loss:  1.7839024066925049 s_loss:  11.035279273986816\n",
            "140 / 157   t_loss:  1.78750479221344 s_loss:  10.975983619689941\n",
            "150 / 157   t_loss:  1.7892078161239624 s_loss:  11.104681968688965\n",
            "Accuracy: 45.3%, Avg loss: 0.024792\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "t_lr,s_lr 9.780219643494177e-07 9.776705808373675e-06\n",
            "0 / 157   t_loss:  1.7882075309753418 s_loss:  11.020452499389648\n",
            "10 / 157   t_loss:  1.7871627807617188 s_loss:  10.823895454406738\n",
            "20 / 157   t_loss:  1.7879663705825806 s_loss:  10.85787296295166\n",
            "30 / 157   t_loss:  1.7874932289123535 s_loss:  11.058713912963867\n",
            "40 / 157   t_loss:  1.7890496253967285 s_loss:  10.930194854736328\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# finetune\n",
        "# model = student_model\n",
        "model = avg_student_model\n",
        "model.drop = nn.Identity()\n",
        "# labeled_loader = DataLoader(finetune_dataset, batch_size=128, num_workers=4, pin_memory=True) # batch_size=512\n",
        "labeled_loader = DataLoader(finetune_dataset, batch_size=128, pin_memory=True) # batch_size=512\n",
        "optimizer = optim.SGD(model.parameters(), lr=3e-5, momentum=0.9, weight_decay=0, nesterov=True)\n",
        "# scaler = amp.GradScaler()\n",
        "for epoch in range(1): #625\n",
        "    # train_ls = strain(labeled_loader, model, loss_fn, optimizer, scheduler)\n",
        "    train_ls = strain(labeled_loader, model, criterion, optimizer)\n",
        "    evaluate(test_loader, student_model, criterion)\n"
      ],
      "metadata": {
        "id": "4GuEvf0MzBTD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "correct, test_loss = evaluate(test_loader, student_model, criterion)\n",
        "print(correct, test_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 624
        },
        "id": "JlQClGYfs5L1",
        "outputId": "f25b2c4d-77bd-4e2a-fb3b-efbee9865e6d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "BackendCompilerFailed",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mBrokenProcessPool\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\u001b[0m in \u001b[0;36mcall_user_compiler\u001b[0;34m(self, gm)\u001b[0m\n\u001b[1;32m    669\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 670\u001b[0;31m                 \u001b[0mcompiled_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompiler_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfake_example_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    671\u001b[0m             \u001b[0m_step_logger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mINFO\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"done compiler function {name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/debug_utils.py\u001b[0m in \u001b[0;36mdebug_wrapper\u001b[0;34m(gm, example_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1054\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1055\u001b[0;31m             \u001b[0mcompiled_gm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompiler_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexample_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1056\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, model_, inputs_)\u001b[0m\n\u001b[1;32m   1389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1390\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcompile_fx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig_patches\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\u001b[0m in \u001b[0;36mcompile_fx\u001b[0;34m(model_, example_inputs_, inner_compile, config_patches)\u001b[0m\n\u001b[1;32m    454\u001b[0m         \u001b[0;31m# once torchdynamo is merged into pytorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 455\u001b[0;31m         return aot_autograd(\n\u001b[0m\u001b[1;32m    456\u001b[0m             \u001b[0mfw_compiler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfw_compiler\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py\u001b[0m in \u001b[0;36mcompiler_fn\u001b[0;34m(gm, example_inputs)\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0menable_aot_logging\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m                 \u001b[0mcg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maot_module_simplified\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexample_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m                 \u001b[0mcounters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"aot_autograd\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ok\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\u001b[0m in \u001b[0;36maot_module_simplified\u001b[0;34m(mod, args, fw_compiler, bw_compiler, partition_fn, decompositions, hasher_type, static_argnums, keep_inference_input_mutations)\u001b[0m\n\u001b[1;32m   2821\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2822\u001b[0;31m     compiled_fn = create_aot_dispatcher_function(\n\u001b[0m\u001b[1;32m   2823\u001b[0m         \u001b[0mfunctional_call\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py\u001b[0m in \u001b[0;36mtime_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0mt0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m             \u001b[0mtime_spent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\u001b[0m in \u001b[0;36mcreate_aot_dispatcher_function\u001b[0;34m(flat_fn, flat_args, aot_config)\u001b[0m\n\u001b[1;32m   2514\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2515\u001b[0;31m         \u001b[0mcompiled_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompiler_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfake_flat_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maot_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\u001b[0m in \u001b[0;36maot_wrapper_dedupe\u001b[0;34m(flat_fn, flat_args, aot_config, compiler_fn)\u001b[0m\n\u001b[1;32m   1714\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mok\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcompiler_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleaf_flat_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maot_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1716\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\u001b[0m in \u001b[0;36maot_dispatch_base\u001b[0;34m(flat_fn, flat_args, aot_config)\u001b[0m\n\u001b[1;32m   1327\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack_graph_compiling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maot_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"inference\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1328\u001b[0;31m         \u001b[0mcompiled_fw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maot_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfw_compiler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfw_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflat_args_with_views_handled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py\u001b[0m in \u001b[0;36mtime_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0mt0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m             \u001b[0mtime_spent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\u001b[0m in \u001b[0;36mfw_compiler\u001b[0;34m(model, example_inputs)\u001b[0m\n\u001b[1;32m    429\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_outplace_to_inplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m         return inner_compile(\n\u001b[0m\u001b[1;32m    431\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/debug_utils.py\u001b[0m in \u001b[0;36mdebug_wrapper\u001b[0;34m(gm, example_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    594\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 595\u001b[0;31m             \u001b[0mcompiled_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompiler_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexample_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    596\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_inductor/debug.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    238\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mDebugContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/contextlib.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recreate_cm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\u001b[0m in \u001b[0;36mcompile_fx_inner\u001b[0;34m(gm, example_inputs, cudagraphs, num_fixed, is_backward, graph_id)\u001b[0m\n\u001b[1;32m    176\u001b[0m             \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mexample_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m             \u001b[0mcompiled_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile_to_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\u001b[0m in \u001b[0;36mcompile_to_fn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    585\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcompile_to_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile_to_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py\u001b[0m in \u001b[0;36mtime_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0mt0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m             \u001b[0mtime_spent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\u001b[0m in \u001b[0;36mcompile_to_module\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    574\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m         \u001b[0mmod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPyCodeCache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstants\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, source_code)\u001b[0m\n\u001b[1;32m    527\u001b[0m                 \u001b[0mmod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m                 \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    529\u001b[0m                 \u001b[0;31m# another thread might set this first\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/torchinductor_root/ct/cctkyasmqvbytrcidwtkblpx7upcucgq4eqix5cjimdxr6gwanse.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m triton__0 = async_compile.triton('''\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtriton\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\u001b[0m in \u001b[0;36mtriton\u001b[0;34m(self, source_code)\u001b[0m\n\u001b[1;32m    682\u001b[0m             \u001b[0mcc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmajor\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mminor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 683\u001b[0;31m             future = self.process_pool().submit(\n\u001b[0m\u001b[1;32m    684\u001b[0m                 \u001b[0m_worker_compile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/concurrent/futures/process.py\u001b[0m in \u001b[0;36msubmit\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    714\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_broken\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 715\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mBrokenProcessPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_broken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    716\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown_thread\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mBrokenProcessPool\u001b[0m: A child process terminated abruptly, the process pool is not usable anymore",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mBackendCompilerFailed\u001b[0m                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-a84d171632d6>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcorrect\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudent_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorrect\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-270aae46b7e7>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(test_loader, model, criterion, verbose)\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mamp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mtest_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdynamo_ctx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_orig_mod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py\u001b[0m in \u001b[0;36m_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m             \u001b[0mdynamic_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m                 \u001b[0mset_eval_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprior\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py\u001b[0m in \u001b[0;36mcatch_errors\u001b[0;34m(frame, cache_size)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mcompile_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m     \u001b[0mcatch_errors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_torchdynamo_orig_callable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallback\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\u001b[0m in \u001b[0;36m_convert_frame\u001b[0;34m(frame, cache_size, hooks)\u001b[0m\n\u001b[1;32m    402\u001b[0m         \u001b[0mcounters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"frames\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"total\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 404\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minner_convert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    405\u001b[0m             \u001b[0mcounters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"frames\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ok\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\u001b[0m in \u001b[0;36m_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_from_src\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfx_forward_from_src_skip_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprior_grad_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\u001b[0m in \u001b[0;36m_convert_frame_assert\u001b[0;34m(frame, cache_size, hooks)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0minitial_grad_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m         return _compile(\n\u001b[0m\u001b[1;32m    263\u001b[0m             \u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_code\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m             \u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_globals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py\u001b[0m in \u001b[0;36mtime_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m                 \u001b[0mcompilation_metrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0mt0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m             \u001b[0mtime_spent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0;31m# print(f\"Dynamo timer: key={key}, latency={latency:.2f} sec\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\u001b[0m in \u001b[0;36m_compile\u001b[0;34m(code, globals, locals, builtins, compiler_fn, one_graph, export, hooks, frame)\u001b[0m\n\u001b[1;32m    322\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mattempt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m                 \u001b[0mout_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform_code_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m                 \u001b[0morig_code_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mout_code\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\u001b[0m in \u001b[0;36mtransform_code_object\u001b[0;34m(code, transformations, safe)\u001b[0m\n\u001b[1;32m    443\u001b[0m     \u001b[0mpropagate_line_nums\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstructions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m     \u001b[0mtransformations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstructions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    446\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mclean_and_assemble_instructions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstructions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(instructions, code_options)\u001b[0m\n\u001b[1;32m    309\u001b[0m             \u001b[0mmutated_closure_cell_contents\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m         )\n\u001b[0;32m--> 311\u001b[0;31m         \u001b[0mtracer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtracer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1724\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1725\u001b[0m         \u001b[0m_step_logger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mINFO\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"torchdynamo start tracing {self.f_code.co_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1726\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1727\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1728\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmatch_nested_cell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    574\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minstruction_pointer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m                 \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_exit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 576\u001b[0;31m                 \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    577\u001b[0m             ):\n\u001b[1;32m    578\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    538\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m                 \u001b[0munimplemented\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"missing: {inst.opname}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 540\u001b[0;31m             \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    541\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0minst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopname\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"RETURN_VALUE\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36mRETURN_VALUE\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m   1790\u001b[0m         )\n\u001b[1;32m   1791\u001b[0m         \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RETURN_VALUE triggered compile\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1792\u001b[0;31m         self.output.compile_subgraph(\n\u001b[0m\u001b[1;32m   1793\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mGraphCompileReason\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"return_value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframe_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1794\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\u001b[0m in \u001b[0;36mcompile_subgraph\u001b[0;34m(self, tx, partial_convert, reason)\u001b[0m\n\u001b[1;32m    515\u001b[0m             \u001b[0;31m# optimization to generate better code in a common case\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m             self.add_output_instructions(\n\u001b[0;32m--> 517\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile_and_call_fx_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreversed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m                 \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcreate_instruction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"UNPACK_SEQUENCE\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\u001b[0m in \u001b[0;36mcompile_and_call_fx_graph\u001b[0;34m(self, tx, rv, root)\u001b[0m\n\u001b[1;32m    586\u001b[0m         \u001b[0massert_no_fake_params_or_buffers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtracing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtracing_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m             \u001b[0mcompiled_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_user_compiler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m         \u001b[0mcompiled_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdisable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompiled_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py\u001b[0m in \u001b[0;36mtime_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m                 \u001b[0mcompilation_metrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0mt0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m             \u001b[0mtime_spent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0;31m# print(f\"Dynamo timer: key={key}, latency={latency:.2f} sec\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\u001b[0m in \u001b[0;36mcall_user_compiler\u001b[0;34m(self, gm)\u001b[0m\n\u001b[1;32m    673\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m             \u001b[0mcompiled_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mBackendCompilerFailed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompiler_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcompiled_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mBackendCompilerFailed\u001b[0m: debug_wrapper raised BrokenProcessPool: A child process terminated abruptly, the process pool is not usable anymore\n\nSet torch._dynamo.config.verbose=True for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    torch._dynamo.config.suppress_errors = True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title save\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "checkpoint = {\n",
        "'epoch': t+1,\n",
        "'teacher_model': teacher_model.state_dict(),\n",
        "'student_model': student_model.state_dict(),\n",
        "'avg_student_model': avg_student_model.state_dict(),\n",
        "'t_optimizer': t_optimizer.state_dict(),\n",
        "'s_optimizer': s_optimizer.state_dict(),\n",
        "'t_scheduler': t_scheduler.state_dict(),\n",
        "'s_scheduler': s_scheduler.state_dict(),}\n",
        "# torch.save(checkpoint, pth)\n",
        "torch.save(checkpoint, 'ckpt.pth')\n"
      ],
      "metadata": {
        "id": "w_2qJyKBwL-V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f256b549-a283-4823-907b-7982d27c80d3"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    }
  ]
}