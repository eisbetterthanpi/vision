{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eisbetterthanpi/vision/blob/main/Meta_Pseudo_Labels_try.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v9XrV9ZACgjm"
      },
      "outputs": [],
      "source": [
        "# Meta Pseudo Labels mar 2021 https://arxiv.org/pdf/2003.10580v4.pdf\n",
        "# https://github.com/kekmodel/MPL-pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "z9AEoUWZekls"
      },
      "outputs": [],
      "source": [
        "# @title download\n",
        "# # google images unlabeled\n",
        "!gdown 1ncx2DJ-GXqrQd6nL5UEmj6GLT4w-9qYs -O house.zip\n",
        "!unzip /content/house.zip -d /\n",
        "!rm -R /content/house/.ipynb_checkpoints\n",
        "\n",
        "# # 70k+gmap\n",
        "!gdown 1-CZp7TbhJLeRQpbKQCyT8ofGg89Yt137 -O gsv.zip\n",
        "!unzip /content/gsv.zip -d /\n",
        "!rm -R /content/gsv70kg/.ipynb_checkpoints\n",
        "!rm -R /content/gsv70kg/01/.ipynb_checkpoints\n",
        "!rm -R /content/gsv70kg/02/.ipynb_checkpoints\n",
        "!rm -R /content/gsv70kg/03/.ipynb_checkpoints\n",
        "!rm -R /content/gsv70kg/04/.ipynb_checkpoints\n",
        "!rm -R /content/gsv70kg/05/.ipynb_checkpoints\n",
        "!rm -R /content/gsv70kg/06/.ipynb_checkpoints\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "1MAfJG1xTW3b"
      },
      "outputs": [],
      "source": [
        "# @title data\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "from torchvision import datasets, transforms\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
        "\n",
        "labeled_dir='/content/gsv70kg'\n",
        "\n",
        "labeled_data = datasets.ImageFolder(labeled_dir, transform=transform)\n",
        "torch.manual_seed(0)\n",
        "train_data, test_data = torch.utils.data.random_split(labeled_data, [.9,.1])\n",
        "# train_data, _ = torch.utils.data.random_split(train_data, [.01,.99])\n",
        "# test_data, _ = torch.utils.data.random_split(test_data, [.01,.99])\n",
        "finetune_dataset = train_data\n",
        "\n",
        "unlabel_dir='/content/house'\n",
        "# unlabel_data = datasets.ImageFolder(unlabel_dir, transform=transform)\n",
        "\n",
        "import os\n",
        "from PIL import Image\n",
        "class Datasetme(torch.utils.data.Dataset):\n",
        "    def __init__(self, dir, transform=None):\n",
        "        self.dir = dir\n",
        "        self.data = os.listdir(dir)\n",
        "        self.transform = transform\n",
        "    def __getitem__(self, index):\n",
        "        img_file = self.data[index]\n",
        "        img_file = os.path.join(self.dir, img_file)\n",
        "        image = Image.open(img_file).convert(\"RGB\")\n",
        "        if self.transform: image = self.transform(image)\n",
        "        return image\n",
        "    def __len__(self): return len(self.data)\n",
        "\n",
        "unlabel_data = Datasetme(unlabel_dir, transform=transform)\n",
        "# unlabel_data, _ = torch.utils.data.random_split(unlabel_data, [.01,.99])\n",
        "\n",
        "\n",
        "batch_size = 64 # 16 is max for res152; default 64/ mainargs128\n",
        "grad_acc = 1\n",
        "\n",
        "# res152 batch16 gradacc4\n",
        "\n",
        "num_batches=int(np.ceil(len(train_data)/batch_size))\n",
        "\n",
        "# train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, num_workers=4, pin_memory=True)\n",
        "# test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, num_workers=4, pin_memory=True)\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, pin_memory=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, pin_memory=True)\n",
        "\n",
        "train_sampler = RandomSampler\n",
        "# labeled_loader = DataLoader(labeled_data, sampler=train_sampler(labeled_data), batch_size=batch_size, num_workers=4, drop_last=True)\n",
        "# unlabeled_loader = DataLoader(unlabel_data, sampler=train_sampler(unlabel_data), batch_size=batch_size * 7, num_workers=4, drop_last=True) # mu=7 ,coefficient of unlabeled batch size\n",
        "# test_loader = DataLoader(test_data, sampler=SequentialSampler(test_data), batch_size=batch_size, num_workers=4)\n",
        "\n",
        "labeled_loader = DataLoader(labeled_data, sampler=train_sampler(labeled_data), batch_size=batch_size*grad_acc, drop_last=True)\n",
        "mu=7 # coefficient of unlabeled batch size\n",
        "unlabeled_loader = DataLoader(unlabel_data, sampler=train_sampler(unlabel_data), batch_size=batch_size*grad_acc * mu, drop_last=True)\n",
        "test_loader = DataLoader(test_data, sampler=SequentialSampler(test_data), batch_size=batch_size*grad_acc)\n",
        "\n",
        "del labeled_data, train_data, test_data, unlabel_data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "6iYyVffJcYB5"
      },
      "outputs": [],
      "source": [
        "# @title torch augment\n",
        "# https://github.com/facebookresearch/vicreg/blob/main/augmentations.py\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "class TrainTransform(object):\n",
        "    def __init__(self):\n",
        "        # self.transform = transforms.RandomApply([transforms.Compose([\n",
        "        self.transform = transforms.Compose([\n",
        "                transforms.RandomPerspective(distortion_scale=0.3, p=0.5), # me\n",
        "                transforms.RandomResizedCrop((400,640), scale=(0.7, 1.0), ratio=(0.8, 1.25), interpolation=transforms.InterpolationMode.BICUBIC),\n",
        "                # transforms.RandomResizedCrop((32,32), scale=(0.7, 1.0), ratio=(0.8, 1.25), interpolation=transforms.InterpolationMode.BICUBIC),\n",
        "                transforms.RandomHorizontalFlip(p=0.5), # 0.5\n",
        "                transforms.Lambda(lambda x : torch.clamp(x, 0., 1.)), # clamp else ColorJitter will return nan https://discuss.pytorch.org/t/input-is-nan-after-transformation/125455/6\n",
        "                transforms.RandomApply([transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1)], p=0.8,), # brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1)], p=0.8\n",
        "                transforms.RandomGrayscale(p=0.2), # 0.2\n",
        "                # # transforms.RandomChoice(transforms.ColorJitter , transforms.RandomGrayscale(p=1.)\n",
        "                transforms.RandomApply([transforms.GaussianBlur(kernel_size=5, sigma=(0.1, 2.0)),], p=1.0),\n",
        "                # # transforms.RandomSolarize(threshold=130, p=0.5)\n",
        "                transforms.RandomErasing(p=1., scale=(0.1, 0.11), ratio=(1,1), value=(0.485, 0.456, 0.406)),\n",
        "                # transforms.ToTensor(), # ToTensored at dataset level, no need to ToTensor again\n",
        "                # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), # normalised at dataset level. default 0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225\n",
        "                ])\n",
        "            # ], p=1.)\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        dims = len(sample.shape)\n",
        "        if dims==3: x1 = self.transform(sample) # same transforms per minibatch\n",
        "        elif dims==4: x1 = transforms.Lambda(lambda x: torch.stack([self.transform(x_) for x_ in x]))(sample) # diff transforms per img in minibatch\n",
        "        # x1 = self.transform(sample)\n",
        "        return x1\n",
        "\n",
        "trs=TrainTransform()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "q_bJYwwSDNNy"
      },
      "outputs": [],
      "source": [
        "# @title utils\n",
        "# https://github.com/kekmodel/MPL-pytorch/blob/main/utils.py\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "def create_loss_fn():\n",
        "    label_smoothing = 0 # default 0 / mainargs 0.15\n",
        "    # if label_smoothing > 0:\n",
        "    #     criterion = SmoothCrossEntropyV2(alpha=label_smoothing)\n",
        "    # else:\n",
        "    criterion = nn.CrossEntropyLoss(label_smoothing=label_smoothing)\n",
        "    return criterion.to(device)\n",
        "\n",
        "from collections import OrderedDict\n",
        "def module_load_state_dict(model, state_dict):\n",
        "    try:\n",
        "        new_state_dict = OrderedDict()\n",
        "        for k, v in state_dict.items():\n",
        "            name = k[7:]  # remove `module.`\n",
        "            new_state_dict[name] = v\n",
        "        model.load_state_dict(new_state_dict)\n",
        "    except:\n",
        "        new_state_dict = OrderedDict()\n",
        "        for k, v in state_dict.items():\n",
        "            name = f'module.{k}'  # add `module.`\n",
        "            new_state_dict[name] = v\n",
        "        model.load_state_dict(new_state_dict)\n",
        "\n",
        "def model_load_state_dict(model, state_dict):\n",
        "    try: model.load_state_dict(state_dict)\n",
        "    except: module_load_state_dict(model, state_dict)\n",
        "\n",
        "\n",
        "def accuracy(output, target, topk=(1,)):\n",
        "    output = output.to(torch.device('cpu'))\n",
        "    target = target.to(torch.device('cpu'))\n",
        "    maxk = max(topk)\n",
        "    batch_size = target.shape[0]\n",
        "    _, idx = output.sort(dim=1, descending=True)\n",
        "    pred = idx.narrow(1, 0, maxk).t()\n",
        "    correct = pred.eq(target.reshape(1, -1).expand_as(pred))\n",
        "    res = []\n",
        "    for k in topk:\n",
        "        correct_k = correct[:k].reshape(-1).float().sum(dim=0, keepdim=True)\n",
        "        res.append(correct_k.mul_(100.0 / batch_size))\n",
        "    return res\n",
        "\n",
        "\n",
        "class SmoothCrossEntropy(nn.Module):\n",
        "    def __init__(self, alpha=0.1):\n",
        "        super(SmoothCrossEntropy, self).__init__()\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def forward(self, logits, labels):\n",
        "        if self.alpha == 0:\n",
        "            loss = F.cross_entropy(logits, labels)\n",
        "        else:\n",
        "            num_classes = logits.shape[-1]\n",
        "            alpha_div_k = self.alpha / num_classes\n",
        "            target_probs = F.one_hot(labels, num_classes=num_classes).float() * (1. - self.alpha) + alpha_div_k\n",
        "            loss = (-(target_probs * torch.log_softmax(logits, dim=-1)).sum(dim=-1)).mean()\n",
        "        return loss\n",
        "\n",
        "class SmoothCrossEntropyV2(nn.Module):\n",
        "    \"\"\"NLL loss with label smoothing.\"\"\"\n",
        "    def __init__(self, label_smoothing=0.1):\n",
        "        super().__init__()\n",
        "        assert label_smoothing < 1.0\n",
        "        self.smoothing = label_smoothing\n",
        "        self.confidence = 1. - label_smoothing\n",
        "\n",
        "    def forward(self, x, target):\n",
        "        if self.smoothing == 0:\n",
        "            loss = F.cross_entropy(x, target)\n",
        "        else:\n",
        "            logprobs = F.log_softmax(x, dim=-1)\n",
        "            nll_loss = -logprobs.gather(dim=-1, index=target.unsqueeze(1))\n",
        "            nll_loss = nll_loss.squeeze(1)\n",
        "            smooth_loss = -logprobs.mean(dim=-1)\n",
        "            loss = (self.confidence * nll_loss + self.smoothing * smooth_loss).mean()\n",
        "        return loss\n",
        "\n",
        "\n",
        "# from main\n",
        "import math\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "def get_cosine_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps, num_wait_steps=0, num_cycles=0.5, last_epoch=-1):\n",
        "    def lr_lambda(current_step):\n",
        "        if current_step < num_wait_steps:\n",
        "            return 0.0\n",
        "        if current_step < num_warmup_steps + num_wait_steps:\n",
        "            return float(current_step) / float(max(1, num_warmup_steps + num_wait_steps))\n",
        "        progress = float(current_step - num_warmup_steps - num_wait_steps) / float(max(1, num_training_steps - num_warmup_steps - num_wait_steps))\n",
        "        return max(0.0, 0.5 * (1.0 + math.cos(math.pi * float(num_cycles) * 2.0 * progress)))\n",
        "    return LambdaLR(optimizer, lr_lambda, last_epoch)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "4J3GeedWa6QO"
      },
      "outputs": [],
      "source": [
        "# @title ModelEMA\n",
        "# expopnential moving average, smoothen model parameters\n",
        "# https://github.com/kekmodel/MPL-pytorch/blob/main/models.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from copy import deepcopy\n",
        "\n",
        "class ModelEMA(nn.Module):\n",
        "    def __init__(self, model, decay=0.9999, device=None):\n",
        "        super().__init__()\n",
        "        self.module = deepcopy(model)\n",
        "        self.module.eval()\n",
        "        self.decay = decay\n",
        "        self.device = device\n",
        "        if self.device is not None:\n",
        "            self.module.to(device=device)\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.module(input)\n",
        "\n",
        "    def _update(self, model, update_fn):\n",
        "        with torch.no_grad():\n",
        "            for ema_v, model_v in zip(self.module.parameters(), model.parameters()):\n",
        "                if self.device is not None:\n",
        "                    model_v = model_v.to(device=self.device)\n",
        "                ema_v.copy_(update_fn(ema_v, model_v))\n",
        "            for ema_v, model_v in zip(self.module.buffers(), model.buffers()):\n",
        "                if self.device is not None:\n",
        "                    model_v = model_v.to(device=self.device)\n",
        "                ema_v.copy_(model_v)\n",
        "\n",
        "    def update_parameters(self, model):\n",
        "        self._update(model, update_fn=lambda e, m: self.decay * e + (1. - self.decay) * m)\n",
        "\n",
        "    def state_dict(self):\n",
        "        return self.module.state_dict()\n",
        "\n",
        "    def load_state_dict(self, state_dict):\n",
        "        self.module.load_state_dict(state_dict)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "2n6C1jpb-WOk"
      },
      "outputs": [],
      "source": [
        "# @title big teacher\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "# !gdown 1ysJfdsvwMiWbCdkvFHwNqAUnJTtm6KbT -O res152adamw71.pth # ty\n",
        "# !gdown 1VaPxGoaLjmt7K9VHi0FWbJ5efEZTLhwd -O res18teacher.pth # A\n",
        "# !pip install bitsandbytes\n",
        "\n",
        "model = models.resnet18(weights='DEFAULT') # 18 34 50 101 152\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Sequential( # og (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
        "    # nn.Linear(num_ftrs, num_classes, bias=False),\n",
        "    # nn.Softmax(dim=1),\n",
        "    )\n",
        "\n",
        "# model.load_state_dict(torch.load('/content/bigTeacher.pth'))\n",
        "# _, modelsd, _,_ = torch.load('/content/bigTeacher.pth').values()\n",
        "# _, modelsd, _,_ = torch.load('/content/res152adamw71.pth').values()\n",
        "_, modelsd, _,_ = torch.load('/content/res18teacher.pth').values()\n",
        "model.load_state_dict(modelsd, strict=False)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "# model = torch.compile(model.to(device)) # compiling teacher leads to significantly higher vram usage\n",
        "\n",
        "\n",
        "model.eval()\n",
        "print('uh')\n",
        "\n",
        "\n",
        "# torch.save(model.state_dict(), '/content/model.pth')\n",
        "# modelsd = torch.load('/content/model.pth')\n",
        "# model.load_state_dict(modelsd, strict=False)\n",
        "# # model = torch.compile(model.to(device))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "PVFBeYZgUya1"
      },
      "outputs": [],
      "source": [
        "# @title model\n",
        "\n",
        "num_classes = 6\n",
        "# if dataset == \"cifar10\": depth, widen_factor = 28, 2\n",
        "# elif dataset == 'cifar100': depth, widen_factor = 28, 8\n",
        "# teacher_model = WideResNet(num_classes=num_classes, depth=depth, widen_factor=widen_factor, dropout=0, dense_dropout=0.2)\n",
        "# student_model = WideResNet(num_classes=num_classes, depth=depth, widen_factor=widen_factor, dropout=0, dense_dropout=0.2)\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "def get_resnet():\n",
        "    model = models.resnet18(weights='DEFAULT') # 18 34 50 101 152\n",
        "    num_ftrs = model.fc.in_features\n",
        "    model.fc = nn.Sequential( # og (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
        "        nn.Linear(num_ftrs, num_classes),\n",
        "        # nn.Linear(num_ftrs, num_classes, bias=False),\n",
        "        # nn.Softmax(dim=1),\n",
        "        )\n",
        "    # model = model.to(device)\n",
        "    model = torch.compile(model.to(device))\n",
        "    return model\n",
        "\n",
        "class Small(nn.Module):\n",
        "    def __init__(self, embed_dim, output_dim):\n",
        "        super(Small, self).__init__()\n",
        "        hidden_size=512\n",
        "        self.lin = nn.Sequential(\n",
        "            nn.Linear(embed_dim, hidden_size), nn.ReLU(),\n",
        "            nn.Linear(hidden_size, hidden_size), nn.ReLU(),\n",
        "            nn.Linear(hidden_size, hidden_size), nn.ReLU(),\n",
        "            nn.Linear(hidden_size, hidden_size), nn.ReLU(),\n",
        "            nn.Linear(hidden_size, output_dim),\n",
        "            # nn.Linear(hidden_size, output_dim, bias=False),\n",
        "            # nn.Softmax(dim=1), # teacher need output logits!, not softmax\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        logits = self.lin(x)\n",
        "        return logits\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# @title ensemble\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Ensemble(nn.Module):\n",
        "    def __init__(self, embed_dim, output_dim):\n",
        "        super().__init__()\n",
        "        self.output_dim = output_dim # 6\n",
        "        self.embed_dim = embed_dim\n",
        "        h_dim = 512\n",
        "        self.fwd = nn.Sequential(\n",
        "            nn.Linear(self.embed_dim, h_dim), nn.ReLU(),\n",
        "            # nn.Linear(h_dim, h_dim), nn.ReLU(),\n",
        "            # Block(h_dim, h_dim, 0.5),\n",
        "            Block(h_dim, h_dim),\n",
        "            Block(h_dim, h_dim),\n",
        "            Block(h_dim, h_dim),\n",
        "            nn.Linear(h_dim, self.output_dim),\n",
        "            # nn.Linear(h_dim, self.output_dim, bias=False),\n",
        "            # nn.Softmax(dim=1),\n",
        "            )\n",
        "    def forward(self, x):\n",
        "        out = self.fwd(x)\n",
        "        return out\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim, drop=None):\n",
        "        super().__init__()\n",
        "        if drop: self.fwd = nn.Sequential(nn.BatchNorm1d(in_dim), nn.Dropout(drop), nn.Linear(in_dim, out_dim), nn.ReLU(),)\n",
        "        else: self.fwd = nn.Sequential(nn.Linear(in_dim, out_dim), nn.ReLU(),)\n",
        "    def forward(self, x):\n",
        "        return x + self.fwd(x)\n",
        "\n",
        "# teacher_model = Ensemble(2048, 6).to(device)\n",
        "# teacher_model = torch.compile(Ensemble(2048, 6).to(device))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# teacher_model = Small(num_ftrs,6).to(device)\n",
        "teacher_model = torch.compile(Small(num_ftrs,6).to(device))\n",
        "# teacher_model = get_resnet()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "student_model = get_resnet()\n",
        "_, modelsd, _,_ = torch.load('/content/res18teacher.pth').values()\n",
        "student_model.load_state_dict(modelsd, strict=False)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "student_model = student_model.to(device)\n",
        "# student_model = torch.compile(student_model.to(device)) #\n",
        "\n",
        "\n",
        "avg_student_model = None\n",
        "ema = 0.995 # default 0 / mainargs 0.995\n",
        "\n",
        "if ema > 0: avg_student_model = ModelEMA(student_model, ema)\n",
        "\n",
        "no_decay = ['bn']\n",
        "weight_decay = 5e-4 # default 0 / mainargs 5e-4\n",
        "teacher_parameters = [{'params': [p for n, p in teacher_model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': weight_decay},\n",
        "    {'params': [p for n, p in teacher_model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}]\n",
        "student_parameters = [{'params': [p for n, p in student_model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': weight_decay},\n",
        "    {'params': [p for n, p in student_model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}]\n",
        "\n",
        "# teacher_model.zero_grad()\n",
        "# student_model.zero_grad()\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ABqGijQ6H27_"
      },
      "outputs": [],
      "source": [
        "# @title try\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# from torchvision import models\n",
        "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# # # 1.9, 1.7, 1.2\n",
        "# im = torch.rand(32,3,400,680,device=device) #\n",
        "# # print(32*3*400*680*32/8)\n",
        "# # print(im.element_size() * im.nelement())\n",
        "\n",
        "# def count_parameters(model): return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "# # print(count_parameters(model))\n",
        "\n",
        "\n",
        "# model = models.resnet152(weights='DEFAULT') # 18 34 50 101 152\n",
        "# model.fc = nn.Sequential()\n",
        "# model = model.to(device)\n",
        "# # model = torch.compile(model.to(device))\n",
        "# # 32, 50:3.3-5=1.7\n",
        "# # print(count_parameters(model))\n",
        "\n",
        "# # amp res152 student cant train on batch 32, compile or not\n",
        "\n",
        "# torch.backends.cuda.matmul.allow_tf32 = True\n",
        "# torch.backends.cudnn.allow_tf32 = True\n",
        "# model.eval()\n",
        "# student_model.eval()\n",
        "# with torch.cuda.amp.autocast():\n",
        "#     with torch.no_grad(): #\n",
        "#         # out = model(im) # 1:2.7, 4:6.3, 8:12.8\n",
        "#         out = student_model(im) #\n",
        "# # 16*15: 11.6\n",
        "# 32:1.3-8.7=6.4\n",
        "\n",
        "# student_model.train()\n",
        "# out = student_model(im) #\n",
        "\n",
        "# model.train()\n",
        "# with torch.cuda.amp.autocast():\n",
        "#     out = model(im) #\n",
        "\n",
        "\n",
        "# # 152:3.2\n",
        "# # im = torch.rand(4,3,400,680,device=device) #\n",
        "# # out = model(im) # 5.9\n",
        "\n",
        "# im = torch.rand(16,2048,device=device) #\n",
        "# out = teacher_model(im) # 5.9\n",
        "\n",
        "# torch.cuda.empty_cache()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "2UGs03MKAHxs"
      },
      "outputs": [],
      "source": [
        "# @title wandb\n",
        "# https://docs.wandb.ai/quickstart\n",
        "!pip install wandb\n",
        "import wandb\n",
        "wandb.login() # 487a2109e55dce4e13fc70681781de9f50f27be7\n",
        "run = wandb.init(\n",
        "    project=\"mpl\",\n",
        "    config={\n",
        "        \"optim\": \"adamw\",\n",
        "        # \"lr\": lr,\n",
        "        # \"epochs\": epochs,\n",
        "    })\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "wWBABdFBjZ9u"
      },
      "outputs": [],
      "source": [
        "# @title main mpl train\n",
        "# ########## https://github.com/kekmodel/MPL-pytorch/blob/main/main.py\n",
        "# import random\n",
        "# import numpy as np\n",
        "# import torch\n",
        "# from torch.cuda import amp\n",
        "# from torch import nn\n",
        "# from torch.nn import functional as F\n",
        "\n",
        "# ema = 0.995 # default 0 / mainargs 0.995\n",
        "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# random.seed(0)\n",
        "# np.random.seed(0)\n",
        "# torch.manual_seed(0)\n",
        "# torch.cuda.manual_seed_all(0)\n",
        "\n",
        "# torch.backends.cuda.matmul.allow_tf32 = True\n",
        "# torch.backends.cudnn.allow_tf32 = True\n",
        "\n",
        "# t_scaler = amp.GradScaler()\n",
        "# s_scaler = amp.GradScaler()\n",
        "# def train(labeled_loader, unlabeled_loader, teacher_model, student_model,\n",
        "#         avg_student_model, criterion, t_optimizer, s_optimizer, t_scheduler, s_scheduler):\n",
        "#     labeled_iter = iter(labeled_loader)\n",
        "#     unlabeled_iter = iter(unlabeled_loader)\n",
        "\n",
        "#     # for author's code formula\n",
        "#     # moving_dot_product = torch.empty(1).to(device)\n",
        "#     # limit = 3.0**(0.5)  # 3 = 6 / (f_in + f_out)\n",
        "#     # nn.init.uniform_(moving_dot_product, -limit, limit)\n",
        "\n",
        "#     # eval_step = 10#1000\n",
        "#     # start_step=0\n",
        "#     # for step in range(start_step, total_steps):\n",
        "#     for step in range(len(unlabeled_loader)):\n",
        "#         teacher_model.train()\n",
        "#         student_model.train()\n",
        "\n",
        "#         try: images_l, targets = next(labeled_iter)\n",
        "#         except:\n",
        "#             labeled_iter = iter(labeled_loader)\n",
        "#             images_l, targets = next(labeled_iter)\n",
        "#         try: images_uw = next(unlabeled_iter) # images_uw, _ = next(unlabeled_iter)\n",
        "#         except:\n",
        "#             unlabeled_iter = iter(unlabeled_loader)\n",
        "#             images_uw = next(unlabeled_iter) # me\n",
        "#         images_l, targets = images_l.to(device), targets.to(device)\n",
        "#         # images_uw, images_us = images_uw.to(device), images_us.to(device)\n",
        "#         images_uw = images_uw.to(device)\n",
        "#         images_us = trs(images_uw)\n",
        "\n",
        "#         with amp.autocast():\n",
        "#             batch_size = images_l.shape[0]\n",
        "#             # print(images_l.shape, images_uw.shape, images_us.shape) # [16, 3, 400, 640], [112, 3, 400, 640], [112, 3, 400, 640] mu*batch_size\n",
        "#             t_images = torch.cat((images_l, images_uw, images_us))\n",
        "#             t_logits = teacher_model(t_images)\n",
        "#             t_logits_l = t_logits[:batch_size]\n",
        "#             t_logits_uw, t_logits_us = t_logits[batch_size:].chunk(2)\n",
        "#             del t_logits\n",
        "\n",
        "#             t_loss_l = criterion(t_logits_l, targets)\n",
        "\n",
        "#             temperature = 1 # default 1 / mainargs 0.7\n",
        "#             soft_pseudo_label = torch.softmax(t_logits_uw.detach() / temperature, dim=-1)\n",
        "#             max_probs, hard_pseudo_label = torch.max(soft_pseudo_label, dim=-1)\n",
        "\n",
        "#             threshold = 0.95 # default 0.95 / mainargs 0.6\n",
        "#             mask = max_probs.ge(threshold).float()\n",
        "#             t_loss_u = torch.mean(-(soft_pseudo_label * torch.log_softmax(t_logits_us, dim=-1)).sum(dim=-1) * mask)\n",
        "#             lambda_u = 8 # default 1 / mainargs 8 coefficient of unlabeled loss\n",
        "#             uda_steps = 10 # default 1 / mainargs 5000 warmup steps of lambda-u\n",
        "#             weight_u = lambda_u * min(1., (step + 1) / uda_steps)\n",
        "#             t_loss_uda = t_loss_l + weight_u * t_loss_u\n",
        "\n",
        "#             s_images = torch.cat((images_l, images_us))\n",
        "#             s_logits = student_model(s_images)\n",
        "#             s_logits_l = s_logits[:batch_size]\n",
        "#             s_logits_us = s_logits[batch_size:]\n",
        "#             del s_logits\n",
        "\n",
        "#             s_loss_l_old = F.cross_entropy(s_logits_l.detach(), targets)\n",
        "\n",
        "#             # print(\"s_logits_us, hard_pseudo_label: \", s_logits_us.shape, hard_pseudo_label.shape) # [448, 10] [224]\n",
        "#             s_loss = criterion(s_logits_us, hard_pseudo_label)\n",
        "\n",
        "#         s_scaler.scale(s_loss).backward()\n",
        "\n",
        "#         # if grad_clip > 0:\n",
        "#         s_scaler.unscale_(s_optimizer)\n",
        "#         nn.utils.clip_grad_norm_(student_model.parameters(), 1e9)\n",
        "\n",
        "#         s_scaler.step(s_optimizer)\n",
        "#         s_scaler.update()\n",
        "#         s_scheduler.step()\n",
        "\n",
        "#         if ema > 0: avg_student_model.update_parameters(student_model)\n",
        "\n",
        "#         with amp.autocast():\n",
        "#             with torch.no_grad():\n",
        "#                 s_logits_l = student_model(images_l)\n",
        "#             s_loss_l_new = F.cross_entropy(s_logits_l.detach(), targets)\n",
        "\n",
        "#             dot_product = s_loss_l_old - s_loss_l_new # theoretically correct formula (https://github.com/kekmodel/MPL-pytorch/issues/6)\n",
        "#             # dot_product = s_loss_l_new - s_loss_l_old # author's code formula\n",
        "#             # # moving_dot_product = moving_dot_product * 0.99 + dot_product * 0.01\n",
        "#             # # dot_product = dot_product - moving_dot_product\n",
        "\n",
        "#             _, hard_pseudo_label = torch.max(t_logits_us.detach(), dim=-1)\n",
        "#             t_loss_mpl = dot_product * F.cross_entropy(t_logits_us, hard_pseudo_label)\n",
        "\n",
        "#             # t_loss_mpl = torch.tensor(0.).to(device) # test\n",
        "#             t_loss = t_loss_uda + t_loss_mpl\n",
        "\n",
        "#         t_scaler.scale(t_loss).backward()\n",
        "\n",
        "#         # if grad_clip > 0:\n",
        "#         t_scaler.unscale_(t_optimizer)\n",
        "#         nn.utils.clip_grad_norm_(teacher_model.parameters(), 1e9)\n",
        "\n",
        "#         t_scaler.step(t_optimizer)\n",
        "#         t_scaler.update()\n",
        "#         t_scheduler.step()\n",
        "\n",
        "#         teacher_model.zero_grad()\n",
        "#         student_model.zero_grad()\n",
        "\n",
        "#         # if (step + 1) % eval_step == 0:\n",
        "#         #     # print(s_losses, t_losses, t_losses_l, t_losses_u, t_losses_mpl, mean_mask)\n",
        "#         #     test_model = avg_student_model if avg_student_model is not None else student_model\n",
        "#         #     test_loss, top1, top5 = evaluate(test_loader, test_model, criterion)\n",
        "#     return\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "s9rTVKxUZhmn"
      },
      "outputs": [],
      "source": [
        "# @title mpl try 1\n",
        "# # https://github.com/kekmodel/MPL-pytorch/blob/main/main.py\n",
        "# import random\n",
        "# import numpy as np\n",
        "# import torch\n",
        "# from torch.cuda import amp\n",
        "# from torch import nn\n",
        "# from torch.nn import functional as F\n",
        "\n",
        "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# random.seed(0)\n",
        "# np.random.seed(0)\n",
        "# torch.manual_seed(0)\n",
        "# torch.cuda.manual_seed_all(0)\n",
        "\n",
        "# torch.backends.cuda.matmul.allow_tf32 = True\n",
        "# torch.backends.cudnn.allow_tf32 = True\n",
        "\n",
        "# t_scaler = amp.GradScaler()\n",
        "# s_scaler = amp.GradScaler()\n",
        "# def train(labeled_loader, unlabeled_loader, teacher_model, student_model,\n",
        "#         avg_student_model, criterion, t_optimizer, s_optimizer, t_scheduler, s_scheduler):\n",
        "#     labeled_iter = iter(labeled_loader)\n",
        "#     unlabeled_iter = iter(unlabeled_loader)\n",
        "#     size = len(unlabeled_loader)\n",
        "\n",
        "#     # for author's code formula\n",
        "#     # moving_dot_product = torch.empty(1).to(device)\n",
        "#     # limit = 3.0**(0.5)  # 3 = 6 / (f_in + f_out)\n",
        "#     # nn.init.uniform_(moving_dot_product, -limit, limit)\n",
        "\n",
        "#     teacher_model.train()\n",
        "#     student_model.train()\n",
        "#     # eval_step = 10#1000\n",
        "#     # start_step=0\n",
        "#     # for step in range(start_step, total_steps):\n",
        "#     for step in range(len(unlabeled_loader)):\n",
        "\n",
        "#         try: images_l, targets = next(labeled_iter)\n",
        "#         except:\n",
        "#             labeled_iter = iter(labeled_loader)\n",
        "#             images_l, targets = next(labeled_iter)\n",
        "#         try: images_uw = next(unlabeled_iter) # images_uw, _ = next(unlabeled_iter)\n",
        "#         except:\n",
        "#             unlabeled_iter = iter(unlabeled_loader)\n",
        "#             images_uw = next(unlabeled_iter) # me\n",
        "#         images_l, targets = images_l.to(device), targets.to(device)\n",
        "#         images_uw = images_uw.to(device)\n",
        "#         images_us = trs(images_uw)\n",
        "\n",
        "#         with amp.autocast():\n",
        "#             batch_size = images_l.shape[0]\n",
        "\n",
        "#             t_images = torch.cat((images_l, images_us))\n",
        "#             with torch.no_grad(): t_images = model(t_images) # reduced pml # big teacher\n",
        "#             t_logits = teacher_model(t_images)\n",
        "#             t_logits_l, t_logits_us = t_logits[:batch_size], t_logits[batch_size:]\n",
        "#             del t_logits\n",
        "#             torch.cuda.empty_cache()\n",
        "#             with torch.no_grad(): # t_logits_uw no need grad\n",
        "#                 images_uw = model(images_uw) # reduced pml # big teacher\n",
        "#                 t_logits_uw = teacher_model(images_uw)\n",
        "#             torch.cuda.empty_cache()\n",
        "#             temperature = 1 # default 1 / mainargs 0.7\n",
        "#             # soft_pseudo_label = torch.softmax(t_logits_uw.detach() / temperature, dim=-1)\n",
        "#             soft_pseudo_label = torch.softmax(t_logits_uw / temperature, dim=-1)\n",
        "#             max_probs, hard_pseudo_label = torch.max(soft_pseudo_label, dim=-1)\n",
        "\n",
        "\n",
        "#         with amp.autocast():\n",
        "#             t_loss_l = criterion(t_logits_l, targets)\n",
        "#         # t_scaler.scale(t_loss_l).backward() # me backward first\n",
        "\n",
        "#         with amp.autocast():\n",
        "#             threshold = 0.95 # default 0.95 / mainargs 0.6\n",
        "#             mask = max_probs.ge(threshold).float()\n",
        "#             t_loss_u = torch.mean(-(soft_pseudo_label * torch.log_softmax(t_logits_us, dim=-1)).sum(dim=-1) * mask)\n",
        "#             lambda_u = 8 # default 1 / mainargs 8 coefficient of unlabeled loss\n",
        "#             uda_steps = 10 # default 1 / mainargs 5000 warmup steps of lambda-u\n",
        "#             weight_u = lambda_u * min(1., (step + 1) / uda_steps)\n",
        "#             t_loss_uda = t_loss_l + weight_u * t_loss_u\n",
        "#         # t_scaler.scale(weight_u * t_loss_u).backward()\n",
        "#         # del soft_pseudo_label, t_loss_u\n",
        "\n",
        "\n",
        "\n",
        "#         with amp.autocast():\n",
        "#             s_logits_us = student_model(images_us)\n",
        "#             with torch.no_grad():\n",
        "#                 s_logits_l = student_model(images_l)\n",
        "#             # s_loss_l_old = F.cross_entropy(s_logits_l.detach(), targets)\n",
        "#             s_loss_l_old = F.cross_entropy(s_logits_l, targets)\n",
        "#             s_loss = criterion(s_logits_us, hard_pseudo_label)\n",
        "#         s_scaler.scale(s_loss).backward()\n",
        "\n",
        "\n",
        "#         # if grad_clip > 0:\n",
        "#         s_scaler.unscale_(s_optimizer)\n",
        "#         nn.utils.clip_grad_norm_(student_model.parameters(), 1e9)\n",
        "#         s_scaler.step(s_optimizer)\n",
        "#         s_scaler.update()\n",
        "#         s_scheduler.step()\n",
        "#         student_model.zero_grad()\n",
        "#         if ema > 0: avg_student_model.update_parameters(student_model)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#         with amp.autocast():\n",
        "#             with torch.no_grad():\n",
        "#                 s_logits_l = student_model(images_l)\n",
        "#             # s_loss_l_new = F.cross_entropy(s_logits_l.detach(), targets)\n",
        "#             s_loss_l_new = F.cross_entropy(s_logits_l, targets)\n",
        "\n",
        "#             dot_product = s_loss_l_old - s_loss_l_new # theoretically correct formula (https://github.com/kekmodel/MPL-pytorch/issues/6)\n",
        "#             # dot_product = s_loss_l_new - s_loss_l_old # author's code formula\n",
        "#             # # moving_dot_product = moving_dot_product * 0.99 + dot_product * 0.01\n",
        "#             # # dot_product = dot_product - moving_dot_product\n",
        "\n",
        "#             _, hard_pseudo_label = torch.max(t_logits_us.detach(), dim=-1)\n",
        "#             t_loss_mpl = dot_product * F.cross_entropy(t_logits_us, hard_pseudo_label) # dot_product no grad\n",
        "\n",
        "#             # t_loss_mpl = torch.tensor(0.).to(device) # test\n",
        "#             t_loss = t_loss_uda + t_loss_mpl\n",
        "#         t_scaler.scale(t_loss).backward()\n",
        "#         print(\"t_loss, s_loss\", t_loss.item(), s_loss.item())\n",
        "#         # t_scaler.scale(t_loss_mpl).backward()\n",
        "\n",
        "\n",
        "#         t_scaler.unscale_(t_optimizer) # if grad_clip > 0:\n",
        "#         nn.utils.clip_grad_norm_(teacher_model.parameters(), 1e9)\n",
        "#         t_scaler.step(t_optimizer)\n",
        "#         t_scaler.update()\n",
        "#         t_scheduler.step()\n",
        "#         teacher_model.zero_grad()\n",
        "\n",
        "#     return\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "0V8rI3DFJK3Y"
      },
      "outputs": [],
      "source": [
        "# @title mpl try\n",
        "# # https://github.com/kekmodel/MPL-pytorch/blob/main/main.py\n",
        "# import random\n",
        "# import numpy as np\n",
        "# import torch\n",
        "# from torch.cuda import amp\n",
        "# from torch import nn\n",
        "# from torch.nn import functional as F\n",
        "\n",
        "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# random.seed(0)\n",
        "# np.random.seed(0)\n",
        "# torch.manual_seed(0)\n",
        "# torch.cuda.manual_seed_all(0)\n",
        "\n",
        "# torch.backends.cuda.matmul.allow_tf32 = True\n",
        "# torch.backends.cudnn.allow_tf32 = True\n",
        "\n",
        "# t_scaler = amp.GradScaler()\n",
        "# s_scaler = amp.GradScaler()\n",
        "# def train(labeled_loader, unlabeled_loader, teacher_model, student_model,\n",
        "#         avg_student_model, criterion, t_optimizer, s_optimizer, t_scheduler, s_scheduler):\n",
        "#     labeled_iter = iter(labeled_loader)\n",
        "#     unlabeled_iter = iter(unlabeled_loader)\n",
        "#     size = len(unlabeled_loader)\n",
        "\n",
        "#     # for author's code formula\n",
        "#     # moving_dot_product = torch.empty(1).to(device)\n",
        "#     # limit = 3.0**(0.5)  # 3 = 6 / (f_in + f_out)\n",
        "#     # nn.init.uniform_(moving_dot_product, -limit, limit)\n",
        "\n",
        "#     teacher_model.train()\n",
        "#     student_model.train()\n",
        "#     # eval_step = 10#1000\n",
        "#     # start_step=0\n",
        "#     # for step in range(start_step, total_steps):\n",
        "#     for step in range(len(unlabeled_loader)):\n",
        "\n",
        "#         try: images_l, targets = next(labeled_iter)\n",
        "#         except:\n",
        "#             labeled_iter = iter(labeled_loader)\n",
        "#             images_l, targets = next(labeled_iter)\n",
        "#         try: images_uw = next(unlabeled_iter) # images_uw, _ = next(unlabeled_iter)\n",
        "#         except:\n",
        "#             unlabeled_iter = iter(unlabeled_loader)\n",
        "#             images_uw = next(unlabeled_iter) # me\n",
        "#         images_l, targets = images_l.to(device), targets.to(device)\n",
        "#         images_uw = images_uw.to(device)\n",
        "#         images_us = trs(images_uw)\n",
        "\n",
        "#         with amp.autocast():\n",
        "#             batch_size = images_l.shape[0]\n",
        "\n",
        "#             t_images = torch.cat((images_l, images_us))\n",
        "#             with torch.no_grad(): t_images = model(t_images) # reduced pml # big teacher\n",
        "#             t_logits = teacher_model(t_images)\n",
        "#             t_logits_l, t_logits_us = t_logits[:batch_size], t_logits[batch_size:]\n",
        "#             del t_logits, t_images\n",
        "#             torch.cuda.empty_cache()\n",
        "#             with torch.no_grad(): # t_logits_uw no need grad\n",
        "#                 images_uw = model(images_uw) # reduced pml # big teacher\n",
        "#                 t_logits_uw = teacher_model(images_uw)\n",
        "#             torch.cuda.empty_cache()\n",
        "#             temperature = 1 # default 1 / mainargs 0.7\n",
        "#             # soft_pseudo_label = torch.softmax(t_logits_uw.detach() / temperature, dim=-1)\n",
        "#             soft_pseudo_label = torch.softmax(t_logits_uw / temperature, dim=-1)\n",
        "#             max_probs, hard_pseudo_label = torch.max(soft_pseudo_label, dim=-1) # all no grads\n",
        "\n",
        "\n",
        "#         with amp.autocast():\n",
        "#             t_loss_l = criterion(t_logits_l, targets)\n",
        "#         t_scaler.scale(t_loss_l).backward(retain_graph=True) # me backward first\n",
        "#         del t_loss_l\n",
        "\n",
        "#         with amp.autocast():\n",
        "#             threshold = 0.95 # default 0.95 / mainargs 0.6\n",
        "#             mask = max_probs.ge(threshold).float()\n",
        "#             t_loss_u = torch.mean(-(soft_pseudo_label * torch.log_softmax(t_logits_us, dim=-1)).sum(dim=-1) * mask)\n",
        "#             lambda_u = 8 # default 1 / mainargs 8 coefficient of unlabeled loss\n",
        "#             uda_steps = 10 # default 1 / mainargs 5000 warmup steps of lambda-u\n",
        "#             weight_u = lambda_u * min(1., (step + 1) / uda_steps)\n",
        "#             # t_loss_uda = t_loss_l + weight_u * t_loss_u\n",
        "#         t_scaler.scale(weight_u * t_loss_u).backward(retain_graph=True)\n",
        "#         del t_loss_u, soft_pseudo_label# t_loss_l\n",
        "#         torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "\n",
        "#         with amp.autocast():\n",
        "#             s_logits_us = student_model(images_us)\n",
        "#             with torch.no_grad():\n",
        "#                 s_logits_l = student_model(images_l)\n",
        "#             # s_loss_l_old = F.cross_entropy(s_logits_l.detach(), targets)\n",
        "#             s_loss_l_old = F.cross_entropy(s_logits_l, targets)\n",
        "#             s_loss = criterion(s_logits_us, hard_pseudo_label)\n",
        "#         s_scaler.scale(s_loss).backward()\n",
        "#         del s_logits_l, s_logits_us, hard_pseudo_label#, s_loss\n",
        "#         torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "#         # if grad_clip > 0:\n",
        "#         s_scaler.unscale_(s_optimizer)\n",
        "#         nn.utils.clip_grad_norm_(student_model.parameters(), 1e9)\n",
        "#         s_scaler.step(s_optimizer)\n",
        "#         s_scaler.update()\n",
        "#         s_scheduler.step()\n",
        "#         student_model.zero_grad()\n",
        "#         if ema > 0: avg_student_model.update_parameters(student_model)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#         with amp.autocast():\n",
        "#             with torch.no_grad():\n",
        "#                 s_logits_l = student_model(images_l)\n",
        "#             # s_loss_l_new = F.cross_entropy(s_logits_l.detach(), targets)\n",
        "#             s_loss_l_new = F.cross_entropy(s_logits_l, targets)\n",
        "\n",
        "#             dot_product = s_loss_l_old - s_loss_l_new # theoretically correct formula (https://github.com/kekmodel/MPL-pytorch/issues/6)\n",
        "#             # dot_product = s_loss_l_new - s_loss_l_old # author's code formula\n",
        "#             # # moving_dot_product = moving_dot_product * 0.99 + dot_product * 0.01\n",
        "#             # # dot_product = dot_product - moving_dot_product\n",
        "\n",
        "#             _, hard_pseudo_label = torch.max(t_logits_us.detach(), dim=-1)\n",
        "#             t_loss_mpl = dot_product * F.cross_entropy(t_logits_us, hard_pseudo_label) # dot_product no grad\n",
        "\n",
        "#             # t_loss_mpl = torch.tensor(0.).to(device) # test\n",
        "#             # t_loss = t_loss_uda + t_loss_mpl\n",
        "#         # t_scaler.scale(t_loss).backward()\n",
        "#         t_scaler.scale(t_loss_mpl).backward()\n",
        "#         # print(\"t_loss, s_loss\", t_loss.item(), s_loss.item())\n",
        "#         # t_scaler.scale(t_loss_mpl).backward()\n",
        "#         del s_logits_l, dot_product, s_loss_l_old, s_loss_l_new, t_logits_us, hard_pseudo_label, #t_loss, t_loss_uda, t_loss_mpl\n",
        "#         del s_loss\n",
        "#         torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "#         t_scaler.unscale_(t_optimizer) # if grad_clip > 0:\n",
        "#         nn.utils.clip_grad_norm_(teacher_model.parameters(), 1e9)\n",
        "#         t_scaler.step(t_optimizer)\n",
        "#         t_scaler.update()\n",
        "#         t_scheduler.step()\n",
        "#         teacher_model.zero_grad()\n",
        "\n",
        "#     return\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "XHoq81Wqy1kJ"
      },
      "outputs": [],
      "source": [
        "# @title mpl looop mu\n",
        "# # https://github.com/kekmodel/MPL-pytorch/blob/main/main.py\n",
        "# import random\n",
        "# import numpy as np\n",
        "# import torch\n",
        "# from torch.cuda import amp\n",
        "# from torch import nn\n",
        "# from torch.nn import functional as F\n",
        "\n",
        "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# random.seed(0)\n",
        "# np.random.seed(0)\n",
        "# torch.manual_seed(0)\n",
        "# torch.cuda.manual_seed_all(0)\n",
        "\n",
        "# torch.backends.cuda.matmul.allow_tf32 = True\n",
        "# torch.backends.cudnn.allow_tf32 = True\n",
        "\n",
        "# t_scaler = amp.GradScaler()\n",
        "# s_scaler = amp.GradScaler()\n",
        "# def train(labeled_loader, unlabeled_loader, teacher_model, student_model,\n",
        "#         avg_student_model, criterion, t_optimizer, s_optimizer, t_scheduler, s_scheduler):\n",
        "#     labeled_iter = iter(labeled_loader)\n",
        "#     unlabeled_iter = iter(unlabeled_loader)\n",
        "#     size = len(unlabeled_loader)\n",
        "\n",
        "#     # for author's code formula\n",
        "#     # moving_dot_product = torch.empty(1).to(device)\n",
        "#     # limit = 3.0**(0.5)  # 3 = 6 / (f_in + f_out)\n",
        "#     # nn.init.uniform_(moving_dot_product, -limit, limit)\n",
        "\n",
        "#     teacher_model.train()\n",
        "#     student_model.train()\n",
        "#     for step in range(len(unlabeled_loader)):\n",
        "\n",
        "#         try: images_l, targets = next(labeled_iter)\n",
        "#         except:\n",
        "#             labeled_iter = iter(labeled_loader)\n",
        "#             images_l, targets = next(labeled_iter)\n",
        "#         try: images_uw = next(unlabeled_iter) # images_uw, _ = next(unlabeled_iter)\n",
        "#         except:\n",
        "#             unlabeled_iter = iter(unlabeled_loader)\n",
        "#             images_uw = next(unlabeled_iter) # me\n",
        "#         images_l, targets = images_l.to(device), targets.to(device)\n",
        "#         images_uw = images_uw.to(device)\n",
        "#         images_us = trs(images_uw)\n",
        "\n",
        "#         # torch.cuda.empty_cache()\n",
        "#         with amp.autocast():\n",
        "#             batch_size = images_l.shape[0]\n",
        "\n",
        "#             with torch.no_grad(): m_logits_l = model(images_l) # reduced pml # big teacher\n",
        "#             t_logits_l = teacher_model(m_logits_l)\n",
        "#             t_loss_l = criterion(t_logits_l, targets)\n",
        "#         # t_scaler.scale(t_loss_l).backward(retain_graph=True) # me backward first\n",
        "#         t_scaler.scale(t_loss_l).backward() # me backward first\n",
        "#         # del t_loss_l\n",
        "\n",
        "\n",
        "\n",
        "#         # once, grad:s_loss_l_old\n",
        "#         with amp.autocast():\n",
        "#             with torch.no_grad(): s_logits_l = student_model(images_l)\n",
        "#             s_loss_l_old = F.cross_entropy(s_logits_l, targets)\n",
        "\n",
        "#         s_loss = 0\n",
        "#         t_loss_uda = t_loss_l\n",
        "#         t_logits_us = torch.empty(0, device=device)\n",
        "#         for i_us, i_uw in zip(images_us.chunk(mu), images_uw.chunk(mu)):\n",
        "#             with amp.autocast():\n",
        "#                 with torch.no_grad():\n",
        "#                     m_i_us = model(i_us) # reduced pml # big teacher\n",
        "#                     m_i_uw = model(i_uw) # reduced pml # big teacher\n",
        "#                     t_l_uw = teacher_model(m_i_uw) # t_logits_uw no need grad\n",
        "#                 t_l_us = teacher_model(m_i_us)\n",
        "#                 t_logits_us = torch.cat((t_logits_us, t_l_us))\n",
        "\n",
        "#                 temperature = 1 # default 1 / mainargs 0.7\n",
        "#                 # soft_pseudo_label = torch.softmax(t_logits_uw.detach() / temperature, dim=-1)\n",
        "#                 soft_pseudo_label = torch.softmax(t_l_uw / temperature, dim=-1)\n",
        "#                 max_probs, hard_pseudo_label = torch.max(soft_pseudo_label, dim=-1) # all no grads\n",
        "\n",
        "#                 threshold = 0.95 # default 0.95 / mainargs 0.6\n",
        "#                 mask = max_probs.ge(threshold).float()\n",
        "#                 t_loss_u = torch.mean(-(soft_pseudo_label * torch.log_softmax(t_l_us, dim=-1)).sum(dim=-1) * mask)\n",
        "#                 lambda_u = 8 # default 1 / mainargs 8 coefficient of unlabeled loss\n",
        "#                 uda_steps = 10 # default 1 / mainargs 5000 warmup steps of lambda-u\n",
        "#                 weight_u = lambda_u * min(1., (step + 1) / uda_steps)\n",
        "#                 # t_loss_uda = t_loss_l + weight_u * t_loss_u\n",
        "\n",
        "#                 # i_us.retain_grad()\n",
        "#                 i_us.requires_grad=True\n",
        "#                 s_l_us = student_model(i_us)\n",
        "#                 s_l = criterion(s_l_us, hard_pseudo_label)\n",
        "#             s_scaler.scale(s_l).backward()\n",
        "#             s_loss += s_l\n",
        "#             t_scaler.scale(weight_u * t_loss_u).backward(retain_graph=True)\n",
        "#             # t_scaler.scale(weight_u * t_loss_u).backward()\n",
        "#             t_loss_uda += weight_u * t_loss_u\n",
        "#         # del t_loss_u, soft_pseudo_label# t_loss_l\n",
        "#         # del s_logits_l, s_logits_us, hard_pseudo_label#, s_loss\n",
        "\n",
        "\n",
        "\n",
        "#         # if grad_clip > 0:\n",
        "#         s_scaler.unscale_(s_optimizer)\n",
        "#         nn.utils.clip_grad_norm_(student_model.parameters(), 1e9)\n",
        "#         s_scaler.step(s_optimizer)\n",
        "#         s_scaler.update()\n",
        "#         s_scheduler.step()\n",
        "#         student_model.zero_grad()\n",
        "#         if ema > 0: avg_student_model.update_parameters(student_model)\n",
        "\n",
        "#         # once, grad:t_logits_us, s_loss_l_old\n",
        "#         with amp.autocast():\n",
        "#             with torch.no_grad(): s_logits_l = student_model(images_l)\n",
        "#             s_loss_l_new = F.cross_entropy(s_logits_l, targets)\n",
        "#             dot_product = s_loss_l_old - s_loss_l_new\n",
        "#             # # moving_dot_product = moving_dot_product * 0.99 + dot_product * 0.01\n",
        "#             # # dot_product = dot_product - moving_dot_product\n",
        "#         t_loss_mpl = 0\n",
        "#         for t_l_us in t_logits_us.chunk(mu):\n",
        "#             with amp.autocast():\n",
        "#                 _, hard_pseudo_label = torch.max(t_l_us.detach(), dim=-1)\n",
        "#                 t_l_mpl = dot_product * F.cross_entropy(t_l_us, hard_pseudo_label) # dot_product no grad\n",
        "#             t_scaler.scale(t_l_mpl).backward(retain_graph=True)\n",
        "#             t_loss_mpl += t_l_mpl\n",
        "\n",
        "#         t_loss = t_loss_uda + t_loss_mpl\n",
        "#         # print(\"t_loss, s_loss\", t_loss.item(), s_loss.item())\n",
        "#         if step%10==0: print(step,\"/\",size,\" \", \"t_loss: \", t_loss.item(), \"s_loss: \", s_loss.item())\n",
        "#         try: wandb.log({\"t_loss\": t_loss.item(), \"s_loss\": s_loss.item()})\n",
        "#         except: pass\n",
        "#         # del s_logits_l, dot_product, s_loss_l_old, s_loss_l_new, t_logits_us, hard_pseudo_label, #t_loss, t_loss_uda, t_loss_mpl\n",
        "#         # del s_loss\n",
        "\n",
        "\n",
        "#         t_scaler.unscale_(t_optimizer) # if grad_clip > 0:\n",
        "#         nn.utils.clip_grad_norm_(teacher_model.parameters(), 1e9)\n",
        "#         t_scaler.step(t_optimizer)\n",
        "#         t_scaler.update()\n",
        "#         t_scheduler.step()\n",
        "#         teacher_model.zero_grad()\n",
        "\n",
        "#     return\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "om2hOCE1BgrE"
      },
      "outputs": [],
      "source": [
        "# @title mpl grad acc\n",
        "# https://github.com/kekmodel/MPL-pytorch/blob/main/main.py\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.cuda import amp\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "random.seed(0)\n",
        "np.random.seed(0)\n",
        "torch.manual_seed(0)\n",
        "torch.cuda.manual_seed_all(0)\n",
        "\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "\n",
        "t_scaler = amp.GradScaler()\n",
        "s_scaler = amp.GradScaler()\n",
        "def train(labeled_loader, unlabeled_loader, teacher_model, student_model,\n",
        "        avg_student_model, criterion, t_optimizer, s_optimizer, t_scheduler, s_scheduler):\n",
        "    labeled_iter = iter(labeled_loader)\n",
        "    unlabeled_iter = iter(unlabeled_loader)\n",
        "    size = len(unlabeled_loader)\n",
        "\n",
        "    # for author's code formula\n",
        "    # moving_dot_product = torch.empty(1).to(device)\n",
        "    # limit = 3.0**(0.5)  # 3 = 6 / (f_in + f_out)\n",
        "    # nn.init.uniform_(moving_dot_product, -limit, limit)\n",
        "\n",
        "    teacher_model.train()\n",
        "    student_model.train()\n",
        "    for step in range(len(unlabeled_loader)):\n",
        "\n",
        "        try: cimages_l, ctargets = next(labeled_iter)\n",
        "        except:\n",
        "            labeled_iter = iter(labeled_loader)\n",
        "            cimages_l, ctargets = next(labeled_iter)\n",
        "        try: cimages_uw = next(unlabeled_iter) # images_uw, _ = next(unlabeled_iter)\n",
        "        except:\n",
        "            unlabeled_iter = iter(unlabeled_loader)\n",
        "            cimages_uw = next(unlabeled_iter) # me\n",
        "        cimages_l, ctargets = cimages_l.to(device), ctargets.to(device)\n",
        "        cimages_uw = cimages_uw.to(device)\n",
        "        cimages_us = trs(cimages_uw)\n",
        "\n",
        "        t_loss_l = 0\n",
        "        t_loss_wu = 0\n",
        "        s_loss = 0\n",
        "        s_loss_l_old = 0\n",
        "        ct_logits_us = torch.empty(0, device=device)\n",
        "        for images_l, targets, images_uw, images_us in zip(cimages_l.chunk(grad_acc), ctargets.chunk(grad_acc), cimages_uw.chunk(grad_acc), cimages_us.chunk(grad_acc)): # for grad acc 1/2\n",
        "        # for images_l, targets, images_uw, images_us, s_loss_l_old in zip(cimages_l.chunk(grad_acc), ctargets.chunk(grad_acc), cimages_uw.chunk(grad_acc), cimages_us.chunk(grad_acc), cs_loss_l_old.chunk(grad_acc)): # for grad acc 1/2\n",
        "            with amp.autocast():\n",
        "                batch_size = images_l.shape[0]\n",
        "                with torch.no_grad(): m_logits_l = model(images_l) # reduced pml # big teacher\n",
        "                t_logits_l = teacher_model(m_logits_l)\n",
        "                t_l_l = criterion(t_logits_l, targets)\n",
        "            # t_scaler.scale(t_loss_l).backward(retain_graph=True) # me backward first\n",
        "            t_scaler.scale(t_l_l).backward() # me backward first\n",
        "            t_loss_l += t_l_l\n",
        "            # del t_loss_l\n",
        "\n",
        "            with amp.autocast():\n",
        "                with torch.no_grad(): s_logits_l = student_model(images_l)\n",
        "                s_l_l_old = F.cross_entropy(s_logits_l, targets)\n",
        "                s_loss_l_old += s_l_l_old\n",
        "\n",
        "            for i_us, i_uw in zip(images_us.chunk(mu), images_uw.chunk(mu)):\n",
        "                with amp.autocast():\n",
        "                    with torch.no_grad():\n",
        "                        m_i_us = model(i_us) # reduced pml # big teacher\n",
        "                        m_i_uw = model(i_uw) # reduced pml # big teacher\n",
        "                        t_l_uw = teacher_model(m_i_uw) # t_logits_uw no need grad\n",
        "                    t_l_us = teacher_model(m_i_us)\n",
        "                    ct_logits_us = torch.cat((ct_logits_us, t_l_us))\n",
        "\n",
        "                    temperature = 0.7 # default 1 / mainargs 0.7\n",
        "                    # soft_pseudo_label = torch.softmax(t_logits_uw.detach() / temperature, dim=-1)\n",
        "                    soft_pseudo_label = torch.softmax(t_l_uw / temperature, dim=-1)\n",
        "                    max_probs, hard_pseudo_label = torch.max(soft_pseudo_label, dim=-1) # all no grads\n",
        "\n",
        "                    threshold = 0.95 # default 0.95 / mainargs 0.6\n",
        "                    mask = max_probs.ge(threshold).float()\n",
        "                    t_loss_u = torch.mean(-(soft_pseudo_label * torch.log_softmax(t_l_us, dim=-1)).sum(dim=-1) * mask)\n",
        "                    lambda_u = 8 # default 1 / mainargs 8 coefficient of unlabeled loss\n",
        "                    uda_steps = 10 # default 1 / mainargs 5000 warmup steps of lambda-u\n",
        "                    weight_u = lambda_u * min(1., (step + 1) / uda_steps) # >0\n",
        "                    # t_loss_uda = t_loss_l + weight_u * t_loss_u\n",
        "\n",
        "                    # i_us.retain_grad()\n",
        "                    i_us.requires_grad=True\n",
        "                    s_l_us = student_model(i_us)\n",
        "                    s_l = criterion(s_l_us, hard_pseudo_label)\n",
        "                s_scaler.scale(s_l).backward()\n",
        "                s_loss += s_l\n",
        "                t_scaler.scale(weight_u * t_loss_u).backward(retain_graph=True)\n",
        "                # t_scaler.scale(weight_u * t_loss_u).backward()\n",
        "                t_loss_wu += weight_u * t_loss_u\n",
        "                # del t_loss_u, soft_pseudo_label# t_loss_l\n",
        "                # del s_logits_l, s_logits_us, hard_pseudo_label#, s_loss\n",
        "\n",
        "        t_loss_uda = t_loss_l + t_loss_wu\n",
        "        print('t_loss_l',t_loss_l.item())\n",
        "        print('t_loss_wu',t_loss_wu.item())\n",
        "\n",
        "        # if grad_clip > 0:\n",
        "        s_scaler.unscale_(s_optimizer)\n",
        "        nn.utils.clip_grad_norm_(student_model.parameters(), 1e9)\n",
        "        s_scaler.step(s_optimizer)\n",
        "        s_scaler.update()\n",
        "        s_scheduler.step()\n",
        "        student_model.zero_grad()\n",
        "        if ema > 0: avg_student_model.update_parameters(student_model)\n",
        "\n",
        "\n",
        "        s_loss_l_new = 0\n",
        "        # for images_l, targets, images_uw, images_us in zip(cimages_l.chunk(grad_acc), ctargets.chunk(grad_acc), cimages_uw.chunk(grad_acc), cimages_us.chunk(grad_acc)): # for grad acc 2/2\n",
        "        # for images_l, targets, images_uw, images_us, t_logits_us in zip(cimages_l.chunk(grad_acc), ctargets.chunk(grad_acc), cimages_uw.chunk(grad_acc), cimages_us.chunk(grad_acc), ct_logits_us.chunk(grad_acc)): # for grad acc 2/2\n",
        "        for images_l, targets in zip(cimages_l.chunk(grad_acc), ctargets.chunk(grad_acc)): # for grad acc 2/2\n",
        "            with amp.autocast():\n",
        "                with torch.no_grad(): s_logits_l = student_model(images_l)\n",
        "                s_l_l_new = F.cross_entropy(s_logits_l, targets)\n",
        "                s_loss_l_new += s_l_l_new\n",
        "        dot_product = s_loss_l_old - s_loss_l_new\n",
        "        # # moving_dot_product = moving_dot_product * 0.99 + dot_product * 0.01\n",
        "        # # dot_product = dot_product - moving_dot_product\n",
        "\n",
        "        t_loss_mpl = 0\n",
        "        # for images_l, targets, images_uw, images_us, t_logits_us in zip(cimages_l.chunk(grad_acc), ctargets.chunk(grad_acc), cimages_uw.chunk(grad_acc), cimages_us.chunk(grad_acc), ct_logits_us.chunk(grad_acc)): # for grad acc 2/2\n",
        "        for t_logits_us in ct_logits_us.chunk(grad_acc): # for grad acc 2/2\n",
        "            for t_l_us in t_logits_us.chunk(mu):\n",
        "                with amp.autocast():\n",
        "                    _, hard_pseudo_label = torch.max(t_l_us.detach(), dim=-1)\n",
        "                    t_l_mpl = dot_product * F.cross_entropy(t_l_us, hard_pseudo_label) # dot_product no grad\n",
        "                t_scaler.scale(t_l_mpl).backward(retain_graph=True)\n",
        "                t_loss_mpl += t_l_mpl\n",
        "\n",
        "        t_loss = t_loss_uda + t_loss_mpl\n",
        "        print('t_loss_mpl',t_loss_mpl.item())\n",
        "        # print(\"t_loss, s_loss\", t_loss.item(), s_loss.item())\n",
        "        if step%10==0: print(step,\"/\",size,\" \", \"t_loss: \", t_loss.item(), \"s_loss: \", s_loss.item())\n",
        "        try: wandb.log({\"t_loss\": t_loss.item(), \"s_loss\": s_loss.item()})\n",
        "        except: pass\n",
        "        # del s_logits_l, dot_product, s_loss_l_old, s_loss_l_new, t_logits_us, hard_pseudo_label, #t_loss, t_loss_uda, t_loss_mpl\n",
        "        # del s_loss\n",
        "\n",
        "\n",
        "        t_scaler.unscale_(t_optimizer) # if grad_clip > 0:\n",
        "        nn.utils.clip_grad_norm_(teacher_model.parameters(), 1e9)\n",
        "        t_scaler.step(t_optimizer)\n",
        "        t_scaler.update()\n",
        "        t_scheduler.step()\n",
        "        teacher_model.zero_grad()\n",
        "\n",
        "    return\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "W9URMRghTLc_"
      },
      "outputs": [],
      "source": [
        "# @title strain eval\n",
        "\n",
        "def evaluate(test_loader, model, criterion, verbose=True):\n",
        "    size = len(test_loader.dataset)\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for step, (images, targets) in enumerate(test_loader):\n",
        "            batch_size = images.shape[0]\n",
        "            images = images.to(device)\n",
        "            targets = targets.to(device)\n",
        "            with amp.autocast():\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, targets)\n",
        "            test_loss += loss.item()\n",
        "        #     acc1, acc5 = accuracy(outputs, targets, (1, 5))\n",
        "        #     losses, top1, top5 = loss.item(), acc1[0], acc5[0]\n",
        "        # # return losses, top1, top5\n",
        "            correct += (outputs.argmax(1) == targets).type(torch.float).sum().item()\n",
        "    test_loss /= size\n",
        "    correct /= size\n",
        "    try: wandb.log({\"test loss\": test_loss})\n",
        "    except: pass\n",
        "    if verbose: print(f\"Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f}\")\n",
        "    return correct, test_loss\n",
        "\n",
        "\n",
        "# @title train test function\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "# https://github.com/prigoyal/pytorch_memonger/blob/master/models/optimized/resnet_new.py\n",
        "from torch.utils.checkpoint import checkpoint, checkpoint_sequential\n",
        "\n",
        "trs=TrainTransform() # for image augmentation during train time\n",
        "# train function with automatic mixed precision\n",
        "def strain(dataloader, model, loss_fn, optimizer, scheduler=None, verbose=True):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    loss_list = []\n",
        "    for batch, (x, y) in enumerate(dataloader):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        with torch.cuda.amp.autocast(): # automatic mixed percision\n",
        "            x = trs(x) # image augmentation during train time to use gpu\n",
        "            pred = model(x) # default\n",
        "            loss = loss_fn(pred, y)\n",
        "        scaler.scale(loss).backward()\n",
        "        if ((batch + 1) % 4 == 0) or (batch + 1 == len(dataloader)): # gradient accumulation\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            optimizer.zero_grad()\n",
        "            if scheduler is not None:\n",
        "                scheduler.step()\n",
        "                # print(\"### lr: \", optimizer.param_groups[0][\"lr\"])\n",
        "\n",
        "        # print(model.state_dict()['_orig_mod.bn1.running_mean'][0])\n",
        "        train_loss = loss.item()/len(y)\n",
        "        loss_list.append(train_loss)\n",
        "        try: wandb.log({\"train loss\": train_loss})\n",
        "        except: pass\n",
        "        # if batch % (size//(10* len(y))) == 0:\n",
        "        current = batch * len(x)\n",
        "        if verbose: print(f\"loss: {train_loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "    return loss_list\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "w_2qJyKBwL-V"
      },
      "outputs": [],
      "source": [
        "# @title save\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "# checkpoint = {\n",
        "# 'epoch': t+1,\n",
        "# 'teacher_model': teacher_model.state_dict(),\n",
        "# 'student_model': student_model.state_dict(),\n",
        "# 'avg_student_model': avg_student_model.state_dict(),\n",
        "# 't_optimizer': t_optimizer.state_dict(),\n",
        "# 's_optimizer': s_optimizer.state_dict(),\n",
        "# 't_scheduler': t_scheduler.state_dict(),\n",
        "# 's_scheduler': s_scheduler.state_dict(),}\n",
        "# # torch.save(checkpoint, pth)\n",
        "# torch.save(checkpoint, 'ckpt.pth')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aw0nwnUAh2Of"
      },
      "outputs": [],
      "source": [
        "# @title wwwwwwwwwww\n",
        "\n",
        "criterion = create_loss_fn()\n",
        "\n",
        "from torch import optim\n",
        "# lr default 0.01/ mainargs 0.05\n",
        "# og:t0.05s0.05 , psl:t1e-3s3e-4, rpsl:t\n",
        "# t_optimizer = optim.SGD(teacher_parameters, lr=0.05, momentum=0.9, nesterov=True)\n",
        "# s_optimizer = optim.SGD(student_parameters, lr=0.05, momentum=0.9, nesterov=True)\n",
        "t_optimizer = optim.SGD(teacher_parameters, lr=1e-6, momentum=0.9, nesterov=True)\n",
        "s_optimizer = optim.SGD(student_parameters, lr=1e-5, momentum=0.9, nesterov=True)\n",
        "# optimizer = bnb.optim.AdamW(model.parameters(), lr=1e-5, betas=(0.9, 0.999), optim_bits=8)\n",
        "# optimizer = Lamb(model.parameters(), lr=1e-5, betas=(0.9, 0.999), eps=1e-08, weight_decay=3e-6)\n",
        "\n",
        "# 3e-5 27.9->25.0\n",
        "# 3e-3 27.9->26.5\n",
        "# 0.05 29.4 -> 22.1\n",
        "\n",
        "epochs = 5\n",
        "num_batches=len(train_loader)\n",
        "total_steps=int(np.ceil(num_batches/grad_acc)*epochs +1) # +1 to excluse uptick at the end of onecycle\n",
        "# total_steps=30 # 300000\n",
        "warmup_steps = 10 # default 0 / mainargs 5000\n",
        "# t_scheduler = get_cosine_schedule_with_warmup(t_optimizer, warmup_steps, total_steps)\n",
        "t_scheduler = get_cosine_schedule_with_warmup(t_optimizer, warmup_steps, total_steps,10)\n",
        "student_wait_steps = 6 # default 0 / mainargs 3000\n",
        "s_scheduler = get_cosine_schedule_with_warmup(s_optimizer, warmup_steps, total_steps, student_wait_steps)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "vfpCXUfEzHZV"
      },
      "outputs": [],
      "source": [
        "\n",
        "# scheduler = torch.optim.lr_scheduler.PolynomialLR(optimizer, total_iters=int(np.ceil(num_batches/4)*3), power=1.0)\n",
        "# scheduler = PolynomialLR(optimizer, total_iters=4, power=1.0)\n",
        "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=10**(-1/2))\n",
        "\n",
        "\n",
        "# t_optimizer.param_groups[0][\"lr\"]\n",
        "# s_optimizer.param_groups[0][\"lr\"]\n",
        "\n",
        "# import time\n",
        "# start = time.time()\n",
        "\n",
        "# pth='/content/mpl.pth' # ty\n",
        "pth='/content/drive/MyDrive/frame/mpl18.pth' # M\n",
        "\n",
        "for t in range(0,epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    t_lr=t_optimizer.param_groups[0][\"lr\"]\n",
        "    s_lr=s_optimizer.param_groups[0][\"lr\"]\n",
        "    print('t_lr,s_lr',t_lr,s_lr)\n",
        "    train(labeled_loader, unlabeled_loader, teacher_model, student_model,\n",
        "        avg_student_model, criterion, t_optimizer, s_optimizer, t_scheduler, s_scheduler)\n",
        "\n",
        "    # evaluate(test_loader, student_model, criterion)\n",
        "    evaluate(test_loader, avg_student_model, criterion)\n",
        "\n",
        "    checkpoint = {\n",
        "    'epoch': t+1,\n",
        "    'teacher_model': teacher_model.state_dict(),\n",
        "    'student_model': student_model.state_dict(),\n",
        "    'avg_student_model': avg_student_model.state_dict(),\n",
        "    't_optimizer': t_optimizer.state_dict(),\n",
        "    's_optimizer': s_optimizer.state_dict(),\n",
        "    't_scheduler': t_scheduler.state_dict(),\n",
        "    's_scheduler': s_scheduler.state_dict(),}\n",
        "    torch.save(checkpoint, pth)\n",
        "\n",
        "\n",
        "# res34, batch4 8.8\n",
        "# res18, batch16 11.6 nocompilemodel\n",
        "\n",
        "# 16m28s\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4GuEvf0MzBTD"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# finetune\n",
        "# model = student_model\n",
        "model = avg_student_model\n",
        "model.drop = nn.Identity()\n",
        "# labeled_loader = DataLoader(finetune_dataset, batch_size=128, num_workers=4, pin_memory=True) # batch_size=512\n",
        "labeled_loader = DataLoader(finetune_dataset, batch_size=128, pin_memory=True) # batch_size=512\n",
        "optimizer = optim.SGD(model.parameters(), lr=3e-5, momentum=0.9, weight_decay=0, nesterov=True)\n",
        "# scaler = amp.GradScaler()\n",
        "for epoch in range(1): #625\n",
        "    # train_ls = strain(labeled_loader, model, loss_fn, optimizer, scheduler)\n",
        "    train_ls = strain(labeled_loader, model, criterion, optimizer)\n",
        "    evaluate(test_loader, student_model, criterion)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JlQClGYfs5L1"
      },
      "outputs": [],
      "source": [
        "correct, test_loss = evaluate(test_loader, student_model, criterion)\n",
        "print(correct, test_loss)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}