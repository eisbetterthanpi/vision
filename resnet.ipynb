{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO42331Z/F6xl0xv9IAJ0iU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eisbetterthanpi/vision/blob/main/resnet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h7iCNId6oXUK",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title data\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "# import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "# https://github.com/python-engineer/pytorchTutorial/blob/master/14_cnn.py\n",
        "\n",
        "# dataset has PILImage images of range [0, 1], transform them to Tensors of normalized range [-1, 1]\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "# transform = transforms.Compose(transforms.ToTensor())\n",
        "\n",
        "# CIFAR10: 60000 32x32 color images in 10 classes, with 6000 images per class\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "batch_size = 4\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "dataiter = iter(train_loader) # get some random training images\n",
        "images, labels = dataiter.next()\n",
        "# imshow(torchvision.utils.make_grid(images))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m3VJeCmgSjHP",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title simplifi\n",
        "# https://github.com/JayPatwardhan/ResNet-PyTorch/blob/master/ResNet/ResNet.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    expansion = 1\n",
        "    def __init__(self, in_channels, out_channels, i_downsample=None, stride=1):\n",
        "        super(Block, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, stride=stride, bias=False),\n",
        "            nn.BatchNorm2d(out_channels), nn.ReLU(),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, stride=stride, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "        )\n",
        "        self.i_downsample = i_downsample\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x.clone()\n",
        "        x = self.conv(x)\n",
        "        if self.i_downsample is not None:\n",
        "            identity = self.i_downsample(identity)\n",
        "        x += identity\n",
        "        x = nn.ReLU()(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "    def __init__(self, in_channels, out_channels, i_downsample=None, stride=1):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0),\n",
        "            nn.BatchNorm2d(out_channels), nn.ReLU(),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1),\n",
        "            nn.BatchNorm2d(out_channels), nn.ReLU(),\n",
        "            nn.Conv2d(out_channels, out_channels*self.expansion, kernel_size=1, stride=1, padding=0),\n",
        "            nn.BatchNorm2d(out_channels*self.expansion),\n",
        "        )\n",
        "        self.i_downsample = i_downsample\n",
        "        \n",
        "    def forward(self, x):\n",
        "        identity = x.clone()\n",
        "        x = self.conv(x)\n",
        "        if self.i_downsample is not None: #downsample if needed\n",
        "            identity = self.i_downsample(identity)\n",
        "        x += identity #add identity\n",
        "        x = nn.ReLU()(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, ResBlock, layer_list, num_classes, num_channels=3):\n",
        "        super(ResNet, self).__init__()\n",
        "        plane_list=[4,8,16,32]\n",
        "        # self.in_channels = 64\n",
        "        self.in_channels = plane_list[0]\n",
        "        self.conv = nn.Sequential(\n",
        "            # nn.Conv2d(num_channels, 64, kernel_size=7, stride=2, padding=3, bias=False),\n",
        "            # nn.BatchNorm2d(64), nn.ReLU(),\n",
        "            nn.Conv2d(num_channels, plane_list[0], kernel_size=7, stride=2, padding=3, bias=False),\n",
        "            nn.BatchNorm2d(plane_list[0]), nn.ReLU(),\n",
        "\n",
        "            nn.MaxPool2d(kernel_size = 3, stride=2, padding=1),\n",
        "\n",
        "            # self._make_layer(ResBlock, layer_list[0], planes=4),#planes=64\n",
        "            # self._make_layer(ResBlock, layer_list[1], planes=8, stride=2),#planes=128\n",
        "            # self._make_layer(ResBlock, layer_list[2], planes=16, stride=2),#planes=256\n",
        "            # self._make_layer(ResBlock, layer_list[3], planes=32, stride=2),#planes=512\n",
        "            self._make_layer(ResBlock, layer_list[0], plane_list[0]),\n",
        "            self._make_layer(ResBlock, layer_list[1], plane_list[1], stride=2),\n",
        "            self._make_layer(ResBlock, layer_list[2], plane_list[2], stride=2),\n",
        "            self._make_layer(ResBlock, layer_list[3], plane_list[3], stride=2),\n",
        "\n",
        "            nn.AdaptiveAvgPool2d((1,1)),\n",
        "        )\n",
        "        # self.fc = nn.Linear(512*ResBlock.expansion, num_classes)\n",
        "        self.fc = nn.Linear(plane_list[3]*ResBlock.expansion, num_classes)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = x.reshape(x.shape[0], -1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "        \n",
        "    def _make_layer(self, ResBlock, blocks, planes, stride=1):\n",
        "        ii_downsample = None\n",
        "        layers = []\n",
        "        if stride != 1 or self.in_channels != planes*ResBlock.expansion:\n",
        "            ii_downsample = nn.Sequential(\n",
        "                nn.Conv2d(self.in_channels, planes*ResBlock.expansion, kernel_size=1, stride=stride),\n",
        "                nn.BatchNorm2d(planes*ResBlock.expansion)\n",
        "            )\n",
        "        layers.append(ResBlock(self.in_channels, planes, i_downsample=ii_downsample, stride=stride))\n",
        "        self.in_channels = planes*ResBlock.expansion\n",
        "        for i in range(blocks-1):\n",
        "            layers.append(ResBlock(self.in_channels, planes))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "        \n",
        "        \n",
        "def ResNet50(num_classes, channels=3):\n",
        "    return ResNet(Bottleneck, [3,4,6,3], num_classes, channels)\n",
        "    \n",
        "def ResNet101(num_classes, channels=3):\n",
        "    return ResNet(Bottleneck, [3,4,23,3], num_classes, channels)\n",
        "\n",
        "def ResNet152(num_classes, channels=3):\n",
        "    return ResNet(Bottleneck, [3,8,36,3], num_classes, channels)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# model = ResNet50(num_classes=10, channels=3).to(device)\n",
        "model = ResNet(Bottleneck, [3,4,6,3], num_classes=10, channels=3).to(device)\n",
        "# print(model)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title train/test\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
        "\n",
        "def train():\n",
        "    model.train()\n",
        "    num_epochs = 5\n",
        "    n_total_steps = len(train_loader)\n",
        "    for epoch in range(num_epochs):\n",
        "        for i, (images, labels) in enumerate(train_loader): # origin shape: [4, 3, 32, 32] = 4, 3, 1024 input_layer: 3 input channels, 6 output channels, 5 kernel size\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            if i % 2000 == 0:\n",
        "                print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
        "train()\n",
        "print('Finished Training')\n",
        "PATH = './cnn.pth'\n",
        "torch.save(model.state_dict(), PATH)\n",
        "\n",
        "def test():\n",
        "    model.eval()\n",
        "    n_correct = 0\n",
        "    n_samples = 0\n",
        "    with torch.no_grad():\n",
        "        n_class_correct = [0 for i in range(10)]\n",
        "        n_class_samples = [0 for i in range(10)]\n",
        "        for images, labels in test_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(images)\n",
        "            # max returns (value ,index)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            n_samples += labels.size(0)\n",
        "            n_correct += (predicted == labels).sum().item()\n",
        "            for i in range(batch_size):\n",
        "                label = labels[i]\n",
        "                pred = predicted[i]\n",
        "                if (label == pred):\n",
        "                    n_class_correct[label] += 1\n",
        "                n_class_samples[label] += 1\n",
        "        acc = 100.0 * n_correct / n_samples\n",
        "        print(f'Accuracy of the network: {acc} %')\n",
        "        for i in range(10):\n",
        "            acc = 100.0 * n_class_correct[i] / n_class_samples[i]\n",
        "            print(f'Accuracy of {classes[i]}: {acc} %')\n",
        "\n",
        "test()"
      ],
      "metadata": {
        "id": "HxreASu8WUCe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b74ae1e9-a0c4-472e-f908-71f264bc02fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5], Step [1/12500], Loss: 2.2956\n",
            "Epoch [1/5], Step [2001/12500], Loss: 2.4555\n",
            "Epoch [1/5], Step [4001/12500], Loss: 2.7042\n",
            "Epoch [1/5], Step [6001/12500], Loss: 2.0968\n",
            "Epoch [1/5], Step [8001/12500], Loss: 1.9578\n",
            "Epoch [1/5], Step [10001/12500], Loss: 1.8977\n",
            "Epoch [1/5], Step [12001/12500], Loss: 2.1082\n",
            "Epoch [2/5], Step [1/12500], Loss: 2.0538\n",
            "Epoch [2/5], Step [2001/12500], Loss: 2.0453\n",
            "Epoch [2/5], Step [4001/12500], Loss: 1.7704\n",
            "Epoch [2/5], Step [6001/12500], Loss: 1.6398\n",
            "Epoch [2/5], Step [8001/12500], Loss: 2.0163\n",
            "Epoch [2/5], Step [10001/12500], Loss: 1.8784\n",
            "Epoch [2/5], Step [12001/12500], Loss: 2.2474\n",
            "Epoch [3/5], Step [1/12500], Loss: 2.3727\n",
            "Epoch [3/5], Step [2001/12500], Loss: 2.2510\n",
            "Epoch [3/5], Step [4001/12500], Loss: 1.8058\n",
            "Epoch [3/5], Step [6001/12500], Loss: 1.9301\n",
            "Epoch [3/5], Step [8001/12500], Loss: 2.0612\n",
            "Epoch [3/5], Step [10001/12500], Loss: 2.0406\n",
            "Epoch [3/5], Step [12001/12500], Loss: 1.5219\n",
            "Epoch [4/5], Step [1/12500], Loss: 1.9874\n",
            "Epoch [4/5], Step [2001/12500], Loss: 2.2669\n",
            "Epoch [4/5], Step [4001/12500], Loss: 2.0645\n",
            "Epoch [4/5], Step [6001/12500], Loss: 2.3544\n",
            "Epoch [4/5], Step [8001/12500], Loss: 1.9010\n",
            "Epoch [4/5], Step [10001/12500], Loss: 2.3575\n",
            "Epoch [4/5], Step [12001/12500], Loss: 1.8455\n",
            "Epoch [5/5], Step [1/12500], Loss: 1.6921\n",
            "Epoch [5/5], Step [2001/12500], Loss: 2.3696\n",
            "Epoch [5/5], Step [4001/12500], Loss: 2.1501\n",
            "Epoch [5/5], Step [6001/12500], Loss: 2.6486\n",
            "Epoch [5/5], Step [8001/12500], Loss: 2.2888\n",
            "Epoch [5/5], Step [10001/12500], Loss: 2.6660\n",
            "Epoch [5/5], Step [12001/12500], Loss: 2.2911\n",
            "Finished Training\n",
            "Accuracy of the network: 31.61 %\n",
            "Accuracy of plane: 38.4 %\n",
            "Accuracy of car: 62.1 %\n",
            "Accuracy of bird: 15.2 %\n",
            "Accuracy of cat: 15.9 %\n",
            "Accuracy of deer: 27.6 %\n",
            "Accuracy of dog: 14.4 %\n",
            "Accuracy of frog: 21.8 %\n",
            "Accuracy of horse: 45.3 %\n",
            "Accuracy of ship: 52.6 %\n",
            "Accuracy of truck: 22.8 %\n"
          ]
        }
      ]
    }
  ]
}