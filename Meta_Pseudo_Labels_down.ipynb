{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eisbetterthanpi/vision/blob/main/Meta_Pseudo_Labels_down.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "v9XrV9ZACgjm"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Meta Pseudo Labels mar 2021 https://arxiv.org/pdf/2003.10580v4.pdf\n",
        "# https://github.com/kekmodel/MPL-pytorch\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title torch augment\n",
        "# https://github.com/facebookresearch/vicreg/blob/main/augmentations.py\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "class TrainTransform(object):\n",
        "    def __init__(self):\n",
        "        # self.transform = transforms.RandomApply([transforms.Compose([\n",
        "        self.transform = transforms.Compose([\n",
        "                transforms.RandomPerspective(distortion_scale=0.3, p=0.5), # me\n",
        "                # transforms.RandomResizedCrop((400,640), scale=(0.7, 1.0), ratio=(0.8, 1.25), interpolation=transforms.InterpolationMode.BICUBIC),\n",
        "                transforms.RandomResizedCrop((32,32), scale=(0.7, 1.0), ratio=(0.8, 1.25), interpolation=transforms.InterpolationMode.BICUBIC),\n",
        "                transforms.RandomHorizontalFlip(p=0.5), # 0.5\n",
        "                transforms.Lambda(lambda x : torch.clamp(x, 0., 1.)), # clamp else ColorJitter will return nan https://discuss.pytorch.org/t/input-is-nan-after-transformation/125455/6\n",
        "                transforms.RandomApply([transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1)], p=0.8,), # brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1)], p=0.8\n",
        "                transforms.RandomGrayscale(p=0.2), # 0.2\n",
        "                # # transforms.RandomChoice(transforms.ColorJitter , transforms.RandomGrayscale(p=1.)\n",
        "                transforms.RandomApply([transforms.GaussianBlur(kernel_size=5, sigma=(0.1, 2.0)),], p=1.0),\n",
        "                # # transforms.RandomSolarize(threshold=130, p=0.5)\n",
        "                transforms.RandomErasing(p=1., scale=(0.1, 0.11), ratio=(1,1), value=(0.485, 0.456, 0.406)),\n",
        "                # transforms.ToTensor(), # ToTensored at dataset level, no need to ToTensor again\n",
        "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), # normalised at dataset level. default 0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225\n",
        "                ])\n",
        "            # ], p=1.)\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        dims = len(sample.shape)\n",
        "        if dims==3: x1 = self.transform(sample) # same transforms per minibatch\n",
        "        elif dims==4: x1 = transforms.Lambda(lambda x: torch.stack([self.transform(x_) for x_ in x]))(sample) # diff transforms per img in minibatch\n",
        "        # x1 = self.transform(sample)\n",
        "        return x1\n",
        "\n",
        "trs=TrainTransform()\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "6iYyVffJcYB5"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title utils\n",
        "# https://github.com/kekmodel/MPL-pytorch/blob/main/utils.py\n",
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "def create_loss_fn():\n",
        "    label_smoothing = 0 # default 0 / mainargs 0.15\n",
        "    # if label_smoothing > 0:\n",
        "    #     criterion = SmoothCrossEntropyV2(alpha=label_smoothing)\n",
        "    # else:\n",
        "    criterion = nn.CrossEntropyLoss(label_smoothing=label_smoothing)\n",
        "    return criterion.to(device)\n",
        "\n",
        "from collections import OrderedDict\n",
        "def module_load_state_dict(model, state_dict):\n",
        "    try:\n",
        "        new_state_dict = OrderedDict()\n",
        "        for k, v in state_dict.items():\n",
        "            name = k[7:]  # remove `module.`\n",
        "            new_state_dict[name] = v\n",
        "        model.load_state_dict(new_state_dict)\n",
        "    except:\n",
        "        new_state_dict = OrderedDict()\n",
        "        for k, v in state_dict.items():\n",
        "            name = f'module.{k}'  # add `module.`\n",
        "            new_state_dict[name] = v\n",
        "        model.load_state_dict(new_state_dict)\n",
        "\n",
        "def model_load_state_dict(model, state_dict):\n",
        "    try: model.load_state_dict(state_dict)\n",
        "    except: module_load_state_dict(model, state_dict)\n",
        "\n",
        "\n",
        "def accuracy(output, target, topk=(1,)):\n",
        "    output = output.to(torch.device('cpu'))\n",
        "    target = target.to(torch.device('cpu'))\n",
        "    maxk = max(topk)\n",
        "    batch_size = target.shape[0]\n",
        "    _, idx = output.sort(dim=1, descending=True)\n",
        "    pred = idx.narrow(1, 0, maxk).t()\n",
        "    correct = pred.eq(target.reshape(1, -1).expand_as(pred))\n",
        "    res = []\n",
        "    for k in topk:\n",
        "        correct_k = correct[:k].reshape(-1).float().sum(dim=0, keepdim=True)\n",
        "        res.append(correct_k.mul_(100.0 / batch_size))\n",
        "    return res\n",
        "\n",
        "\n",
        "class SmoothCrossEntropy(nn.Module):\n",
        "    def __init__(self, alpha=0.1):\n",
        "        super(SmoothCrossEntropy, self).__init__()\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def forward(self, logits, labels):\n",
        "        if self.alpha == 0:\n",
        "            loss = F.cross_entropy(logits, labels)\n",
        "        else:\n",
        "            num_classes = logits.shape[-1]\n",
        "            alpha_div_k = self.alpha / num_classes\n",
        "            target_probs = F.one_hot(labels, num_classes=num_classes).float() * (1. - self.alpha) + alpha_div_k\n",
        "            loss = (-(target_probs * torch.log_softmax(logits, dim=-1)).sum(dim=-1)).mean()\n",
        "        return loss\n",
        "\n",
        "class SmoothCrossEntropyV2(nn.Module):\n",
        "    \"\"\"NLL loss with label smoothing.\"\"\"\n",
        "    def __init__(self, label_smoothing=0.1):\n",
        "        super().__init__()\n",
        "        assert label_smoothing < 1.0\n",
        "        self.smoothing = label_smoothing\n",
        "        self.confidence = 1. - label_smoothing\n",
        "\n",
        "    def forward(self, x, target):\n",
        "        if self.smoothing == 0:\n",
        "            loss = F.cross_entropy(x, target)\n",
        "        else:\n",
        "            logprobs = F.log_softmax(x, dim=-1)\n",
        "            nll_loss = -logprobs.gather(dim=-1, index=target.unsqueeze(1))\n",
        "            nll_loss = nll_loss.squeeze(1)\n",
        "            smooth_loss = -logprobs.mean(dim=-1)\n",
        "            loss = (self.confidence * nll_loss + self.smoothing * smooth_loss).mean()\n",
        "        return loss\n",
        "\n"
      ],
      "metadata": {
        "id": "q_bJYwwSDNNy",
        "cellView": "form"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ModelEMA\n",
        "# expopnential moving average, smoothen model parameters\n",
        "# https://github.com/kekmodel/MPL-pytorch/blob/main/models.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from copy import deepcopy\n",
        "\n",
        "class ModelEMA(nn.Module):\n",
        "    def __init__(self, model, decay=0.9999, device=None):\n",
        "        super().__init__()\n",
        "        self.module = deepcopy(model)\n",
        "        self.module.eval()\n",
        "        self.decay = decay\n",
        "        self.device = device\n",
        "        if self.device is not None:\n",
        "            self.module.to(device=device)\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.module(input)\n",
        "\n",
        "    def _update(self, model, update_fn):\n",
        "        with torch.no_grad():\n",
        "            for ema_v, model_v in zip(self.module.parameters(), model.parameters()):\n",
        "                if self.device is not None:\n",
        "                    model_v = model_v.to(device=self.device)\n",
        "                ema_v.copy_(update_fn(ema_v, model_v))\n",
        "            for ema_v, model_v in zip(self.module.buffers(), model.buffers()):\n",
        "                if self.device is not None:\n",
        "                    model_v = model_v.to(device=self.device)\n",
        "                ema_v.copy_(model_v)\n",
        "\n",
        "    def update_parameters(self, model):\n",
        "        self._update(model, update_fn=lambda e, m: self.decay * e + (1. - self.decay) * m)\n",
        "\n",
        "    def state_dict(self):\n",
        "        return self.module.state_dict()\n",
        "\n",
        "    def load_state_dict(self, state_dict):\n",
        "        self.module.load_state_dict(state_dict)\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "4J3GeedWa6QO"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# @title main down\n",
        "# https://github.com/kekmodel/MPL-pytorch/blob/main/main.py\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "import time\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.cuda import amp\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch import optim\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "from tqdm import tqdm\n",
        "\n",
        "ema = 0.995 # default 0 / mainargs 0.995\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "random.seed(0)\n",
        "np.random.seed(0)\n",
        "torch.manual_seed(0)\n",
        "torch.cuda.manual_seed_all(0)\n",
        "\n",
        "\n",
        "def get_cosine_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps, num_wait_steps=0, num_cycles=0.5, last_epoch=-1):\n",
        "    def lr_lambda(current_step):\n",
        "        if current_step < num_wait_steps:\n",
        "            return 0.0\n",
        "        if current_step < num_warmup_steps + num_wait_steps:\n",
        "            return float(current_step) / float(max(1, num_warmup_steps + num_wait_steps))\n",
        "        progress = float(current_step - num_warmup_steps - num_wait_steps) / float(max(1, num_training_steps - num_warmup_steps - num_wait_steps))\n",
        "        return max(0.0, 0.5 * (1.0 + math.cos(math.pi * float(num_cycles) * 2.0 * progress)))\n",
        "    return LambdaLR(optimizer, lr_lambda, last_epoch)\n",
        "\n",
        "\n",
        "t_scaler = amp.GradScaler()\n",
        "s_scaler = amp.GradScaler()\n",
        "# def train_loop(labeled_loader, unlabeled_loader, test_loader, finetune_dataset,\n",
        "#                teacher_model, student_model, avg_student_model, criterion,\n",
        "#                t_optimizer, s_optimizer, t_scheduler, s_scheduler, t_scaler, s_scaler):\n",
        "def train(labeled_loader, unlabeled_loader, teacher_model, student_model,\n",
        "        avg_student_model, criterion, t_optimizer, s_optimizer, t_scheduler, s_scheduler):\n",
        "    labeled_iter = iter(labeled_loader)\n",
        "    unlabeled_iter = iter(unlabeled_loader)\n",
        "\n",
        "    # for author's code formula\n",
        "    # moving_dot_product = torch.empty(1).to(device)\n",
        "    # limit = 3.0**(0.5)  # 3 = 6 / (f_in + f_out)\n",
        "    # nn.init.uniform_(moving_dot_product, -limit, limit)\n",
        "\n",
        "    # eval_step = 10#1000\n",
        "    # start_step=0\n",
        "    # for step in range(start_step, total_steps):\n",
        "    for step in range(len(unlabeled_loader)):\n",
        "        teacher_model.train()\n",
        "        student_model.train()\n",
        "\n",
        "        try:\n",
        "            images_l, targets = next(labeled_iter)\n",
        "        except:\n",
        "            labeled_iter = iter(labeled_loader)\n",
        "            images_l, targets = next(labeled_iter)\n",
        "        try:\n",
        "            # (images_uw, images_us), _ = next(unlabeled_iter)\n",
        "            images_uw, _ = next(unlabeled_iter)\n",
        "            images_us = trs(images_uw)\n",
        "        except:\n",
        "            unlabeled_iter = iter(unlabeled_loader)\n",
        "            # (images_uw, images_us), _ = next(unlabeled_iter)\n",
        "            images_uw, _ = next(unlabeled_iter)\n",
        "            images_us = trs(images_uw)\n",
        "        images_l, targets = images_l.to(device), targets.to(device)\n",
        "        images_uw, images_us = images_uw.to(device), images_us.to(device)\n",
        "\n",
        "        with amp.autocast():\n",
        "            batch_size = images_l.shape[0]\n",
        "            # print(images_l.shape, images_uw.shape, images_us.shape) # [64, 3, 32, 32]. [448, 3, 32, 32], [448, 3, 32, 32]\n",
        "            t_images = torch.cat((images_l, images_uw, images_us))\n",
        "            t_logits = teacher_model(t_images)\n",
        "            t_logits_l = t_logits[:batch_size]\n",
        "            t_logits_uw, t_logits_us = t_logits[batch_size:].chunk(2)\n",
        "            del t_logits\n",
        "\n",
        "            t_loss_l = criterion(t_logits_l, targets)\n",
        "\n",
        "            temperature = 1 # default 1 / mainargs 0.7\n",
        "            soft_pseudo_label = torch.softmax(t_logits_uw.detach() / temperature, dim=-1)\n",
        "            max_probs, hard_pseudo_label = torch.max(soft_pseudo_label, dim=-1)\n",
        "\n",
        "            threshold = 0.95 # default 0.95 / mainargs 0.6\n",
        "            mask = max_probs.ge(threshold).float()\n",
        "            t_loss_u = torch.mean(-(soft_pseudo_label * torch.log_softmax(t_logits_us, dim=-1)).sum(dim=-1) * mask)\n",
        "            lambda_u = 8 # default 1 / mainargs 8 coefficient of unlabeled loss\n",
        "            uda_steps = 10 # default 1 / mainargs 5000 warmup steps of lambda-u\n",
        "            weight_u = lambda_u * min(1., (step + 1) / uda_steps)\n",
        "            t_loss_uda = t_loss_l + weight_u * t_loss_u\n",
        "\n",
        "            s_images = torch.cat((images_l, images_us))\n",
        "            s_logits = student_model(s_images)\n",
        "            s_logits_l = s_logits[:batch_size]\n",
        "            s_logits_us = s_logits[batch_size:]\n",
        "            del s_logits\n",
        "\n",
        "            s_loss_l_old = F.cross_entropy(s_logits_l.detach(), targets)\n",
        "\n",
        "            # print(\"s_logits_us, hard_pseudo_label: \", s_logits_us.shape, hard_pseudo_label.shape) # [448, 10] [224]\n",
        "            s_loss = criterion(s_logits_us, hard_pseudo_label)\n",
        "\n",
        "        s_scaler.scale(s_loss).backward()\n",
        "\n",
        "        # if grad_clip > 0:\n",
        "        s_scaler.unscale_(s_optimizer)\n",
        "        nn.utils.clip_grad_norm_(student_model.parameters(), 1e9)\n",
        "\n",
        "        s_scaler.step(s_optimizer)\n",
        "        s_scaler.update()\n",
        "        s_scheduler.step()\n",
        "\n",
        "        if ema > 0: avg_student_model.update_parameters(student_model)\n",
        "\n",
        "        with amp.autocast():\n",
        "            with torch.no_grad():\n",
        "                s_logits_l = student_model(images_l)\n",
        "            s_loss_l_new = F.cross_entropy(s_logits_l.detach(), targets)\n",
        "\n",
        "            dot_product = s_loss_l_old - s_loss_l_new # theoretically correct formula (https://github.com/kekmodel/MPL-pytorch/issues/6)\n",
        "            # dot_product = s_loss_l_new - s_loss_l_old # author's code formula\n",
        "            # # moving_dot_product = moving_dot_product * 0.99 + dot_product * 0.01\n",
        "            # # dot_product = dot_product - moving_dot_product\n",
        "\n",
        "            _, hard_pseudo_label = torch.max(t_logits_us.detach(), dim=-1)\n",
        "            t_loss_mpl = dot_product * F.cross_entropy(t_logits_us, hard_pseudo_label)\n",
        "\n",
        "            # t_loss_mpl = torch.tensor(0.).to(device) # test\n",
        "            t_loss = t_loss_uda + t_loss_mpl\n",
        "\n",
        "        t_scaler.scale(t_loss).backward()\n",
        "\n",
        "        # if grad_clip > 0:\n",
        "        t_scaler.unscale_(t_optimizer)\n",
        "        nn.utils.clip_grad_norm_(teacher_model.parameters(), 1e9)\n",
        "\n",
        "        t_scaler.step(t_optimizer)\n",
        "        t_scaler.update()\n",
        "        t_scheduler.step()\n",
        "\n",
        "        teacher_model.zero_grad()\n",
        "        student_model.zero_grad()\n",
        "\n",
        "        # if (step + 1) % eval_step == 0:\n",
        "        #     # print(s_losses.avg, t_losses.avg, t_losses_l.avg, t_losses_u.avg, t_losses_mpl.avg, mean_mask.avg)\n",
        "\n",
        "        #     test_model = avg_student_model if avg_student_model is not None else student_model\n",
        "        #     test_loss, top1, top5 = evaluate(test_loader, test_model, criterion)\n",
        "\n",
        "    # # finetune\n",
        "    # ckpt_name = f'{save_path}/{name}_best.pth.tar'\n",
        "    # gpu=0\n",
        "    # loc = f'cuda:{gpu}'\n",
        "    # checkpoint = torch.load(ckpt_name, map_location=loc)\n",
        "    # if checkpoint['avg_state_dict'] is not None:\n",
        "    #     model_load_state_dict(student_model, checkpoint['avg_state_dict'])\n",
        "    # else:\n",
        "    #     model_load_state_dict(student_model, checkpoint['student_state_dict'])\n",
        "    # finetune(finetune_dataset, test_loader, student_model, criterion)\n",
        "    return\n",
        "\n",
        "\n",
        "def evaluate(test_loader, model, criterion, verbose=True):\n",
        "    size = len(test_loader.dataset)\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for step, (images, targets) in enumerate(test_loader):\n",
        "            batch_size = images.shape[0]\n",
        "            images = images.to(device)\n",
        "            targets = targets.to(device)\n",
        "            with amp.autocast():\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, targets)\n",
        "            test_loss += loss.item()\n",
        "        #     acc1, acc5 = accuracy(outputs, targets, (1, 5))\n",
        "        #     loss.item(), acc1[0], acc5[0]\n",
        "        # # return losses.avg, top1.avg, top5.avg\n",
        "            correct += (outputs.argmax(1) == targets).type(torch.float).sum().item()\n",
        "    test_loss /= size\n",
        "    correct /= size\n",
        "    try: wandb.log({\"test loss\": test_loss})\n",
        "    except: pass\n",
        "    if verbose: print(f\"Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f}\")\n",
        "    return correct, test_loss\n",
        "\n",
        "\n",
        "\n",
        "# # def finetune(finetune_dataset, test_loader, model, criterion):\n",
        "# def finetune(finetune_dataset, model, criterion):\n",
        "#     # model.drop = nn.Identity()\n",
        "#     # labeled_loader = DataLoader(finetune_dataset, batch_size=512, num_workers=4, pin_memory=True)\n",
        "#     # optimizer = optim.SGD(model.parameters(), lr=3e-5, momentum=0.9, weight_decay=0, nesterov=True)\n",
        "#     # scaler = amp.GradScaler()\n",
        "#     # for epoch in range(1): #625\n",
        "\n",
        "\n",
        "# @title train test function\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "# https://github.com/prigoyal/pytorch_memonger/blob/master/models/optimized/resnet_new.py\n",
        "from torch.utils.checkpoint import checkpoint, checkpoint_sequential\n",
        "\n",
        "trs=TrainTransform() # for image augmentation during train time\n",
        "# train function with automatic mixed precision\n",
        "def strain(dataloader, model, loss_fn, optimizer, scheduler=None, verbose=True):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    loss_list = []\n",
        "    for batch, (x, y) in enumerate(dataloader):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        with torch.cuda.amp.autocast(): # automatic mixed percision\n",
        "            x = trs(x) # image augmentation during train time to use gpu\n",
        "            pred = model(x) # default\n",
        "            loss = loss_fn(pred, y)\n",
        "        scaler.scale(loss).backward()\n",
        "        if ((batch + 1) % 4 == 0) or (batch + 1 == len(dataloader)): # gradient accumulation\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            optimizer.zero_grad()\n",
        "            if scheduler is not None:\n",
        "                scheduler.step()\n",
        "                # print(\"### lr: \", optimizer.param_groups[0][\"lr\"])\n",
        "\n",
        "        # print(model.state_dict()['_orig_mod.bn1.running_mean'][0])\n",
        "        train_loss = loss.item()/len(y)\n",
        "        loss_list.append(train_loss)\n",
        "        try: wandb.log({\"train loss\": train_loss})\n",
        "        except: pass\n",
        "        if batch % (size//(10* len(y))) == 0:\n",
        "            current = batch * len(x)\n",
        "            if verbose: print(f\"loss: {train_loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "    return loss_list\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wWBABdFBjZ9u"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title wwwwwwwwwww\n",
        "\n",
        "\n",
        "# labeled_dataset, unlabeled_dataset, test_dataset, finetune_dataset = DATASET_GETTERS[dataset](args)\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
        "train_data = datasets.CIFAR10(root=\"data\", train=True, download=True,transform=transform)\n",
        "# test_data = datasets.CIFAR10(root=\"data\", train=False, download=True,transform=transform)\n",
        "labeled_dataset, unlabeled_dataset = torch.utils.data.random_split(train_data, [.1,.9])\n",
        "test_dataset = datasets.CIFAR10(root=\"data\", train=False, download=True,transform=transform)\n",
        "finetune_dataset = labeled_dataset\n",
        "\n",
        "\n",
        "batch_size = 64 # default 64/ mainargs128\n",
        "train_sampler = RandomSampler\n",
        "labeled_loader = DataLoader(labeled_dataset, sampler=train_sampler(labeled_dataset), batch_size=batch_size, num_workers=4, drop_last=True)\n",
        "unlabeled_loader = DataLoader(unlabeled_dataset, sampler=train_sampler(unlabeled_dataset), batch_size=batch_size * 7, num_workers=4, drop_last=True) # mu=7 ,coefficient of unlabeled batch size\n",
        "test_loader = DataLoader(test_dataset, sampler=SequentialSampler(test_dataset), batch_size=batch_size, num_workers=4)\n",
        "\n",
        "num_classes = 10\n",
        "# if dataset == \"cifar10\": depth, widen_factor = 28, 2\n",
        "# elif dataset == 'cifar100': depth, widen_factor = 28, 8\n",
        "# teacher_model = WideResNet(num_classes=num_classes, depth=depth, widen_factor=widen_factor, dropout=0, dense_dropout=0.2)\n",
        "# student_model = WideResNet(num_classes=num_classes, depth=depth, widen_factor=widen_factor, dropout=0, dense_dropout=0.2)\n",
        "\n",
        "\n",
        "from torchvision import models\n",
        "def get_resnet():\n",
        "    model = models.resnet152(weights='DEFAULT') # 18 34 50 101 152\n",
        "    num_ftrs = model.fc.in_features\n",
        "    model.fc = nn.Sequential( # og (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
        "        nn.Linear(num_ftrs, num_classes, bias=False),\n",
        "        nn.Softmax(dim=1),\n",
        "        )\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    # model = model.to(device)\n",
        "    model = torch.compile(model.to(device))\n",
        "    return model\n",
        "\n",
        "teacher_model = get_resnet()\n",
        "student_model = get_resnet()\n",
        "\n",
        "\n",
        "# teacher_model.to(device)\n",
        "# student_model.to(device)\n",
        "avg_student_model = None\n",
        "if ema > 0: avg_student_model = ModelEMA(student_model, ema)\n",
        "\n",
        "\n",
        "criterion = create_loss_fn()\n",
        "\n",
        "no_decay = ['bn']\n",
        "weight_decay = 5e-4 # default 0 / mainargs 5e-4\n",
        "teacher_parameters = [{'params': [p for n, p in teacher_model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': weight_decay},\n",
        "    {'params': [p for n, p in teacher_model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}]\n",
        "student_parameters = [{'params': [p for n, p in student_model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': weight_decay},\n",
        "    {'params': [p for n, p in student_model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}]\n",
        "\n",
        "# lr default 0.01/ mainargs 0.05\n",
        "t_optimizer = optim.SGD(teacher_parameters, lr=0.05, momentum=0.9, nesterov=True)\n",
        "s_optimizer = optim.SGD(student_parameters, lr=0.05, momentum=0.9, nesterov=True)\n",
        "\n",
        "total_steps=30 # 300000\n",
        "warmup_steps = 10 # default 0 / mainargs 5000\n",
        "t_scheduler = get_cosine_schedule_with_warmup(t_optimizer, warmup_steps, total_steps)\n",
        "student_wait_steps = 3 # default 0 / mainargs 3000\n",
        "s_scheduler = get_cosine_schedule_with_warmup(s_optimizer, warmup_steps, total_steps, student_wait_steps)\n",
        "\n",
        "\n",
        "# finetune(finetune_dataset, test_loader, student_model, criterion)\n",
        "# evaluate(test_loader, student_model, criterion)\n",
        "\n",
        "# teacher_model.zero_grad()\n",
        "# student_model.zero_grad()\n",
        "# train_loop(labeled_loader, unlabeled_loader, test_loader, finetune_dataset,\n",
        "#             teacher_model, student_model, avg_student_model, criterion,\n",
        "#             t_optimizer, s_optimizer, t_scheduler, s_scheduler, t_scaler, s_scaler)\n",
        "\n",
        "\n",
        "\n",
        "# import time\n",
        "# start = time.time()\n",
        "\n",
        "# optimizer = bnb.optim.AdamW(model.parameters(), lr=1e-5, betas=(0.9, 0.999), optim_bits=8)\n",
        "# optimizer = Lamb(model.parameters(), lr=1e-5, betas=(0.9, 0.999), eps=1e-08, weight_decay=3e-6)\n",
        "\n",
        "# scheduler = torch.optim.lr_scheduler.PolynomialLR(optimizer, total_iters=int(np.ceil(num_batches/4)*3), power=1.0)\n",
        "pth='/content/mpl.pth' # ty\n",
        "\n",
        "# scheduler = PolynomialLR(optimizer, total_iters=4, power=1.0)\n",
        "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=10**(-1/2))\n",
        "# for epoch in range(5):\n",
        "#     scheduler.step()\n",
        "epochs = 2\n",
        "for t in range(0,epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    t_lr=t_optimizer.param_groups[0][\"lr\"]\n",
        "    s_lr=s_optimizer.param_groups[0][\"lr\"]\n",
        "    print(t_lr,s_lr)\n",
        "    train(labeled_loader, unlabeled_loader, teacher_model, student_model,\n",
        "        avg_student_model, criterion, t_optimizer, s_optimizer, t_scheduler, s_scheduler)\n",
        "\n",
        "    # correct, test_loss = test(test_loader, student_model, loss_fn)\n",
        "    # correct, test_loss = test(test_loader, avg_student_model, loss_fn)\n",
        "    # evaluate(test_loader, student_model, criterion)\n",
        "    evaluate(test_loader, avg_student_model, criterion)\n",
        "\n",
        "    # train_lst.extend(train_ls)\n",
        "    # test_lst.append(test_loss)\n",
        "    # acc_lst.append(correct)\n",
        "\n",
        "    checkpoint = {\n",
        "    'epoch': t+1,\n",
        "    'teacher_model': teacher_model.state_dict(),\n",
        "    'student_model': student_model.state_dict(),\n",
        "    'avg_student_model': avg_student_model.state_dict(),\n",
        "    't_optimizer': t_optimizer.state_dict(),\n",
        "    's_optimizer': s_optimizer.state_dict(),\n",
        "    't_scheduler': t_scheduler.state_dict(),\n",
        "    's_scheduler': s_scheduler.state_dict(),}\n",
        "    torch.save(checkpoint, pth)\n",
        "\n",
        "\n",
        "\n",
        "# model = student_model\n",
        "model = avg_student_model\n",
        "model.drop = nn.Identity()\n",
        "labeled_loader = DataLoader(finetune_dataset, batch_size=128, num_workers=4, pin_memory=True) # batch_size=512\n",
        "optimizer = optim.SGD(model.parameters(), lr=3e-5, momentum=0.9, weight_decay=0, nesterov=True)\n",
        "# scaler = amp.GradScaler()\n",
        "for epoch in range(1): #625\n",
        "    # train_ls = strain(labeled_loader, model, loss_fn, optimizer, scheduler)\n",
        "    train_ls = strain(labeled_loader, model, criterion, optimizer)\n",
        "    evaluate(test_loader, student_model, criterion)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aw0nwnUAh2Of",
        "outputId": "483a1f32-fbf3-45ee-9f78-47d62251ea6b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "0.0 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 14.7%, Avg loss: 0.035975\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "0.025000000000000012 0.001688194264891091\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 22.4%, Avg loss: 0.035331\n",
            "loss: 0.017570  [    0/ 5000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 0.017508  [  384/ 5000]\n",
            "loss: 0.017203  [  768/ 5000]\n",
            "loss: 0.017683  [ 1152/ 5000]\n",
            "loss: 0.017355  [ 1536/ 5000]\n",
            "loss: 0.017640  [ 1920/ 5000]\n",
            "loss: 0.017366  [ 2304/ 5000]\n",
            "loss: 0.017347  [ 2688/ 5000]\n",
            "loss: 0.017757  [ 3072/ 5000]\n",
            "loss: 0.017323  [ 3456/ 5000]\n",
            "loss: 0.017645  [ 3840/ 5000]\n",
            "loss: 0.017492  [ 4224/ 5000]\n",
            "loss: 0.017430  [ 4608/ 5000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 32.5%, Avg loss: 0.033508\n"
          ]
        }
      ]
    }
  ]
}