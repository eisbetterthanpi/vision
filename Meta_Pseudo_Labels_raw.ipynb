{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP6NSc/gpnsO9uscD06S507",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eisbetterthanpi/vision/blob/main/Meta_Pseudo_Labels_raw.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v9XrV9ZACgjm"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Meta Pseudo Labels mar 2021 https://arxiv.org/pdf/2003.10580v4.pdf\n",
        "# https://github.com/kekmodel/MPL-pytorch\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title utils\n",
        "# https://github.com/kekmodel/MPL-pytorch/blob/main/utils.py\n",
        "\n",
        "import logging\n",
        "import os\n",
        "import shutil\n",
        "from collections import OrderedDict\n",
        "\n",
        "import torch\n",
        "from torch import distributed as dist\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "def reduce_tensor(tensor, n):\n",
        "    rt = tensor.clone()\n",
        "    dist.all_reduce(rt, op=dist.ReduceOp.SUM)\n",
        "    rt /= n\n",
        "    return rt\n",
        "\n",
        "\n",
        "def create_loss_fn(args):\n",
        "    # if args.label_smoothing > 0:\n",
        "    #     criterion = SmoothCrossEntropyV2(alpha=args.label_smoothing)\n",
        "    # else:\n",
        "    criterion = nn.CrossEntropyLoss(label_smoothing=args.label_smoothing)\n",
        "    return criterion.to(args.device)\n",
        "\n",
        "\n",
        "def module_load_state_dict(model, state_dict):\n",
        "    try:\n",
        "        new_state_dict = OrderedDict()\n",
        "        for k, v in state_dict.items():\n",
        "            name = k[7:]  # remove `module.`\n",
        "            new_state_dict[name] = v\n",
        "        model.load_state_dict(new_state_dict)\n",
        "    except:\n",
        "        new_state_dict = OrderedDict()\n",
        "        for k, v in state_dict.items():\n",
        "            name = f'module.{k}'  # add `module.`\n",
        "            new_state_dict[name] = v\n",
        "        model.load_state_dict(new_state_dict)\n",
        "\n",
        "\n",
        "def model_load_state_dict(model, state_dict):\n",
        "    try:\n",
        "        model.load_state_dict(state_dict)\n",
        "    except:\n",
        "        module_load_state_dict(model, state_dict)\n",
        "\n",
        "\n",
        "def save_checkpoint(args, state, is_best, finetune=False):\n",
        "    os.makedirs(args.save_path, exist_ok=True)\n",
        "    if finetune:\n",
        "        name = f'{args.name}_finetune'\n",
        "    else:\n",
        "        name = args.name\n",
        "    filename = f'{args.save_path}/{name}_last.pth.tar'\n",
        "    torch.save(state, filename, _use_new_zipfile_serialization=False)\n",
        "    if is_best:\n",
        "        shutil.copyfile(filename, f'{args.save_path}/{args.name}_best.pth.tar')\n",
        "\n",
        "\n",
        "def accuracy(output, target, topk=(1,)):\n",
        "    output = output.to(torch.device('cpu'))\n",
        "    target = target.to(torch.device('cpu'))\n",
        "    maxk = max(topk)\n",
        "    batch_size = target.shape[0]\n",
        "\n",
        "    _, idx = output.sort(dim=1, descending=True)\n",
        "    pred = idx.narrow(1, 0, maxk).t()\n",
        "    correct = pred.eq(target.reshape(1, -1).expand_as(pred))\n",
        "\n",
        "    res = []\n",
        "    for k in topk:\n",
        "        correct_k = correct[:k].reshape(-1).float().sum(dim=0, keepdim=True)\n",
        "        res.append(correct_k.mul_(100.0 / batch_size))\n",
        "    return res\n",
        "\n",
        "\n",
        "class SmoothCrossEntropy(nn.Module):\n",
        "    def __init__(self, alpha=0.1):\n",
        "        super(SmoothCrossEntropy, self).__init__()\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def forward(self, logits, labels):\n",
        "        if self.alpha == 0:\n",
        "            loss = F.cross_entropy(logits, labels)\n",
        "        else:\n",
        "            num_classes = logits.shape[-1]\n",
        "            alpha_div_k = self.alpha / num_classes\n",
        "            target_probs = F.one_hot(labels, num_classes=num_classes).float() * \\\n",
        "                (1. - self.alpha) + alpha_div_k\n",
        "            loss = (-(target_probs * torch.log_softmax(logits, dim=-1)).sum(dim=-1)).mean()\n",
        "        return loss\n",
        "\n",
        "\n",
        "class SmoothCrossEntropyV2(nn.Module):\n",
        "    \"\"\"\n",
        "    NLL loss with label smoothing.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, label_smoothing=0.1):\n",
        "        \"\"\"\n",
        "        Constructor for the LabelSmoothing module.\n",
        "        :param smoothing: label smoothing factor\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        assert label_smoothing < 1.0\n",
        "        self.smoothing = label_smoothing\n",
        "        self.confidence = 1. - label_smoothing\n",
        "\n",
        "    def forward(self, x, target):\n",
        "        if self.smoothing == 0:\n",
        "            loss = F.cross_entropy(x, target)\n",
        "        else:\n",
        "            logprobs = F.log_softmax(x, dim=-1)\n",
        "            nll_loss = -logprobs.gather(dim=-1, index=target.unsqueeze(1))\n",
        "            nll_loss = nll_loss.squeeze(1)\n",
        "            smooth_loss = -logprobs.mean(dim=-1)\n",
        "            loss = (self.confidence * nll_loss + self.smoothing * smooth_loss).mean()\n",
        "        return loss\n",
        "\n",
        "\n",
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\n",
        "       Imported from https://github.com/pytorch/examples/blob/master/imagenet/main.py#L247-L262\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n"
      ],
      "metadata": {
        "id": "q_bJYwwSDNNy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title main\n",
        "# https://github.com/kekmodel/MPL-pytorch/blob/main/main.py\n",
        "\n",
        "import argparse\n",
        "import logging\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.cuda import amp\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch import optim\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "# import wandb\n",
        "from tqdm import tqdm\n",
        "\n",
        "from data import DATASET_GETTERS\n",
        "from models import WideResNet, ModelEMA\n",
        "from utils import AverageMeter, accuracy, create_loss_fn, save_checkpoint, reduce_tensor, model_load_state_dict\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--name', type=str, required=True, help='experiment name')\n",
        "parser.add_argument('--data-path', default='./data', type=str, help='data path')\n",
        "parser.add_argument('--save-path', default='./checkpoint', type=str, help='save path')\n",
        "parser.add_argument('--dataset', default='cifar10', type=str,\n",
        "                    choices=['cifar10', 'cifar100'], help='dataset name')\n",
        "parser.add_argument('--num-labeled', type=int, default=4000, help='number of labeled data')\n",
        "parser.add_argument(\"--expand-labels\", action=\"store_true\", help=\"expand labels to fit eval steps\")\n",
        "parser.add_argument('--total-steps', default=300000, type=int, help='number of total steps to run')\n",
        "parser.add_argument('--eval-step', default=1000, type=int, help='number of eval steps to run')\n",
        "parser.add_argument('--start-step', default=0, type=int,\n",
        "                    help='manual epoch number (useful on restarts)')\n",
        "parser.add_argument('--workers', default=4, type=int, help='number of workers')\n",
        "parser.add_argument('--num-classes', default=10, type=int, help='number of classes')\n",
        "parser.add_argument('--resize', default=32, type=int, help='resize image')\n",
        "parser.add_argument('--batch-size', default=64, type=int, help='train batch size')\n",
        "parser.add_argument('--teacher-dropout', default=0, type=float, help='dropout on last dense layer')\n",
        "parser.add_argument('--student-dropout', default=0, type=float, help='dropout on last dense layer')\n",
        "parser.add_argument('--teacher_lr', default=0.01, type=float, help='train learning late')\n",
        "parser.add_argument('--student_lr', default=0.01, type=float, help='train learning late')\n",
        "parser.add_argument('--momentum', default=0.9, type=float, help='SGD Momentum')\n",
        "parser.add_argument('--nesterov', action='store_true', help='use nesterov')\n",
        "parser.add_argument('--weight-decay', default=0, type=float, help='train weight decay')\n",
        "parser.add_argument('--ema', default=0, type=float, help='EMA decay rate')\n",
        "parser.add_argument('--warmup-steps', default=0, type=int, help='warmup steps')\n",
        "parser.add_argument('--student-wait-steps', default=0, type=int, help='warmup steps')\n",
        "parser.add_argument('--grad-clip', default=1e9, type=float, help='gradient norm clipping')\n",
        "parser.add_argument('--resume', default='', type=str, help='path to checkpoint')\n",
        "parser.add_argument('--evaluate', action='store_true', help='only evaluate model on validation set')\n",
        "parser.add_argument('--finetune', action='store_true',\n",
        "                    help='only finetune model on labeled dataset')\n",
        "parser.add_argument('--finetune-epochs', default=625, type=int, help='finetune epochs')\n",
        "parser.add_argument('--finetune-batch-size', default=512, type=int, help='finetune batch size')\n",
        "parser.add_argument('--finetune-lr', default=3e-5, type=float, help='finetune learning late')\n",
        "parser.add_argument('--finetune-weight-decay', default=0, type=float, help='finetune weight decay')\n",
        "parser.add_argument('--finetune-momentum', default=0.9, type=float, help='finetune SGD Momentum')\n",
        "parser.add_argument('--seed', default=None, type=int, help='seed for initializing training')\n",
        "parser.add_argument('--label-smoothing', default=0, type=float, help='label smoothing alpha')\n",
        "parser.add_argument('--mu', default=7, type=int, help='coefficient of unlabeled batch size')\n",
        "parser.add_argument('--threshold', default=0.95, type=float, help='pseudo label threshold')\n",
        "parser.add_argument('--temperature', default=1, type=float, help='pseudo label temperature')\n",
        "parser.add_argument('--lambda-u', default=1, type=float, help='coefficient of unlabeled loss')\n",
        "parser.add_argument('--uda-steps', default=1, type=float, help='warmup steps of lambda-u')\n",
        "parser.add_argument(\"--randaug\", nargs=\"+\", type=int, help=\"use it like this. --randaug 2 10\")\n",
        "parser.add_argument(\"--amp\", action=\"store_true\", help=\"use 16-bit (mixed) precision\")\n",
        "parser.add_argument('--world-size', default=-1, type=int,\n",
        "                    help='number of nodes for distributed training')\n",
        "parser.add_argument(\"--local_rank\", type=int, default=-1,\n",
        "                    help=\"For distributed training: local_rank\")\n",
        "\n",
        "\n",
        "def set_seed(args):\n",
        "    random.seed(args.seed)\n",
        "    np.random.seed(args.seed)\n",
        "    torch.manual_seed(args.seed)\n",
        "    torch.cuda.manual_seed_all(args.seed)\n",
        "\n",
        "\n",
        "def get_cosine_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps, num_wait_steps=0, num_cycles=0.5, last_epoch=-1):\n",
        "    def lr_lambda(current_step):\n",
        "        if current_step < num_wait_steps:\n",
        "            return 0.0\n",
        "        if current_step < num_warmup_steps + num_wait_steps:\n",
        "            return float(current_step) / float(max(1, num_warmup_steps + num_wait_steps))\n",
        "        progress = float(current_step - num_warmup_steps - num_wait_steps) / \\\n",
        "            float(max(1, num_training_steps - num_warmup_steps - num_wait_steps))\n",
        "        return max(0.0, 0.5 * (1.0 + math.cos(math.pi * float(num_cycles) * 2.0 * progress)))\n",
        "    return LambdaLR(optimizer, lr_lambda, last_epoch)\n",
        "\n",
        "def get_lr(optimizer):\n",
        "    return optimizer.param_groups[0]['lr']\n",
        "\n",
        "\n",
        "def train_loop(args, labeled_loader, unlabeled_loader, test_loader, finetune_dataset,\n",
        "               teacher_model, student_model, avg_student_model, criterion,\n",
        "               t_optimizer, s_optimizer, t_scheduler, s_scheduler, t_scaler, s_scaler):\n",
        "    logger.info(\"***** Running Training *****\")\n",
        "    logger.info(f\"   Task = {args.dataset}@{args.num_labeled}\")\n",
        "    logger.info(f\"   Total steps = {args.total_steps}\")\n",
        "\n",
        "    if args.world_size > 1:\n",
        "        labeled_epoch = 0\n",
        "        unlabeled_epoch = 0\n",
        "        labeled_loader.sampler.set_epoch(labeled_epoch)\n",
        "        unlabeled_loader.sampler.set_epoch(unlabeled_epoch)\n",
        "\n",
        "    labeled_iter = iter(labeled_loader)\n",
        "    unlabeled_iter = iter(unlabeled_loader)\n",
        "\n",
        "    # for author's code formula\n",
        "    # moving_dot_product = torch.empty(1).to(args.device)\n",
        "    # limit = 3.0**(0.5)  # 3 = 6 / (f_in + f_out)\n",
        "    # nn.init.uniform_(moving_dot_product, -limit, limit)\n",
        "\n",
        "    for step in range(args.start_step, args.total_steps):\n",
        "        if step % args.eval_step == 0:\n",
        "            pbar = tqdm(range(args.eval_step), disable=args.local_rank not in [-1, 0])\n",
        "            batch_time = AverageMeter()\n",
        "            data_time = AverageMeter()\n",
        "            s_losses = AverageMeter()\n",
        "            t_losses = AverageMeter()\n",
        "            t_losses_l = AverageMeter()\n",
        "            t_losses_u = AverageMeter()\n",
        "            t_losses_mpl = AverageMeter()\n",
        "            mean_mask = AverageMeter()\n",
        "\n",
        "        teacher_model.train()\n",
        "        student_model.train()\n",
        "        end = time.time()\n",
        "\n",
        "        try:\n",
        "            # error occurs ↓\n",
        "            # images_l, targets = labeled_iter.next()\n",
        "            images_l, targets = next(labeled_iter)\n",
        "        except:\n",
        "            if args.world_size > 1:\n",
        "                labeled_epoch += 1\n",
        "                labeled_loader.sampler.set_epoch(labeled_epoch)\n",
        "            labeled_iter = iter(labeled_loader)\n",
        "            # error occurs ↓\n",
        "            # images_l, targets = labeled_iter.next()\n",
        "            images_l, targets = next(labeled_iter)\n",
        "\n",
        "        try:\n",
        "            # error occurs ↓\n",
        "            # (images_uw, images_us), _ = unlabeled_iter.next()\n",
        "            (images_uw, images_us), _ = next(unlabeled_iter)\n",
        "        except:\n",
        "            if args.world_size > 1:\n",
        "                unlabeled_epoch += 1\n",
        "                unlabeled_loader.sampler.set_epoch(unlabeled_epoch)\n",
        "            unlabeled_iter = iter(unlabeled_loader)\n",
        "            # error occurs ↓\n",
        "            # (images_uw, images_us), _ = unlabeled_iter.next()\n",
        "            (images_uw, images_us), _ = next(unlabeled_iter)\n",
        "\n",
        "        data_time.update(time.time() - end)\n",
        "\n",
        "        images_l = images_l.to(args.device)\n",
        "        images_uw = images_uw.to(args.device)\n",
        "        images_us = images_us.to(args.device)\n",
        "        targets = targets.to(args.device)\n",
        "        with amp.autocast(enabled=args.amp):\n",
        "            batch_size = images_l.shape[0]\n",
        "            t_images = torch.cat((images_l, images_uw, images_us))\n",
        "            t_logits = teacher_model(t_images)\n",
        "            t_logits_l = t_logits[:batch_size]\n",
        "            t_logits_uw, t_logits_us = t_logits[batch_size:].chunk(2)\n",
        "            del t_logits\n",
        "\n",
        "            t_loss_l = criterion(t_logits_l, targets)\n",
        "\n",
        "            soft_pseudo_label = torch.softmax(t_logits_uw.detach() / args.temperature, dim=-1)\n",
        "            max_probs, hard_pseudo_label = torch.max(soft_pseudo_label, dim=-1)\n",
        "            mask = max_probs.ge(args.threshold).float()\n",
        "            t_loss_u = torch.mean(-(soft_pseudo_label * torch.log_softmax(t_logits_us, dim=-1)).sum(dim=-1) * mask)\n",
        "            weight_u = args.lambda_u * min(1., (step + 1) / args.uda_steps)\n",
        "            t_loss_uda = t_loss_l + weight_u * t_loss_u\n",
        "\n",
        "            s_images = torch.cat((images_l, images_us))\n",
        "            s_logits = student_model(s_images)\n",
        "            s_logits_l = s_logits[:batch_size]\n",
        "            s_logits_us = s_logits[batch_size:]\n",
        "            del s_logits\n",
        "\n",
        "            s_loss_l_old = F.cross_entropy(s_logits_l.detach(), targets)\n",
        "            s_loss = criterion(s_logits_us, hard_pseudo_label)\n",
        "\n",
        "        s_scaler.scale(s_loss).backward()\n",
        "        if args.grad_clip > 0:\n",
        "            s_scaler.unscale_(s_optimizer)\n",
        "            nn.utils.clip_grad_norm_(student_model.parameters(), args.grad_clip)\n",
        "        s_scaler.step(s_optimizer)\n",
        "        s_scaler.update()\n",
        "        s_scheduler.step()\n",
        "        if args.ema > 0:\n",
        "            avg_student_model.update_parameters(student_model)\n",
        "\n",
        "        with amp.autocast(enabled=args.amp):\n",
        "            with torch.no_grad():\n",
        "                s_logits_l = student_model(images_l)\n",
        "            s_loss_l_new = F.cross_entropy(s_logits_l.detach(), targets)\n",
        "\n",
        "            # theoretically correct formula (https://github.com/kekmodel/MPL-pytorch/issues/6)\n",
        "            # dot_product = s_loss_l_old - s_loss_l_new\n",
        "\n",
        "            # author's code formula\n",
        "            dot_product = s_loss_l_new - s_loss_l_old\n",
        "            # moving_dot_product = moving_dot_product * 0.99 + dot_product * 0.01\n",
        "            # dot_product = dot_product - moving_dot_product\n",
        "\n",
        "            _, hard_pseudo_label = torch.max(t_logits_us.detach(), dim=-1)\n",
        "            t_loss_mpl = dot_product * F.cross_entropy(t_logits_us, hard_pseudo_label)\n",
        "            # test\n",
        "            # t_loss_mpl = torch.tensor(0.).to(args.device)\n",
        "            t_loss = t_loss_uda + t_loss_mpl\n",
        "\n",
        "        t_scaler.scale(t_loss).backward()\n",
        "        if args.grad_clip > 0:\n",
        "            t_scaler.unscale_(t_optimizer)\n",
        "            nn.utils.clip_grad_norm_(teacher_model.parameters(), args.grad_clip)\n",
        "        t_scaler.step(t_optimizer)\n",
        "        t_scaler.update()\n",
        "        t_scheduler.step()\n",
        "\n",
        "        teacher_model.zero_grad()\n",
        "        student_model.zero_grad()\n",
        "\n",
        "        if args.world_size > 1:\n",
        "            s_loss = reduce_tensor(s_loss.detach(), args.world_size)\n",
        "            t_loss = reduce_tensor(t_loss.detach(), args.world_size)\n",
        "            t_loss_l = reduce_tensor(t_loss_l.detach(), args.world_size)\n",
        "            t_loss_u = reduce_tensor(t_loss_u.detach(), args.world_size)\n",
        "            t_loss_mpl = reduce_tensor(t_loss_mpl.detach(), args.world_size)\n",
        "            mask = reduce_tensor(mask, args.world_size)\n",
        "\n",
        "        s_losses.update(s_loss.item())\n",
        "        t_losses.update(t_loss.item())\n",
        "        t_losses_l.update(t_loss_l.item())\n",
        "        t_losses_u.update(t_loss_u.item())\n",
        "        t_losses_mpl.update(t_loss_mpl.item())\n",
        "        mean_mask.update(mask.mean().item())\n",
        "\n",
        "        batch_time.update(time.time() - end)\n",
        "        pbar.set_description(\n",
        "            f\"Train Iter: {step+1:3}/{args.total_steps:3}. \"\n",
        "            f\"LR: {get_lr(s_optimizer):.4f}. Data: {data_time.avg:.2f}s. \"\n",
        "            f\"Batch: {batch_time.avg:.2f}s. S_Loss: {s_losses.avg:.4f}. \"\n",
        "            f\"T_Loss: {t_losses.avg:.4f}. Mask: {mean_mask.avg:.4f}. \")\n",
        "        pbar.update()\n",
        "        if args.local_rank in [-1, 0]:\n",
        "            args.writer.add_scalar(\"lr\", get_lr(s_optimizer), step)\n",
        "#             wandb.log({\"lr\": get_lr(s_optimizer)})\n",
        "\n",
        "        args.num_eval = step // args.eval_step\n",
        "        if (step + 1) % args.eval_step == 0:\n",
        "            pbar.close()\n",
        "            if args.local_rank in [-1, 0]:\n",
        "                args.writer.add_scalar(\"train/1.s_loss\", s_losses.avg, args.num_eval)\n",
        "                args.writer.add_scalar(\"train/2.t_loss\", t_losses.avg, args.num_eval)\n",
        "                args.writer.add_scalar(\"train/3.t_labeled\", t_losses_l.avg, args.num_eval)\n",
        "                args.writer.add_scalar(\"train/4.t_unlabeled\", t_losses_u.avg, args.num_eval)\n",
        "                args.writer.add_scalar(\"train/5.t_mpl\", t_losses_mpl.avg, args.num_eval)\n",
        "                args.writer.add_scalar(\"train/6.mask\", mean_mask.avg, args.num_eval)\n",
        "#                 wandb.log({\"train/1.s_loss\": s_losses.avg,\n",
        "#                            \"train/2.t_loss\": t_losses.avg,\n",
        "#                            \"train/3.t_labeled\": t_losses_l.avg,\n",
        "#                            \"train/4.t_unlabeled\": t_losses_u.avg,\n",
        "#                            \"train/5.t_mpl\": t_losses_mpl.avg,\n",
        "#                            \"train/6.mask\": mean_mask.avg})\n",
        "\n",
        "                test_model = avg_student_model if avg_student_model is not None else student_model\n",
        "                test_loss, top1, top5 = evaluate(args, test_loader, test_model, criterion)\n",
        "\n",
        "                args.writer.add_scalar(\"test/loss\", test_loss, args.num_eval)\n",
        "                args.writer.add_scalar(\"test/acc@1\", top1, args.num_eval)\n",
        "                args.writer.add_scalar(\"test/acc@5\", top5, args.num_eval)\n",
        "#                 wandb.log({\"test/loss\": test_loss,\n",
        "#                            \"test/acc@1\": top1,\n",
        "#                            \"test/acc@5\": top5})\n",
        "\n",
        "                is_best = top1 > args.best_top1\n",
        "                if is_best:\n",
        "                    args.best_top1 = top1\n",
        "                    args.best_top5 = top5\n",
        "\n",
        "                logger.info(f\"top-1 acc: {top1:.2f}\")\n",
        "                logger.info(f\"Best top-1 acc: {args.best_top1:.2f}\")\n",
        "\n",
        "                save_checkpoint(args, {\n",
        "                    'step': step + 1,\n",
        "                    'teacher_state_dict': teacher_model.state_dict(),\n",
        "                    'student_state_dict': student_model.state_dict(),\n",
        "                    'avg_state_dict': avg_student_model.state_dict() if avg_student_model is not None else None,\n",
        "                    'best_top1': args.best_top1,\n",
        "                    'best_top5': args.best_top5,\n",
        "                    'teacher_optimizer': t_optimizer.state_dict(),\n",
        "                    'student_optimizer': s_optimizer.state_dict(),\n",
        "                    'teacher_scheduler': t_scheduler.state_dict(),\n",
        "                    'student_scheduler': s_scheduler.state_dict(),\n",
        "                    'teacher_scaler': t_scaler.state_dict(),\n",
        "                    'student_scaler': s_scaler.state_dict(),\n",
        "                }, is_best)\n",
        "\n",
        "    if args.local_rank in [-1, 0]:\n",
        "        args.writer.add_scalar(\"result/test_acc@1\", args.best_top1)\n",
        "#         wandb.log({\"result/test_acc@1\": args.best_top1})\n",
        "\n",
        "    # finetune\n",
        "    del t_scaler, t_scheduler, t_optimizer, teacher_model, labeled_loader, unlabeled_loader\n",
        "    del s_scaler, s_scheduler, s_optimizer\n",
        "    ckpt_name = f'{args.save_path}/{args.name}_best.pth.tar'\n",
        "    loc = f'cuda:{args.gpu}'\n",
        "    checkpoint = torch.load(ckpt_name, map_location=loc)\n",
        "    logger.info(f\"=> loading checkpoint '{ckpt_name}'\")\n",
        "    if checkpoint['avg_state_dict'] is not None:\n",
        "        model_load_state_dict(student_model, checkpoint['avg_state_dict'])\n",
        "    else:\n",
        "        model_load_state_dict(student_model, checkpoint['student_state_dict'])\n",
        "    finetune(args, finetune_dataset, test_loader, student_model, criterion)\n",
        "    return\n",
        "\n",
        "\n",
        "def evaluate(args, test_loader, model, criterion):\n",
        "    batch_time = AverageMeter()\n",
        "    data_time = AverageMeter()\n",
        "    losses = AverageMeter()\n",
        "    top1 = AverageMeter()\n",
        "    top5 = AverageMeter()\n",
        "    model.eval()\n",
        "    test_iter = tqdm(test_loader, disable=args.local_rank not in [-1, 0])\n",
        "    with torch.no_grad():\n",
        "        end = time.time()\n",
        "        for step, (images, targets) in enumerate(test_iter):\n",
        "            data_time.update(time.time() - end)\n",
        "            batch_size = images.shape[0]\n",
        "            images = images.to(args.device)\n",
        "            targets = targets.to(args.device)\n",
        "            with amp.autocast(enabled=args.amp):\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, targets)\n",
        "\n",
        "            acc1, acc5 = accuracy(outputs, targets, (1, 5))\n",
        "            losses.update(loss.item(), batch_size)\n",
        "            top1.update(acc1[0], batch_size)\n",
        "            top5.update(acc5[0], batch_size)\n",
        "            batch_time.update(time.time() - end)\n",
        "            end = time.time()\n",
        "            test_iter.set_description(\n",
        "                f\"Test Iter: {step+1:3}/{len(test_loader):3}. Data: {data_time.avg:.2f}s. \"\n",
        "                f\"Batch: {batch_time.avg:.2f}s. Loss: {losses.avg:.4f}. \"\n",
        "                f\"top1: {top1.avg:.2f}. top5: {top5.avg:.2f}. \")\n",
        "\n",
        "        test_iter.close()\n",
        "        return losses.avg, top1.avg, top5.avg\n",
        "\n",
        "\n",
        "def finetune(args, finetune_dataset, test_loader, model, criterion):\n",
        "    model.drop = nn.Identity()\n",
        "    train_sampler = RandomSampler if args.local_rank == -1 else DistributedSampler\n",
        "    labeled_loader = DataLoader(\n",
        "        finetune_dataset,\n",
        "        batch_size=args.finetune_batch_size,\n",
        "        num_workers=args.workers,\n",
        "        pin_memory=True)\n",
        "    optimizer = optim.SGD(model.parameters(), lr=args.finetune_lr, momentum=args.finetune_momentum, weight_decay=args.finetune_weight_decay, nesterov=True)\n",
        "    scaler = amp.GradScaler(enabled=args.amp)\n",
        "\n",
        "    logger.info(\"***** Running Finetuning *****\")\n",
        "    logger.info(f\"   Finetuning steps = {len(labeled_loader)*args.finetune_epochs}\")\n",
        "\n",
        "    for epoch in range(args.finetune_epochs):\n",
        "        if args.world_size > 1:\n",
        "            labeled_loader.sampler.set_epoch(epoch + 624)\n",
        "\n",
        "        batch_time = AverageMeter()\n",
        "        data_time = AverageMeter()\n",
        "        losses = AverageMeter()\n",
        "        model.train()\n",
        "        end = time.time()\n",
        "        labeled_iter = tqdm(labeled_loader, disable=args.local_rank not in [-1, 0])\n",
        "        for step, (images, targets) in enumerate(labeled_iter):\n",
        "            data_time.update(time.time() - end)\n",
        "            batch_size = images.shape[0]\n",
        "            images = images.to(args.device)\n",
        "            targets = targets.to(args.device)\n",
        "            with amp.autocast(enabled=args.amp):\n",
        "                model.zero_grad()\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, targets)\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            if args.world_size > 1:\n",
        "                loss = reduce_tensor(loss.detach(), args.world_size)\n",
        "            losses.update(loss.item(), batch_size)\n",
        "            batch_time.update(time.time() - end)\n",
        "            labeled_iter.set_description(\n",
        "                f\"Finetune Epoch: {epoch+1:2}/{args.finetune_epochs:2}. Data: {data_time.avg:.2f}s. \"\n",
        "                f\"Batch: {batch_time.avg:.2f}s. Loss: {losses.avg:.4f}. \")\n",
        "        labeled_iter.close()\n",
        "        if args.local_rank in [-1, 0]:\n",
        "            args.writer.add_scalar(\"finetune/train_loss\", losses.avg, epoch)\n",
        "            test_loss, top1, top5 = evaluate(args, test_loader, model, criterion)\n",
        "            args.writer.add_scalar(\"finetune/test_loss\", test_loss, epoch)\n",
        "            args.writer.add_scalar(\"finetune/acc@1\", top1, epoch)\n",
        "            args.writer.add_scalar(\"finetune/acc@5\", top5, epoch)\n",
        "#             wandb.log({\"finetune/train_loss\": losses.avg,\n",
        "#                        \"finetune/test_loss\": test_loss,\n",
        "#                        \"finetune/acc@1\": top1,\n",
        "#                        \"finetune/acc@5\": top5})\n",
        "\n",
        "            is_best = top1 > args.best_top1\n",
        "            if is_best:\n",
        "                args.best_top1 = top1\n",
        "                args.best_top5 = top5\n",
        "\n",
        "            logger.info(f\"top-1 acc: {top1:.2f}\")\n",
        "            logger.info(f\"Best top-1 acc: {args.best_top1:.2f}\")\n",
        "\n",
        "            save_checkpoint(args, {\n",
        "                'step': step + 1,\n",
        "                'best_top1': args.best_top1,\n",
        "                'best_top5': args.best_top5,\n",
        "                'student_state_dict': model.state_dict(),\n",
        "                'avg_state_dict': None,\n",
        "                'student_optimizer': optimizer.state_dict(),\n",
        "            }, is_best, finetune=True)\n",
        "        if args.local_rank in [-1, 0]:\n",
        "            args.writer.add_scalar(\"result/finetune_acc@1\", args.best_top1)\n",
        "#             wandb.log({\"result/finetune_acc@1\": args.best_top1})\n",
        "    return\n",
        "\n",
        "\n",
        "def main():\n",
        "    args = parser.parse_args()\n",
        "    args.best_top1 = 0.\n",
        "    args.best_top5 = 0.\n",
        "\n",
        "    if args.local_rank != -1:\n",
        "        args.gpu = args.local_rank\n",
        "        torch.distributed.init_process_group(backend='nccl')\n",
        "        args.world_size = torch.distributed.get_world_size()\n",
        "    else:\n",
        "        args.gpu = 0\n",
        "        args.world_size = 1\n",
        "\n",
        "    args.device = torch.device('cuda', args.gpu)\n",
        "\n",
        "    logging.basicConfig(\n",
        "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
        "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
        "        level=logging.INFO if args.local_rank in [-1, 0] else logging.WARNING)\n",
        "\n",
        "    logger.warning(\n",
        "        f\"Process rank: {args.local_rank}, \"\n",
        "        f\"device: {args.device}, \"\n",
        "        f\"distributed training: {bool(args.local_rank != -1)}, \"\n",
        "        f\"16-bits training: {args.amp}\")\n",
        "\n",
        "    logger.info(dict(args._get_kwargs()))\n",
        "\n",
        "    if args.local_rank in [-1, 0]:\n",
        "        args.writer = SummaryWriter(f\"results/{args.name}\")\n",
        "#         wandb.init(name=args.name, project='MPL', config=args)\n",
        "\n",
        "    if args.seed is not None:\n",
        "        set_seed(args)\n",
        "\n",
        "    if args.local_rank not in [-1, 0]:\n",
        "        torch.distributed.barrier()\n",
        "\n",
        "    labeled_dataset, unlabeled_dataset, test_dataset, finetune_dataset = DATASET_GETTERS[args.dataset](args)\n",
        "\n",
        "    if args.local_rank == 0:\n",
        "        torch.distributed.barrier()\n",
        "\n",
        "    train_sampler = RandomSampler if args.local_rank == -1 else DistributedSampler\n",
        "    labeled_loader = DataLoader(labeled_dataset, sampler=train_sampler(labeled_dataset), batch_size=args.batch_size, num_workers=args.workers, drop_last=True)\n",
        "    unlabeled_loader = DataLoader(unlabeled_dataset, sampler=train_sampler(unlabeled_dataset), batch_size=args.batch_size * args.mu, num_workers=args.workers, drop_last=True)\n",
        "\n",
        "    test_loader = DataLoader(test_dataset,\n",
        "                             sampler=SequentialSampler(test_dataset),\n",
        "                             batch_size=args.batch_size,\n",
        "                             num_workers=args.workers)\n",
        "\n",
        "    if args.dataset == \"cifar10\":\n",
        "        depth, widen_factor = 28, 2\n",
        "    elif args.dataset == 'cifar100':\n",
        "        depth, widen_factor = 28, 8\n",
        "\n",
        "    if args.local_rank not in [-1, 0]:\n",
        "        torch.distributed.barrier()\n",
        "\n",
        "    teacher_model = WideResNet(num_classes=args.num_classes,\n",
        "                               depth=depth,\n",
        "                               widen_factor=widen_factor,\n",
        "                               dropout=0,\n",
        "                               dense_dropout=args.teacher_dropout)\n",
        "    student_model = WideResNet(num_classes=args.num_classes,\n",
        "                               depth=depth,\n",
        "                               widen_factor=widen_factor,\n",
        "                               dropout=0,\n",
        "                               dense_dropout=args.student_dropout)\n",
        "\n",
        "    if args.local_rank == 0:\n",
        "        torch.distributed.barrier()\n",
        "\n",
        "    logger.info(f\"Model: WideResNet {depth}x{widen_factor}\")\n",
        "    logger.info(f\"Params: {sum(p.numel() for p in teacher_model.parameters())/1e6:.2f}M\")\n",
        "\n",
        "    teacher_model.to(args.device)\n",
        "    student_model.to(args.device)\n",
        "    avg_student_model = None\n",
        "    if args.ema > 0:\n",
        "        avg_student_model = ModelEMA(student_model, args.ema)\n",
        "\n",
        "    criterion = create_loss_fn(args)\n",
        "\n",
        "    no_decay = ['bn']\n",
        "    teacher_parameters = [\n",
        "        {'params': [p for n, p in teacher_model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': args.weight_decay},\n",
        "        {'params': [p for n, p in teacher_model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "    ]\n",
        "    student_parameters = [\n",
        "        {'params': [p for n, p in student_model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': args.weight_decay},\n",
        "        {'params': [p for n, p in student_model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "    ]\n",
        "\n",
        "    t_optimizer = optim.SGD(teacher_parameters, lr=args.teacher_lr, momentum=args.momentum, nesterov=args.nesterov)\n",
        "    s_optimizer = optim.SGD(student_parameters, lr=args.student_lr, momentum=args.momentum, nesterov=args.nesterov)\n",
        "\n",
        "    t_scheduler = get_cosine_schedule_with_warmup(t_optimizer, args.warmup_steps, args.total_steps)\n",
        "    s_scheduler = get_cosine_schedule_with_warmup(s_optimizer, args.warmup_steps, args.total_steps, args.student_wait_steps)\n",
        "\n",
        "    t_scaler = amp.GradScaler(enabled=args.amp)\n",
        "    s_scaler = amp.GradScaler(enabled=args.amp)\n",
        "\n",
        "    # optionally resume from a checkpoint\n",
        "    if args.resume:\n",
        "        if os.path.isfile(args.resume):\n",
        "            logger.info(f\"=> loading checkpoint '{args.resume}'\")\n",
        "            loc = f'cuda:{args.gpu}'\n",
        "            checkpoint = torch.load(args.resume, map_location=loc)\n",
        "            args.best_top1 = checkpoint['best_top1'].to(torch.device('cpu'))\n",
        "            args.best_top5 = checkpoint['best_top5'].to(torch.device('cpu'))\n",
        "            if not (args.evaluate or args.finetune):\n",
        "                args.start_step = checkpoint['step']\n",
        "                t_optimizer.load_state_dict(checkpoint['teacher_optimizer'])\n",
        "                s_optimizer.load_state_dict(checkpoint['student_optimizer'])\n",
        "                t_scheduler.load_state_dict(checkpoint['teacher_scheduler'])\n",
        "                s_scheduler.load_state_dict(checkpoint['student_scheduler'])\n",
        "                t_scaler.load_state_dict(checkpoint['teacher_scaler'])\n",
        "                s_scaler.load_state_dict(checkpoint['student_scaler'])\n",
        "                model_load_state_dict(teacher_model, checkpoint['teacher_state_dict'])\n",
        "                if avg_student_model is not None:\n",
        "                    model_load_state_dict(avg_student_model, checkpoint['avg_state_dict'])\n",
        "\n",
        "            else:\n",
        "                if checkpoint['avg_state_dict'] is not None:\n",
        "                    model_load_state_dict(student_model, checkpoint['avg_state_dict'])\n",
        "                else:\n",
        "                    model_load_state_dict(student_model, checkpoint['student_state_dict'])\n",
        "\n",
        "            logger.info(f\"=> loaded checkpoint '{args.resume}' (step {checkpoint['step']})\")\n",
        "        else:\n",
        "            logger.info(f\"=> no checkpoint found at '{args.resume}'\")\n",
        "\n",
        "    if args.local_rank != -1:\n",
        "        teacher_model = nn.parallel.DistributedDataParallel(\n",
        "            teacher_model, device_ids=[args.local_rank],\n",
        "            output_device=args.local_rank, find_unused_parameters=True)\n",
        "        student_model = nn.parallel.DistributedDataParallel(\n",
        "            student_model, device_ids=[args.local_rank],\n",
        "            output_device=args.local_rank, find_unused_parameters=True)\n",
        "\n",
        "    if args.finetune:\n",
        "        del t_scaler, t_scheduler, t_optimizer, teacher_model, unlabeled_loader\n",
        "        del s_scaler, s_scheduler, s_optimizer\n",
        "        finetune(args, finetune_dataset, test_loader, student_model, criterion)\n",
        "        return\n",
        "\n",
        "    if args.evaluate:\n",
        "        del t_scaler, t_scheduler, t_optimizer, teacher_model, unlabeled_loader, labeled_loader\n",
        "        del s_scaler, s_scheduler, s_optimizer\n",
        "        evaluate(args, test_loader, student_model, criterion)\n",
        "        return\n",
        "\n",
        "    teacher_model.zero_grad()\n",
        "    student_model.zero_grad()\n",
        "    train_loop(args, labeled_loader, unlabeled_loader, test_loader, finetune_dataset,\n",
        "               teacher_model, student_model, avg_student_model, criterion,\n",
        "               t_optimizer, s_optimizer, t_scheduler, s_scheduler, t_scaler, s_scaler)\n",
        "    return\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n",
        "\n"
      ],
      "metadata": {
        "id": "LZVx1qyfCwGa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "python main.py \\\n",
        "    --seed 2 \\\n",
        "    --name cifar10-4K.2 \\\n",
        "    --expand-labels \\\n",
        "    --dataset cifar10 \\\n",
        "    --num-classes 10 \\\n",
        "    --num-labeled 4000 \\\n",
        "    --total-steps 300000 \\\n",
        "    --eval-step 1000 \\\n",
        "    --randaug 2 16 \\\n",
        "    --batch-size 128 \\\n",
        "    --teacher_lr 0.05 \\\n",
        "    --student_lr 0.05 \\\n",
        "    --weight-decay 5e-4 \\\n",
        "    --ema 0.995 \\\n",
        "    --nesterov \\\n",
        "    --mu 7 \\\n",
        "    --label-smoothing 0.15 \\\n",
        "    --temperature 0.7 \\\n",
        "    --threshold 0.6 \\\n",
        "    --lambda-u 8 \\\n",
        "    --warmup-steps 5000 \\\n",
        "    --uda-steps 5000 \\\n",
        "    --student-wait-steps 3000 \\\n",
        "    --teacher-dropout 0.2 \\\n",
        "    --student-dropout 0.2 \\\n",
        "    --finetune-epochs 625 \\\n",
        "    --finetune-batch-size 512 \\\n",
        "    --finetune-lr 3e-5 \\\n",
        "    --finetune-weight-decay 0 \\\n",
        "    --finetune-momentum 0.9 \\\n",
        "    --amp\n"
      ],
      "metadata": {
        "id": "94MPIEHyCvPL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}