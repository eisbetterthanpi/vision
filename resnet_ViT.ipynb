{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eisbetterthanpi/vision/blob/main/resnet_ViT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "2kQGpnynh2oQ"
      },
      "outputs": [],
      "source": [
        "# @title download\n",
        "# original 10k\n",
        "# # https://drive.google.com/file/d/1NkCNecLpFG3i7bo3Vl9RQSwzBpSRQ29q/view?usp=sharing\n",
        "# !gdown 1NkCNecLpFG3i7bo3Vl9RQSwzBpSRQ29q -O gsv.zip\n",
        "# !unzip /content/gsv.zip -d /content\n",
        "# import shutil\n",
        "# shutil.rmtree('/content/google_street_view/meta_data', ignore_errors=True) # delete the meta_data folder\n",
        "\n",
        "# # clip cleaned\n",
        "# # https://drive.google.com/file/d/1-xcHyVAMeTkY7SUdUyQRVAn_FFXPSDGB/view?usp=share_link\n",
        "# !gdown 1-xcHyVAMeTkY7SUdUyQRVAn_FFXPSDGB -O gsv.zip\n",
        "# !unzip /content/gsv.zip -d /\n",
        "# !rm -R /content/gsv/.ipynb_checkpoints\n",
        "# !rm -R /content/gsv/01/.ipynb_checkpoints\n",
        "# !rm -R /content/gsv/02/.ipynb_checkpoints\n",
        "# !rm -R /content/gsv/03/.ipynb_checkpoints\n",
        "# !rm -R /content/gsv/04/.ipynb_checkpoints\n",
        "# !rm -R /content/gsv/05/.ipynb_checkpoints\n",
        "# !rm -R /content/gsv/06/.ipynb_checkpoints\n",
        "\n",
        "# # gsv 70k\n",
        "# # https://drive.google.com/file/d/1-7ZC29k4VxXQkpnOuLfj7Ag_SFTM4LV8/view?usp=share_link\n",
        "# !gdown 1-7ZC29k4VxXQkpnOuLfj7Ag_SFTM4LV8 -O gsv.zip\n",
        "# !unzip /content/gsv.zip -d /\n",
        "\n",
        "# # # !ls\n",
        "# !ls -a /content/gsv70k\n",
        "# !rm -R /content/gsv70k/.ipynb_checkpoints\n",
        "# # # !rm -R /content/gsv/06/.ipynb_checkpoints\n",
        "\n",
        "# # 70k+gmap\n",
        "# # https://drive.google.com/file/d/1-CZp7TbhJLeRQpbKQCyT8ofGg89Yt137/view?usp=sharing\n",
        "!gdown 1-CZp7TbhJLeRQpbKQCyT8ofGg89Yt137 -O gsv.zip\n",
        "!unzip /content/gsv.zip -d /\n",
        "!rm -R /content/gsv70kg/.ipynb_checkpoints\n",
        "!rm -R /content/gsv70kg/01/.ipynb_checkpoints\n",
        "!rm -R /content/gsv70kg/02/.ipynb_checkpoints\n",
        "!rm -R /content/gsv70kg/03/.ipynb_checkpoints\n",
        "!rm -R /content/gsv70kg/04/.ipynb_checkpoints\n",
        "!rm -R /content/gsv70kg/05/.ipynb_checkpoints\n",
        "!rm -R /content/gsv70kg/06/.ipynb_checkpoints\n",
        "\n",
        "# # https://bestasoff.medium.com/how-to-fine-tune-very-large-model-if-it-doesnt-fit-on-your-gpu-3561e50859af\n",
        "!pip install bitsandbytes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "cellView": "form",
        "id": "hEUffQ24mkRY"
      },
      "outputs": [],
      "source": [
        "# @title torch augment\n",
        "# https://github.com/facebookresearch/vicreg/blob/main/augmentations.py\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "class TrainTransform(object):\n",
        "    def __init__(self):\n",
        "        # self.transform = transforms.RandomApply([transforms.Compose([\n",
        "        self.transform = transforms.Compose([\n",
        "                transforms.RandomPerspective(distortion_scale=0.3, p=0.5), # me\n",
        "                transforms.RandomResizedCrop((400,640), scale=(0.7, 1.0), ratio=(0.8, 1.25), interpolation=transforms.InterpolationMode.BICUBIC),\n",
        "                transforms.RandomHorizontalFlip(p=0.5), # 0.5\n",
        "                transforms.Lambda(lambda x : torch.clamp(x, 0., 1.)), # clamp else ColorJitter will return nan https://discuss.pytorch.org/t/input-is-nan-after-transformation/125455/6\n",
        "                transforms.RandomApply([transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1)], p=0.8,), # brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1)], p=0.8\n",
        "                transforms.RandomGrayscale(p=0.2), # 0.2\n",
        "                # # transforms.RandomChoice(transforms.ColorJitter , transforms.RandomGrayscale(p=1.)\n",
        "                transforms.RandomApply([transforms.GaussianBlur(kernel_size=5, sigma=(0.1, 2.0)),], p=1.0),\n",
        "                # # transforms.RandomSolarize(threshold=130, p=0.5)\n",
        "                transforms.RandomErasing(p=1., scale=(0.1, 0.11), ratio=(1,1), value=(0.485, 0.456, 0.406)),\n",
        "                # transforms.ToTensor(), # ToTensored at dataset level, no need to ToTensor again\n",
        "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), # normalised at dataset level. default 0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225\n",
        "                ])\n",
        "            # ], p=1.)\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        dims = len(sample.shape)\n",
        "        if dims==3: x1 = self.transform(sample) # same transforms per minibatch\n",
        "        elif dims==4: x1 = transforms.Lambda(lambda x: torch.stack([self.transform(x_) for x_ in x]))(sample) # diff transforms per img in minibatch\n",
        "        # x1 = self.transform(sample)\n",
        "        return x1\n",
        "\n",
        "trs=TrainTransform()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yjix6KhUqxY1",
        "outputId": "71db8d44-d8df-433e-a4b7-80c530adb2e1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ],
      "source": [
        "# @title data (old)\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
        "\n",
        "dir='/content/gsv70kg'\n",
        "\n",
        "data = datasets.ImageFolder(dir, transform=transform)\n",
        "# data = datasets.ImageFolder(dir, transform=None)\n",
        "torch.manual_seed(0)\n",
        "train_data, test_data = torch.utils.data.random_split(data, [.9,.1])\n",
        "\n",
        "batch_size = 16 # 64\n",
        "num_batches=int(np.ceil(len(train_data)/batch_size))\n",
        "\n",
        "# train_loader = torch.utils.data.DataLoader(train_data, sampler=train_sampler, batch_size=batch_size, num_workers=4, pin_memory=True)\n",
        "# test_loader = torch.utils.data.DataLoader(test_data, sampler=test_sampler, batch_size=batch_size, num_workers=4, pin_memory=True)\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, num_workers=4, pin_memory=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, num_workers=4, pin_memory=True)\n",
        "del data, train_data, test_data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pqw5n--6WYEG",
        "outputId": "75e37a27-3cb9-4227-a8de-a20ea043940d",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ],
      "source": [
        "# @title data weighted\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# dataset has PILImage images of range [0, 1], transform them to Tensors of normalized range [-1, 1]\n",
        "# transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
        "# transform = transforms.Compose(transforms.ToTensor())\n",
        "\n",
        "# dir='/content/gsv'\n",
        "# dir='/content/gsv70k'\n",
        "dir='/content/gsv70kg'\n",
        "\n",
        "# data = datasets.ImageFolder(dir, transform=transform)\n",
        "data = datasets.ImageFolder(dir, transform=None)\n",
        "torch.manual_seed(0)\n",
        "\n",
        "# split data manually so that can work with weighted random sampler\n",
        "# train_data, test_data = torch.utils.data.random_split(data, [.85,.15])\n",
        "# https://www.scaler.com/topics/pytorch/how-to-split-a-torch-dataset/\n",
        "data_size = len(data)\n",
        "indices = np.arange(data_size)\n",
        "np.random.shuffle(indices)\n",
        "split_index = int(np.floor(0.9 * data_size))\n",
        "# split_index = int(np.floor(0.002 * data_size))\n",
        "train_idx, test_idx = indices[:split_index], indices[split_index:]\n",
        "# train_idx, test_idx = indices[:split_index], indices[split_index:split_index*2]\n",
        "# train_idx, test_idx = indices[:145], indices[10000:10017]\n",
        "train_data = torch.utils.data.Subset(data, train_idx)\n",
        "test_data = torch.utils.data.Subset(data, test_idx)\n",
        "targets = np.array(data.targets)\n",
        "train_targets = targets[train_idx]\n",
        "test_targets = targets[test_idx]\n",
        "\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "class DatasetWrap(Dataset):\n",
        "    def __init__(self, dataset, transform=None):\n",
        "        super(DatasetWrap, self).__init__()\n",
        "        self.dataset = dataset\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        x, y = self.dataset[index]\n",
        "        if self.transform:\n",
        "            x = self.transform(x)\n",
        "        return x, y\n",
        "\n",
        "# dataset wrapper in order to apply transforms to train data only\n",
        "# train_data = DatasetWrap(train_data, TrainTransform()) # apply data augmentation to train dataset only\n",
        "# train_data = DatasetWrap(train_data, transform) # apply transform during training to use gpu\n",
        "train_data = DatasetWrap(train_data, transforms.ToTensor()) # apply transform during training to use gpu\n",
        "test_data = DatasetWrap(test_data, transform)\n",
        "\n",
        "# use batch size 16 for resnet 152/ vit with grad accumulation\n",
        "# can use batch size 64 for inception v3 without grad accumulation?\n",
        "batch_size = 16 # 64/16\n",
        "num_batches=int(np.ceil(len(train_data)/batch_size))\n",
        "\n",
        "# oversampling\n",
        "# https://stackoverflow.com/questions/62319228/number-of-instances-per-class-in-pytorch-dataset\n",
        "from collections import Counter\n",
        "class_count = torch.tensor(list(Counter(data.targets).values()))\n",
        "weights=1./class_count\n",
        "# weights=sum(class_count)/class_count\n",
        "# print(weights)\n",
        "# https://pytorch.org/docs/stable/data.html#torch.utils.data.WeightedRandomSampler\n",
        "\n",
        "train_weight = weights[train_targets]\n",
        "test_weight = weights[test_targets]\n",
        "train_sampler = torch.utils.data.WeightedRandomSampler(train_weight, len(train_weight))\n",
        "test_sampler = torch.utils.data.WeightedRandomSampler(test_weight, len(test_weight))\n",
        "train_loader = torch.utils.data.DataLoader(train_data, sampler=train_sampler, batch_size=batch_size, num_workers=4, pin_memory=True)\n",
        "# train_loader = torch.utils.data.DataLoader(train_data, sampler=train_sampler, batch_size=batch_size, num_workers=4, pin_memory=True, drop_last=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, sampler=test_sampler, batch_size=batch_size, num_workers=4, pin_memory=True)\n",
        "# test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, num_workers=4, pin_memory=True)\n",
        "# test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, num_workers=4, pin_memory=True, drop_last=True)\n",
        "del data, train_data, test_data\n",
        "\n",
        "\n",
        "# test oversampling: occurence of each class should be roughly equal\n",
        "# c=0\n",
        "# print(len(test_loader))\n",
        "# # for batch, (x, y) in enumerate(train_loader):\n",
        "# for batch, (x, y) in enumerate(test_loader):\n",
        "#     print(torch.bincount(y)) # torch count number of elements with value in tensor\n",
        "#     c+=1\n",
        "#     if c>5: break\n",
        "\n",
        "# display img from torch tensor\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "# trainiter = iter(train_loader)\n",
        "# images, labels = next(trainiter)\n",
        "# images=trs(images)\n",
        "# imshow(torchvision.utils.make_grid(images,nrow=4))\n",
        "# del trainiter\n",
        "\n",
        "# print(labels)\n",
        "\n",
        "# testiter = iter(test_loader)\n",
        "# images, labels = next(testiter)\n",
        "# imshow(torchvision.utils.make_grid(images,nrow=4))\n",
        "# del testiter\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "cellView": "form",
        "id": "7oYDr8kuA5Bl"
      },
      "outputs": [],
      "source": [
        "# @title resnet\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "# # https://pytorch.org/vision/0.12/models.html#id10\n",
        "model = models.resnet152(weights='DEFAULT') # 18 34 50 101 152\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Sequential( # og (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
        "    nn.Linear(num_ftrs, 6, bias=False),\n",
        "    nn.Softmax(dim=1),\n",
        "    )\n",
        "\n",
        "# print(model)\n",
        "\n",
        "# # model.mods = [module for k, module in model._modules.items()]\n",
        "# # modules = [module for k, module in model._modules.items()]\n",
        "\n",
        "# torch._dynamo.config.suppress_errors = True\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# model = model.to(device)\n",
        "model = torch.compile(model.to(device))\n",
        "# model = torch.compile(model.to(device),mode='reduce-overhead')\n",
        "# model = torch.compile(model.to(device),mode='max-autotune') # max needs grad ckpt\n",
        "\n",
        "# resnet152 batch16 compile gradacc nogradckpt lr3e-6,3e-5\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "rJw9_Ort2Sek"
      },
      "outputs": [],
      "source": [
        "# @title vit\n",
        "# https://arxiv.org/pdf/2010.11929.pdf\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "# https://pytorch.org/vision/main/models/vision_transformer.html\n",
        "# https://pytorch.org/vision/main/models/generated/torchvision.models.vit_b_16.html#torchvision.models.vit_b_16\n",
        "# model = models.vit_l_16(weights='DEFAULT') # small vit_b_16 vit_b_32 vit_l_16 vit_l_32 vit_h_14 big\n",
        "# # VisionTransformer(image_size, patch_size, num_layers, num_heads, hidden_dim, mlp_dim)\n",
        "# num_ftrs = model.heads.head.in_features\n",
        "# # num_ftrs = model.heads[-1].in_features\n",
        "# model.heads = nn.Sequential(\n",
        "#     # nn.Dropout(0.2),\n",
        "#     nn.Linear(num_ftrs, 6, bias=False),\n",
        "#     nn.Softmax(dim=1),\n",
        "#     )\n",
        "\n",
        "\n",
        "!pip install timm\n",
        "# https://github.com/huggingface/pytorch-image-models/issues/908\n",
        "import timm\n",
        "# model = timm.create_model('vit_base_patch16_224', pretrained=True)\n",
        "model = timm.create_model('vit_base_patch16_224', img_size=(400, 640), pretrained=True)\n",
        "# [print(x) for x in timm.list_models('vit*',pretrained=True)]\n",
        "# https://huggingface.co/google/vit-base-patch16-224\n",
        "# https://github.com/huggingface/pytorch-image-models/blob/main/timm/models/vision_transformer.py\n",
        "# vit_base_patch16_224 compile,no ckpt # patch_size=16, embed_dim=768, depth=12, num_heads=12\n",
        "# vit_base_patch16_384\n",
        "# vit_large_patch16_224 explodesgpu # patch_size=16, embed_dim=1024, depth=24, num_heads=16\n",
        "# vit_large_patch16_384 # patch_size=16, embed_dim=1024, depth=24, num_heads=16\n",
        "\n",
        "# or fine tune huge\n",
        "# vit_large_patch14_224 # patch_size=16, embed_dim=1024, depth=24, num_heads=16\n",
        "# vit_large_patch16_384\n",
        "\n",
        "\n",
        "num_ftrs = model.head.in_features\n",
        "model.head = nn.Sequential(\n",
        "    nn.Linear(num_ftrs, 6, bias=False),\n",
        "    nn.Softmax(dim=1),\n",
        "    )\n",
        "# model.set_grad_checkpointing()\n",
        "\n",
        "# print(model.patch_embed.grid_size) # (25, 40)\n",
        "# print(model.pos_embed.shape) # [1, 1001, 768]\n",
        "# https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py\n",
        "\n",
        "# print(model)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# model = model.to(device)\n",
        "model = torch.compile(model.to(device))\n",
        "# model = torch.compile(model.to(device),mode='reduce-overhead')\n",
        "# model = torch.compile(model.to(device),mode='max-autotune')\n",
        "\n",
        "# vit_base_patch16_224 batch16 maxcompile nockpt gradacc lr1e-5,1e-4\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-oKYpG8n2fBI",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title inception\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "# # https://pytorch.org/vision/0.12/models.html#id10\n",
        "model = models.inception_v3(pretrained=True)\n",
        "# https://discuss.pytorch.org/t/inception-v3-is-not-working-very-well/38296/16\n",
        "# https://colab.research.google.com/github/CaoCharles/Deep-Learning-with-PyTorch/blob/master/2_Inception.ipynb\n",
        "model.aux_logits = False\n",
        "num_ftrs = model.fc.in_features # 2048\n",
        "model.fc = nn.Sequential( # og: (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
        "    nn.Linear(num_ftrs, 6, bias=False),\n",
        "    nn.Softmax(dim=1),\n",
        "    )\n",
        "# pytorch \"inception\" v3 \"gradient checkpointing\" https://github.com/jianweif/OptimalGradCheckpointing\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# model = model.to(device)\n",
        "model = torch.compile(model.to(device))\n",
        "# model = torch.compile(model.to(device),mode='max-autotune')\n",
        "\n",
        "# inception batch64 compile nogradacc nogradckpt lr3e-6,3e-5\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vEZCFg5YSS9J",
        "outputId": "6bd41d04-a4aa-4f33-8edd-6a61197b2330"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([16, 6])\n",
            "tensor([2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "# @title try\n",
        "\n",
        "# # check model's input and output dimensions are correct\n",
        "# X = torch.rand(64, 3, 32, 32, device=device)\n",
        "X = torch.rand(16, 3, 400, 640, device=device)\n",
        "# X = torch.rand(16, 3, 224, 224, device=device)\n",
        "model.eval()\n",
        "\n",
        "# 224x224\n",
        "# 16x16 / 32x32 patch\n",
        "# -> 14x14=196 7x7=49 seq length\n",
        "# 400x640 -> 25x40=1000 seq length\n",
        "\n",
        "\n",
        "logits = model(X)\n",
        "\n",
        "# modules = [module for k, module in model._modules.items()]\n",
        "# for i,x in enumerate(modules):\n",
        "#     print(i,x)\n",
        "\n",
        "# logits = checkpoint_sequential(functions=modules, segments=1, input=X)\n",
        "\n",
        "print(logits.shape)\n",
        "# print(logits[0])\n",
        "# print(logits[0].argmax(1))\n",
        "\n",
        "pred_probab = nn.Softmax(dim=1)(logits)\n",
        "y_pred = pred_probab.argmax(1)\n",
        "print(y_pred)\n",
        "del X, logits\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title wandb\n",
        "# https://docs.wandb.ai/quickstart\n",
        "!pip install wandb\n",
        "import wandb\n",
        "wandb.login() # 487a2109e55dce4e13fc70681781de9f50f27be7\n",
        "run = wandb.init(\n",
        "    project=\"resnet_vit\",\n",
        "    config={\n",
        "        # \"model\": \"resnet\",\n",
        "        \"model\": \"vit\",\n",
        "        # \"model\": \"inception\",\n",
        "        \"optim\": \"adamw\",\n",
        "        # \"learning_rate\": lr,\n",
        "        # \"epochs\": epochs,\n",
        "    })\n"
      ],
      "metadata": {
        "id": "uT9m-J1BUWyz",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "fsealXK3OPQa",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title train test function\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "# https://github.com/prigoyal/pytorch_memonger/blob/master/models/optimized/resnet_new.py\n",
        "from torch.utils.checkpoint import checkpoint, checkpoint_sequential\n",
        "\n",
        "trs=TrainTransform() # for image augmentation during train time\n",
        "# train function with automatic mixed precision\n",
        "def strain(dataloader, model, loss_fn, optimizer, scheduler=None, verbose=True):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    loss_list = []\n",
        "    for batch, (x, y) in enumerate(dataloader):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        with torch.cuda.amp.autocast(): # automatic mixed percision\n",
        "            x = trs(x) # image augmentation during train time to use gpu\n",
        "            pred = model(x) # default\n",
        "            # modules = [module for k, module in model._modules.items()]\n",
        "            # pred = checkpoint_sequential(functions=modules, segments=1, input=x) # gradient checkpointing for resnet and inception only\n",
        "            # # pred = checkpoint_sequential(functions=model.mods, segments=1, input=x)\n",
        "            # print(\"train\",pred[0])\n",
        "            # print((pred.argmax(1)==y).sum().item(), pred.argmax(1).tolist(), y.tolist())\n",
        "            loss = loss_fn(pred, y)\n",
        "        scaler.scale(loss).backward()\n",
        "        if ((batch + 1) % 4 == 0) or (batch + 1 == len(dataloader)): # gradient accumulation\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            optimizer.zero_grad()\n",
        "            if scheduler is not None:\n",
        "                scheduler.step()\n",
        "                # print(\"### lr: \", optimizer.param_groups[0][\"lr\"])\n",
        "\n",
        "        # print(model.state_dict()['_orig_mod.bn1.running_mean'][0])\n",
        "        train_loss = loss.item()/len(y)\n",
        "        loss_list.append(train_loss)\n",
        "        try: wandb.log({\"train loss\": train_loss})\n",
        "        except: pass\n",
        "        if batch % (size//(10* len(y))) == 0:\n",
        "            # loss, current = loss.item(), batch * len(x)\n",
        "            # loss, current = loss.item()/len(y), batch * len(x)\n",
        "            current = batch * len(x)\n",
        "            if verbose: print(f\"loss: {train_loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "    return loss_list\n",
        "\n",
        "def test(dataloader, model, loss_fn, verbose=True):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in dataloader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            pred = model(x)\n",
        "            # print(pred[0])\n",
        "            loss = loss_fn(pred, y)\n",
        "            # predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            # print((pred.argmax(1)==y).sum().item(), pred.argmax(1).tolist(), y.tolist())\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    # test_loss /= num_batches\n",
        "    test_loss /= size\n",
        "    correct /= size\n",
        "    # should not use weighted rand sampler for test?\n",
        "    try: wandb.log({\"test loss\": test_loss})\n",
        "    except: pass\n",
        "    if verbose: print(f\"Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f}\")\n",
        "    return correct, test_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "a5CHCIMo7ZC5"
      },
      "outputs": [],
      "source": [
        "# @title Lamb\n",
        "# https://github.com/cybertronai/pytorch-lamb/blob/master/pytorch_lamb/lamb.py\n",
        "import torch\n",
        "from torch.optim import Optimizer\n",
        "\n",
        "class Lamb(Optimizer):\n",
        "    \"\"\"Large Batch Optimization for Deep Learning: Training BERT in 76 minutes: https://arxiv.org/abs/1904.00962\n",
        "        adam (bool, optional): always use trust ratio = 1, which turns this into Adam\"\"\"\n",
        "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-6, weight_decay=0, adam=False): # eps=1e-8, weight_decay=0\n",
        "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n",
        "        self.adam = adam\n",
        "        super(Lamb, self).__init__(params, defaults)\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        loss = None\n",
        "        if closure is not None: loss = closure() # closure (callable, optional): A closure that reevaluates the model and returns the loss.\n",
        "        for group in self.param_groups:\n",
        "            for p in group['params']:\n",
        "                if p.grad is None: continue\n",
        "                grad = p.grad.data\n",
        "                if grad.is_sparse: raise RuntimeError('Lamb does not support sparse gradients, consider SparseAdam instad.')\n",
        "                state = self.state[p] # State initialization\n",
        "                if len(state) == 0:\n",
        "                    state['step'] = 0\n",
        "                    state['exp_avg'] = torch.zeros_like(p.data) # Exponential moving average of gradient values\n",
        "                    state['exp_avg_sq'] = torch.zeros_like(p.data) # Exponential moving average of squared gradient values\n",
        "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
        "                beta1, beta2 = group['betas']\n",
        "                state['step'] += 1\n",
        "\n",
        "                # Decay the first and second moment running average coefficient\n",
        "                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1) # m_t\n",
        "                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2) # v_t\n",
        "\n",
        "                # Paper v3 does not use debiasing.\n",
        "                # bias_correction1 = 1 - beta1 ** state['step']\n",
        "                # bias_correction2 = 1 - beta2 ** state['step']\n",
        "                # Apply bias to lr to avoid broadcast.\n",
        "                step_size = group['lr'] # * math.sqrt(bias_correction2) / bias_correction1\n",
        "                weight_norm = p.data.pow(2).sum().sqrt().clamp(0, 10)\n",
        "                adam_step = exp_avg / exp_avg_sq.sqrt().add(group['eps'])\n",
        "                if group['weight_decay'] != 0: adam_step.add_(p.data, alpha=group['weight_decay'])\n",
        "                adam_norm = adam_step.pow(2).sum().sqrt()\n",
        "                if weight_norm == 0 or adam_norm == 0: trust_ratio = 1\n",
        "                else: trust_ratio = weight_norm / adam_norm\n",
        "                state['weight_norm'] = weight_norm\n",
        "                state['adam_norm'] = adam_norm\n",
        "                state['trust_ratio'] = trust_ratio\n",
        "                if self.adam: trust_ratio = 1\n",
        "                p.data.add_(adam_step, alpha=-step_size * trust_ratio)\n",
        "        return loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "iuumbm2SB_lX"
      },
      "outputs": [],
      "source": [
        "# @title LR range test\n",
        "# gives insight into good LR range to use.\n",
        "# for accurate results, be sure to use a new model for range test;\n",
        "# also reset the model before training bec range test destroys the model\n",
        "# 1cycle super convergencehttps://arxiv.org/pdf/1708.07120.pdf\n",
        "# # cyclic lr https://arxiv.org/pdf/1506.01186.pdf\n",
        "# Note the learning rate value when the accuracy starts to\n",
        "# increase and when the accuracy slows, becomes ragged, or starts to fall\n",
        "\n",
        "# one training run of the network for a few epochs\n",
        "# pth='/content/lr.pth'\n",
        "# torch.save(model.state_dict(), pth) # save temporary model for lr finding\n",
        "# model.load_state_dict(torch.load(\"lr.pth\"))\n",
        "# torch.autograd.set_detect_anomaly(True)\n",
        "epochs=1\n",
        "min_lr= 1e-6\n",
        "max_lr= 1e-1 # 1e-2\n",
        "# 152: 1e-7 - 1e-4      result 3e-7 - 3e-6\n",
        "# inception: 1e-7 - 1e1      result 3e-7 - 3e-6\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "# optimizer = torch.optim.AdamW(model.parameters(), lr=start_lr, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01)\n",
        "# optimizer = torch.optim.SGD(model.parameters(), lr=start_lr, momentum=0.9)\n",
        "# import bitsandbytes as bnb\n",
        "# optimizer = bnb.optim.AdamW(model.parameters(), lr=min_lr, betas=(0.9, 0.999), optim_bits=8)\n",
        "optimizer = Lamb(model.parameters(), lr=min_lr, betas=(0.9, 0.999), eps=1e-08, weight_decay=3e-6)\n",
        "\n",
        "num_batches=len(test_loader)\n",
        "# num_batches=len(train_loader)\n",
        "\n",
        "# total_steps=int(num_batches*epochs)\n",
        "total_steps=int(np.ceil(num_batches/4)*epochs) # for grad accumulation\n",
        "\n",
        "# min_lr* gamma^total_steps = max_lr\n",
        "gamma = np.exp(np.log(max_lr/min_lr)/total_steps)\n",
        "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=gamma) # 0.75(20)-0.9(100)\n",
        "\n",
        "total_steps=total_steps*4 # for grad accumulation\n",
        "gamma = np.exp(np.log(max_lr/min_lr)/total_steps)\n",
        "lr_list=np.ones(total_steps)*min_lr*gamma**np.arange(total_steps)\n",
        "train_lst, test_lst=[],[]\n",
        "\n",
        "\n",
        "print(\"lr: \", optimizer.param_groups[0][\"lr\"])\n",
        "for i in range(epochs):\n",
        "    train_ls = strain(test_loader, model, loss_fn, optimizer, scheduler)\n",
        "    # train_ls = strain(train_loader, model, loss_fn, optimizer, scheduler)\n",
        "    train_lst.extend(train_ls)\n",
        "print(optimizer.param_groups[0][\"lr\"])\n",
        "\n",
        "# https://stackoverflow.com/a/53472966/13359815\n",
        "from scipy.ndimage.filters import gaussian_filter1d\n",
        "train_lstsm = gaussian_filter1d(train_lst, sigma=30)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(lr_list, train_lst)\n",
        "plt.plot(lr_list, train_lstsm)\n",
        "plt.xscale('log')\n",
        "# plt.plot(train_lst)\n",
        "# plt.plot(np.linspace(0,len(train_lst),len(test_lst)), test_lst)\n",
        "plt.show()\n",
        "plt.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "wkBp7WjC8LLA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43422ccf-d446-4769-dade-9791c89df7fd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "# @title wwwwwwwww\n",
        "acc_lst, train_lst, test_lst=[],[],[]\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# base_lr, max_lr = 3e-7, 3e-6 # resnet\n",
        "# base_lr, max_lr = 1e-3, 1e-2 # resnet lamb\n",
        "base_lr, max_lr = 1e-6, 1e-5 # vit\n",
        "# base_lr, max_lr = 3e-6, 3e-5 # inception\n",
        "# end_lr, start_lr = 1e-5, 1e-3 # 0.0001,0.1\n",
        "tp=0\n",
        "epochs = 5 #5 20\n",
        "# optimizer = torch.optim.SGD(model.parameters(), lr = start_lr, momentum=0.9)\n",
        "# optimizer = torch.optim.AdamW(model.parameters(), lr=base_lr, betas=(0.9, 0.999), eps=1e-08, weight_decay=3e-6)\n",
        "import bitsandbytes as bnb # 8bit optimizer\n",
        "# optimizer = bnb.optim.AdamW(model.parameters(), lr=base_lr, betas=(0.9, 0.999), optim_bits=8)\n",
        "# optimizer = bnb.optim.AdamW(model.parameters(), lr=3e-6, betas=(0.9, 0.999), optim_bits=8)\n",
        "optimizer = bnb.optim.AdamW(model.parameters(), lr=3e-5, betas=(0.9, 0.999), optim_bits=8)\n",
        "# optimizer = Lamb(model.parameters(), lr=base_lr, betas=(0.9, 0.999), eps=1e-08, weight_decay=3e-6)\n",
        "# optimizer = Lamb(model.parameters(), lr=3e-6, betas=(0.9, 0.999), eps=1e-08, weight_decay=3e-6)\n",
        "# 152 1e-5\n",
        "# cnn 3e-4\n",
        "\n",
        "div_factor = max_lr/base_lr\n",
        "num_batches=len(train_loader)\n",
        "# total_steps=int(num_batches*epochs)+1 # +1 to excluse uptick at the end of onecycle\n",
        "total_steps=int(np.ceil(num_batches/4)*epochs +1) # /4 for when using grad accumulation\n",
        "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=max_lr, total_steps=total_steps, pct_start=0.45, anneal_strategy='cos', cycle_momentum=True, base_momentum=0.85, max_momentum=0.95, div_factor=div_factor, final_div_factor=100.0, three_phase=True,)\n",
        "# scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=max_lr, epochs=epochs, steps_per_epoch=num_batches, pct_start=0.45, anneal_strategy='cos', cycle_momentum=True, base_momentum=0.85, max_momentum=0.95, div_factor=div_factor, final_div_factor=10000.0, three_phase=True,)\n",
        "# gamma = np.exp(np.log(end_lr/start_lr)/(num_batches*epochs)) # for scheduler step every optimizer step\n",
        "# scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=gamma) # 0.75(20)-0.9(100)\n",
        "\n",
        "# pth='/content/res15270kg.pth'\n",
        "# pth='/content/drive/MyDrive/frame/res15236.pth' # B\n",
        "# pth='/content/drive/MyDrive/frame/res15270kold.pth' # M\n",
        "# pth='/content/drive/MyDrive/frame/res152trackf.pth' # B\n",
        "# pth='/content/drive/MyDrive/frame/res152plus.pth' # B\n",
        "pth='/content/res152adamw71.pth' # ty\n",
        "\n",
        "\n",
        "# pth='/content/vit.pth'\n",
        "# pth='/content/drive/MyDrive/frame/vit.pth'\n",
        "# pth='/content/drive/MyDrive/frame/vit3736.pth'\n",
        "# pth='/content/drive/MyDrive/frame/vitadamw.pth' # M\n",
        "\n",
        "# pth='/content/drive/MyDrive/frame/inception1.pth'\n",
        "# pth='/content/inception2.pth'\n",
        "# pth='/content/inception.pth'\n",
        "\n",
        "\n",
        "# # # to continue training\n",
        "tp, modelsd, optimsd, schedsd = torch.load(pth).values()\n",
        "# tp, modelsd, optimsd = torch.load(pth).values()\n",
        "model.load_state_dict(modelsd)\n",
        "# model.load_state_dict(modelsd,strict=False)\n",
        "# optimizer.load_state_dict(optimsd)\n",
        "# # scheduler.load_state_dict(schedsd)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "REiP7-nvhc4s",
        "outputId": "8e618917-eb39-4b7f-f720-3fc53604597a",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1ysJfdsvwMiWbCdkvFHwNqAUnJTtm6KbT\n",
            "To: /content/res152adamw71.pth\n",
            "100% 351M/351M [00:03<00:00, 92.2MB/s]\n"
          ]
        }
      ],
      "source": [
        "# @title save\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# # pth='/content/drive/MyDrive/frame/vit.pth'\n",
        "# pth='/content/drive/MyDrive/frame/res55.pth'\n",
        "# pth='/content/drive/MyDrive/frame/resnet152.pth'\n",
        "# pth='/content/drive/MyDrive/frame/res152373605aug.pth'\n",
        "# pth='/content/res152373605aug.pth' # B\n",
        "# pth='/content/drive/MyDrive/frame/res152lamb12.pth' # ty\n",
        "# pth='/content/drive/MyDrive/frame/res1522.pth' # A\n",
        "# pth='/content/drive/MyDrive/frame/res1522do.pth' # A\n",
        "# pth='/content/drive/MyDrive/frame/res152fol6470kg.pth' # Ty B\n",
        "\n",
        "# pth='/content/drive/MyDrive/frame/inception.pth'\n",
        "# pth='/content/inception2.pth'\n",
        "\n",
        "# pth='/content/res15270kg.pth'\n",
        "\n",
        "# pth='/content/drive/MyDrive/frame/vit3736.pth'\n",
        "\n",
        "\n",
        "# tp, modelsd, optimsd, schedsd = torch.load(pth).values()\n",
        "# model.load_state_dict(modelsd)\n",
        "# # optimizer.load_state_dict(optimsd)\n",
        "# # scheduler.load_state_dict(schedsd)\n",
        "\n",
        "\n",
        "# checkpoint = { # https://discuss.pytorch.org/t/saving-model-and-optimiser-and-scheduler/52030\n",
        "# 'epoch': 3,\n",
        "# 'model': model.state_dict(),\n",
        "# 'optimizer': optimizer.state_dict(),\n",
        "# # # 'lr_sched': scheduler.state_dict()\n",
        "# }\n",
        "# torch.save(checkpoint, pth)\n",
        "\n",
        "\n",
        "\n",
        "# pth='/content/model.pth'\n",
        "# torch.save(model.state_dict(), pth)\n",
        "# model.load_state_dict(torch.load(pth))\n",
        "# # model.load_state_dict(torch.load(\"model.pth\"))\n",
        "\n",
        "# !gdown 1---4fdFbOUBTrS-VP5Va6pKowfgoU2UN -O inception2.pth\n",
        "# !gdown 1visTNvWmnuV7jAm2TBiAIIrNjbOAi1Fv -O resnet152.pth\n",
        "# !gdown 1-3oA1cKxgw4cfrqx079h_MKtMqRT1UFq -O res152lamb12.pth # S\n",
        "# !gdown 1cxu8Qq4-FoH3W3nQdRHT5mwe0xOxZ0Kb -O res152adamw.pth # M\n",
        "!gdown 1ysJfdsvwMiWbCdkvFHwNqAUnJTtm6KbT -O res152adamw71.pth # ty\n",
        "\n",
        "\n",
        "# !gdown 1qsbDQgnPgWLIwdwsb6MKJnxAf5SRTFz5 -O inception.pth # Ty 9jun\n",
        "# !gdown 1-032red_AZ4nABCXmYTvJN4s31xGctAC -O inception1615.pth # Ty\n",
        "\n",
        "# !gdown 1SAC0TW-KNJcFqVb4kA79sQb859eT-wME -O vit.pth # A 9jun\n",
        "# !gdown 1-0-GTDs5vEWnezoo-vUVagHi_niATwrN -O vit3736.pth # A 9jun\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# # t, modelsd, optimsd, scheduler = torch.load('/content/drive/MyDrive/frame/resnet152.pth').values()\n",
        "# t, modelsd, optimsd, scheduler = torch.load('/content/resnet152.pth').values()\n",
        "# model.load_state_dict(modelsd)\n",
        "# # optimizer.load_state_dict(optimsd)\n",
        "\n",
        "# # matt152 # https://drive.google.com/file/d/1MQ0xLfHbio458uEVbn2VyMpD3bij2A4J/view?usp=sharing\n",
        "# !gdown 1MQ0xLfHbio458uEVbn2VyMpD3bij2A4J -O res152.pth\n",
        "# model.load_state_dict(torch.load(\"res152.pth\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDBEk-l-Oxjn",
        "outputId": "e836e670-4abd-4a1f-e106-52e9d2a5fad7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "1e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 0.100319  [    0/61144]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 0.094134  [ 6112/61144]\n",
            "loss: 0.102401  [12224/61144]\n",
            "loss: 0.094886  [18336/61144]\n",
            "loss: 0.104073  [24448/61144]\n",
            "loss: 0.107287  [30560/61144]\n",
            "loss: 0.102769  [36672/61144]\n",
            "loss: 0.099126  [42784/61144]\n",
            "loss: 0.106741  [48896/61144]\n",
            "loss: 0.096996  [55008/61144]\n",
            "loss: 0.098195  [61120/61144]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 51.5%, Avg loss: 0.096032\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "1e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 0.097990  [    0/61144]\n",
            "loss: 0.094380  [ 6112/61144]\n",
            "loss: 0.098996  [12224/61144]\n",
            "loss: 0.088746  [18336/61144]\n",
            "loss: 0.086935  [24448/61144]\n",
            "loss: 0.093182  [30560/61144]\n",
            "loss: 0.100590  [36672/61144]\n",
            "loss: 0.098809  [42784/61144]\n",
            "loss: 0.088718  [48896/61144]\n",
            "loss: 0.099220  [55008/61144]\n",
            "loss: 0.096896  [61120/61144]\n",
            "Accuracy: 55.0%, Avg loss: 0.093889\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "1e-05\n",
            "loss: 0.103366  [    0/61144]\n",
            "loss: 0.100016  [ 6112/61144]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import time\n",
        "start = time.time()\n",
        "\n",
        "# optimizer = bnb.optim.AdamW(model.parameters(), lr=1e-5, betas=(0.9, 0.999), optim_bits=8)\n",
        "optimizer = Lamb(model.parameters(), lr=1e-5, betas=(0.9, 0.999), eps=1e-08, weight_decay=3e-6)\n",
        "\n",
        "# scheduler = torch.optim.lr_scheduler.PolynomialLR(optimizer, total_iters=int(np.ceil(num_batches/4)*3), power=1.0)\n",
        "# pth='/content/drive/MyDrive/frame/res152adamw.pth' # M\n",
        "\n",
        "# scheduler = PolynomialLR(optimizer, total_iters=4, power=1.0)\n",
        "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=10**(-1/2))\n",
        "# for epoch in range(5):\n",
        "#     scheduler.step()\n",
        "for t in range(0,epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    lr=optimizer.param_groups[0][\"lr\"]\n",
        "    print(lr)\n",
        "    # train_ls = strain(train_loader, model, loss_fn, optimizer, scheduler)\n",
        "    train_ls = strain(train_loader, model, loss_fn, optimizer)\n",
        "    correct, test_loss = test(test_loader, model, loss_fn)\n",
        "    train_lst.extend(train_ls)\n",
        "    test_lst.append(test_loss)\n",
        "    acc_lst.append(correct)\n",
        "\n",
        "    checkpoint = { # https://discuss.pytorch.org/t/saving-model-and-optimiser-and-scheduler/52030\n",
        "    'epoch': t+1,\n",
        "    'model': model.state_dict(),\n",
        "    'optimizer': optimizer.state_dict(),\n",
        "    'lr_sched': scheduler.state_dict()}\n",
        "    torch.save(checkpoint, pth)\n",
        "    # torch.save(model.state_dict(), pth)\n",
        "\n",
        "# print(\"Done!\")\n",
        "\n",
        "# end = time.time()\n",
        "# print(\"time: \",end - start)\n",
        "\n",
        "# print(len(train_lst), len(test_lst))\n",
        "# import matplotlib.pyplot as plt\n",
        "# plt.plot(train_lst)\n",
        "# plt.plot(np.linspace(0,len(train_lst),len(test_lst)), test_lst)\n",
        "# plt.show()\n",
        "# plt.plot(acc_lst)\n",
        "# plt.show()\n",
        "# plt.close()\n",
        "\n",
        "\n",
        "# torch.save(model.state_dict(), \"model.pth\")\n",
        "\n",
        "# resnet 18, 60/61 38.4%, 528s\n",
        "# resnet 18, 58/61 39.8%, 523s\n",
        "# resnet 18 compile , 58/61 40.4%, 555s\n",
        "# resnet 18 compile augment , 58/61 36.4%, 1941 # augment on cpu, takes longer\n",
        "# resnet 18 augment lr3e-4:3e-3, 58/61 37.7%, 1863s\n",
        "# resnet 18 augment 10epoch lr1e-5:3e-4, 58/61 33.5%, 3387s\n",
        "# resnet 18 compile lr1e-5:3e-4, 58/61 35.0%, 493s\n",
        "# resnet 18 compile scratch lr1e-5:1e-3, 58/61 26.8%, 475s\n",
        "# resnet 18 compile lr1e-5:1e-3, 55/61 47.3%, 480s\n",
        "# resnet 18 compile lr1e-5:1e-3, 52/61 51.7% 503s\n",
        "# resnet 18 compile lr1e-5:1e-3, unfreeze 51.0%, 550s\n",
        "# resnet 18 compile lr1e-5:1e-4, unfreeze 52.7%, 518s\n",
        "# resnet 34 compile lr1e-5:1e-4, unfreeze bitsadamW batch16*4\n",
        "# resnet 152 compileoverhead lr3e-7:3e-6, bitsadamW batch16*4 ckpt 53.8%, 2066s\n",
        "# resnet 152 from53.8% augment+cutout lr3e-7:3e-6, 53.8%, 2088s\n",
        "# resnet 152 comile augment+cutout lr1e-5 /4 1epoch 48.3%, 454s\n",
        "# resnet 152 comile augment+cutout lr1e-5 1epoch 48.4%, 446s\n",
        "# resnet 152 comilemaxautotue augment+cutout lr1e-5 1epoch 47.7%, 448\n",
        "# resnet 152 clipclean comilemax augment+cutout lr1e-5 10epoch 45.1%, 1585 *2\n",
        "# resnet 152 clipclean comilemax augmax lr1e-6:3e-5 20epoch\n",
        "# resnet 152 clipclean compilemax lr3e-7:1e-5,\n",
        "# resnet 152 70k augment compilemax lr3e-7:3e-6 5epoch, 65.4% 11585s*5/3=19300s = 5h20m\n",
        "\n",
        "# resnet 152 70k augment compile adamw const lr3e-6, 59.8%\n",
        "# resnet 152 70k augment compile adamw const3e-6 stepdown\n",
        "\n",
        "\n",
        "\n",
        "# resnet 152 70k augment compile lamb lr1e-2 not learning\n",
        "# resnet 152 70k augment compile lamb lr3e-3 not learning\n",
        "# resnet 152 70k augment compile lamb lr1e-3:1e-2 5epoch,\n",
        "# 2/5epochwarmup 55.0%\n",
        "\n",
        "# resnet 152 70kg augment1 cut1,-1 compilemax adamw lr3e-7:3e-6 1/5epoch 31.1% ty\n",
        "# resnet 152 70k augment1 cut1,-1 compilemax adamw lr3e-7:3e-6 1/5epoch 28.1% A\n",
        "\n",
        "# resnet 152 70k dataold augment1 cut1,0 compilemax adamw lr3e-7:3e-6 M nantest\n",
        "# resnet 152 70kg augment1 cut1,0 compilemax adamw lr3e-7:3e-6 B\n",
        "# resnet 152 70k augment1 from64.5 cut(mean) compilemax adamw lr3e-7:3e-6 4/5epochs 58.1% res152fol64 A 61.5% res152kplus M\n",
        "# resnet 152 70kg augment1 from64.5 cut(mean) compilemax adamw lr3e-7:3e-6 5epochs+ 63.8% res152fol64kg res152plus B\n",
        "# 73.6% 67% 66.1%\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# vit b16 lr1e-5 5epochs 41.3%, 466s # 4.4ram, 5.5vram\n",
        "# vit l16 lr1e-5 5epochs # 32.0%, 1242s 4.5ram, 8.0vram\n",
        "# vit l16 lr3e-7;1e-5 5epochs # 45.4%, 1315s 4.5ram, 8.0vram\n",
        "# vit l32 lr1e-5 5epochs # .ram, .vram\n",
        "# vit_large_patch16_384\n",
        "# vit_base_patch16_224 maxcompile nockpt lr3e-7;1e-5 5epochs # 45.2%, 2272s 5.3ram, 11.0vram\n",
        "# vit_base_patch16_224 maxcompile nockpt gradacc batch16 1e-5, 1e-2 explode\n",
        "# vit_base_patch16_224 maxcompile nockpt gradacc batch16 1e-6, 1e-5 low acc, test nan\n",
        "# vit_base_patch16_224 maxcompile nockpt gradacc batch16 3e-7, 3e-6 4/5 epoch50%\n",
        "# vit_base_patch16_224 maxcompile nockpt gradacc batch16 1e-6, 1e-5 0.5aug\n",
        "# vit_base_patch16_224 maxcompile nockpt gradacc batch16 aug1 noblur Lamb lr1e-5 vitadamw B\n",
        "\n",
        "\n",
        "\n",
        "# inception\n",
        "# inception og 10kclean 1e-7, 3e-5 batch64 nockpt 5epochs 0.020348  51.4%, 0.024312 20m; 1st 40.5% max 52.1\n",
        "# inception hid 10kclean 1e-7, 3e-5 batch64 nockpt 1/5epochs 0.019635 53.6%, 0.024120 1st 32.8%, max 53.6%\n",
        "# inception og 10kclean 1e-7, 1e-4 batch64 nockpt 2/10epochs 0.020348  0.020923 39.9%, 0.026105\n",
        "# inception og 70kg 1e-7 nope\n",
        "# inception og 70kg 3e-6 3e-5 54.8%\n",
        "# inception og 70kg 1e-6, 1e-5\n",
        "# inception og 70kg 3e-6 3e-5 0.5aug\n",
        "# adamw 3e-5 A 51.8% 53.3%\n",
        "# inception 70kg 3e-5 S\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "86I4ghMp-dgP"
      },
      "outputs": [],
      "source": [
        "\n",
        "# scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=max_lr, total_steps=total_steps, pct_start=0.45, anneal_strategy='cos', cycle_momentum=True, base_momentum=0.85, max_momentum=0.95, div_factor=div_factor, final_div_factor=100.0, three_phase=True,)\n",
        "\n",
        "poptimizer = bnb.optim.AdamW(model.parameters(), lr=3e-6, betas=(0.9, 0.999), optim_bits=8)\n",
        "slist=[]\n",
        "pscheduler = torch.optim.lr_scheduler.PolynomialLR(poptimizer, total_iters=100, power=1.0)\n",
        "for epoch in range(100):\n",
        "# for epoch in range(int(np.ceil(num_batches/4)*3)):\n",
        "    lr=poptimizer.param_groups[0][\"lr\"]\n",
        "    # print(lr)\n",
        "    slist.append(lr)\n",
        "    pscheduler.step()\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(slist)\n",
        "# plt.xscale('log')\n",
        "# plt.plot(train_lst)\n",
        "# plt.plot(np.linspace(0,len(train_lst),len(test_lst)), test_lst)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def deactivate_batchnorm(m):\n",
        "    if isinstance(m, nn.BatchNorm2d):\n",
        "        m.reset_parameters()\n",
        "        m.eval()\n",
        "        with torch.no_grad():\n",
        "            m.weight.fill_(1.0)\n",
        "            m.bias.zero_()\n",
        "\n",
        "model.apply(deactivate_batchnorm)\n"
      ],
      "metadata": {
        "id": "8mzr-IlDjSQU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "PFsnIRVz7Xp5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6cf0106-30ed-47ad-c0c4-94b85f9f16ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 71.6%, Avg loss: 0.082712\n",
            "0.7163673829849867 0.08271206086756448\n"
          ]
        }
      ],
      "source": [
        "correct, test_loss = test(test_loader, model, loss_fn)\n",
        "print(correct, test_loss)\n",
        "\n",
        "# # https://discuss.pytorch.org/t/model-predictions-changing-with-no-grad-and-eval/126543/2\n",
        "# self.drop_layer = nn.Dropout(p = .5)\n",
        "# out = F.dropout(x, training=self.training)\n",
        "# # https://discuss.pytorch.org/t/trained-resnet-doesnt-work-in-eval-mode-behaves-strangely/121242/8\n",
        "# self.bn = torch.nn.BatchNorm2d(input_features,track_running_stats=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IyVU_i594fMh",
        "outputId": "34e9f18d-c411-4b90-bd36-521188a7de3f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6794\n",
            "425\n"
          ]
        }
      ],
      "source": [
        "print(len(test_loader.dataset))\n",
        "print(len(test_loader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "GWt4B_YoCxKB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520
        },
        "outputId": "615f7d42-6a21-4216-b14c-fd942606a4a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 500x500 with 2 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbsAAAHACAYAAAA7jMYcAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAB45klEQVR4nO3ddVhU2RvA8e+QAhZt99pd2LWu8bO7xVjXLtRV7Fpxbdfu7ly7sAMDJFTsWoOyY0Vg5vcH6+jIgKAzDs68n33u8yxn3nvve7gjZ86559xRqFQqFUIIIYQRMzN0AkIIIYS+SWMnhBDC6EljJ4QQwuhJYyeEEMLoSWMnhBDC6EljJ4QQwuhJYyeEEMLoSWMnhBDC6EljJ4QQwuhZGDoBfXgzrJmhU9CpBitfGjoFnfJ5ct3QKeiMmUJh6BR0ysLM3NAp6NSryLeGTkFnot4/1O3xIm7r7FiWTjl0dix9McrGTgghxBcoYwydwXclw5hCCCGMnvTshBDCFKmUhs7gu5LGTgghTJHStBo7GcYUQghh9KRnJ4QQJkglw5hCCCGMngxjCiGEEMZFenZCCGGKZBhTCCGE0ZNF5UIIIYRxkZ6dEEKYIhnGFEIIYfRkNqYQQghhXKRnJ4QQJkgWlQshhDB+MowphBBCGBfp2QkhhCmSYUwhhBBGTxaVCyGEEMZFenZCCGGKZBhTCCGE0ZPZmEIIIYRxkZ6dEEKYIhnGFEIIYfRkGNN0WbjVxGbgHGxHryFFtwmYZcoVb2yKzqOx+2NTnM26vac6xjx/aVJ0GI7tsKXY/bEJs/TZvkMtYtV3r8fq0yvYc2Mns3bMJE/RPPHGZs2dlVELRrD69AoO/bOfxp0bxYkxMzOjw8D2rDq1gt03drDy5DLa9G2tzypo+K1rOy4HnyDi6VWOHNtGiZJFEoxv1Oh/+F08RMTTq5w9t5caNatovD50WF/8Lh4iNPwy/zz0Z+euVZQsVVR/FfhMl9/aEXTlOGFPgjl8dCslShROML5ho9pc8DtI2JNgzmipj+fQvlzwO8jjsEvce3CRv3etouQXfke60rlLG/wvHeFR+CUOHt5M8S/UpUHDWvj47uNR+CVO+uyieo3K8cZOnTGWp69u0K1HBx1nHb/u3dy5cd2HVy9vcerkTkqVLJpgfJMmdQkKOsarl7e46HeIWrWqabzesGFt9uxeS8jjS0S9f0iRIgX0mL2IjzR2/zEvVA6r/7kTdXgT/84ZjDLkHik6DAO71Frj362dwluvLh+3mf1RxcQQE3RGHaOwSkHMvau837/6e1UDgCr1KtNtxG+smrGGbv/rye0rt5m46g/SOqbRGp/CxprH9x+zeOJSnoQ+0RrTokdz6rWry+wRc+hUtQuLJiyhRbdmNOzYQJ9VAaBJkzp4TRyG14SZVChXl0tBwWz/ewXOzo5a493cirNsxUxWrNhI+bJ12LXrIOs3LCB//tzqmBs37uDhMQq3UrWoUb0Z9+4/5O8dK3ByctB7fRo3qcOEiUOZ6PUXFcvXIygomK1/r8ApnvqUdivO0uUzWblyIxXK1WX3zgOsXT+ffJ/U5+bNOwwcMJqypWtT85fm3L/3gG07VuKo5/o0avw/xnsNZdLE2VSt0JBLl4LZvG1pvL/H0m7FWLRsOmtWbqZKhQbs2XWI1evmki/fT3Fi69T7hZKlivLoUYhe6/CpZs3qM3nyKMaPn0Zpt1oEBl5h9+418b7XypYpyepVc1i2bB2lStfk7x372bJ5CQUKfPxwaWdny6nT5xg69I/vVY1EUalidLb9CBQqlUpl6CR07c2wZkneJ0W3CSgf3uL9ziWxBQoFNr/PJ/rMXqKOb//i/hbl/ofVzy14O/E3iIrUeE2R1hnbQXP5d/YglI/vJjm3BitfJil+1o6ZXAu4zuwRc2LPr1Cw7txqti/7m/VzNya47+rTK9i6ZDtbl2zTKB+/bCzPIp4xddB0ddmoBSOIfBfJxL6TkpSfz5PrSYo/cmwbfr6BDPAYBcTW59qN08yft4JpU+fHiV+xcha2djY0a/Kruuzw0a0EBV6hb5/hWs+RKlVKHocGUfd/bTh69HSiczNTKJJUlw+5+PkGMnDAaCC2PsHXT7Fg/kqma6nPshV/YWdnS/OmH+vjfWQLgYHB9O8bf30ehgRSr05bjiWhPhZm5kmqy8HDm/HzC2TwwLHqugRdPc6iBauYOW1hnPgly2dga2dLq2a/qcsOHN5EUGAwA/qNVJelT+/KwSObadqwI+s3L2L+3BXMn7s8SbkBvIp8m6T4Uyd3cuFCAH37DVfX587t88yZu4zJk+fEiV+zZh52trY0bOSuLjt5YicBAZfp2WuIRmzWrJm4eeMsJUvVICDgcpLrEvX+YZL3Scg7/106O1aKonV1dix9kZ4dgLkFZhlyEHMz8GOZSkXMzUDMsuSOf79PWJb4meig03Eauu/NwtKC3IV+wu+kn7pMpVLhd+Ii+Uvk/+rjXva9QrHyRcmYPSMAOfLloGCpApw7cv6bc06IpaUlxYoV5MiRk+oylUrFkcOnKO1WXOs+pd2KceTwKY0y70PHKV1ae7ylpSUdO7Xi+fOXBAUF6y75eM5VtFhBjhz5mJ9KpeLokVOULl1M6z6l3Ypz9Mjn9TlBaTft8ZaWlnTo1FLv9bG0tKRIsQIajalKpeLY0dOUiqcupUoX49gRzcb38KETlCpdVP2zQqFg3qLJzJq5mKtXb+old20sLS0pXrww3odPqMtUKhWHD5+kTJkSWvcp41aCw5/EAxw4eDTeeGE4MkEFUNimQmFujur1C41y1esXmDln/OL+ZplyYZYuC5Hb5ukrxURL45AacwtznoU/1yh/FvGMzLkyf/Vx18/ZgF1KW5YdXYwyRomZuRnLJi3n8PYj35hxwhyd7LGwsCAsNEKjPCwsgtx5cmrdx9XVmfCwuPGurs4aZbVqV2P5ir+wtbUhJCSM+vXa8eTJM91W4DOOjrH10ZZf7tzx1ceJsMTUp1Y1lq6Yqa5Pw3rtearH+sRXl/CwJ+T+SXtdXOKpi8sndenr8Rsx0TEsmLdC90knwMnJQet7LTQsnDzxvNfSpXMmNCxcoywsNO61SZZkgkry8c8//9CpU6cEYyIjI3n58qXGFhn9fceQLUpUQxlyD+WD7/cp9HurXK8S1RpVY0LviXT/X08m9Z9Cs65N+aVpdUOn9tWOHztDuTJ1+LlqEw4ePMbKVbPjvTfzIzh+/AwVytbll2pNOXTwOMtXzYr3PmByVaRoAbp2d6dnt8GGTsX4qZS6234Aybqxe/r0KStWJPzpzsvLizRp0mhsU05fTdJ5VG9foYqJQZFScwKHImUaVK+fJ7yzpTUWhcsTdeFwks6pLy+eviQmOgZ757Qa5fZO9jwL//pP+b8N68L6uRs4uuMYd67e5dBWb7Ys3kqrni2/MeOEPYl4RnR0NC6uThrlLi5OhIaGa90nNDQcZ5cvx799+y+3b9/j/Hl/enYfQnR0NO3dm+u2Ap958iS2PonJ74PQ0AhcklifXj2GEBMdo9f6xFcXZxfHOL2dD8LiqUvYf3UpW64Uzs6OBAYfI+xZMGHPgsmSNRPjJgzB/5J+RxEiIp5qfa+5ujgTEs+1CQkJx9VFsxfn4hr/tRSGY9DGbseOHQluR458+c3t6enJixcvNLaB5fImLZGYaJSPbmOes9DHMoUC85yFUN5PeDKFRcGyYG5BtP/xpJ1TT6KjorkedIPi5T/eM1EoFBSrUJQrvle++rgpbKxRKTXnMiljlJiZJX2CRlJERUVx8eIlqlQpry5TKBRUqVqOc2f9tO5z7uxFqlQtr1FWtVoFzp3THv+BmZkZ1tZW3550AqKiovC/eIkqVcqpyxQKBZWrlOPcuYta9zl31o/Kn8QDVK1WnnNntcd/YGamwNpKf/WJiooi4OJlKlUuqy5TKBRUrlyO8/HU5fy5i1SqUlajrEq18pw/5w/AhvXbqVimLpXL1Vdvjx6FMGvmYpo2SniU51tFRUXh5xdItaoV1GUKhYKqVSvg4+OrdR+fs75UrVZBo6z6z5XijU9WlDG6234ABr1n17BhQxQKBQlNCFV8YbabtbU11tbWGmVvLJI2owwg6tQurJv0RPnwFjEPbmJZrg4KK2uifGMbXKumvVC9fErUgbUa+1mUrEZM8Hn493Xcg9qkxCytE4pU9rF1ccqAGaB69fzLPcZvsGXRVn6fNpBrgde55n+Nxp0bkcImBfs2HgBg8PRBRIREsOTPZbF1sLQg609ZYv/fyhKndI7kzJ+Df9++49HdRwCcOeRD694tCXsYxt3r98hVMCdNujRm34YDeqvHB7P/WsyCRVPx8wvE90IAPXt1wtbWltWrNgOwcNFUHj0KYfSoyQDMnbOMfQfW07vPr+zfd5imzepRvHgh+vQaCoCtrQ2DBvdkz65DhISE4+hkz29d25EhQzq2bd2j//rMWsL8hVO4eDGICxcC6NGzo0Z9FiyawqNHoYz5rz7z5i5n7/519OrTmf37jtC0aT2KFS9En97D1PUZ+HtP9u4+REhIGI6ODnTp2o70GdKxbZt+6zN39lLmLJiE/8VL+PkG0q1HB2xtbVi7akvs6wsm8fhxKONGT42t27wV7Ny7hp69O3Fg/1EaN6lD0WIF6d87dvbjs6fPefb0ucY5oqOiCQuN4OaNO3qtC8CMmYtYumQ6vn6BnD9/kT69u2BnZ8OKFRsAWLZ0Jg8fPWb48IlA7LX09t5Mv35d2bv3EM2bN6BEicJ07/G7+pj29mnJkiUj6dO7AqjvzYaEhBm2B/iDDD/qikEbu/Tp0zN37lwaNNC+Vsvf358SJb7PrKaYoNO8t0uN5c8tsEqVFuXju7xb/ge8iZ20YpbGCeVnjbLCKQPm2fLx79JxWo9pkbck1k17qn9O0bI/AO+9NxJ1eJOeagJHdx4jjUMaOgxoj72zPbeu3Maz3TCeRzwHwCWjM8pP3uiOro4s2P9xck3zbs1o3q0ZAWcCGNA89h/t7BFz6TDQnT5/9CKtU1qehD5h95o9rJqxRm/1+GDLlt04OTsyfIQHrq5OBAYG06hhB/VEh8yZM6D85Gb72bN+dOrQjxGjBjB6zEBu3bxLyxZduXIltpceExNDntw5abOuCY6O9jx9+hxf30Bq/NKc4OAbeq/P1i27cXJyYOjw/ri6OhEUGEyThh3UEz0yZdKsz7mzfnTu2I8RIwcwavRAbt26S+uW3Qj+pD65c+ekdZvG6vr4+QZS65cWXNVzfbZt3YOjkwOew/ri4urMpcBgmjXuTHh47HrNTJkzaPy7OXf2Ir918mDoyP4MHzWA27fu0rZVj+/ye0+MTZt24OzkwKiRA0mXzpmAgMvUrds23vfaGZ8LtGvfizFjfmf8uMHcuHmHJk07c/nyNXVMvbo1WLLk45KdtWti/62NHTeVceOmfaeaCYOus6tfvz5FixZl7NixWl8PCAigWLFiGm+uxPiadXbJWVLX2SV3SV1nl5x9zTq75Cyp6+ySu6Sus0vOdL7OzmeDzo6VokwLnR1LXwzasxs0aBBv3ryJ9/VcuXIl6r6dEEKIJJJhzO+nYsWKCb5uZ2dH5crxPzdPCCGESAxZVC6EEKbIxBaVS2MnhBCmyMQau2S9qFwIIYTQBenZCSGECfpRvppHV6SxE0IIUyTDmEIIIYRxkZ6dEEKYIllnJ4QQwujJMKYQQghhXKRnJ4QQpkiGMYUQQhg9GcYUQgghjIv07IQQwhTJMKYQQgijJ8OYQgghhHGRnp0QQpgiE+vZSWMnhBCmyMTu2ckwphBCCKMnPTshhDBFMowphBDC6MkwphBCCGFcpGcnhBCmSIYxhRBCGD0ZxhRCCCGMi/TshBDCFMkw5o+vw+r3hk5BpzaVjjR0CjqV+5CVoVPQmeiYGEOnoFNmCoWhU9AphZHVR6dMrLGTYUwhhBBGzyh7dkIIIb5ApTJ0Bt+VNHZCCGGKZBhTCCGEMC7S2AkhhClSKnW3fYU5c+aQLVs2UqRIgZubG+fOnUswfsaMGeTJkwcbGxsyZ85M//79effuXaLPJ8OYQghhigy4qHzDhg14eHgwf/583NzcmDFjBjVr1uTatWu4uLjEiV+7di1Dhgxh6dKllCtXjuvXr9OhQwcUCgXTpk1L1DmlZyeEEOK7mjZtGl26dKFjx47kz5+f+fPnY2try9KlS7XGnz59mvLly9O6dWuyZctGjRo1aNWq1Rd7g5+Sxk4IIUyRDocxIyMjefnypcYWGal9ffD79+/x9fWlevXq6jIzMzOqV6/OmTNntO5Trlw5fH191Y3b7du32bNnD//73/8SXV1p7IQQwhSpVDrbvLy8SJMmjcbm5eWl9bQRERHExMTg6uqqUe7q6kpISIjWfVq3bs3YsWOpUKEClpaW5MyZkypVqjB06NBEV1caOyGEEN/E09OTFy9eaGyenp46O/7Ro0eZMGECc+fOxc/Pj61bt7J7927GjRuX6GPIBBUhhDBFOlxnZ21tjbW1daJinZycMDc3JzQ0VKM8NDSUdOnSad1nxIgRtGvXjl9//RWAQoUK8ebNG3777TeGDRuGmdmX+23SsxNCCFNkoKUHVlZWlChRAm9v709SUeLt7U3ZsmW17vP27ds4DZq5uTkAqkQ+CUZ6dkIIIb4rDw8P3N3dKVmyJKVLl2bGjBm8efOGjh07AtC+fXsyZsyovu9Xr149pk2bRrFixXBzc+PmzZuMGDGCevXqqRu9L5HGTgghTJEB19m1aNGC8PBwRo4cSUhICEWLFmXfvn3qSSv379/X6MkNHz4chULB8OHDefjwIc7OztSrV48//vgj0edUqBLbB/yBNMvawNAp6NTCki8MnYJO5T702NAp6IyxfcWPeSLuffxIXka+NXQKOvM+8oFOj/d2YX+dHcv2t+k6O5a+GNc7WwghhNBChjGFEMIUmdi3HkhjJ4QQpsiA9+wMQYYxhRBCGD3p2QkhhClSGt3cxARJYyeEEKbIxO7ZyTCmEEIIoyc9OyGEMEUm1rOTxk4IIUyR8T1PJEEyjCmEEMLoSWP3iZrt/8eckwtZc20TE7ZPJleRn+KN/bnlL4zdNIFlgWtYFriGEWvGasSbW5jTZkh7pu6fyargDSw4t4xe0/ph7+LwPaqCda2GpJ63nrTrDpDKay7mufImGK+wTYnNr31Js3gLadcfIPWsVVgUd1O/blWzPqmmLSHtqt2kXbWbVBPmYFGstL6rodbp19b4BnrzT2gg+7w3Uqx4oQTj6zesxenze/knNJBjp3dQ/ZdKGq/PmutF+ItrGtuGLYv1WQUNv/7WloDLR3kccZmDRzZTvEThBOMbNKrNWb/9PI64zKmzu/mlRuV4Y6fNHMuz1zfp1qODjrPWrlOXNvgFHeZBWBD7D2+i2BfqUr9hLc5c2MeDsCCOn9lJ9c/qMmveRCJeXtfYNmz9ftemWzd3rl87w8sXNzl5YiclSxZNML5J4zoEBR7l5Yub+PkeolatahqvN2xQm9271/D4URDvIx9QpHB+PWafBAb61gNDkcbuP+XqVsB9eCc2zdzA4Loe3Au+w7BVo0ntmEZrfIGyhTi54wRjWg5nWKPfefIoguGrRuPgGtuYWdtYk6NgTjb/tZHBdTyY0tWLDDkyMnjJML3XxbJcVWw69ODdxuW8HNSFmHu3SDliMorUabXvYGFBylFTMHdJx+vJo3jZuz1v501B9SRCHaJ6Es6/qxfy8vffePl7V6Iu+ZFy8B+YZc6m9/o0bFybsRM8mfLnHH6u1IjLl66ycdsSnJy0f3AoVboYC5ZMZc2qzVSr2JC9u71ZsXYOefNpfnjxPnicAj+VV2+/dfbQe10AGjX5H+O9hvKn1yyqVGjApUtX2bJ9GU7O2utT2q0Yi5dNZ/WKTVQuX5/duw6yev088uWP+2GsTr1fKFmqKI8eaf/GZ11r2Ph/jJvgyeSJs6lWsSGXg66yaWvC12bh0mmsWbmJqhUasmf3IVZquTaHDh4nf65y6u23Tt/n2jRrWo/Jk0Yy/o/puLnVJjDoCrt3rcbZ2VFrfJkyJVi1ag7Llq+ntFstduzYx+ZNiymQP486xs7OltOnzjN02ITvUodEU6p0t/0A5EHQ/5mwfTK3Am+wZORCABQKBfN9lrB3+W62z9vyxf3NzMxYFriGJSMXcnzrEa0xOQvnYuLOqXQv25mIRxFaY7RJ6oOgU3nNJfrWNf5dPDO2QKEgzYKNvNu7jchta+PEW9WoT4oGLXjZpz0k4cHGaZbv4N9V83nvvSdJ+SX1QdD7vDfi7xfEkEGx30qsUCgIuHKMxQtX8df0RXHiFy2bjq2tDW1adFOX7T20gUtBVxnUfxQQ27NLnSY17m16JimXz33Ng6APHtnMRb8gfh8wBoitz6VrJ1g0fxUzpi2IE79kxUzsbG1o2ew3ddmBw5u5FHQFj74j1WXp07ty8OgWmjbsyIbNi5g3Zznz5y5PUm5JfRD0/sObuOgXxJCBY9V1CQw+zqIFq/hr+sI48YuXzcDWzobWzbuqy/Z5b+RSYDADP1ybeRNJkyY17Vv3SFIu2iT1QdAnT+zkgm8A/foNB2Lrc/vWeebOXcbkKXPixK9ZPRdbO1saNeqgLjtxfAcBgZfp1Uvzm7qzZs3Ejes+lCpVg4DAK0mui84fBD3lV50dy3bg9+t5fy3p2QEWlhbkKJSTwJMB6jKVSkXgyQByF8+TwJ4fWdlYY2Fpzuvnr+KNsU1lh1Kp5M3LN9+cc7wsLDDPmYfoQN+PZSoVUYG+WOTWPnxiVaoc0deuYNulH2mWbCX19GWkaNwG4vvDZ2aGZflqKFKkIPraZT1U4iNLS0uKFC3AsaOn1WUqlYrjR09TslQxrfuULFWU40fPaJQd8T5JyVJFNcrKVyjNlZunOXNhH5OmjcbePq2u04/D0tKSosUKcvTIKXWZSqXi2JHTlCqtvT6lSxfj6JHTGmWHvU9oxCsUCuYvnsKsmYu4GnxDP8l/Rn1tjmhem2NHT1OqdFGt+5QsXVTjWsJ/1+azupevUJrgW2fw8d3H5GmjsXdIq+v047C0tKR48UIcPnxCXaZSqTh8+ARlyhTXuo+bWwmNeICDB49Rxq2EXnPVCZVSd9sPQGZjAqnsU2NuYc6LiOca5S8inpMxZ6ZEHaOtZ3uehj4l6FSA1tctrS1p69meUztO8O/rf7815XgpUqVBYW6O8vlTjXLVi2eYZ8yidR8z1wxYFEzH+xMHef3HEMzSZcT2t35gbsG7TSs+xmXJTuoJc8HKCtW7f3k9aQTKB/f0VhcAB0d7LCwsCA97olEeFv6EXLlzaN3HxdWJsDDNnnN4+BNcXJ3UP3t7n2DXzoPcv/eAbNkzM2ykB+u3LKJ29RYo9XgPwjGe+oSHRfBTAvUJD4+IE+/i6qz+uZ9HV6KjY1gwd8Xnu+uNui5ackuwLp9dm7CwCI1rc/jQCXbvOMC9ew/Ilj0Lw0d5sGHLYmr93Fyv18bJyQELCwtCQ8Pj5JcnTy6t+6RL50xYqGZ9QsPCcf3k2iRbP8jwo64YvLH7999/8fX1xcHBgfz5NXse7969Y+PGjbRv3z7e/SMjI4mMjNQoi1HFYK5I3LfX6kLD7k0oX68io1oMIyoyKs7r5hbmeMz5HRQKFg2b993ySjSFAtWLZ7ydPxWUSmJuX+edoxMpGrTUaOyUj/7h5cBfUdjaYVm2Mna9PHk1sq/eGzx92L7l49Br8JXrXLl8jQsB3pSvWJoTx3wMmFnSFSlagK493KlS3ji+x3Hblt3q//9wbXwDvSlf0Y0Tx84ksKcQ8TPoMOb169fJly8flSpVolChQlSuXJnHjz/ez3nx4oX6a9rj4+XlRZo0aTS2qy+SNozz6tlLYqJjSOOUVqM8jVNanoc/S3Dfer81pGH3xoxrO5r7V+P+0f/Q0DlldGZcm1F67dUBqF69QBUTg1lazQkCijT2cXp7HyifPSHm8QONWVUxD+5hZu8IFp98HoqORhnyMLYxXLOImHu3SFGniV7q8cHTJ8+Ijo7G2UVzgoCLs2OcT9QfhIVG4OLipFHmnEA8wL27D4iIeEr2HFm/PekEPImnPs4uTgnWx9nZSUt8bA+kbLlSODs7EnT1OOHPrxL+/CpZsmZivJcnAZeP6qUe8EldEsjtc2GhETh/dm1cEqg7wL27/xAR8ZQcObSPTOhKRMRToqOj4/TKXFycCA0N07pPSEi4Rq8UwNXFOU7vMDlSKZU6234EBm3sBg8eTMGCBQkLC+PatWukSpWK8uXLc//+/UQfw9PTkxcvXmhsedPEv2RAm+ioaG4H3aJQ+Y9TphUKBYXKF+a637V496vftRFNezfnD/cx3A66Gef1Dw1duuzpGddmZIL383QmOpqYW9ewKPTJPQaFAsvCJYi+rv2mePTVS5ilywgKhbrMPENmlE8jIDo6/nMpFGBppavMtYqKiiLA/zKVKpf95LQKKlYuy4XzF7Xuc+G8PxUrl9Eoq1y1HBfO+8d7nvQZXHFwSEtoiH7/SEVFReF/8RKVq5RTlykUCipVKcf5c9rrc+7cRY14gKpVy6vjN6zfToUydahUrp56e/QohFkzFtOkYcIfFr+1LgH+l6lURfPaVKpclvPn/LXuc+Gcv8a1hP+uTTx1h+97bfz8gqhatYK6TKFQULVqBXx8/LTuc/asL9U+iQf4+eeK+Jz11RqfrJjYbEyDDmOePn2aQ4cO4eTkhJOTEzt37qRHjx5UrFiRI0eOYGdn98VjWFtbY21trVH2NUOYuxb/Tc+pfbkVeJObATeo06ke1rYpOLLpEAC9pvXjacgT1k5aBUCDbo1p4dGamX2nEv4gjLTOaQF49+Yd796+w9zCnAHzBpO9YE4mdhqHmbmZOub189dERyXQiHyjdzs3Ydfbk5hb14i+EUyKuk3BOgXvD+8FwLa3J8qnEbxbEzuTMXL/36So3QibTr2J3LMVs/SZSNG4DZF7tqqPmaJNF6IvnkUZHgY2NlhVrI5FgaK8HjdIb/X4YP6cZcya9yf+Fy/h5xtI1x7u2NrZsG51bH6z5/9JyONQxo+ZBsDCeSv5e88quvfqyMH9x2jU5H8ULVaQAf/NXLSzs2XgkF7s+ns/YWERZMuemVFjB3Hn9j2OeJ+INw9dmTt7KXMXTOaiXxB+voF079kBO1sb1qzeDMC8hZN5/CiUsaOnALBg7nJ27VtLz96dObD/CI2b1qVo8YL06xO7jOXZ0+c8e/pc4xzRUdGEhoZz88YdvdZl3uxlzJ7/37W5EEi3Hu7Y2tqwbnXsDOY5Cybx+FEo48dMja3LvBXs2LuaHr06cWD/URo3rUPRYgXx6DMCiL02g4b0YueO/YSFRpAtexZG/3dtDn+HazNz5kKWLJmOn28A5y/407v3r9jZ2bBi5QYAli6ZwaNHIQwfMRGAWbOX4H1oM/36/cbevd40b9aAEiUK06PHYPUx7e3TkiVzBtJnSAdA7tw5AQgJDf8heoDGwqCN3b///ovFJ8NkCoWCefPm0atXLypXrszatXGnyevL6V0nSe2YmhYerUnrbM/dK3f4o/0YXkTETvt3yuCk0V2v0bYWltaWDJw/ROM4G6evY9OM9Tikc6RUjdhF2VP2zdSIGdViGFd8LumtLlGnj/BvmrSkaNkRs7QOxNy5yevxv6N6ETska+bkqvGoINWTcF6NG4Rtx15YT1uK8mk4kbu38G77OnWMWZq02PYeipm9A6q3b4i5d5vX4wZpzvrUk+1b9+Lo6MDgoX1wcXXmUlAwLRr/Snh47CSPTJnSa1yb8+cu0u3XgXgO78ewkR7cvnUX99Y91bMUY2JiKFAgNy1aNSRNmlSEPA7j6JFTTBw/k/fv495z1bVtW/bg5OTI0OH9cHF1JijwCk0bdVJPWsmUOYPGRIxzZy/SpZMHw0b0Z8ToAdy+dZe2LbsTfOX7zLpMyPate3B0cmDIJ9emeZPOGtdG+dm16dp5AENH9GPYqNhr0/6za5O/YB5atG708docPoXX+Bnf5dps2rwTJ2dHRo4cSLp0zgQEXKFuvXbqCU+ZM2fUqI+Pjy/t2/dizJjfGTd2MDdv3qFps1+5fOXjiFDdur+wZPF09c9r1sTetx83bhrjxk/Te53i9YPMotQVg66zK126NL1796Zdu3ZxXuvVqxdr1qzh5cuXxCRxLdPXrLNLzpK6zi65S+o6u+Tsa9bZJWdJXWeX3CV1nV1yput1dm/GttHZsexGrtHZsfTFoO/sRo0asW7dOq2vzZ49m1atWmGEa96FEEJ8ZwZt7Dw9PdmzJ/6nb8ydO1ev62qEEMJkmdizMQ2+zk4IIYQB/CCzKHXFuAbohRBCCC2kZyeEEKbIxGZjSmMnhBCmSIYxhRBCCOMiPTshhDBBP8ozLXVFenZCCCGMnvTshBDCFJnYPTtp7IQQwhSZWGMnw5hCCCGMnvTshBDCFMk6OyGEEEZPhjGFEEII4yI9OyGEMEEqE+vZSWMnhBCmyMQaOxnGFEIIYfSkZyeEEKbIxB4XJo2dEEKYIhnGFEIIIYyL9OyEEMIUmVjPTho7IYQwQSqVaTV2MowphBDC6EnPTgghTJEMYwohhDB6JtbYyTCmEEIIo2eUPbsLr+8aOgWdanzWxdAp6FRwmXSGTkFnKlx8a+gUdOrRmyeGTkGnTG0SRlLIszGFEEIYPxNr7GQYUwghhNGTnp0QQpgi03o0pjR2Qghhikztnp0MYwohhDB60rMTQghTZGI9O2nshBDCFJnYPTsZxhRCCGH0pGcnhBAmyNQmqEhjJ4QQpkiGMYUQQgjjIj07IYQwQTKMKYQQwvjJMKYQQghhXKRnJ4QQJkhlYj07aeyEEMIUmVhjJ8OYQgghjJ707IQQwgSZ2jCm9OyEEMIUKXW4fYU5c+aQLVs2UqRIgZubG+fOnUsw/vnz5/Ts2ZP06dNjbW1N7ty52bNnT6LPJz07IYQQ39WGDRvw8PBg/vz5uLm5MWPGDGrWrMm1a9dwcXGJE//+/Xt++eUXXFxc2Lx5MxkzZuTevXukTZs20eeUxk4IIUyQIYcxp02bRpcuXejYsSMA8+fPZ/fu3SxdupQhQ4bEiV+6dClPnz7l9OnTWFpaApAtW7YknVOGMYUQwgSplLrbIiMjefnypcYWGRmp9bzv37/H19eX6tWrq8vMzMyoXr06Z86c0brPjh07KFu2LD179sTV1ZWCBQsyYcIEYmJiEl1faeyEEEJ8Ey8vL9KkSaOxeXl5aY2NiIggJiYGV1dXjXJXV1dCQkK07nP79m02b95MTEwMe/bsYcSIEUydOpXx48cnOkcZxhRCCBOky2FMT09PPDw8NMqsra11dnylUomLiwsLFy7E3NycEiVK8PDhQyZPnsyoUaMSdQxp7IQQwhSpFDo7lLW1daIbNycnJ8zNzQkNDdUoDw0NJV26dFr3SZ8+PZaWlpibm6vL8uXLR0hICO/fv8fKyuqL55VhzE+069yCExf3cPXhObYdWE2R4gUTjP9f/V845LOdqw/PsffEZqpUr6Dxuq2dDWP+9OR00AGCH5zlwOmttO7QTJ9VUGvoXp91Z1ax/+Zu5u78i7xF88Qbmy13VsYsHMm6M6s48uAgTTo3ihPz4bXPt77je+uzGmop6jXEfsV6HHceIM3MeVjkyZtgvMIuJXY9++GwdiuOOw9iv2Q1lqXc1K9bFCxM6jFe2K/dgtP+Y1iVrZDA0XSvdadmeF/4m4D7J9mwdxmFiuWPNzZXnhz8tfRPvC/8zdWw87T/rdU3H1OXuvzWjqArxwl7Eszho1spUaJwgvENG9Xmgt9Bwp4Ec+bcXmrUrKLxuufQvlzwO8jjsEvce3CRv3etomTJInqsgabu3dy5cd2HVy9vcerkTkqVLJpgfJMmdQkKOsarl7e46HeIWrWqabzesGFt9uxeS8jjS0S9f0iRIgX0mH3yZ2VlRYkSJfD29laXKZVKvL29KVu2rNZ9ypcvz82bN1EqP3ZHr1+/Tvr06RPV0IE0dmp1GtZk2LiBzJy8gLrVWhJ86RorNs3D0clBa3zxUkWYuWgiG1dvo07VFhzcc4QFq2aQO28udczwcQOpVK0c/bsNpXrZRiybv4Yxfw6heq3Keq1L1XqV6T6yKyumr+a32t25deU2k1Z7kdYxrdZ4axtrHt1/zEKvJTwJfaI1pludXjQu1ly9DWj5OwBHdx/TVzXUrCpXxe63nrxds4LnPbsQc/sWqf+YgiJNWu07WFiQ2msq5q7peDl+JM9+bcerGZNRPolQhyhS2BB9+yZvZs/Qe/6fq93gF4aM6cecKYtpXL0d1y7fYPGGWTg42WuNT2GTgn/uPWTq+NmEhUZojUnqMXWlcZM6TJg4lIlef1GxfD2CgoLZ+vcKnJwdtcaXdivO0uUzWblyIxXK1WX3zgOsXT+ffPlzq2Nu3rzDwAGjKVu6NjV/ac79ew/YtmNlvP8WdalZs/pMnjyK8eOnUdqtFoGBV9i9ew3O8dSnbJmSrF41h2XL1lGqdE3+3rGfLZuXUKDAxw+Xdna2nDp9jqFD/9B7/kmhywkqSeXh4cGiRYtYsWIFwcHBdO/enTdv3qhnZ7Zv3x5PT091fPfu3Xn69Cl9+/bl+vXr7N69mwkTJtCzZ89En1OhUqmM7kuNsjsm/VPgtgOrCbx4mVGDY2+qKhQKTgcdYMWidcyfuTRO/KzFk7CxteHX1h97Nlv3r+JK0DWGD4y9abrv5BZ2b9vPrKkL1TE7vNdxzPskUyfMSXRu2WzirjtJyNydf3E14Dp/DZ+trsuG82vZtmw76+ZsSHDfdWdWsXnxVrYs2ZZgXM/R3Slb3Y22FTokKTeATQXeJyk+zcx5RF+/yps5M2MLFArsV2/i3d9b+Xfj2jjxKerUx6ZpS5792g4SMVvLaf8xXo4exvszJ5OUF0CFi2+TvM+Gvcu45H+FcZ6Tgdjrc9R/F6sXb2TRrBUJ7ut94W9WLFzPyoXrdHbMTz16o/3DTnwOH92Kn28gAweMVp83+PopFsxfyfSp8+PEL1vxF3Z2tjRv+uvHOh3ZQmBgMP37Dtd6jlSpUvIwJJB6ddpy7OjpJOX3b5T2GYHxOXVyJxcuBNC3X2wuCoWCO7fPM2fuMiZPjvtvds2aedjZ2tKwkbu67OSJnQQEXKZnL80p9FmzZuLmjbOULFWDgIDLScoLIOr9wyTvk5DHFarq7FjpTx5J8j6zZ89m8uTJhISEULRoUf766y/c3GJHX6pUqUK2bNlYvny5Ov7MmTP0798ff39/MmbMSOfOnRk8eLDG0GZCpGcHWFpaULBIPk4e81GXqVQqTh3zoXgp7UMyxUoV5tQn8QDHD5/WiPc758/PtSvjmj62sSpToRTZc2XlxBHt02t1wcLSgtyFcuN7wk9dplKp8DvhR4HiuhnWsrC04JfGP7N3/X6dHC/hk1lg8VNuovx8P5apVERd9MUiv/bhIKsy5YkKvkzKXv1xWL+NtAuWYdOyLZgZ/u1uaWlBgSJ5OX3849MiVCoVZ46fo2jJQsnmmIk7ryVFixXkyJFTGuc9euQUpUsX07pPabfiHP0kHsD70AlKu2mPt7S0pEOnljx//pKgoGDdJR/PuYoXL4z34RPqMpVKxeHDJylTpoTWfcq4leDwJ/EABw4ejTdefNSrVy/u3btHZGQkZ8+eVTd0AEePHtVo6ADKli2Lj48P796949atWwwdOjTRDR1IYweAvaM9FhYWRIRpfqqNCHuCs4uT1n2cXZyICP8sPlwzfvSQidy8dhufSwe5HnKB5RvnMur3CZw74/f54XQmjUMazC3MeRb+TKP8WcQzHFx0M6RVoWY5UqZOyb5NB3RyvISYpU6DwtwC5XPN+iifPcPMXvuwlln69FhXrAxmZrwYPpi3a1di06Q5Nq3a6T3fL7F3SIuFhQVPwp9qlEeEP8XJRftQmSGOmRiO//27CQ/THFoNC4vA1dVZ6z6urk6EJSK+Vq1qPAoNIvxpMD17daJhvfY8faL5HtA1JycHLCws4gwVh4aFky6e+qRL50xoWLhGWVho/PVPTgw5jGkIBp+NGRwcjI+PD2XLliVv3rxcvXqVmTNnEhkZSdu2balWrVqC+0dGRsZZvKhSKVEoDN+Ou3dpRbGShfm1dR8e/vOI0uVKMGbSUEJDwjl17Kyh0/tq/2tZm7NHzsV7f8/QFAozlM+f83rmFFAqibl5nbeOztg2bcm/axI/pCcM5/jxM1QoWxdHR3vcO7Zk+apZVKvSOM4HTPH1VDqcjfkjMGiLsG/fPooWLcrAgQMpVqwY+/bto1KlSty8eZN79+5Ro0YNDh8+nOAxtC1mfP5vWJLyePbkGdHR0XE+BTu5OMb51PpBeFhEnJvwTs4f461TWDNweB/GD5+C9/5jXL1yg5WL17N723669HTXdkidePH0BTHRMdg7a/bi7J3seRr27Z+MXTO6ULxiMfas2/vNx0oM5csXqGKiMUurWR8ze3uUz55q3+fpE2Ie/gOfzNyKuX8PM0dHsDDs57tnT58THR2No7Nmr9TJ2SHOyIIhj5kYT/77d/P56IeLixOhoeFa9wkNjcAlEfFv3/7L7dv3OH/en149hhATHUN79+a6rcBnIiKeEh0djYurZn6uLs6ExFOfkJBwXF00e3EurvHXXxiOQRu7sWPHMmjQIJ48ecKyZcto3bo1Xbp04eDBg3h7ezNo0CAmTpyY4DE8PT158eKFxpY2iRM6oqKiuRQQTPlKH8eMFQoF5Sq54Xc+UOs+F88HasQDVKhSRh1vaWmBlZWlxlRZgJgYJWZ6vHcUHRXN9aDrFK/w8R6IQqGgeIViXPa78s3Hr9WiJs8jnnPG+zv1TKOjib5xHctin9wDUSiwLFqc6Cvab/JHXbmEefqMoPj4ydU8UyZinkRAdLS+M05QVFQ0lwOuUrZiKXWZQqGgTMVS+F8ISjbHTNx5o/C/eIkqVcppnLdylXKcO3dR6z7nzvpR+ZN4gKrVynPurPb4D8zMFFgncor514qKisLPL5BqVT8uQ1EoFFStWgEfH1+t+/ic9aVqNc1lK9V/rhRvfHJiasOYBm3sLl++TIcOHQBo3rw5r169omnTpurX27RpQ2Cg9sbmA2tra1KnTq2xfc0Q5uK5q2jZrjGNW9YjZ+7sjJ8yHFtbGzav3Q7A1LnjGTSijzp+2YI1VPq5HL/2aE+On7LR9/duFCpagJWL1wPw+tUbfE6ex3OMB27lS5IpS0aatKpP4xZ12b/bW1sKOrNp4RbqtvofNZv+QpZcWejv1YcUNinYtyF2QonnjN/5dUgndbyFpQU58+ckZ/6cWFha4pTeiZz5c5IhWwaN4yoUCmo1r8n+zQdRxny/d/i/WzeSonYdrKvXxDxzVux6e6BIYcO7A7G9y5SDhmLbsYs6/t2u7ShSpcauex/MMmbCsnQZbFu25d3OT2aYprDBPEcuzHPELhUxS5ce8xy5MHNO2gelr7F8/lqatW1IwxZ1yPFTNkZPHoKNrQ1b1+8EYOLs0XgM+zil2tLSgrwFc5O3YG4srSxxTe9M3oK5yZI9U6KPqS+zZy3BvWNLWrdpTO48OZk+cxy2trasXrUZgAWLpjBqzCB1/Ly5y6n+SyV69enMT7lz4Dm0L8WKF2LhgpUA2NraMHL0QEqVKkrmzBkoWrQgc+b9SfoM6di2LfFf5/K1ZsxcROfOrWnXrhl58+ZizuyJ2NnZsGJF7CzmZUtnMn78x1mWs2ctoWaNKvTr15U8eXIyYoQHJUoUZu68ZeoYe/u0FClSgHz5YpdX5M6dkyJFChj8vp5KqdDZ9iMw+D07xX+fvs3MzEiRIgVp0qRRv5YqVSpevHjxXfLYvX0/jk72eAzpgZOLE8GXrtGheQ8i/rvpnyFjOo1emt/5APr95smAYb0YOLw3d2/fp2u7fly/elMd07vLYH4f0ZcZC7xImzY1Dx88Zsofs1mzbJNe63Jk5zHSOKalw0B3HJztuXXlFoPbDeVZxHMAXDK6oFR+XHHi6OrI4gMfp4m37Naclt2a438mgP7NBqrLS1QsTrpMruxdv0+v+X/u/bEjvEmTFtv2nTCzdyD69k1eDhuE6r9JK+bOLhpDlsrwcF4OG4Rd157Yz1+KMiKCf7dv0VimYJk7D2kmz1T/nLJbLwDeHdjL66kJjyZ8q71/H8TBMS29f++Ks4sjwZeu06VlH/UEkwwZ06H65Pq4pHNm++E16p8792xH557tOHfKl/aNuiXqmPqydctunJwcGDq8P66uTgQFBtOkYQf1cH6mTBk0/t2cO+tH5479GDFyAKNGD+TWrbu0btmN4CvXAYiJiSF37py0btMYR0d7nj59jp9vILV+acHV4Bt6rQvApk07cHZyYNTIgaRL50xAwGXq1m2rnlSTObNmfc74XKBd+16MGfM748cN5sbNOzRp2pnLl6+pY+rVrcGSJdPVP69dMw+AseOmMm7cNL3XScQy6Dq7IkWK8Oeff1KrVi0ALl26RN68ebH4777KiRMncHd35/bt20k67tess0vOkrrOLrlL6jq75Oxr1tklZ0ldZ5fcJXWdXXKm63V290v+rLNjZbmg39EqXTBoz6579+4aX9FQsKDm47n27t37xdmYQgghku5HGX7UFYM2dt26dUvw9QkTJnynTIQQQhgzg9+zE0II8f1Jz04IIYTRM76nIifM8I8ZEUIIIfRMenZCCGGCZBhTCCGE0ZNnYwohhBBGJlE9ux07diT6gPXr1//qZIQQQnwfP8ozLXUlUY1dw4YNE3UwhUKhsUhcCCFE8qQ0sWHMRDV2nz+5XwghhPiRyAQVIYQwQaY2QeWrGrs3b95w7Ngx7t+/z/v3mg/17dOnTzx7CSGESC5k6cEXXLx4kf/973+8ffuWN2/e4ODgQEREBLa2tri4uEhjJ4QQItlJ8tKD/v37U69ePZ49e4aNjQ0+Pj7cu3ePEiVKMGXKFH3kKIQQQsdUKt1tP4IkN3b+/v4MGDAAMzMzzM3NiYyMJHPmzEyaNImhQ4fqI0chhBA6ZmrfVJ7kxs7S0hIzs9jdXFxcuH//PgBp0qThn3/+0W12QgghhA4k+Z5dsWLFOH/+PD/99BOVK1dm5MiRREREsGrVqjhfviqEECJ5MrV1dknu2U2YMIH06dMD8Mcff2Bvb0/37t0JDw9n4cKFOk9QCCGE7qlUCp1tP4Ik9+xKliyp/n8XFxf27dun04SEEEIIXZNF5UIIYYJ+lFmUupLkxi579uwoFPF3W2/fvv1NCQkhhNA/U7tnl+TGrl+/fho/R0VFcfHiRfbt28egQYN0lZcQQgihM0lu7Pr27au1fM6cOVy4cOGbExJCCKF/P8rEEl3R2Ze31q5dmy1btujqcEIIIfRInqDylTZv3oyDg4OuDieEEELozFctKv90gopKpSIkJITw8HDmzp2r0+SEEELoh0xQ+YIGDRpoNHZmZmY4OztTpUoV8ubNq9PkRKwHkU8NnYJOlfY1ni8D3uOQ3tAp6NTAFC6GTkGnDoQGGDqFZMvU7tklubEbPXq0HtIQQggh9CfJ9+zMzc0JCwuLU/7kyRPMzc11kpQQQgj9UqoUOtt+BEnu2animXoTGRmJlZXVNyckhBBC/36QSZQ6k+jG7q+//gJAoVCwePFiUqZMqX4tJiaG48ePyz07IYQQyVKiG7vp06cDsT27+fPnawxZWllZkS1bNubPn6/7DIUQQujcjzL8qCuJbuzu3LkDQNWqVdm6dSv29vZ6S0oIIYR+yWzMLzhy5Ig+8hBCCCH0JsmzMZs0acKff/4Zp3zSpEk0a9ZMJ0kJIYTQL6UOtx9Bkhu748eP87///S9Oee3atTl+/LhOkhJCCKFfKhQ6234ESW7sXr9+rXWJgaWlJS9fvtRJUkIIIYQuJbmxK1SoEBs2bIhTvn79evLnz6+TpIQQQuiXUqW77UeQ5AkqI0aMoHHjxty6dYtq1aoB4O3tzdq1a9m8ebPOExRCCKF7yh9k+FFXktzY1atXj+3btzNhwgQ2b96MjY0NRYoU4fDhw/IVP0IIIZKlJDd2AHXq1KFOnToAvHz5knXr1jFw4EB8fX2JiYnRaYJCCCF070eZWKIrX/3lrcePH8fd3Z0MGTIwdepUqlWrho+Pjy5zE0IIoSemtvQgST27kJAQli9fzpIlS3j58iXNmzcnMjKS7du3y+QUIYQQyVaie3b16tUjT548BAYGMmPGDB49esSsWbP0mZsQQgg9MbV1donu2e3du5c+ffrQvXt3fvrpJ33mJIQQQs9+lOFHXUl0z+7kyZO8evWKEiVK4ObmxuzZs4mIiNBnbkIIIYROJLqxK1OmDIsWLeLx48d07dqV9evXkyFDBpRKJQcPHuTVq1f6zFMIIYQOmdoElSTPxrSzs6NTp06cPHmSoKAgBgwYwMSJE3FxcaF+/fr6yFEIIYSOmdo9u69eegCQJ08eJk2axIMHD1i3bp2uchJCCCF06qsWlX/O3Nychg0b0rBhQ10cTgghhJ4pf4wOmc7opLETQgjxYzG1Z2N+0zCmEEII8SOQnp0QQpigH+SbeXRGenafaNe5BScu7uHqw3NsO7CaIsULJhj/v/q/cMhnO1cfnmPvic1UqV5B43VbOxvG/OnJ6aADBD84y4HTW2ndoZk+q6DWplMzjvju5NI/p9m8bwWFixWINzZXnhzMXjaJI747uRHuS4eurb75mLrWrlNzjvvtJviBD1v3r/ziuWvXr87BM1sJfuDD3uMb41wbJ2cHJs0aw5lLB7h8/zTLNswmW44s+qyChrRt6pLz8DJyB20n66bppCicO97YNI2qk/f6Ho0td9B2jRin3m3Ivm8Buf238tP5DWRe/gcpCufRcy1i1WlfhyWnlrL1+jam/j2N3EXir0vNVjX5c/OfrA/awPqgDYxf+4fW+DYebVl5YRVbrm9l/No/yJAtgz6roKFbN3euXzvDyxc3OXliJyVLFk0wvknjOgQFHuXli5v4+R6iVq1qGq83bFCb3bvX8PhREO8jH1CkcPJ4tKIsPTBRdRrWZNi4gcycvIC61VoSfOkaKzbNw9FJ+9cWFS9VhJmLJrJx9TbqVG3BwT1HWLBqBrnz5lLHDB83kErVytG/21Cql23EsvlrGPPnEKrXqqzXuvyv4S8MHevB7CkLafhzG4IvX2fpxtk4ONlrjbexScE/dx8yZdwswkK1PyggqcfUpToNazB03AD+mryAetVaE3z5Ois2zcUxnnMXL1WEmQu92LhmO3WrtuLAnqPMXzmN3HlzqmPmr5xOlmyZ6NquH3WrteLhg8es2jIfG9sUeq9Pqv9VwsWzCxGz13K3YW8ir94m85JxmDukiXefmFdvuFGujXq7VaWDxuvv7zwkdOw87tTrwb1Wg4h6GEbmZeMxt0+t17pUrFeRX0d0Yd2MtfSt04c7wXcYu3ocaRy116VQmUIc+/s4ni08GdhwAOGPwhm7ehyOro7qmCbdm1KvYz3meM5hQH0P3r19x9jV47C0ttRrXQCaNa3H5EkjGf/HdNzcahMYdIXdu1bj7OyoNb5MmRKsWjWHZcvXU9qtFjt27GPzpsUUyP/xg4adnS2nT51n6LAJes9fxE+hUqmSVW9WpVKhUHzbjdPsjkWSvM+2A6sJvHiZUYO9AFAoFJwOOsCKReuYP3NpnPhZiydhY2vDr617q8u27l/FlaBrDB84HoB9J7ewe9t+Zk1dqI7Z4b2OY94nmTphTqJzszBL2mjz5n0rCPS/zNghk9R1OR6wh1WLN7Dwr+UJ7nvEdycrFq5l+QLNpSTfcszPxaiS9llw6/6VBF68zOghf6rPfSpwHysXrWf+X8vixP+1eCK2tjb82rqvumzLvhUEX7rO8IF/kD1nFrzP/k3N8k24ce22+phnrxxiyh+z2bh6W6Jz2+OQPkl1Aci6aTrvgq4TOnYe/52cnMdX8GzVTp4u3BQnPk2j6rgM+40bJZsn+hxmdjbkvriF++6evD0TkOj9Br5L2r+9qX9P40bAdeaPnA/E/h6Xn13OzuW72Dw3bl3i5GlmxvqgDcwfOY/DWw4DsPLCKrYt3Ma2hVsBsE1ly2rfNcwYMJ3jO48nKb8DoYmvO8DJEzu54BtAv37D1fW5fes8c+cuY/KUuP9m16yei62dLY0adVCXnTi+g4DAy/Tq5akRmzVrJm5c96FUqRoEBF5JUl4A7yMfJHmfhGxO30Znx2r6eI3OjqUvya5nZ21tTXBw8Hc9p6WlBQWL5OPksY9fUaRSqTh1zIfipQpr3adYqcKcOqb5lUbHD5/WiPc758/PtSvjmt4FgDIVSpE9V1ZOHDmjh1rEsrS0oECRvJw+dk5dplKpOH38HMVKFko2x0zKuQsWycepY2c1zn3q2FmKxXNtipcsrBEPcOLIGYqVjI23srICIDLyvcYx379/T0m3ojquwWcsLUhRIBdvTvt/LFOpeHvaH5uieePdzczWhpxHlpPz2Aoyzh2BVa4EhlwtLUjbojYxL18TefWO7nL/jIWlBbkK5cL/pL+6TKVS4X/Sn7zF46/Lp6xtrDG3NOfV89gnMLlmSYeDi4PGMd++ess1/2vkLZG4Y34tS0tLihcvxOHDJ9RlKpWKw4dPUKZMca37uLmV0IgHOHjwGGXcSug1V11Q6XD7ERhsgoqHh4fW8piYGCZOnIijY+ywwbRp0xI8TmRkJJGRkRplKpUShSLx7bi9oz0WFhZEhD3RKI8Ie0LOn7Jr3cfZxYmI8M/iw5/g7OKk/nn0kIlMmD4Sn0sHiYqKQqlUMbT/GM6d8Ut0bkll75A2ti6f5fYk7Ak5c2VLNsdM9Lk/XJvwpxrlEeFPyPmT9nM7uTjFjQ97grNL7Hvq1o27PPznMYOG92bYgPH8+/ZfOnVrS4aM6XBxddJ2SJ2xsE+NwsKc6IhnGuXREc+xzZFZ6z6Rdx7weOh0Iq/dxSylLY6dm5B1w1Tu/K8b0aEfr4ldldJknD4YhY010eFP+afjMGKevdRbXVI7pMbcwpznEc81yp9HPCdTTu11+VwHz448DX2qbtzsne3/O4bm7+d5xHPSOut3yNzJyQELCwtCQ8M1ysPCIsiTJ5fWfdKlc44z9B8aFo6rq7Pe8hRfx2CN3YwZMyhSpAhp06bVKFepVAQHB2NnZ5eo4UwvLy/GjBmjUZYmhQv2tul0me5Xce/SimIlC/Nr6z48/OcRpcuVYMykoYSGhMfpeYjvJzo6mu4dBjBxxij8bx0nOjqaU8fOcvTgSfjGIXR9eOd/lXf+V9U/P7gYTI69C0jb8n9EzFylLn97NoA7DXphbp+atM1rkWGGJ/ea9Sfm6QtDpP1FTXs0o1L9Sng2H0JUZJSh0zE5P8rEEl0x2DDmhAkTePHiBSNGjODIkSPqzdzcnOXLl3PkyBEOHz78xeN4enry4sULjS2tjUuScnn25BnR0dE4uWjehHZycSQ8TPuEjfCwCJw+u2nt5Pwx3jqFNQOH92H88Cl47z/G1Ss3WLl4Pbu37adLT/ck5ZcUz54+j63LZ7k5JlAXQxwz0ef+cG2cNScKxf6un2jdJyIsIm68i2b8pYBg6lZtSZHsFSlToAYdW/QirUMa/rmn2/sin4t+9hJVdAwWn02usXBKS/RnvdH4DxLDuyu3sMqqeb9Q9W8kUfcf8y7gGiHDZkJMDGma1dRV6nG8fPqSmOgY0jql1ShP65SWZ+HPtO/0n0a/NaZp96aMaDucu1fvqss/7Jf2s99PWqe0PP/CMb9VRMRToqOj4/TKXFycCA0N07pPSEh4nNEAVxfnOL3D5Eip0N32NebMmUO2bNlIkSIFbm5unDt37ss7AevXr0ehUCT5iV0Ga+yGDBnChg0b6N69OwMHDiQq6us+2VlbW5M6dWqNLSlDmABRUdFcCgimfCU3dZlCoaBcJTf8zgdq3efi+UCNeIAKVcqo4y0tLbCyskSp1Pz8FBOjxMxMf7/2qKhoLgdcpWylUuoyhUJBuYqluHghKNkcMynnvhQQTLk416Y0F+O5Nn4XAilXqbRGWfnKZbh4IW78q1evefrkGdlyZKFQ0fwc3HtUp/nHERXNu8s3sSv7ySQqhQLbskX595PeW4LMzLDOk43oL/3xNzPDzEp/Mxijo6K5GXSTIuWLqssUCgVFyhflql/8dWnSrQkt+7RkVPuR3Ay8qfFa6P0QnoY9pWj5j78fm5Q25Cmah6u+ifz9fKWoqCj8/IKoWvXjMhWFQkHVqhXw8dF+6+HsWV+qVdVc1vLzzxXxOeur11x/dBs2bMDDw4NRo0bh5+dHkSJFqFmzJmFh2j9UfHD37l0GDhxIxYoVk3xOg05QKVWqFL6+voSHh1OyZEkuXbr0zTMxv9biuato2a4xjVvWI2fu7IyfMhxbWxs2r90OwNS54xk0oo86ftmCNVT6uRy/9mhPjp+y0ff3bhQqWoCVi9cD8PrVG3xOnsdzjAdu5UuSKUtGmrSqT+MWddm/21uvdVk6fzUt2jaiUYu65PwpG2Mne2Jja8OWdTsAmDR7DAOG91LHW1pakK9gbvIVzI2llSWu6VzIVzA3WbJnSvQx9WnJvNW0bNeIxi3qkfOn7IybMjT22qz7G4Apc8YxaPjHWbHLF6yjUrVydO7Rjhy5stH3964UKppffW0gdh2eW/kSZM6akeq1q7By8zwO7jnKyaM+cc6va0+XbSNN81qkbvQzVjkz4zqmJ2Y21rzYchCA9JMG4DyggzresWcrbMsXwzJzOqzz5yTDlIFYZnDh+cZ9AChsrHHycCdFkTxYZHDBukAu0k3oh4WrIy/3ntCWgs5sX7yNmq1qUq3pz2TKlZkeE3qSwjYFhzbG1sVjugfugz+OZDTp3pS2A9oxc9AMQh+EkdbZnrTO9qT4ZMnH30v+pkWflpT+xY2sebLiMX0AT8OecuaA/iZ2fTBz5kI6d2pFu7ZNyZs3F7Nne2FnZ8OKlRsAWLpkBuPHDVHHz5q9hBo1qtCv32/kyZOTEcM9KFGiMPPmLlfH2NunpUjh/OTLF7ueMHfunBQpnN/g9/WUKHS2JdW0adPo0qULHTt2JH/+/MyfPx9bW1uWLo078/2DmJgY2rRpw5gxY8iRI0eSz2nwJ6ikTJmSFStWsH79eqpXr05MTIxB8ti9fT+OTvZ4DOmBk4sTwZeu0aF5D/VEhwwZ02n00vzOB9DvN08GDOvFwOG9uXv7Pl3b9eP61Y+fVHt3GczvI/oyY4EXadOm5uGDx0z5YzZrln15Sva32LP9IA6O9vQd3A1nF0eCL12nc4vePPlQl0zp+HTFiUs6Z3Yc+bjU4Nde7fm1V3vOnrpA24ZdE3VMfdq9/QAOjvb0H9IdJxfH/65Nz4/XJpOWa9N1KAOG9mTgsF7cvX2fbu09uH711sc6uzozbNyA2OHQ0Ai2btjF7E+WiOjTqz3HMXdIjXOfdpg72xMZfJt/Oo8k5slzACzTO8Mn9TFPnZL04/ti7myP8sUr3l2+yb0WA3h/65/YgBgl1jkykabRMMzt0xDz7CXvgq5zv/Ug3t+8r9e6nNh5gjQOaWjr0RZ7Z3tuX7nNyHYj1ZNWnDM4o1R+fK/9r+3/sLS2ZOiCYRrHWTt9DWunrwVgy7zNpLBJQW+v3tiltuPKhSuMbDfiu9zX27R5J07OjowcOZB06ZwJCLhC3XrtCPtvuD5z5owa7zUfH1/at+/FmDG/M27sYG7evEPTZr9y+co1dUzdur+wZPF09c9r1sQuORk3bhrjxic8AU+fdDmLUttEQWtra6ytrePEvn//Hl9fXzw9Py7NMDMzo3r16pw5E/8HmrFjx+Li4kLnzp05cSLpH+KS1Tq7Bw8e4OvrS/Xq1bGzs/vq43zNOrvkLKnr7JK7pK6zS86+Zp1dcpbUdXbJXVLX2SVnul5ntzpDW50d6+ZvueJMFBw1ahSjR4+OE/vo0SMyZszI6dOnKVu2rLr8999/59ixY5w9G3fy3smTJ2nZsiX+/v44OTnRoUMHnj9/zvbt2xOdY7L6K5opUyYyZcr05UAhhBDfRJdf8ePp6RlnOZm2Xt3XePXqFe3atWPRokU4OX390qBk1dgJIYT4PnQ5vhLfkKU2Tk5OmJubExoaqlEeGhpKunRxl4zdunWLu3fvUq9ePXXZh6FkCwsLrl27Rs6cOePs97lk9wQVIYQQxsvKyooSJUrg7f1xop5SqcTb21tjWPODvHnzEhQUhL+/v3qrX78+VatWxd/fn8yZE/cAA+nZCSGECTLkZA0PDw/c3d0pWbIkpUuXZsaMGbx584aOHTsC0L59ezJmzIiXlxcpUqSgYEHNb6D58DCSz8sTIo2dEEKYIF3es0uqFi1aEB4ezsiRIwkJCaFo0aLs27cPV1dXAO7fv6/z9cjJajamrshszORNZmMmXzIbM/nS9WzMJZl0Nxuz84PVOjuWvhjXX1EhhBCJYjwfORNHGjshhDBBptbYyWxMIYQQRk96dkIIYYJUxnV79ouksRNCCBMkw5hCCCGEkZGenRBCmCBT69lJYyeEECbI6BZYf4EMYwohhDB60rMTQggTZMjHhRmCNHZCCGGCTO2enQxjCiGEMHrSsxNCCBNkaj07aeyEEMIEyWxMIYQQwshIz04IIUyQzMYUQghh9Eztnp0MYwohhDB60rMTQggTZGoTVKSxE0IIE6Q0sebOKBu7V1H/GjoFnTJXGNdoc5QyxtAp6Ez1iPuGTkGnro8sb+gUdMrR85KhUxDJhFE2dkIIIRJmahNUpLETQggTZFqDmDIbUwghhAmQnp0QQpggGcYUQghh9EztCSoyjCmEEMLoSc9OCCFMkKyzE0IIYfRMq6mTYUwhhBAmQHp2QghhgmQ2phBCCKNnavfsZBhTCCGE0ZOenRBCmCDT6tdJYyeEECbJ1O7ZyTCmEEIIoyc9OyGEMEGmNkFFGjshhDBBptXUyTCmEEIIEyA9OyGEMEGmNkFFGjshhDBBKhMbyJRhTCGEEEZPenZCCGGCZBhTCCGE0TO1pQcyjCmEEMLoSc9OCCFMkGn166SxE0IIkyTDmCasU5c2+AUd5kFYEPsPb6JYicIJxtdvWIszF/bxICyI42d2Ur1GZY3XZ82bSMTL6xrbhq2L9VkFtY6/tuZ8oDf3QgPY672BYsULJRhfr2FNTp7fw73QAI6e3sHPv1SKE/NT7hysXDeXG/fPc+eRH/uObCJjpvT6qoKGzl3a4H/pCI/CL3Hw8GaKf+HaNGhYCx/ffTwKv8RJn11xrs2nps4Yy9NXN+jWo4OOs46fe+eWnPHfz81Hvuw8uJaixQsmGF+nQQ2O+uzg5iNfDp3cSrXqFTVef/D0ktatW++O+qwGABaFq5Ci0x/Y9JqNdcshmLlmizfWuqkHtv0WxNmsG/TSiFPYp8OqXg9sus/ApudfWLf0RJHKXs81idW1a3uuXj3Js2fXOH58OyVLFkkwvnHj/+Hv782zZ9c4f34/NWtWVb9mYWHB+PFDOH9+PxERwdy+fY7Fi6eRPr2LvqshPiON3X8aNv4f4yZ4MnnibKpVbMjloKts2roEJycHrfGlShdj4dJprFm5iaoVGrJn9yFWrp1D3nw/acQdOnic/LnKqbffOnnovS4NGtdmzIQhTP1zDr9UaszlS9dYv21xvHUpWboY85dMZe2qzVSv2Ii9uw+xfO1sjbpkzZ6ZHfvXcuPGbRrVbU+V8g2YPmkuke8i9V6fRo3/x3ivoUyaOJuqFRpy6VIwm7ctjbc+pd2KsWjZdNas3EyVCg3Ys+sQq9fNJd9n1wagTr1fKFmqKI8ehei7Gmr1GtVi5PjfmT5pHrWrNuPKpWus3rwAx3jqU6J0UeYsmsT6NduoVaUZ+/YcZvHqv8iTL5c6pljeyhqbR6/hKJVK9uw4qNe6mOcuiWWlpkT57Obd2j9QhT/AulEfsEmlNT5y53zeLhyk3v5dORqVMoboG77qGEUaJ1I0H4TqWQjvNk/l3eqxRJ/bjSo6Wq91AWjatC5//jmcP/6YSdmydQkMDGbHjlU4OztqjS9TpgQrVsxixYqNlClTh507D7Bx40Ly588NgK2tDUWLFmTixL8oW7YOLVt2JXfuHGzatETvdfkSpQ63H4FCpVIZXV/WKXXuJO+z//AmLvoFMWTgWAAUCgWBwcdZtGAVf01fGCd+8bIZ2NrZ0Lp5V3XZPu+NXAoMZmD/UUBszy5NmtS0b93jK2sSy1yRtM8ke703cNHvEkMHjQNi63LxylGWLFzNrOmL4sQvXDYNW1tb2rbopi7bc2g9l4Ku8nv/0QAsWDqVqKhoenUd/PUV+U+UMiZJ8QcPb8bPL5DBn1yboKux12bmtLjXZsnyGdja2dKq2W/qsgOHNxEUGMyAfiPVZenTu3LwyGaaNuzI+s2LmD93BfPnLk9SbraW1kmKB9h5cC0BfpcYPniCuj7ngw6xbNFa5syM+0dw7pIp2Nra0KFVT3XZjgNruBx0Dc8BY7WeY/GqmaRMaUfLRr8mKbfrI8snKd665RCUIXeJOrr+vxIFKX71Itr/CNEX9n9xf4tiP2NZph7/Lvodot8DYFX7V1DG8H7/siTloo2j574kxR8/vh1f30D69499nygUCm7e9GHevOVMmTIvTvyqVbOxtbWlSZNO6rJjx7YREHCFPn2GaT1HiRKFOXlyJ7lzl+Wffx4lOrd//72XpLp8ya/ZmursWIvvbtbZsfRFenaApaUlRYoW4NiR0+oylUrFsaOnKVW6qNZ9SpYuyrGjpzXKjnifpGTpYhpl5SuUJvjWGXx89zF52mjsHdLqOn0NlpaWFC5agBNHNety/OgZSpYqqnWfEqWKcjxOXU6p4xUKBdVrVOHWzbus37qYyzdPsdd7A7Xr/KyvaqhZWlpSpFgBjd/1x2tTTOs+pUoX07iWAIcPndC4lgqFgnmLJjNr5mKuXr2pl9y1sbS0oFCR/Jw45qMuU6lUnDjmQ/FS2ofLSpQqwoljZzTKjh0+TYl44p2cHfm5RiXWr96qu8S1MTPHzCULyn+CPylUobx/FbP0ORJ1CIsC5Ym5fkHd0IEC8+yFUD4LxbpRH2x+m4x1yyGY50x4KFEXLC0tKVasEIcPn1SXqVQqDh8+SenSxbXu4+ZWnCNHTmqUHTx4HDc37fEAqVOnQqlU8vz5S90kLhIlWTV2b968YdmyZQwbNozZs2fz5MmTL+4TGRnJy5cvNTaVKmkda0dHeywsLAgPj9AoDw+LwMXVWes+Lq5OhIdpxoeFReDi6qT++fChE/Ts+juN67kzZuQUylUozYYtizEz09+v3eFDXcI0f3fh4Zq5fSq2LvHHOzk7kjKVHX36d+HIoRM0b9SZPbsOsXT1LMqWL6WfivxHfW3CPr82T3B1if/ahGm9Nh/j+3r8Rkx0DAvmrdB90glQX59wzd93RPiTeK+Ps4sTEZ9fn7AInF20xzdrWZ83r9+yd9ch3SQdD4VNShRm5qjevtIoV719icIuzRf3N3PNhplTRqIvfdJY2KZCYZUCy1K1iLl7mXfbZhJz8yJWdbthljHuMLQuOTnFXhtt75106bS/11xdnbXGu8bzd8Pa2prx4z3ZuHEHr1691k3iX8nUhjENOhszf/78nDx5EgcHB/755x8qVarEs2fPyJ07N7du3WLcuHH4+PiQPXv2eI/h5eXFmDFjNMpsrBywtdY+xv49bduyW/3/wVeuc+XyNXwDvSlf0S3OJ/Xk7EPjvG/PYRbMjW0cLgddpVTpYrh3asmZU+cNmV6SFSlagK7d3alaoaGhU9GLFm0asW3TLiIj33852IDMC5ZHGf4AZehddZlCoQAg5lYA0Re9AYgOf4BZ+pxYFK7E+4c3DJGqTlhYWLB69RwUCkW8Q5zfkzwb8zu6evUq0f/ddPb09CRDhgzcu3ePc+fOce/ePQoXLsywYQm/KTw9PXnx4oXGZmOVtFlbT548Izo6GmdnzU/Kzi5OhIWGa90nLDTuJ2sXFyfCQiO0xgPcu/sPERFPyZEjS5LyS4qnH+riotnYOzvHn1tsXeKPf/rkGVFRUVz/bLjv+vVbep+Nqb42Lp9fG0dCw+K/Ni5ar01sfNlypXB2diQw+Bhhz4IJexZMlqyZGDdhCP6XjuinIv9RX5/PJjw4OTvGe33CwyJw+vz6uMQdWQAoXaY4uXLnYO0qPQ9hAqp/X6NSxqCw1ZyMorBNjerNi4R3trDCIncpoi+finvMmBiUTx9rlj8LQZFK+wQeXYmIiL022t47ISHa32uhoeFa40M/+7thYWHBmjVzyJIlI3XrtjF4r84UJZthzDNnzjB69GjSpIkd/kiZMiVjxozh5MmTCe5nbW1N6tSpNTZFEid0REVFEeB/mUpVyqrLFAoFlSqX5fw5f637XDjnT6XKZTXKKlctx4VzF+M9T/oMrjg4pCU0nn84uhAVFUWg/2UqVtasS8XKZbhw3l/rPr7n/TXi4b+6/BcfFRWFv98lcv6k2cPOmTMbD5Jwg/1rREVFEXDxssbvWqFQULlyOc7H87s+f+6ixrUEqFKtvPpabli/nYpl6lK5XH319uhRCLNmLqZpo05ajqg7UVHRBAVcoUIlN3WZQqGgQmU3/M4HaN3H93wAFSqV0SirWKUsvlriW7ZtTMDFywRfvqbbxLVRxqAMu49Z5nyfFCowy5wX5ePbCe5qnrsEmFsQffVs3GOG3sXM3lWjWJHWBdXLpzpKXLuoqCguXgyiatWPk3QUCgVVq5bn3Dk/rfucPetHlSqak3p+/rkiZ89+jP/Q0OXMmZ06ddrw9OlzveSfVKY2jGnwxu7DsMW7d+9In16zl5AxY0bCw/XXMHxq3uxltHNvTovWjfgpd06mTB+Dra0N61ZvAWDOgkkMHzVAHb9g3gqqVa9Ij16dyPVTDn737E3RYgVZvHA1AHZ2towe9zslShUhc5aMVKxcltXr5nHn9j0Oe5/Qa13mz1lOG/dmNG/VkJ9y52DS9NHY2tmoJyzMmj+RYaM+LoFYOG8VVatXoFuvjuT6KTsDh/SiSLECLF24Rh0z568lNGhcm7buzciWIwudurShRu2qLFu8Vq91AZg7eyntO7SgZetG5M6Tk6kzxmJra8PaVbHXZu6CSYwYrXltfq5ekZ69O/FT7hwM/nBtFqwC4NnT5wQH39DYoqOiCQuN4OaNO3qvz8K5K2nVvilNW9YnV+4ceE0dgY2tDRvWbgdgxtwJDBnRTx2/ZMFqqvxcnt96upPzp+x4DO5B4aIFWP7Z7z5lKjvqNqjBuv9+L99DtN8hLApWwDxfGRT26bD8uTUKSyuir8ROELKq0QHL8g3j7GdRoDwxt/zh3Zu4x/Q9gHnukpgXrIAijTMWRapgnqMw0YFH9VsZ4K+/FtOxY0vatGlCnjy5+OuvP7C1tWXlyk0ALF48jbFjf1fHz5mzjBo1KtO3bxdy587JsGH9KF68EPPnxw73W1hYsHbtPIoXL0zHjn0xNzfH1dUZV1dnLC0t9V6fhChVKp1tPwKDP0Hl559/xsLCgpcvX3Lt2jUKFvy4uPbevXs4On6fe2/bt+7B0cmBIUP74OLqzKWgYJo36ayeSJApU3qUyo+fYc6fu0jXzgMYOqIfw0Z5cPvWXdq37snV4Nh7CjExMeQvmIcWrRuRJk0qQh6HcfTwKbzGz+D9+yi91uXvrXtxdHTg96G9cXF15nJQMK0ad1HXJWOmDCiVH9+gF85dpPuvAxkyvB9DR/bnzq27dGjdS10XgL27DvF7/9H08fiN8X8O49aNO3Ru14dzPto/8erStv+ujeewvrHXJjCYZo0/uTaZM2j8gzt39iK/dfJg6Mj+DB81gNu37tK2VQ+Cg5PH/Z6d2/bh6GjPQM9eOLs4ceXSVdo160aE+vpovtd8z/nT67fB/D60N4OH9+XO7Xv82rYP14I1h5UbNK6NQqHg7y17vltdYq5fIMomJZZl66OwTY0y4gGR2/+C/yatKFI78PmDqRT2rphn/Il3W2doP+Ytf957r8GyVC0UVVqgehbK+10LUD66pd/KAJs378LJyZGRIz1wdXUmMPAKDRq0V09CyZw5g8a18fHxpUOHPowaNZAxYwZx8+Zdmjf/jStXrgOQIUM66tWrAcC5c5rLIGrUaMGJEz6I78Og6+w+n1hSpkwZatasqf550KBBPHjwgHXr1iXpuF+zzi45S+o6u+QuqevskrOvWWeXnCV1nV1yl9R1dsmZrtfZtc3aWGfHWn1P//eIv5VBe3ajRo1K8PXJkyd/p0yEEMK0yLMxhRBCCCNj8Ht2Qgghvj9TW2cnjZ0QQpigH2XJgK7IMKYQQgijJz07IYQwQTJBRQghhDAy0rMTQggTJBNUhBBCGD2ZoCKEEELo2Zw5c8iWLRspUqTAzc2Nc+fOxRu7aNEiKlasiL29Pfb29lSvXj3BeG2ksRNCCBOkUql0tiXVhg0b8PDwYNSoUfj5+VGkSBFq1qxJWFiY1vijR4/SqlUrjhw5wpkzZ8icOTM1atTg4cOHiT6nQZ+NqS/ybMzkTZ6NmXzJszGTL10/G7NBlro6O9bf93clKd7NzY1SpUoxe/ZsAJRKJZkzZ6Z3794MGTLki/vHxMRgb2/P7Nmzad++faLOaVx/RYUQQnx3kZGRvHz5UmOLjIzUGvv+/Xt8fX2pXr26uszMzIzq1atz5syZRJ3v7du3REVF4eCQ+C/0lcZOCCFMkC6/vNXLy4s0adJobF5eXlrPGxERQUxMDK6uml/Q6+rqSkhISKJyHzx4MBkyZNBoML9EZmMKIYQJ0uXSA09PTzw8PDTKrK31M8Q/ceJE1q9fz9GjR0mRIkWi95PGTgghxDextrZOdOPm5OSEubk5oaGhGuWhoaGkS5cuwX2nTJnCxIkTOXToEIULF05SjjKMKYQQJkiJSmdbUlhZWVGiRAm8vb0/5qJU4u3tTdmyZePdb9KkSYwbN459+/ZRsmTJJNdXenZCCGGCDDkR38PDA3d3d0qWLEnp0qWZMWMGb968oWPHjgC0b9+ejBkzqu/7/fnnn4wcOZK1a9eSLVs29b29lClTkjJlykSdUxo7IYQQ31WLFi0IDw9n5MiRhISEULRoUfbt26eetHL//n3MzD4OPM6bN4/379/TtGlTjeOMGjWK0aNHJ+qc0tgJIYQJMvTjwnr16kWvXr20vnb06FGNn+/evfvN55PGTgghTJCpPQhaJqgIIYQwetKzE0IIE2RqX94qjZ0QQpggI3wscoJkGFMIIYTRk56dEEKYIBnGFEIIYfRMbTamUTZ2z9+9MXQKOmVuZlyjzTFKQ6/w0Z2XkW8NnYJOpf49ad9Lltz9++iEoVMQyYRRNnZCCCESpjSxCSrS2AkhhAkyraZOZmMKIYQwAdKzE0IIEySzMYUQQhg9U2vsZBhTCCGE0ZOenRBCmCBTe1yYNHZCCGGCZBhTCCGEMDLSsxNCCBMkjwsTQghh9Eztnp0MYwohhDB60rMTQggTZGoTVKSxE0IIEyTDmEIIIYSRkZ6dEEKYIBnGFEIIYfRMbemBDGMKIYQwetKzE0IIEyTfVC6EEMLoyTCmEEIIYWSkZyeEECZIhjGFEEIYPRnGFEIIIYyM9OyEEMIEmdowpvTsPtG9mzs3r/vw+uUtTp/cSamSRROMb9KkLpeCjvH65S0u+h2idq1qcWJGjxrIP/f8ePXiJvv3ridXrux6yl5Tt67uXLt2mhfPb3Di+A5KfqEujRvXITDgCC+e38D3wkFq1ayq8XqDBrXYvWsNjx4GEvnuHwoXzq/H7OMypmsDxlUfY6oLwLotO6nRxJ3iVevTqks/gq5cizc2KjqaeUvXUKtZR4pXrU9j9x6c9LmgEfPmzVsmzpjPL43dKVG1AW26ehAUHP8xvxeVDv/7EUhj959mzeozZfIoxo2fRim3WgQEXmHP7jU4OztqjS9bpiRrVs1h2bJ1lCxdkx079rNl8xIKFMijjhk0sAe9enaiR68hlKtQjzdv37Jn1xqsra31WpemTesxadII/vhjBm5l/kdQ0BV27VwVb13KlCnBqpWzWb58PW5utdmxcz+bNi0mf/6PdbGzs+XU6XMMGz5Br7lrY0zXxtjqY0x1Adh76BiTZi2ke6c2bFo6izy5stPVYzhPnj3XGj9r4Qo2/b2Xof278/fqBTRv+D/6eo4j+PpNdczIiTM5c/4iXiMHsm3VPMqVLk6XvkMJDY/Qe33ERwqVET762sIqY5L3OX1yJ+cvBNC333AAFAoFd2+fZ87cZUyaPCdO/No187CztaVBI3d12akTO/EPuEzPXkMA+OeeH9NnLGDa9AUApE6dikcP/On0a382btyR6NzMzZL2meTE8R34+gbQr/8IdV1u3TzH3HnLmDJlbpz41avmYmdnQ6PGHdVlx4/9TWDgZXr1HqoRmzVrJq5fO0Op0jUJDLySpLw+iFEqkxSfnK/N1zCm+iT3uvz76ESS4lt16UfBvLkZNqAHAEqlkuqN2tO6aX1+bdc8TnzV+m34zb0lrZrUU5f1Gzoea2sr/hz1O+8iI3H7pTF/TRxF5XKl1THNO/WmQpmS9PnNPc4x42PplCNJdfmSnE7FdXasWxF+OjuWvkjPDrC0tKR48cJ4H/74D0OlUuF9+CRlypTQuk8ZtxIa8QAHDh5Vx2fPnoX06V3xPnxS/frLl684d+4iZdy0H1MXYutSiMOfnFelUnH4yIl4z+tWprhGPMDBQ8dw02OeiWVM1waMqz7GVBeAqKgorly7QZlSRdVlZmZmlClZlIBLwVr3eR8VhZWVlUaZtbUVFwMvAxATHUNMjBJrK8s4MX7/xRiKDGN+R35+fty5c0f986pVqyhfvjyZM2emQoUKrF+//rvk4eTkgIWFBWGhmsMKYWHhpHN11rpPunTOhIaFa5SFhkao49O5uvxX9llMWATp0rnoKvU4PtTl89zCQiNwja8urs6EhkUkOv57MqZrA8ZVH2OqC8Cz5y+JiVHi6GCvUe7oYE/E02da9ynvVoKV67dy75+HKJVKTp/zw/vYacKfPAVih/+LFMzH/OXrCAt/QkxMDDv3Hybg0lUiIp7qtT5Ck0Ebu44dO3Lr1i0AFi9eTNeuXSlZsiTDhg2jVKlSdOnShaVLlyZ4jMjISF6+fKmxGeHIrBAiGRrStytZM2ekXuvfKFalHhOmzaVhnV8wU3z80+o1YiCoVFRr2JbiVeuzZtPf1K5eGUUSb0/omkql1Nn2IzDo0oMbN27w008/ATB37lxmzpxJly5d1K+XKlWKP/74g06dOsV7DC8vL8aMGaNRpjBLicI8daLziIh4SnR0NC6uThrlLi7OhHz2CfODkJBwXF00P726ujqp40NCw/4rcyYkJOxjjIsT/gH6G774UJfPc3NxdYrzafmDkNBwXF2cEh3/PRnTtQHjqo8x1QXAPm1qzM3NePJZL+7J02c4fdbb+8DBPi1/TRxJZOR7nr98iYuTI9PnLSVThnTqmCyZMrB8zmTe/vuON2/e4uzkwIARXhoxhmBq32dn0I8Wtra2RETEDoE8fPiQ0qVLa7zu5uamMcypjaenJy9evNDYFGapkpRHVFQUfn6BVKtaQV2mUCioVrUCPj6+WvfxOetLtWoVNMqq/1xJHX/nzn0ePw7VOGaqVCkpXboYPme1H1MXYusSRNWq5dVlCoWCqlUqxHvesz5+GvEAP1eryFk95plYxnRtwLjqY0x1gdh7kPnz/MTZC/7qMqVSyVlff4oUzJfgvtbWVrg6OxEdE8PBo6eoWrFsnBhbmxQ4Oznw4uUrTp/zpVrFMrqugkiAQXt2tWvXZt68eSxevJjKlSuzefNmihQpon5948aN5MqVK8FjWFtbx5mSrFAokpzL9JmLWLZkOr5+gZw/f5E+vbtgZ2fD8hUbAFi2dCaPHj1m2PCJAMyatYTD3pvp368re/YeokXzBpQoUZhuPX5XH/OvWYsZ6tmHGzdvc/fuP4wZPYhHj0L5++/9Sc4vKWb+tYgli6fh6xfIhfP+9O7dGTs7G1au3AjAkiXTefQohBEj/gRg9pwlHDq4iX59f2PvXm+aNa9PiRKF6dFziPqY9vZpyZw5AxnSuwKQO3dOIPbeir57gMZ0bYytPsZUF4D2LRox7I+pFMj7EwXz52H1xu38+y6ShnV+AcBz3BRcnBzp3z125nLg5auEhj8h7085CAt/wtylq1GpVHRq01R9zFNnfVGpVGTLkon7Dx4xdc4SsmfJRMM6NfRen4SY2u0egzZ2f/75J+XLl6dy5cqULFmSqVOncvToUfLly8e1a9fw8fFh27Zt3yWXTZt24OzkwOiRA0mXzpmAgMvUqduWsP8mbmTJnAHlJ1Pmz/hcoG37Xowd8zvjxw3mxs07NGnamcuXPy4WnTxlLnZ2tsyfO4m0aVNz6tR56tRrS2RkpF7rsnnzTpydHBg5cgDpXJ0JCLhCvfrt1HXJnDkjSuXHN7qPjy/t3XszZvQgxo79nZs379Ks2a9c+WQxbd26v7B40TT1z2tWxy5hGDd+GuPHT9drfYzp2hhbfYypLgC1q1fm2fMXzF68moinT8n7U07mTx2nHsZ8HBqG2ScfpiPfv2fWohU8eBSCrY0NFcuWwmvEIFKnSqmOefX6DTPmLyM0PII0qVPxS+UK9OnqjqWFYR9gZWrDmAZfZ/f8+XMmTpzIzp07uX37NkqlkvTp01O+fHn69+9PyZIlk3zMr1lnl5wldZ1dcpfUdXZCfK2krrNLznS9zi6TQ0GdHevB00s6O5a+GLyx0wdp7JI3aezE9yKNXfwy2hfQ2bEePjPsmsHEkAdBCyGECZIHQQshhBBGRnp2Qghhgn6Ux3zpijR2QghhgoxwukaCZBhTCCGE0ZOenRBCmCBTW2cnjZ0QQpggGcYUQgghjIz07IQQwgSZ2jo7aeyEEMIEyTCmEEIIYWSkZyeEECZIZmMKIYQwejKMKYQQQhgZ6dkJIYQJktmYQgghjJ6pPQhahjGFEEIYPenZCSGECZJhTCGEEEZPZmMKIYQQRkZ6dkIIYYJkgooQQgijp1KpdLZ9jTlz5pAtWzZSpEiBm5sb586dSzB+06ZN5M2blxQpUlCoUCH27NmTpPNJYyeEEOK72rBhAx4eHowaNQo/Pz+KFClCzZo1CQsL0xp/+vRpWrVqRefOnbl48SINGzakYcOGXLp0KdHnVKiM8C6lhVVGQ6egU+ZmxvWZJEapNHQKwkT8++iEoVPQGUunHLo9ng7/Tka9f5ikeDc3N0qVKsXs2bMBUCqVZM6cmd69ezNkyJA48S1atODNmzfs2rVLXVamTBmKFi3K/PnzE3VO4/orKoQQIlFUOtyS4v379/j6+lK9enV1mZmZGdWrV+fMmTNa9zlz5oxGPEDNmjXjjddGJqgIIYT4JpGRkURGRmqUWVtbY21tHSc2IiKCmJgYXF1dNcpdXV25evWq1uOHhIRojQ8JCUl0jkbZ2EUnsUv9NSIjI/Hy8sLT01PrBf3RGFN9jKkuIPVJzn7kuujy7+To0aMZM2aMRtmoUaMYPXq0zs7xrYzynt338PLlS9KkScOLFy9InTq1odP5ZsZUH2OqC0h9kjNjqsu3SErP7v3799ja2rJ582YaNmyoLnd3d+f58+f8/fffcfbJkiULHh4e9OvXT102atQotm/fTkBAQKJylHt2Qgghvom1tTWpU6fW2OLr6VpZWVGiRAm8vb3VZUqlEm9vb8qWLat1n7Jly2rEAxw8eDDeeG2MchhTCCFE8uXh4YG7uzslS5akdOnSzJgxgzdv3tCxY0cA2rdvT8aMGfHy8gKgb9++VK5cmalTp1KnTh3Wr1/PhQsXWLhwYaLPKY2dEEKI76pFixaEh4czcuRIQkJCKFq0KPv27VNPQrl//z5mnyy5KleuHGvXrmX48OEMHTqUn376ie3bt1OwYMFEn1Mau69kbW3NqFGjfrib0vExpvoYU11A6pOcGVNdvrdevXrRq1cvra8dPXo0TlmzZs1o1qzZV59PJqgIIYQwejJBRQghhNGTxk4IIYTRk8ZOCCGE0ZPGTgghhNGTxi6Jjh8/Tr169ciQIQMKhYLt27cbOqWv5uXlRalSpUiVKhUuLi40bNiQa9euGTqtrzZv3jwKFy6sXtRatmxZ9u7da+i0dGLixIkoFAqNJ0j8SEaPHo1CodDY8ubNa+i0vsnDhw9p27Ytjo6O2NjYUKhQIS5cuGDotEQ8pLFLojdv3lCkSBHmzJlj6FS+2bFjx+jZsyc+Pj4cPHiQqKgoatSowZs3bwyd2lfJlCkTEydOxNfXlwsXLlCtWjUaNGjA5cuXDZ3aNzl//jwLFiygcOHChk7lmxQoUIDHjx+rt5MnTxo6pa/27Nkzypcvj6WlJXv37uXKlStMnToVe3t7Q6cm4iHr7JKodu3a1K5d29Bp6MS+ffs0fl6+fDkuLi74+vpSqVIlA2X19erVq6fx8x9//MG8efPw8fGhQIECBsrq27x+/Zo2bdqwaNEixo8fb+h0vomFhQXp0qUzdBo68eeff5I5c2aWLVumLsuePbsBMxJfIj07ofbixQsAHBwcDJzJt4uJiWH9+vW8efMmSc/PS2569uxJnTp14nyX14/oxo0bZMiQgRw5ctCmTRvu379v6JS+2o4dOyhZsiTNmjXDxcWFYsWKsWjRIkOnJRIgPTsBxD6ItV+/fpQvXz5Jj+BJboKCgihbtizv3r0jZcqUbNu2jfz58xs6ra+yfv16/Pz8OH/+vKFT+WZubm4sX76cPHny8PjxY8aMGUPFihW5dOkSqVKlMnR6SXb79m3mzZuHh4cHQ4cO5fz58/Tp0wcrKyvc3d0NnZ7QQho7AcT2IC5duvRD30cByJMnD/7+/rx48YLNmzfj7u7OsWPHfrgG759//qFv374cPHiQFClSGDqdb/bp0H/hwoVxc3Mja9asbNy4kc6dOxsws6+jVCopWbIkEyZMAKBYsWJcunSJ+fPnS2OXTMkwpqBXr17s2rWLI0eOkClTJkOn802srKzIlSsXJUqUwMvLiyJFijBz5kxDp5Vkvr6+hIWFUbx4cSwsLLCwsODYsWP89ddfWFhYEBMTY+gUv0natGnJnTs3N2/eNHQqXyV9+vRxPkDly5fvhx6aNXbSszNhKpWK3r17s23bNo4ePWqUN9iVSmWcL5X8Efz8888EBQVplHXs2JG8efMyePBgzM3NDZSZbrx+/Zpbt27Rrl07Q6fyVcqXLx9nmc7169fJmjWrgTISXyKNXRK9fv1a49PonTt38Pf3x8HBgSxZshgws6Tr2bMna9eu5e+//yZVqlSEhIQAkCZNGmxsbAycXdJ5enpSu3ZtsmTJwqtXr1i7di1Hjx5l//79hk4tyVKlShXn3qmdnR2Ojo4/5D3VgQMHUq9ePbJmzcqjR48YNWoU5ubmtGrVytCpfZX+/ftTrlw5JkyYQPPmzTl37hwLFy5M0verie9MJZLkyJEjKiDO5u7ubujUkkxbPQDVsmXLDJ3aV+nUqZMqa9asKisrK5Wzs7Pq559/Vh04cMDQaelM5cqVVX379jV0Gl+lRYsWqvTp06usrKxUGTNmVLVo0UJ18+ZNQ6f1TXbu3KkqWLCgytraWpU3b17VwoULDZ2SSIB8xY8QQgijJxNUhBBCGD1p7IQQQhg9aeyEEEIYPWnshBBCGD1p7IQQQhg9aeyEEEIYPWnshBBCGD1p7IRIpA4dOtCwYUP1z1WqVDHIN4cfPXoUhULB8+fPv/u5hfhRSWMnfngdOnRAoVCgUCjUD4IeO3Ys0dHRej3v1q1bGTduXKJipYESwrDk2ZjCKNSqVYtly5YRGRnJnj176NmzJ5aWlnh6emrEvX//HisrK52c0xi+5FYIUyE9O2EUrK2tSZcuHVmzZqV79+5Ur16dHTt2qIce//jjDzJkyECePHmA2O+La968OWnTpsXBwYEGDRpw9+5d9fFiYmLw8PAgbdq0ODo68vvvv/P5k/U+H8aMjIxk8ODBZM6cGWtra3LlysWSJUu4e/cuVatWBcDe3h6FQkGHDh2A2G9l8PLyInv27NjY2FCkSBE2b96scZ49e/aQO3dubGxsqFq1qkaeQojEkcZOGCUbGxvev38PgLe3N9euXePgwYPs2rWLqKgoatasSapUqThx4gSnTp0iZcqU1KpVS73P1KlTWb58OUuXLuXkyZM8ffqUbdu2JXjO9u3bs27dOv766y+Cg4NZsGABKVOmJHPmzGzZsgWAa9eu8fjxY/V37Hl5ebFy5Urmz5/P5cuX6d+/P23btuXYsWNAbKPcuHFj6tWrh7+/P7/++itDhgzR169NCONl4AdRC/HN3N3dVQ0aNFCpVCqVUqlUHTx4UGVtba0aOHCgyt3dXeXq6qqKjIxUx69atUqVJ08elVKpVJdFRkaqbGxsVPv371epVCpV+vTpVZMmTVK/HhUVpcqUKZP6PCqV5rcQXLt2TQWoDh48qDXHD9+W8ezZM3XZu3fvVLa2tqrTp09rxHbu3FnVqlUrlUqlUnl6eqry58+v8frgwYPjHEsIkTC5ZyeMwq5du0iZMiVRUVEolUpat27N6NGj6dmzJ4UKFdK4TxcQEMDNmzdJlSqVxjHevXvHrVu3ePHiBY8fP8bNzU39moWFBSVLlowzlPmBv78/5ubmVK5cOdE537x5k7dv3/LLL79olL9//55ixYoBEBwcrJEHQNmyZRN9DiFELGnshFGoWrUq8+bNw8rKigwZMmBh8fGtbWdnpxH7+vVrSpQowZo1a+Icx9nZ+avO/zVfdvv69WsAdu/eTcaMGTVes7a2/qo8hBDaSWMnjIKdnR25cuVKVGzx4sXZsGEDLi4upE6dWmtM+vTpOXv2LJUqVQIgOjoaX19fihcvrjW+UKFCKJVKjh07RvXq1eO8/qFnGRMToy7Lnz8/1tbW3L9/P94eYb58+dixY4dGmY+Pz5crKYTQIBNUhMlp06YNTk5ONGjQgBMnTnDnzh2OHj1Knz59ePDgAQB9+/Zl4sSJbN++natXr9KjR48E18hly5YNd3d3OnXqxPbt29XH3LhxIwBZs2ZFoVCwa9cuwsPDef36NalSpWLgwIH079+fFStWcOvWLfz8/Jg1axYrVqwAoFu3bty4cYNBgwZx7do11q5dy/Lly/X9KxLC6EhjJ0yOra0tx48fJ0uWLDRu3Jh8+fLRuXNn3r17p+7pDRgwgHbt2uHu7k7ZsmVJlSoVjRo1SvC48+bNo2nTpvTo0YO8efPSpUsX3rx5A0DGjBkZM2YMQ4YMwdXVlV69egEwbtw4RowYgZeXF/ny5aNWrVrs3r2b7NmzA5AlSxa2bNnC9u3bKVKkCPPnz2fChAl6/O0IYZwUqvjuuAshhBBGQnp2QgghjJ40dkIIIYyeNHZCCCGMnjR2QgghjJ40dkIIIYyeNHZCCCGMnjR2QgghjJ40dkIIIYyeNHZCCCGMnjR2QgghjJ40dkIIIYyeNHZCCCGM3v8B5yzmoVMGwrwAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# @title confusion matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "def confusion(dataloader, model, loss_fn, verbose=True):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    y_true, y_pred = torch.empty(0, device=device), torch.empty(0, device=device)\n",
        "    # test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in dataloader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            pred = model(x)\n",
        "            # print(pred[0])\n",
        "            # loss = loss_fn(pred, y)\n",
        "            # test_loss += loss_fn(pred, y).item()\n",
        "            # print((pred.argmax(1)==y).sum().item(), pred.argmax(1).tolist(), y.tolist())\n",
        "            y_pred = torch.cat((y_pred, pred.argmax(1)), 0)\n",
        "            y_true = torch.cat((y_true, y), 0)\n",
        "            # correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    # test_loss /= num_batches\n",
        "    # correct /= size\n",
        "    # should not use weighted rand sampler for test?\n",
        "    # if verbose: print(f\"Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f}\")\n",
        "    # return correct, test_loss\n",
        "    return y_true, y_pred\n",
        "    # y_true, y_pred = y_true(), y_pred.cpu()\n",
        "    # cm = confusion_matrix(y_true=y_true, y_pred=y_pred)\n",
        "    # cmn = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "    # fig, ax = plt.subplots(figsize=(5,5))\n",
        "    # sns.heatmap(cmn, annot=True, fmt='.2f', xticklabels=['1','2','3','4','5','6',], yticklabels=['1','2','3','4','5','6',])\n",
        "    # plt.ylabel('Actual')\n",
        "    # plt.xlabel('Predicted')\n",
        "    # plt.show()\n",
        "\n",
        "y_true, y_pred = confusion(test_loader, model, loss_fn)\n",
        "\n",
        "y_true, y_pred = y_true.cpu(), y_pred.cpu()\n",
        "cm = confusion_matrix(y_true=y_true, y_pred=y_pred)\n",
        "cmn = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "fig, ax = plt.subplots(figsize=(5,5))\n",
        "sns.heatmap(cmn, annot=True, fmt='.2f', xticklabels=['1','2','3','4','5','6',], yticklabels=['1','2','3','4','5','6',])\n",
        "plt.ylabel('Actual')\n",
        "plt.xlabel('Predicted')\n",
        "plt.show()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "icF25Bbpp6gR"
      },
      "outputs": [],
      "source": [
        "# @title collect wrong preds\n",
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "# path_rong = '/content/wrong/'\n",
        "# combi = torch.combinations(torch.arange(0,6), with_replacement=False)\n",
        "# for i, j in combi:\n",
        "#     fol_rong = path_rong +str(i.item())+str(j.item())\n",
        "#     if not os.path.exists(fol_rong):\n",
        "#         os.makedirs(fol_rong)\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
        "\n",
        "rongs=[[[] for i in range(6)] for j in range(6)]\n",
        "# print(rongs)\n",
        "# print(rongs[0][0])\n",
        "\n",
        "\n",
        "t=0\n",
        "c=0\n",
        "for cls in range(1,7):\n",
        "    img_dir = '/content/gsv70kg/0'+str(cls)\n",
        "    for filename in os.listdir(img_dir):\n",
        "        # name = os.path.splitext(filename)[0]\n",
        "        img_file=os.path.join(img_dir, filename)\n",
        "        image = Image.open(img_file).convert(\"RGB\")\n",
        "        with torch.no_grad():\n",
        "            x = transform(image).to(device).unsqueeze(0)\n",
        "            y = torch.tensor(cls-1, device=device)\n",
        "            pred = model(x)\n",
        "\n",
        "        p = pred.argmax(1)\n",
        "        # print(\"p, y\", p, y)\n",
        "        if p != y:\n",
        "            # print(filename)\n",
        "            c+=1\n",
        "            rongs[y][p].append(filename)\n",
        "\n",
        "        # plt.figure(figsize=(4, 1.25)) # plt.figure(figsize=(16, 5))\n",
        "        # plt.imshow(image)\n",
        "        # plt.show()\n",
        "\n",
        "        # torch.save(tensor, path_rong +str(i.item())+str(j.item()))\n",
        "        # # torch.save(tensor, path_rong +str(i.item())+str(j.item())+'.pt')\n",
        "        # torch.load('file.pt')\n",
        "\n",
        "        t+=1\n",
        "        # if t >=5: break\n",
        "\n",
        "print(c,t,c*100/t)\n",
        "\n",
        "\n",
        "\n",
        "def confusion(dataloader, model, loss_fn, verbose=True):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    y_true, y_pred = torch.empty(0, device=device), torch.empty(0, device=device)\n",
        "    # test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in dataloader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            pred = model(x)\n",
        "            # print(pred[0])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            # loss = loss_fn(pred, y)\n",
        "            # test_loss += loss_fn(pred, y).item()\n",
        "            print((pred.argmax(1)==y).sum().item(), pred.argmax(1).tolist(), y.tolist())\n",
        "            y_pred = torch.cat((y_pred, pred.argmax(1)), 0)\n",
        "            y_true = torch.cat((y_true, y), 0)\n",
        "            # correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    # test_loss /= num_batches\n",
        "    # correct /= size\n",
        "    # should not use weighted rand sampler for test?\n",
        "    # if verbose: print(f\"Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f}\")\n",
        "    # return correct, test_loss\n",
        "    return y_true, y_pred\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print(rongs)\n",
        "\n",
        "[print([len(s) for s in l]) for l in rongs]\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "plt.axis('off')\n",
        "\n",
        "combi = torch.combinations(torch.arange(0,6), with_replacement=False)\n",
        "for i, j in combi:\n",
        "    print(i,j)\n",
        "    c=0\n",
        "    images=torch.empty(0)\n",
        "    for filename in rongs[i][j]:\n",
        "    # for filename in rongs[5][0]:\n",
        "        # print(filename)\n",
        "        cls=filename[1]\n",
        "        img_dir = '/content/gsv70kg/0'+str(cls)\n",
        "        img_file=os.path.join(img_dir, filename)\n",
        "        image = Image.open(img_file)#.convert(\"RGB\")\n",
        "        image = transform(image).unsqueeze(0)\n",
        "        images=torch.cat((images,image),0)\n",
        "\n",
        "        c+=1\n",
        "        if c>15: break\n",
        "    imshow(torchvision.utils.make_grid(images,nrow=4))\n",
        "\n",
        "\n",
        "\n",
        "# def imshow(img):\n",
        "#     img = img / 2 + 0.5  # unnormalize\n",
        "#     npimg = img.numpy()\n",
        "    # plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    # plt.show()\n",
        "\n",
        "# trainiter = iter(train_loader)\n",
        "# images, labels = next(trainiter)\n",
        "# images=trs(images)\n",
        "# imshow(torchvision.utils.make_grid(images,nrow=4))\n",
        "\n"
      ],
      "metadata": {
        "id": "y-Cjw51AedI7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ayRePbr_rq9F"
      },
      "outputs": [],
      "source": [
        "\n",
        "# pytorch release gpu ram after training\n",
        "# https://discuss.pytorch.org/t/free-all-gpu-memory-used-in-between-runs/168202/2\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# correct, test_loss = test(test_loader, model, loss_fn)\n",
        "# train_lst.extend(train_ls)\n",
        "# test_lst.append(test_loss)\n",
        "# acc_lst.append(correct)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "q_3Cttn1qHcc"
      },
      "outputs": [],
      "source": [
        "# @title trash\n",
        "\n",
        "model.eval()\n",
        "count=0\n",
        "i=138\n",
        "rong_lst=[]\n",
        "\n",
        "while count<20:\n",
        "    img,label=test_data[i]\n",
        "    pred=model(img.unsqueeze(0).to(device))\n",
        "    pred_probab = nn.Softmax(dim=1)(pred)\n",
        "    y_pred = pred_probab.argmax(1)\n",
        "    if y_pred.item() != label:\n",
        "        print(\"pred: \",y_pred.item(),\", actual: \",label)\n",
        "        # plt.imshow(np.transpose(img, (1, 2, 0)))\n",
        "        # plt.show()\n",
        "        imshow(img)\n",
        "        rong_lst.append(img)\n",
        "        count+=1\n",
        "    i+=1\n",
        "\n",
        "\n",
        "# 20/137 wrong\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OJs2bVL0rXnG"
      },
      "outputs": [],
      "source": [
        "print(i)\n",
        "# imshow(torchvision.utils.make_grid(images,nrow=4))\n",
        "# print(torch.stack(rong_lst).shape)\n",
        "# print(len(rong_lst))\n",
        "# print(rong_lst[0].shape)\n",
        "imshow(torchvision.utils.make_grid(torch.stack(rong_lst),nrow=4))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}