{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eisbetterthanpi/vision/blob/main/resnet_ViT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## resnet_ViT"
      ],
      "metadata": {
        "id": "Cac8nXhrnCsy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "2kQGpnynh2oQ"
      },
      "outputs": [],
      "source": [
        "# @title download\n",
        "# original 10k\n",
        "# # https://drive.google.com/file/d/1NkCNecLpFG3i7bo3Vl9RQSwzBpSRQ29q/view?usp=sharing\n",
        "# !gdown 1NkCNecLpFG3i7bo3Vl9RQSwzBpSRQ29q -O gsv.zip\n",
        "# !unzip /content/gsv.zip -d /content\n",
        "# import shutil\n",
        "# shutil.rmtree('/content/google_street_view/meta_data', ignore_errors=True) # delete the meta_data folder\n",
        "\n",
        "# # clip cleaned\n",
        "# # https://drive.google.com/file/d/1-xcHyVAMeTkY7SUdUyQRVAn_FFXPSDGB/view?usp=share_link\n",
        "# !gdown 1-xcHyVAMeTkY7SUdUyQRVAn_FFXPSDGB -O gsv.zip\n",
        "# !unzip /content/gsv.zip -d /\n",
        "# !rm -R /content/gsv/.ipynb_checkpoints\n",
        "# !rm -R /content/gsv/01/.ipynb_checkpoints\n",
        "# !rm -R /content/gsv/02/.ipynb_checkpoints\n",
        "# !rm -R /content/gsv/03/.ipynb_checkpoints\n",
        "# !rm -R /content/gsv/04/.ipynb_checkpoints\n",
        "# !rm -R /content/gsv/05/.ipynb_checkpoints\n",
        "# !rm -R /content/gsv/06/.ipynb_checkpoints\n",
        "\n",
        "# # gsv 70k\n",
        "# # https://drive.google.com/file/d/1-7ZC29k4VxXQkpnOuLfj7Ag_SFTM4LV8/view?usp=share_link\n",
        "# !gdown 1-7ZC29k4VxXQkpnOuLfj7Ag_SFTM4LV8 -O gsv.zip\n",
        "# !unzip /content/gsv.zip -d /\n",
        "\n",
        "# # # !ls\n",
        "# !ls -a /content/gsv70k\n",
        "# !rm -R /content/gsv70k/.ipynb_checkpoints\n",
        "# # # !rm -R /content/gsv/06/.ipynb_checkpoints\n",
        "\n",
        "# # 70k+gmap\n",
        "# # https://drive.google.com/file/d/1-CZp7TbhJLeRQpbKQCyT8ofGg89Yt137/view?usp=sharing\n",
        "!gdown 1-CZp7TbhJLeRQpbKQCyT8ofGg89Yt137 -O gsv.zip\n",
        "!unzip /content/gsv.zip -d /\n",
        "!rm -R /content/gsv70kg/.ipynb_checkpoints\n",
        "!rm -R /content/gsv70kg/01/.ipynb_checkpoints\n",
        "!rm -R /content/gsv70kg/02/.ipynb_checkpoints\n",
        "!rm -R /content/gsv70kg/03/.ipynb_checkpoints\n",
        "!rm -R /content/gsv70kg/04/.ipynb_checkpoints\n",
        "!rm -R /content/gsv70kg/05/.ipynb_checkpoints\n",
        "!rm -R /content/gsv70kg/06/.ipynb_checkpoints\n",
        "\n",
        "# # https://bestasoff.medium.com/how-to-fine-tune-very-large-model-if-it-doesnt-fit-on-your-gpu-3561e50859af\n",
        "!pip install bitsandbytes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hEUffQ24mkRY",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title torch augment\n",
        "# https://github.com/facebookresearch/vicreg/blob/main/augmentations.py\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "class TrainTransform(object):\n",
        "    def __init__(self):\n",
        "        # self.transform = transforms.RandomApply([transforms.Compose([\n",
        "        self.transform = transforms.Compose([\n",
        "                transforms.RandomPerspective(distortion_scale=0.3, p=0.5), # me\n",
        "                transforms.RandomResizedCrop((400,640), scale=(0.7, 1.0), ratio=(0.8, 1.25), interpolation=transforms.InterpolationMode.BICUBIC),\n",
        "                transforms.RandomHorizontalFlip(p=0.5), # 0.5\n",
        "                transforms.Lambda(lambda x : torch.clamp(x, 0., 1.)), # clamp else ColorJitter will return nan https://discuss.pytorch.org/t/input-is-nan-after-transformation/125455/6\n",
        "                transforms.RandomApply([transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1)], p=0.8,), # brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1)], p=0.8\n",
        "                transforms.RandomGrayscale(p=0.2), # 0.2\n",
        "                # # transforms.RandomChoice(transforms.ColorJitter , transforms.RandomGrayscale(p=1.)\n",
        "                transforms.RandomApply([transforms.GaussianBlur(kernel_size=5, sigma=(0.1, 2.0)),], p=1.0),\n",
        "                # # transforms.RandomSolarize(threshold=130, p=0.5)\n",
        "                # transforms.RandomErasing(p=1., scale=(0.1, 0.11), ratio=(1,1), value=(0.485, 0.456, 0.406)),\n",
        "                transforms.RandomErasing(p=1., scale=(0.1, 0.11), ratio=(1,1), value=(0.5, 0.5, 0.5)),\n",
        "                # transforms.ToTensor(), # ToTensored at dataset level, no need to ToTensor again\n",
        "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), # normalised at dataset level. default 0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225\n",
        "                ])\n",
        "            # ], p=1.)\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        dims = len(sample.shape)\n",
        "        if dims==3: x1 = self.transform(sample) # same transforms per minibatch\n",
        "        elif dims==4: x1 = transforms.Lambda(lambda x: torch.stack([self.transform(x_) for x_ in x]))(sample) # diff transforms per img in minibatch\n",
        "        # x1 = self.transform(sample)\n",
        "        return x1\n",
        "\n",
        "trs=TrainTransform()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "yjix6KhUqxY1"
      },
      "outputs": [],
      "source": [
        "# @title data (old)\n",
        "# without oversampling\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
        "\n",
        "dir='/content/gsv70kg'\n",
        "\n",
        "data = datasets.ImageFolder(dir, transform=transform)\n",
        "# data = datasets.ImageFolder(dir, transform=None)\n",
        "torch.manual_seed(0)\n",
        "train_data, test_data = torch.utils.data.random_split(data, [.9,.1])\n",
        "\n",
        "batch_size = 16 # 64\n",
        "num_batches=int(np.ceil(len(train_data)/batch_size))\n",
        "\n",
        "# train_loader = torch.utils.data.DataLoader(train_data, sampler=train_sampler, batch_size=batch_size, num_workers=4, pin_memory=True)\n",
        "# test_loader = torch.utils.data.DataLoader(test_data, sampler=test_sampler, batch_size=batch_size, num_workers=4, pin_memory=True)\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, num_workers=4, pin_memory=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, num_workers=4, pin_memory=True)\n",
        "del data, train_data, test_data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pqw5n--6WYEG",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title data weighted\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# dataset has PILImage images of range [0, 1], transform them to Tensors of normalized range [-1, 1]\n",
        "# transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
        "# transform = transforms.Compose(transforms.ToTensor())\n",
        "\n",
        "dir='/content/gsv70kg'\n",
        "\n",
        "# data = datasets.ImageFolder(dir, transform=transform)\n",
        "data = datasets.ImageFolder(dir, transform=None)\n",
        "# torch.manual_seed(0)\n",
        "\n",
        "# random.seed(0)\n",
        "np.random.seed(0)\n",
        "torch.manual_seed(0)\n",
        "torch.cuda.manual_seed_all(0)\n",
        "\n",
        "\n",
        "# split data manually so that can work with weighted random sampler\n",
        "# train_data, test_data = torch.utils.data.random_split(data, [.85,.15])\n",
        "# https://www.scaler.com/topics/pytorch/how-to-split-a-torch-dataset/\n",
        "data_size = len(data)\n",
        "indices = np.arange(data_size)\n",
        "np.random.shuffle(indices)\n",
        "split_index = int(np.floor(0.9 * data_size))\n",
        "# split_index = int(np.floor(1. * data_size))\n",
        "train_idx, test_idx = indices[:split_index], indices[split_index:]\n",
        "# train_idx, test_idx = indices[:split_index], indices[split_index:split_index*2]\n",
        "train_data = torch.utils.data.Subset(data, train_idx)\n",
        "# train_data, _ = torch.utils.data.random_split(train_data, [.1,.9])\n",
        "test_data = torch.utils.data.Subset(data, test_idx)\n",
        "targets = np.array(data.targets)\n",
        "train_targets = targets[train_idx]\n",
        "test_targets = targets[test_idx]\n",
        "\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "class DatasetWrap(Dataset):\n",
        "    def __init__(self, dataset, transform=None):\n",
        "        super(DatasetWrap, self).__init__()\n",
        "        self.dataset = dataset\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        x, y = self.dataset[index]\n",
        "        if self.transform:\n",
        "            x = self.transform(x)\n",
        "        return x, y\n",
        "\n",
        "# dataset wrapper in order to apply transforms to train data only\n",
        "# train_data = DatasetWrap(train_data, TrainTransform()) # apply data augmentation to train dataset only\n",
        "# train_data = DatasetWrap(train_data, transform) # apply transform during training to use gpu\n",
        "train_data = DatasetWrap(train_data, transforms.ToTensor()) # apply transform during training to use gpu\n",
        "test_data = DatasetWrap(test_data, transform)\n",
        "\n",
        "# use batch size 16 for resnet 152/ vit with grad accumulation\n",
        "# can use batch size 64 for inception v3 without grad accumulation?\n",
        "batch_size = 16 # 64/16\n",
        "grad_acc = 4\n",
        "num_batches=int(np.ceil(len(train_data)/batch_size))\n",
        "\n",
        "# oversampling\n",
        "# https://stackoverflow.com/questions/62319228/number-of-instances-per-class-in-pytorch-dataset\n",
        "from collections import Counter\n",
        "class_count = torch.tensor(list(Counter(data.targets).values()))\n",
        "weights=1./class_count\n",
        "# weights=sum(class_count)/class_count\n",
        "# print(weights)\n",
        "# https://pytorch.org/docs/stable/data.html#torch.utils.data.WeightedRandomSampler\n",
        "\n",
        "train_weight = weights[train_targets]\n",
        "test_weight = weights[test_targets]\n",
        "train_sampler = torch.utils.data.WeightedRandomSampler(train_weight, len(train_weight))\n",
        "test_sampler = torch.utils.data.WeightedRandomSampler(test_weight, len(test_weight))\n",
        "# train_loader = torch.utils.data.DataLoader(train_data, sampler=train_sampler, batch_size=batch_size, num_workers=4, pin_memory=True)\n",
        "# test_loader = torch.utils.data.DataLoader(test_data, sampler=test_sampler, batch_size=batch_size, num_workers=4, pin_memory=True)\n",
        "train_loader = torch.utils.data.DataLoader(train_data, sampler=train_sampler, batch_size=batch_size, pin_memory=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, sampler=test_sampler, batch_size=batch_size, pin_memory=True)\n",
        "del data, train_data, test_data\n",
        "\n",
        "\n",
        "# test oversampling: occurence of each class should be roughly equal\n",
        "# c=0\n",
        "# print(len(test_loader))\n",
        "# # for batch, (x, y) in enumerate(train_loader):\n",
        "# for batch, (x, y) in enumerate(test_loader):\n",
        "#     print(torch.bincount(y)) # torch count number of elements with value in tensor\n",
        "#     c+=1\n",
        "#     if c>5: break\n",
        "\n",
        "# import matplotlib\n",
        "# matplotlib.rcParams['figure.dpi'] = 300\n",
        "# def imshow(img): # display img from torch tensor\n",
        "#     img = img / 2 + 0.5  # unnormalize\n",
        "#     plt.axis('off')\n",
        "#     npimg = img.numpy()\n",
        "#     plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "#     plt.show()\n",
        "\n",
        "# trainiter = iter(train_loader)\n",
        "# images, labels = next(trainiter)\n",
        "# images=trs(images)\n",
        "# imshow(torchvision.utils.make_grid(images,nrow=4))\n",
        "# del trainiter\n",
        "\n",
        "# print(labels)\n",
        "\n",
        "# testiter = iter(test_loader)\n",
        "# images, labels = next(testiter)\n",
        "# imshow(torchvision.utils.make_grid(images,nrow=4))\n",
        "# del testiter\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "7oYDr8kuA5Bl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "525f3c0a-4717-4e40-a895-632ededdec09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet152-f82ba261.pth\" to /root/.cache/torch/hub/checkpoints/resnet152-f82ba261.pth\n",
            "100%|██████████| 230M/230M [00:03<00:00, 64.2MB/s]\n"
          ]
        }
      ],
      "source": [
        "# @title resnet\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "# # https://pytorch.org/vision/0.12/models.html#id10\n",
        "model = models.resnet152(weights='DEFAULT') # 18 34 50 101 152\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Sequential( # og (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
        "    nn.Linear(num_ftrs, 6, bias=False),\n",
        "    nn.Softmax(dim=1),\n",
        "    )\n",
        "\n",
        "# print(model)\n",
        "\n",
        "# # model.mods = [module for k, module in model._modules.items()]\n",
        "# # modules = [module for k, module in model._modules.items()]\n",
        "\n",
        "# torch._dynamo.config.suppress_errors = True\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# model = model.to(device)\n",
        "model = torch.compile(model.to(device))\n",
        "# model = torch.compile(model.to(device),mode='reduce-overhead')\n",
        "# model = torch.compile(model.to(device),mode='max-autotune') # max needs grad ckpt\n",
        "\n",
        "# resnet152 batch16 compile gradacc4 nogradckpt lr3e-6,3e-5\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "rJw9_Ort2Sek"
      },
      "outputs": [],
      "source": [
        "# @title vit\n",
        "# https://arxiv.org/pdf/2010.11929.pdf\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "# https://pytorch.org/vision/main/models/vision_transformer.html\n",
        "# https://pytorch.org/vision/main/models/generated/torchvision.models.vit_b_16.html#torchvision.models.vit_b_16\n",
        "# model = models.vit_l_16(weights='DEFAULT') # small vit_b_16 vit_b_32 vit_l_16 vit_l_32 vit_h_14 big\n",
        "# # VisionTransformer(image_size, patch_size, num_layers, num_heads, hidden_dim, mlp_dim)\n",
        "# num_ftrs = model.heads.head.in_features\n",
        "# # num_ftrs = model.heads[-1].in_features\n",
        "# model.heads = nn.Sequential(\n",
        "#     # nn.Dropout(0.2),\n",
        "#     nn.Linear(num_ftrs, 6, bias=False),\n",
        "#     nn.Softmax(dim=1),\n",
        "#     )\n",
        "\n",
        "\n",
        "!pip install timm\n",
        "# https://github.com/huggingface/pytorch-image-models/issues/908\n",
        "import timm\n",
        "# model = timm.create_model('vit_base_patch16_224', pretrained=True)\n",
        "model = timm.create_model('vit_base_patch16_224', img_size=(400, 640), pretrained=True)\n",
        "# [print(x) for x in timm.list_models('vit*',pretrained=True)]\n",
        "# https://huggingface.co/google/vit-base-patch16-224\n",
        "# https://github.com/huggingface/pytorch-image-models/blob/main/timm/models/vision_transformer.py\n",
        "# vit_base_patch16_224 compile,no ckpt # patch_size=16, embed_dim=768, depth=12, num_heads=12\n",
        "# vit_base_patch16_384\n",
        "# vit_large_patch16_224 explodesgpu # patch_size=16, embed_dim=1024, depth=24, num_heads=16\n",
        "# vit_large_patch16_384 # patch_size=16, embed_dim=1024, depth=24, num_heads=16\n",
        "\n",
        "# or fine tune huge\n",
        "# vit_large_patch14_224 # patch_size=16, embed_dim=1024, depth=24, num_heads=16\n",
        "# vit_large_patch16_384\n",
        "\n",
        "\n",
        "num_ftrs = model.head.in_features\n",
        "model.head = nn.Sequential(\n",
        "    nn.Linear(num_ftrs, 6, bias=False),\n",
        "    nn.Softmax(dim=1),\n",
        "    )\n",
        "# model.set_grad_checkpointing()\n",
        "\n",
        "# print(model.patch_embed.grid_size) # (25, 40)\n",
        "# print(model.pos_embed.shape) # [1, 1001, 768]\n",
        "# https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py\n",
        "\n",
        "# print(model)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# model = model.to(device)\n",
        "model = torch.compile(model.to(device))\n",
        "# model = torch.compile(model.to(device),mode='reduce-overhead')\n",
        "# model = torch.compile(model.to(device),mode='max-autotune')\n",
        "\n",
        "# vit_base_patch16_224 batch16 maxcompile nockpt gradacc lr1e-5,1e-4\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "-oKYpG8n2fBI"
      },
      "outputs": [],
      "source": [
        "# @title inception\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "# # https://pytorch.org/vision/0.12/models.html#id10\n",
        "model = models.inception_v3(pretrained=True)\n",
        "# https://discuss.pytorch.org/t/inception-v3-is-not-working-very-well/38296/16\n",
        "# https://colab.research.google.com/github/CaoCharles/Deep-Learning-with-PyTorch/blob/master/2_Inception.ipynb\n",
        "model.aux_logits = False\n",
        "num_ftrs = model.fc.in_features # 2048\n",
        "model.fc = nn.Sequential( # og: (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
        "    nn.Linear(num_ftrs, 6, bias=False),\n",
        "    nn.Softmax(dim=1),\n",
        "    )\n",
        "# pytorch \"inception\" v3 \"gradient checkpointing\" https://github.com/jianweif/OptimalGradCheckpointing\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# model = model.to(device)\n",
        "model = torch.compile(model.to(device))\n",
        "# model = torch.compile(model.to(device),mode='max-autotune')\n",
        "\n",
        "# inception batch64 compile nogradacc nogradckpt lr3e-6,3e-5\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "vEZCFg5YSS9J"
      },
      "outputs": [],
      "source": [
        "# @title try\n",
        "\n",
        "# # check model's input and output dimensions are correct\n",
        "# X = torch.rand(64, 3, 32, 32, device=device)\n",
        "X = torch.rand(64, 3, 400, 640, device=device)\n",
        "# X = torch.rand(16, 3, 224, 224, device=device)\n",
        "model.train()\n",
        "\n",
        "# 224x224\n",
        "# 16x16 / 32x32 patch\n",
        "# -> 14x14=196 7x7=49 seq length\n",
        "# 400x640 -> 25x40=1000 seq length\n",
        "\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "with torch.cuda.amp.autocast():\n",
        "    logits = model(X)\n",
        "\n",
        "# modules = [module for k, module in model._modules.items()]\n",
        "# for i,x in enumerate(modules):\n",
        "#     print(i,x)\n",
        "\n",
        "# logits = checkpoint_sequential(functions=modules, segments=1, input=X)\n",
        "\n",
        "print(logits.shape)\n",
        "# print(logits[0])\n",
        "# print(logits[0].argmax(1))\n",
        "\n",
        "# pred_probab = nn.Softmax(dim=1)(logits)\n",
        "# y_pred = pred_probab.argmax(1)\n",
        "# print(y_pred)\n",
        "# del X, logits\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()\n"
      ],
      "metadata": {
        "id": "97phwgdcdJ85"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uT9m-J1BUWyz",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title wandb\n",
        "# https://docs.wandb.ai/quickstart\n",
        "!pip install wandb\n",
        "import wandb\n",
        "wandb.login() # 487a2109e55dce4e13fc70681781de9f50f27be7\n",
        "run = wandb.init(\n",
        "    project=\"resnet50teacher\",\n",
        "    config={\n",
        "        # \"model\": \"resnet adamw 3e-6\",\n",
        "        \"model\": \"vit\",\n",
        "        # \"model\": \"inception\",\n",
        "        # \"optim\": \"adamw\",\n",
        "        # \"learning_rate\": lr,\n",
        "        # \"epochs\": epochs,\n",
        "    })\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fsealXK3OPQa",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title train test function\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "# https://github.com/prigoyal/pytorch_memonger/blob/master/models/optimized/resnet_new.py\n",
        "from torch.utils.checkpoint import checkpoint, checkpoint_sequential\n",
        "\n",
        "trs=TrainTransform() # for image augmentation during train time\n",
        "# train function with automatic mixed precision\n",
        "def strain(dataloader, model, loss_fn, optimizer, scheduler=None, verbose=True):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    loss_list = []\n",
        "    for batch, (x, y) in enumerate(dataloader):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        with torch.cuda.amp.autocast(): # automatic mixed percision\n",
        "            x = trs(x) # image augmentation during train time to use gpu\n",
        "            pred = model(x) # default\n",
        "            # modules = [module for k, module in model._modules.items()]\n",
        "            # pred = checkpoint_sequential(functions=modules, segments=1, input=x) # gradient checkpointing for resnet and inception only\n",
        "            # # pred = checkpoint_sequential(functions=model.mods, segments=1, input=x)\n",
        "            # print(\"train\",pred[0])\n",
        "            # print((pred.argmax(1)==y).sum().item(), pred.argmax(1).tolist(), y.tolist())\n",
        "            loss = loss_fn(pred, y)\n",
        "        scaler.scale(loss).backward()\n",
        "        if ((batch + 1) % grad_acc == 0) or (batch + 1 == len(dataloader)): # gradient accumulation\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            optimizer.zero_grad()\n",
        "            if scheduler is not None:\n",
        "                scheduler.step()\n",
        "                # print(\"### lr: \", optimizer.param_groups[0][\"lr\"])\n",
        "\n",
        "        # print(model.state_dict()['_orig_mod.bn1.running_mean'][0])\n",
        "        train_loss = loss.item()/len(y)\n",
        "        loss_list.append(train_loss)\n",
        "        try: wandb.log({\"train loss\": train_loss})\n",
        "        except: pass\n",
        "        if batch % (size//(10* len(y))) == 0:\n",
        "            # loss, current = loss.item(), batch * len(x)\n",
        "            # loss, current = loss.item()/len(y), batch * len(x)\n",
        "            current = batch * len(x)\n",
        "            if verbose: print(f\"loss: {train_loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "    return loss_list\n",
        "\n",
        "\n",
        "def test(dataloader, model, loss_fn, verbose=True):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in dataloader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            pred = model(x)\n",
        "            # print(pred[0])\n",
        "            loss = loss_fn(pred, y)\n",
        "            # predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            # print((pred.argmax(1)==y).sum().item(), pred.argmax(1).tolist(), y.tolist())\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    # test_loss /= num_batches\n",
        "    test_loss /= size\n",
        "    correct /= size\n",
        "    # should not use weighted rand sampler for test?\n",
        "    try: wandb.log({\"test loss\": test_loss})\n",
        "    except: pass\n",
        "    if verbose: print(f\"Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f}\")\n",
        "    return correct, test_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "a5CHCIMo7ZC5"
      },
      "outputs": [],
      "source": [
        "# @title Lamb\n",
        "# lamb optimizer (optional)\n",
        "# its like adamw but with warmup-like properties built in\n",
        "# https://github.com/cybertronai/pytorch-lamb/blob/master/pytorch_lamb/lamb.py\n",
        "import torch\n",
        "from torch.optim import Optimizer\n",
        "\n",
        "class Lamb(Optimizer):\n",
        "    \"\"\"Large Batch Optimization for Deep Learning: Training BERT in 76 minutes: https://arxiv.org/abs/1904.00962\n",
        "        adam (bool, optional): always use trust ratio = 1, which turns this into Adam\"\"\"\n",
        "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-6, weight_decay=0, adam=False): # eps=1e-8, weight_decay=0\n",
        "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n",
        "        self.adam = adam\n",
        "        super(Lamb, self).__init__(params, defaults)\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        loss = None\n",
        "        if closure is not None: loss = closure() # closure (callable, optional): A closure that reevaluates the model and returns the loss.\n",
        "        for group in self.param_groups:\n",
        "            for p in group['params']:\n",
        "                if p.grad is None: continue\n",
        "                grad = p.grad.data\n",
        "                if grad.is_sparse: raise RuntimeError('Lamb does not support sparse gradients, consider SparseAdam instad.')\n",
        "                state = self.state[p] # State initialization\n",
        "                if len(state) == 0:\n",
        "                    state['step'] = 0\n",
        "                    state['exp_avg'] = torch.zeros_like(p.data) # Exponential moving average of gradient values\n",
        "                    state['exp_avg_sq'] = torch.zeros_like(p.data) # Exponential moving average of squared gradient values\n",
        "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
        "                beta1, beta2 = group['betas']\n",
        "                state['step'] += 1\n",
        "\n",
        "                # Decay the first and second moment running average coefficient\n",
        "                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1) # m_t\n",
        "                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2) # v_t\n",
        "\n",
        "                # Paper v3 does not use debiasing.\n",
        "                # bias_correction1 = 1 - beta1 ** state['step']\n",
        "                # bias_correction2 = 1 - beta2 ** state['step']\n",
        "                # Apply bias to lr to avoid broadcast.\n",
        "                step_size = group['lr'] # * math.sqrt(bias_correction2) / bias_correction1\n",
        "                weight_norm = p.data.pow(2).sum().sqrt().clamp(0, 10)\n",
        "                adam_step = exp_avg / exp_avg_sq.sqrt().add(group['eps'])\n",
        "                if group['weight_decay'] != 0: adam_step.add_(p.data, alpha=group['weight_decay'])\n",
        "                adam_norm = adam_step.pow(2).sum().sqrt()\n",
        "                if weight_norm == 0 or adam_norm == 0: trust_ratio = 1\n",
        "                else: trust_ratio = weight_norm / adam_norm\n",
        "                state['weight_norm'] = weight_norm\n",
        "                state['adam_norm'] = adam_norm\n",
        "                state['trust_ratio'] = trust_ratio\n",
        "                if self.adam: trust_ratio = 1\n",
        "                p.data.add_(adam_step, alpha=-step_size * trust_ratio)\n",
        "        return loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iuumbm2SB_lX",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title LR range test\n",
        "# gives insight into good LR range to use.\n",
        "# for accurate results, be sure to use a new model for range test;\n",
        "# also reset the model before training bec range test destroys the model!!!\n",
        "# 1cycle super convergencehttps://arxiv.org/pdf/1708.07120.pdf\n",
        "# # cyclic lr https://arxiv.org/pdf/1506.01186.pdf\n",
        "# Note the learning rate value when the accuracy starts to\n",
        "# increase and when the accuracy slows, becomes ragged, or starts to fall\n",
        "\n",
        "# one training run of the network for a few epochs\n",
        "\n",
        "# torch.autograd.set_detect_anomaly(True)\n",
        "epochs=1\n",
        "min_lr= 1e-7\n",
        "max_lr= 1e-2 # 1e-2\n",
        "# 152: 1e-7 - 1e-4      result 3e-7 - 3e-6\n",
        "# inception: 1e-7 - 1e1      result 3e-7 - 3e-6\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "# optimizer = torch.optim.AdamW(model.parameters(), lr=min_lr, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=min_lr, momentum=0.9)\n",
        "# optimizer = torch.optim.SGD(model.parameters(), lr=min_lr, momentum=0.)\n",
        "# import bitsandbytes as bnb\n",
        "# optimizer = bnb.optim.AdamW(model.parameters(), lr=min_lr, betas=(0.9, 0.999), optim_bits=8)\n",
        "# optimizer = Lamb(model.parameters(), lr=min_lr, betas=(0.9, 0.999), eps=1e-08, weight_decay=3e-6)\n",
        "\n",
        "num_batches=len(test_loader)\n",
        "# num_batches=len(train_loader)\n",
        "\n",
        "# total_steps=int(num_batches*epochs)\n",
        "total_steps=int(np.ceil(num_batches/grad_acc)*epochs) # for grad accumulation\n",
        "\n",
        "# min_lr* gamma^total_steps = max_lr\n",
        "gamma = np.exp(np.log(max_lr/min_lr)/total_steps)\n",
        "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=gamma) # 0.75(20)-0.9(100)\n",
        "\n",
        "gamma = np.exp(np.log(max_lr/min_lr)/(total_steps*grad_acc))\n",
        "# lr_list=np.ones(total_steps)*min_lr*gamma**np.arange(total_steps)\n",
        "lr_list = min_lr*gamma**np.arange(total_steps*grad_acc+1-grad_acc)\n",
        "\n",
        "train_lst, test_lst=[],[]\n",
        "# use test_loader so that the range test is shorter\n",
        "# can do that since we are going to reset the model later anyways\n",
        "\n",
        "print(\"lr: \", optimizer.param_groups[0][\"lr\"])\n",
        "for i in range(epochs):\n",
        "    # train_ls = train(test_loader, model, loss_fn, optimizer, scheduler)\n",
        "    train_ls = strain(test_loader, model, loss_fn, optimizer, scheduler)\n",
        "    # train_ls = strain(train_loader, model, loss_fn, optimizer, scheduler)\n",
        "    train_lst.extend(train_ls)\n",
        "print(optimizer.param_groups[0][\"lr\"])\n",
        "\n",
        "# https://stackoverflow.com/a/53472966/13359815\n",
        "from scipy.ndimage.filters import gaussian_filter1d\n",
        "train_lstsm = gaussian_filter1d(train_lst, sigma=30)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(lr_list[:-1], train_lst[:-1])\n",
        "plt.plot(lr_list[:-1], train_lstsm[:-1])\n",
        "plt.xscale('log')\n",
        "# plt.plot(train_lst)\n",
        "# plt.plot(np.linspace(0,len(train_lst),len(test_lst)), test_lst)\n",
        "plt.show()\n",
        "plt.close()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wkBp7WjC8LLA"
      },
      "outputs": [],
      "source": [
        "# @title wwwwwwwww\n",
        "acc_lst, train_lst, test_lst=[],[],[]\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# base_lr, max_lr = 3e-6, 3e-5 # resnet152?\n",
        "base_lr, max_lr = 3e-7, 3e-6 # resnet152 batch16 gradacc4 adamw\n",
        "# base_lr, max_lr = 3e-3, 1e-1 # resnet152 batch16 gradacc4 sgd\n",
        "# base_lr, max_lr = 3e-5, 3e-4 # resnet50 batch32 gradacc2\n",
        "# base_lr, max_lr = 1e-5, 3e-4 # resnet18 batch128/64 gradacc1\n",
        "# base_lr, max_lr = 1e-6, 1e-5 # vit\n",
        "# base_lr, max_lr = 3e-6, 3e-5 # inception\n",
        "# end_lr, start_lr = 1e-5, 1e-3 # 0.0001,0.1\n",
        "tp=0\n",
        "epochs = 8 #5 20\n",
        "# optimizer = torch.optim.SGD(model.parameters(), lr = base_lr, momentum=0.9)\n",
        "# optimizer = torch.optim.AdamW(model.parameters(), lr=base_lr, betas=(0.9, 0.999), eps=1e-08, weight_decay=3e-6)\n",
        "import bitsandbytes as bnb # 8bit optimizer\n",
        "optimizer = bnb.optim.AdamW(model.parameters(), lr=base_lr, betas=(0.9, 0.999), optim_bits=8)\n",
        "# optimizer = bnb.optim.AdamW(model.parameters(), lr=3e-6, betas=(0.9, 0.999), optim_bits=8)\n",
        "# optimizer = bnb.optim.AdamW(model.parameters(), lr=3e-5, betas=(0.9, 0.999), optim_bits=8)\n",
        "# optimizer = Lamb(model.parameters(), lr=base_lr, betas=(0.9, 0.999), eps=1e-08, weight_decay=3e-6)\n",
        "# optimizer = Lamb(model.parameters(), lr=3e-6, betas=(0.9, 0.999), eps=1e-08, weight_decay=3e-6)\n",
        "# 152 1e-5\n",
        "# cnn 3e-4\n",
        "\n",
        "div_factor = max_lr/base_lr\n",
        "num_batches=len(train_loader)\n",
        "# total_steps=int(num_batches*epochs)+1 # +1 to excluse uptick at the end of onecycle\n",
        "total_steps=int(np.ceil(num_batches/grad_acc)*epochs +1) # /4 for when using grad accumulation\n",
        "\n",
        "# https://github.com/ultralytics/yolov3/issues/902\n",
        "# scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=max_lr, total_steps=total_steps, pct_start=0.45, anneal_strategy='cos', cycle_momentum=True, base_momentum=0.85, max_momentum=0.95, div_factor=div_factor, final_div_factor=100.0, three_phase=True,)\n",
        "# scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=max_lr, epochs=epochs, steps_per_epoch=num_batches, pct_start=0.45, anneal_strategy='cos', cycle_momentum=True, base_momentum=0.85, max_momentum=0.95, div_factor=div_factor, final_div_factor=10000.0, three_phase=True,)\n",
        "# gamma = np.exp(np.log(end_lr/start_lr)/(num_batches*epochs)) # for scheduler step every optimizer step\n",
        "# scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=gamma) # 0.75(20)-0.9(100)\n",
        "\n",
        "# pth='/content/res15270kg.pth'\n",
        "# pth='/content/drive/MyDrive/frame/res15236.pth' # B\n",
        "# pth='/content/drive/MyDrive/frame/res15270kold.pth' # M\n",
        "# pth='/content/drive/MyDrive/frame/res152trackf.pth' # B\n",
        "# pth='/content/drive/MyDrive/frame/res152plus.pth' # B\n",
        "# pth='/content/res152adamw71.pth' # ty\n",
        "# pth='/content/drive/MyDrive/frame/res152Teacher_onecycle.pth' # M\n",
        "# pth='/content/drive/MyDrive/frame/res18Teacher.pth' # A\n",
        "# pth='/content/drive/MyDrive/frame/res50Teacher.pth' # B/\n",
        "# pth='/content/drive/MyDrive/frame/resnet1e1sgd.pth' # M\n",
        "# pth='/content/drive/MyDrive/frame/resnet3e6adamw1.pth' # B\n",
        "# pth='/content/resnet3e6adamw1.pth' # ty\n",
        "\n",
        "\n",
        "# pth='/content/vit.pth'\n",
        "# pth='/content/drive/MyDrive/frame/vit.pth'\n",
        "# pth='/content/drive/MyDrive/frame/vit3736.pth'\n",
        "# pth='/content/drive/MyDrive/frame/vitadamw.pth' # M\n",
        "pth='/content/vitadamw.pth'\n",
        "\n",
        "# pth='/content/drive/MyDrive/frame/inception.pth'\n",
        "# pth='/content/inception2.pth'\n",
        "# pth='/content/inception.pth'\n",
        "# pth='/content/inception11.pth'\n",
        "# pth='/content/drive/MyDrive/frame/inception11.pth' # A, ty\n",
        "\n",
        "\n",
        "# # # to continue training\n",
        "# tp, modelsd, optimsd, schedsd = torch.load(pth).values()\n",
        "tp, modelsd, optimsd = torch.load(pth).values()\n",
        "# model.load_state_dict(modelsd)\n",
        "model.load_state_dict(modelsd,strict=False)\n",
        "optimizer.load_state_dict(optimsd)\n",
        "# # scheduler.load_state_dict(schedsd)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "REiP7-nvhc4s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ea7873c-419a-4ded-eb70-64855c659bcc",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1mzaiehZpHZcLIIuCA2SptuyrAs-sZMsD\n",
            "To: /content/vitadamw.pth\n",
            "100% 520M/520M [00:14<00:00, 34.9MB/s]\n"
          ]
        }
      ],
      "source": [
        "# @title save\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# # pth='/content/drive/MyDrive/frame/vit.pth'\n",
        "# pth='/content/drive/MyDrive/frame/res55.pth'\n",
        "# pth='/content/drive/MyDrive/frame/resnet152.pth'\n",
        "# pth='/content/drive/MyDrive/frame/res152373605aug.pth'\n",
        "# pth='/content/res152373605aug.pth' # B\n",
        "# pth='/content/drive/MyDrive/frame/res152lamb12.pth' # ty\n",
        "# pth='/content/drive/MyDrive/frame/res1522.pth' # A\n",
        "# pth='/content/drive/MyDrive/frame/res1522do.pth' # A\n",
        "# pth='/content/drive/MyDrive/frame/res152fol6470kg.pth' # Ty B\n",
        "\n",
        "# pth='/content/drive/MyDrive/frame/inception.pth'\n",
        "# pth='/content/inception2.pth'\n",
        "\n",
        "# pth='/content/res15270kg.pth'\n",
        "\n",
        "# pth='/content/drive/MyDrive/frame/vit3736.pth'\n",
        "\n",
        "\n",
        "# tp, modelsd, optimsd, schedsd = torch.load(pth).values()\n",
        "# model.load_state_dict(modelsd)\n",
        "# # optimizer.load_state_dict(optimsd)\n",
        "# # scheduler.load_state_dict(schedsd)\n",
        "\n",
        "# pth='/content/drive/MyDrive/frame/res152Teacher3e-6.pth' # M\n",
        "# pth='/content/drive/MyDrive/frame/res152TeacherSGD.pth' # B\n",
        "\n",
        "# checkpoint = { # https://discuss.pytorch.org/t/saving-model-and-optimiser-and-scheduler/52030\n",
        "# 'epoch': 0,\n",
        "# 'model': model.state_dict(),\n",
        "# 'optimizer': optimizer.state_dict(),\n",
        "# 'lr_sched': scheduler.state_dict()\n",
        "# }\n",
        "# torch.save(checkpoint, pth)\n",
        "\n",
        "\n",
        "\n",
        "# pth='/content/model.pth'\n",
        "# torch.save(model.state_dict(), pth)\n",
        "# model.load_state_dict(torch.load(pth))\n",
        "# # model.load_state_dict(torch.load(\"model.pth\"))\n",
        "\n",
        "# !gdown 1---4fdFbOUBTrS-VP5Va6pKowfgoU2UN -O inception2.pth\n",
        "# !gdown 1visTNvWmnuV7jAm2TBiAIIrNjbOAi1Fv -O resnet152.pth\n",
        "# !gdown 1-3oA1cKxgw4cfrqx079h_MKtMqRT1UFq -O res152lamb12.pth # S\n",
        "# !gdown 1cxu8Qq4-FoH3W3nQdRHT5mwe0xOxZ0Kb -O res152adamw.pth # M\n",
        "# !gdown 1ysJfdsvwMiWbCdkvFHwNqAUnJTtm6KbT -O res152adamw71.pth # ty\n",
        "# !gdown 1VaPxGoaLjmt7K9VHi0FWbJ5efEZTLhwd -O res18teacher.pth # A\n",
        "\n",
        "# !gdown 1NBxqdTfVjNC37R47rc79j_cl23Fd93v3 -O res50teacher.pth # B\n",
        "# !gdown 1LAK8lr1_lpd7l48Z-NiB-o5JRa9k6C2V -O res50teacher.pth # M\n",
        "# !gdown 1x3j3ztNPK_3Q3xntPq4J6lKvICYVhvP_ -O resnet3e6adamw1.pth # B,ty\n",
        "\n",
        "\n",
        "\n",
        "# !gdown 1qsbDQgnPgWLIwdwsb6MKJnxAf5SRTFz5 -O inception.pth # Ty 9jun\n",
        "# !gdown 1-032red_AZ4nABCXmYTvJN4s31xGctAC -O inception1615.pth # Ty\n",
        "# !gdown 1SpcKojBbTIcPGbrB4xVk65MI7RhA_Jry -O inception11.pth # Ty\n",
        "\n",
        "\n",
        "# !gdown 1SAC0TW-KNJcFqVb4kA79sQb859eT-wME -O vit.pth # A 9jun\n",
        "# !gdown 1-0-GTDs5vEWnezoo-vUVagHi_niATwrN -O vit3736.pth # A 9jun\n",
        "# !gdown 1uATrbs6bMQGVzy8TN-SyiXNXV4CL26Ee -O vitadamw.pth # S\n",
        "# !gdown 1mzaiehZpHZcLIIuCA2SptuyrAs-sZMsD -O vitadamw.pth # S spoch lin 3\n",
        "\n",
        "\n",
        "\n",
        "# # t, modelsd, optimsd, scheduler = torch.load('/content/drive/MyDrive/frame/resnet152.pth').values()\n",
        "# t, modelsd, optimsd, scheduler = torch.load('/content/resnet152.pth').values()\n",
        "# model.load_state_dict(modelsd)\n",
        "# # optimizer.load_state_dict(optimsd)\n",
        "\n",
        "# # matt152 # https://drive.google.com/file/d/1MQ0xLfHbio458uEVbn2VyMpD3bij2A4J/view?usp=sharing\n",
        "# !gdown 1MQ0xLfHbio458uEVbn2VyMpD3bij2A4J -O res152.pth\n",
        "# model.load_state_dict(torch.load(\"res152.pth\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kDBEk-l-Oxjn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d834921-3f61-4f2f-e9e7-8d2e69f6fc50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "1.2000000000000173e-06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 0.087250  [    0/61144]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 0.090041  [ 6112/61144]\n",
            "loss: 0.080726  [12224/61144]\n",
            "loss: 0.091343  [18336/61144]\n",
            "loss: 0.096101  [24448/61144]\n",
            "loss: 0.080747  [30560/61144]\n",
            "loss: 0.087123  [36672/61144]\n",
            "loss: 0.085356  [42784/61144]\n",
            "loss: 0.086307  [48896/61144]\n",
            "loss: 0.088493  [55008/61144]\n",
            "loss: 0.086020  [61120/61144]\n",
            "Accuracy: 61.3%, Avg loss: 0.088968\n",
            "time:  3941.737517595291\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "6.000000000000025e-07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 0.099682  [    0/61144]\n",
            "loss: 0.087055  [ 6112/61144]\n",
            "loss: 0.087815  [12224/61144]\n",
            "loss: 0.099767  [18336/61144]\n",
            "loss: 0.092121  [24448/61144]\n",
            "loss: 0.086567  [30560/61144]\n",
            "loss: 0.091683  [36672/61144]\n",
            "loss: 0.084151  [42784/61144]\n",
            "loss: 0.077526  [48896/61144]\n",
            "loss: 0.091601  [55008/61144]\n",
            "loss: 0.085967  [61120/61144]\n",
            "Accuracy: 60.2%, Avg loss: 0.090016\n",
            "time:  3772.03626704216\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import time\n",
        "start = time.time()\n",
        "\n",
        "# optimizer = bnb.optim.AdamW(model.parameters(), lr=3e-5, betas=(0.9, 0.999), optim_bits=8)\n",
        "# optimizer = bnb.optim.AdamW(model.parameters(), lr=1e-7, betas=(0.9, 0.999), optim_bits=8)\n",
        "# optimizer = bnb.optim.AdamW(model.parameters(), lr=0, betas=(0.9, 0.999), optim_bits=8)\n",
        "# optimizer = Lamb(model.parameters(), lr=3e-5, betas=(0.9, 0.999), eps=1e-08, weight_decay=3e-6)\n",
        "# optimizer = torch.optim.SGD(model.parameters(), lr=3e-6, momentum=0.9)\n",
        "# optimizer.param_groups[0][\"lr\"]=0\n",
        "# optimizer.param_groups[0][\"lr\"]=3e-6\n",
        "# optimizer.param_groups[0][\"lr\"]=3e-3\n",
        "# optimizer.param_groups[0][\"lr\"]=1e-2\n",
        "# optimizer.param_groups[0][\"lr\"]=3e-2\n",
        "# optimizer.param_groups[0][\"lr\"]=1e-1\n",
        "# resnet152 3e-3 - 1e-2     3e-2 - 1e-1?\n",
        "# vit 3e-5 - 1e-4           3e-5 - 1e-4?\n",
        "\n",
        "optimizer.param_groups[0][\"lr\"]=3e-6\n",
        "\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.PolynomialLR(optimizer, total_iters=int(np.ceil(num_batches/4)*5), power=1.0)\n",
        "# pth='/content/drive/MyDrive/frame/res152Tn.pth' # M\n",
        "# pth='/content/drive/MyDrive/frame/res50Teacher.pth' # B/M\n",
        "# pth='/content/drive/MyDrive/frame/resnet3e6adamw1.pth' # B,ty\n",
        "pth='/content/drive/MyDrive/frame/vitadamw1.pth' # S\n",
        "# pth='/content/drive/MyDrive/frame/inception11.pth' # A\n",
        "\n",
        "# print(optimizer.param_groups[0][\"lr\"])\n",
        "\n",
        "for i in range(int(np.ceil(num_batches/4)*3)):\n",
        "    scheduler.step()\n",
        "\n",
        "\n",
        "# scheduler = PolynomialLR(optimizer, total_iters=4, power=1.0)\n",
        "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=10**(-1/2))\n",
        "# for epoch in range(5):\n",
        "#     scheduler.step()\n",
        "# for t in range(0,epochs):\n",
        "for t in range(2):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    lr=optimizer.param_groups[0][\"lr\"]\n",
        "    print(lr)\n",
        "    train_ls = strain(train_loader, model, loss_fn, optimizer, scheduler)\n",
        "    # train_ls = strain(train_loader, model, loss_fn, optimizer)\n",
        "    correct, test_loss = test(test_loader, model, loss_fn)\n",
        "    # train_lst.extend(train_ls)\n",
        "    # test_lst.append(test_loss)\n",
        "    # acc_lst.append(correct)\n",
        "\n",
        "    checkpoint = { # https://discuss.pytorch.org/t/saving-model-and-optimiser-and-scheduler/52030\n",
        "    'epoch': t+1,\n",
        "    'model': model.state_dict(),\n",
        "    'optimizer': optimizer.state_dict(),\n",
        "    # 'lr_sched': scheduler.state_dict()\n",
        "    }\n",
        "    torch.save(checkpoint, pth)\n",
        "    # torch.save(model.state_dict(), pth)\n",
        "    end = time.time()\n",
        "    print(\"time: \",end - start)\n",
        "    start = end\n",
        "\n",
        "print(\"Done!\")\n",
        "\n",
        "\n",
        "# print(len(train_lst), len(test_lst))\n",
        "# import matplotlib.pyplot as plt\n",
        "# plt.plot(train_lst)\n",
        "# plt.plot(np.linspace(0,len(train_lst),len(test_lst)), test_lst)\n",
        "# plt.show()\n",
        "# plt.plot(acc_lst)\n",
        "# plt.show()\n",
        "# plt.close()\n",
        "\n",
        "\n",
        "# torch.save(model.state_dict(), \"model.pth\")\n",
        "\n",
        "# resnet 18, 60/61 38.4%, 528s\n",
        "# resnet 18, 58/61 39.8%, 523s\n",
        "# resnet 18 compile , 58/61 40.4%, 555s\n",
        "# resnet 18 compile augment , 58/61 36.4%, 1941 # augment on cpu, takes longer\n",
        "# resnet 18 augment lr3e-4:3e-3, 58/61 37.7%, 1863s\n",
        "# resnet 18 augment 10epoch lr1e-5:3e-4, 58/61 33.5%, 3387s\n",
        "# resnet 18 compile lr1e-5:3e-4, 58/61 35.0%, 493s\n",
        "# resnet 18 compile scratch lr1e-5:1e-3, 58/61 26.8%, 475s\n",
        "# resnet 18 compile lr1e-5:1e-3, 55/61 47.3%, 480s\n",
        "# resnet 18 compile lr1e-5:1e-3, 52/61 51.7% 503s\n",
        "# resnet 18 compile lr1e-5:1e-3, unfreeze 51.0%, 550s\n",
        "# resnet 18 compile lr1e-5:1e-4, unfreeze 52.7%, 518s\n",
        "# resnet 34 compile lr1e-5:1e-4, unfreeze bitsadamW batch16*4\n",
        "# resnet 152 compileoverhead lr3e-7:3e-6, bitsadamW batch16*4 ckpt 53.8%, 2066s\n",
        "# resnet 152 from53.8% augment+cutout lr3e-7:3e-6, 53.8%, 2088s\n",
        "# resnet 152 comile augment+cutout lr1e-5 /4 1epoch 48.3%, 454s\n",
        "# resnet 152 comile augment+cutout lr1e-5 1epoch 48.4%, 446s\n",
        "# resnet 152 comilemaxautotue augment+cutout lr1e-5 1epoch 47.7%, 448\n",
        "# resnet 152 clipclean comilemax augment+cutout lr1e-5 10epoch 45.1%, 1585 *2\n",
        "# resnet 152 clipclean comilemax augmax lr1e-6:3e-5 20epoch\n",
        "# resnet 152 clipclean compilemax lr3e-7:1e-5,\n",
        "# resnet 152 70k augment compilemax lr3e-7:3e-6 5epoch, 65.4% 11585s*5/3=19300s = 5h20m\n",
        "\n",
        "# resnet 152 70k augment compile adamw const lr3e-6, 59.8%\n",
        "# resnet 152 70k augment compile adamw const3e-6 stepdown\n",
        "\n",
        "\n",
        "\n",
        "# resnet 152 70k augment compile lamb lr1e-2 not learning\n",
        "# resnet 152 70k augment compile lamb lr3e-3 not learning\n",
        "# resnet 152 70k augment compile lamb lr1e-3:1e-2 5epoch,\n",
        "# 2/5epochwarmup 55.0%\n",
        "\n",
        "# resnet 152 70kg augment1 cut1,-1 compilemax adamw lr3e-7:3e-6 1/5epoch 31.1% ty\n",
        "# resnet 152 70k augment1 cut1,-1 compilemax adamw lr3e-7:3e-6 1/5epoch 28.1% A\n",
        "\n",
        "# resnet 152 70k dataold augment1 cut1,0 compilemax adamw lr3e-7:3e-6 M nantest\n",
        "# resnet 152 70kg augment1 cut1,0 compilemax adamw lr3e-7:3e-6 B\n",
        "# resnet 152 70k augment1 from64.5 cut(mean) compilemax adamw lr3e-7:3e-6 4/5epochs 58.1% res152fol64 A 61.5% res152kplus M\n",
        "# resnet 152 70kg augment1 from64.5 cut(mean) compilemax adamw lr3e-7:3e-6 5epochs+ 63.8% res152fol64kg res152plus B\n",
        "# 73.6% 67% 66.1%\n",
        "# 0 1e5 nan\n",
        "# 0 3e6\n",
        "\n",
        "# adamw 3e-6 57.5% A\n",
        "# sgd 3e-3 60.0% ty\n",
        "# res152 70kg correctedweight adamw3e-6 57m per epoch 57.9% M\n",
        "\n",
        "\n",
        "\n",
        "# vit b16 lr1e-5 5epochs 41.3%, 466s # 4.4ram, 5.5vram\n",
        "# vit l16 lr1e-5 5epochs # 32.0%, 1242s 4.5ram, 8.0vram\n",
        "# vit l16 lr3e-7;1e-5 5epochs # 45.4%, 1315s 4.5ram, 8.0vram\n",
        "# vit l32 lr1e-5 5epochs # .ram, .vram\n",
        "# vit_large_patch16_384\n",
        "# vit_base_patch16_224 maxcompile nockpt lr3e-7;1e-5 5epochs # 45.2%, 2272s 5.3ram, 11.0vram\n",
        "# vit_base_patch16_224 maxcompile nockpt gradacc batch16 1e-5, 1e-2 explode\n",
        "# vit_base_patch16_224 maxcompile nockpt gradacc batch16 1e-6, 1e-5 low acc, test nan\n",
        "# vit_base_patch16_224 maxcompile nockpt gradacc batch16 3e-7, 3e-6 4/5 epoch50%\n",
        "# vit_base_patch16_224 maxcompile nockpt gradacc batch16 1e-6, 1e-5 0.5aug\n",
        "# vit_base_patch16_224 maxcompile nockpt gradacc batch16 aug1 noblur Lamb lr1e-5 vitadamw B\n",
        "# 1h per epoch\n",
        "# 0 1e5 nan\n",
        "# 0 1e6\n",
        "# vit 70kg correctedweight adamw3e-6 64m per epoch 60.2%\n",
        "\n",
        "\n",
        "\n",
        "# inception\n",
        "# inception og 10kclean 1e-7, 3e-5 batch64 nockpt 5epochs 0.020348  51.4%, 0.024312 20m; 1st 40.5% max 52.1\n",
        "# inception hid 10kclean 1e-7, 3e-5 batch64 nockpt 1/5epochs 0.019635 53.6%, 0.024120 1st 32.8%, max 53.6%\n",
        "# inception og 10kclean 1e-7, 1e-4 batch64 nockpt 2/10epochs 0.020348  0.020923 39.9%, 0.026105\n",
        "# inception og 70kg 1e-7 nope\n",
        "# inception og 70kg 3e-6 3e-5 54.8%\n",
        "# inception og 70kg 1e-6, 1e-5\n",
        "# inception og 70kg 3e-6 3e-5 0.5aug\n",
        "# adamw 3e-5 A 51.8% 53.3%\n",
        "# inception 70kg 3e-5 S\n",
        "# inception 70kg 3e-5 3e 61.0% 30m per epoch\n",
        "# inception 70kg correctedweight adamw3e-5 35m per epoch 58.3% ty\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8mzr-IlDjSQU"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def deactivate_batchnorm(m):\n",
        "    if isinstance(m, nn.BatchNorm2d):\n",
        "        m.reset_parameters()\n",
        "        m.eval()\n",
        "        with torch.no_grad():\n",
        "            m.weight.fill_(1.0)\n",
        "            m.bias.zero_()\n",
        "\n",
        "model.apply(deactivate_batchnorm)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PFsnIRVz7Xp5",
        "outputId": "38325727-2121-4b87-8fd8-61b2262a8979"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 60.3%, Avg loss: 0.089894\n",
            "0.6027377097438916 0.08989398208196492\n"
          ]
        }
      ],
      "source": [
        "correct, test_loss = test(test_loader, model, loss_fn)\n",
        "# correct, test_loss = test(train_loader, model, loss_fn) # dont forget to normalise train data\n",
        "print(correct, test_loss)\n",
        "# res152 70 Accuracy: 69.5%, Avg loss: 0.084133 ; trainloader:Accuracy: 68.6%, Avg loss: 0.084691\n",
        "\n",
        "# # https://discuss.pytorch.org/t/model-predictions-changing-with-no-grad-and-eval/126543/2\n",
        "# self.drop_layer = nn.Dropout(p = .5)\n",
        "# out = F.dropout(x, training=self.training)\n",
        "# # https://discuss.pytorch.org/t/trained-resnet-doesnt-work-in-eval-mode-behaves-strangely/121242/8\n",
        "# self.bn = torch.nn.BatchNorm2d(input_features,track_running_stats=False)\n",
        "\n",
        "# res50 Accuracy: 36.3%, Avg loss: 0.052037 Accuracy: 45.6%, Avg loss: 0.049405\n",
        "# res18 Accuracy: 62.3%, Avg loss: 0.044281\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GWt4B_YoCxKB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        },
        "outputId": "abf15f65-7338-45c3-baa4-b317920e8b49"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 500x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# @title confusion matrix\n",
        "# for plotting confusoin matrix\n",
        "# left vertical axis is actual, bottom horizontal axis is predicted\n",
        "from sklearn.metrics import confusion_matrix, f1_score\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "def confusion(dataloader, model, loss_fn, verbose=True):\n",
        "    model.eval()\n",
        "    y_true, y_pred = torch.empty(0, device=device), torch.empty(0, device=device)\n",
        "    for x, y in dataloader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        with torch.no_grad():\n",
        "            pred = model(x)\n",
        "        # print((pred.argmax(1)==y).sum().item(), pred.argmax(1).tolist(), y.tolist())\n",
        "        y_pred = torch.cat((y_pred, pred.argmax(1)), 0)\n",
        "        y_true = torch.cat((y_true, y), 0)\n",
        "    return y_true, y_pred\n",
        "\n",
        "y_true, y_pred = confusion(test_loader, model, loss_fn)\n",
        "\n",
        "y_true, y_pred = y_true.cpu(), y_pred.cpu()\n",
        "cm = confusion_matrix(y_true=y_true, y_pred=y_pred)\n",
        "cmn = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "fig, ax = plt.subplots(figsize=(5,5))\n",
        "sns.heatmap(cmn, annot=True, fmt='.2f', xticklabels=['1','2','3','4','5','6',], yticklabels=['1','2','3','4','5','6',])\n",
        "plt.ylabel('Actual')\n",
        "plt.xlabel('Predicted')\n",
        "plt.show()\n",
        "\n",
        "f1 = f1_score(y_true=y_true, y_pred=y_pred)\n",
        "print(f1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "icF25Bbpp6gR",
        "outputId": "2162156b-9172-4160-fe5d-024c0ee8ad5f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "38767 67938 57.062321528452415\n"
          ]
        }
      ],
      "source": [
        "# @title collect unconfident preds\n",
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
        "\n",
        "t=0\n",
        "for cls in range(1,7):\n",
        "    img_dir = '/content/gsv70kg/0'+str(cls)\n",
        "    for filename in os.listdir(img_dir):\n",
        "        img_file=os.path.join(img_dir, filename)\n",
        "        image = Image.open(img_file).convert(\"RGB\")\n",
        "        with torch.no_grad():\n",
        "            with torch.cuda.amp.autocast(): # automatic mixed percision\n",
        "                x = transform(image).to(device).unsqueeze(0)\n",
        "                y = torch.tensor(cls-1, device=device)\n",
        "                pred = model(x)\n",
        "\n",
        "        p = pred.max(1)\n",
        "        # s.append((p, img_file, y.item(), pred.argmax(1)))\n",
        "        s.append((p, img_file))\n",
        "\n",
        "        t+=1\n",
        "        # if t >=5: break\n",
        "\n",
        "\n",
        "s.sort() # lowest confidence to highest confidence\n",
        "\n",
        "sa=[x[0] for x in s] # p\n",
        "sb=[x[1] for x in s] # img_file\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "matplotlib.rcParams['figure.dpi'] = 300\n",
        "\n",
        "def imshow(img):\n",
        "    # img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.axis('off')\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "from torchvision import datasets, transforms\n",
        "c=3*64\n",
        "images = [Image.open(img_file).convert(\"RGB\") for img_file in sb[c:c+64]]\n",
        "images = [transforms.ToTensor()(im) for im in images]\n",
        "imshow(torchvision.utils.make_grid(images,nrow=8))\n",
        "\n",
        "\n",
        "# print(type(sa[0]))\n",
        "# print(sa[0].values.item())\n",
        "\n",
        "# for i,x in enumerate(sa[:256]):\n",
        "#     # print(x)\n",
        "#     print(i, x.values.item(), sb[i])\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y-Cjw51AedI7",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title collect misclassified\n",
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
        "\n",
        "rongs=[[[] for i in range(6)] for j in range(6)]\n",
        "\n",
        "\n",
        "t=0\n",
        "c=0\n",
        "for cls in range(1,7):\n",
        "    img_dir = '/content/gsv70kg/0'+str(cls)\n",
        "    for filename in os.listdir(img_dir):\n",
        "        # name = os.path.splitext(filename)[0]\n",
        "        img_file=os.path.join(img_dir, filename)\n",
        "        image = Image.open(img_file).convert(\"RGB\")\n",
        "        with torch.no_grad():\n",
        "            with torch.cuda.amp.autocast(): # automatic mixed percision\n",
        "                x = transform(image).to(device).unsqueeze(0)\n",
        "                y = torch.tensor(cls-1, device=device)\n",
        "                pred = model(x)\n",
        "\n",
        "        p = pred.argmax(1)\n",
        "        # print(\"p, y\", p, y)\n",
        "        if p != y:\n",
        "            # print(filename)\n",
        "            c+=1\n",
        "            rongs[y][p].append(filename)\n",
        "\n",
        "        t+=1\n",
        "        # if t >=5: break\n",
        "\n",
        "print(c,t,c*100/t) # correct, total, accuracy\n",
        "# 38767 67938 57.062321528452415\n",
        "\n",
        "# print(rongs)\n",
        "[print([len(s) for s in l]) for l in rongs]\n",
        "\n",
        "\n",
        "# display misclassified images\n",
        "\n",
        "# def imshow(img):\n",
        "#     img = img / 2 + 0.5  # unnormalize\n",
        "#     npimg = img.numpy()\n",
        "    # plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    # plt.show()\n",
        "\n",
        "\n",
        "# combi = torch.combinations(torch.arange(0,6), with_replacement=True)\n",
        "combi = [[i,j] for i in range(6) for j in range(6)]\n",
        "# print(combi)\n",
        "\n",
        "for i, j in combi:\n",
        "    print(\"actual: \", i, \"predicted: \", j)\n",
        "    c=0\n",
        "    images=torch.empty(0)\n",
        "    plt.figure(figsize=(16, 16))\n",
        "    plt.axis('off')\n",
        "    for filename in rongs[i][j]:\n",
        "    # for filename in rongs[5][0]:\n",
        "        # print(filename)\n",
        "        cls=filename[1]\n",
        "        img_dir = '/content/gsv70kg/0'+str(cls)\n",
        "        img_file=os.path.join(img_dir, filename)\n",
        "        image = Image.open(img_file)#.convert(\"RGB\")\n",
        "        image = transform(image).unsqueeze(0)\n",
        "        images=torch.cat((images,image),0)\n",
        "\n",
        "        c+=1\n",
        "        if c>15: break\n",
        "    try: imshow(torchvision.utils.make_grid(images,nrow=4))\n",
        "    except: continue\n",
        "# except: pass\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title collect misclass all\n",
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "def get_img(img_dir, filename):\n",
        "    img_file=os.path.join(img_dir, filename)\n",
        "    image = Image.open(img_file).convert(\"RGB\")\n",
        "    return image\n",
        "\n",
        "from torchvision import transforms\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
        "\n",
        "rongs=[[[] for i in range(6)] for j in range(6)]\n",
        "\n",
        "\n",
        "batch_size = 1024 # 16\n",
        "\n",
        "import time\n",
        "import numpy as np\n",
        "e = ThreadPoolExecutor(batch_size)\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "t=0\n",
        "for cls in range(1,7):\n",
        "    img_dir = '/content/gsv70kg/0'+str(cls)\n",
        "\n",
        "    m = os.listdir(img_dir)\n",
        "    for i in range(0, len(m), batch_size):\n",
        "        # e = ThreadPoolExecutor(min(batch_size, len(m))) #\n",
        "        filenames = m[i: min(i+batch_size, len(m))]\n",
        "        future = [e.submit(img_dir, filename) for filename in filenames]\n",
        "        # result = [f.result() for f in future if f.result() is not None]\n",
        "        images = [f.result() for f in future]\n",
        "        images = torch.tensor(np.stack(images)).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            with torch.cuda.amp.autocast(): # automatic mixed percision\n",
        "                x = transform(image).to(device)#.unsqueeze(0)\n",
        "                # y = torch.tensor(cls-1, device=device)\n",
        "                pred = model(x)\n",
        "        y = cls-1\n",
        "        p = pred.argmax(1)\n",
        "        # print(\"p, y\", p, y)\n",
        "        # rongs[y][p].append(filename)\n",
        "        for i in range(len(filenames)):\n",
        "            rongs[y][p[i]].append(filenames[i])\n",
        "        t+=1\n",
        "\n"
      ],
      "metadata": {
        "id": "apfnRF89cO5h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rongs=[[[] for i in range(6)] for j in range(6)]\n",
        "filenames=['wret','kyd','an','aw5','yhf','pok','dgf','hdm','Wrh','xfb']\n",
        "p=torch.randint(1,6,(10,))\n",
        "y = torch.tensor(1)\n",
        "\n",
        "rongs[y][].extend(filenames[p[==]])\n",
        "\n"
      ],
      "metadata": {
        "id": "rFLPpKEdkBy-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ayRePbr_rq9F"
      },
      "outputs": [],
      "source": [
        "# pytorch release gpu ram after training\n",
        "# https://discuss.pytorch.org/t/free-all-gpu-memory-used-in-between-runs/168202/2\n",
        "torch.cuda.empty_cache()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title plot lr scheduler\n",
        "lr_lst=[]\n",
        "total_steps=100\n",
        "base_lr, max_lr = 3e-5, 3e-4 # resnet50 batch32 gradacc2\n",
        "\n",
        "optimizer = bnb.optim.AdamW(model.parameters(), lr=base_lr, betas=(0.9, 0.999), optim_bits=8)\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=max_lr, total_steps=total_steps, pct_start=0.45, anneal_strategy='cos', cycle_momentum=True, base_momentum=0.85, max_momentum=0.95, div_factor=div_factor, final_div_factor=100.0, three_phase=True,)\n",
        "\n",
        "for t in range(total_steps):\n",
        "    lr=optimizer.param_groups[0][\"lr\"]\n",
        "    lr_lst.append(lr)\n",
        "    scheduler.step()\n",
        "plt.plot(lr_lst)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "fCYDnCuD3UWo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}