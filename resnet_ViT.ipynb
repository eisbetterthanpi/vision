{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eisbetterthanpi/vision/blob/main/resnet_ViT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "2kQGpnynh2oQ"
      },
      "outputs": [],
      "source": [
        "# @title download\n",
        "# original 10k\n",
        "# # https://drive.google.com/file/d/1NkCNecLpFG3i7bo3Vl9RQSwzBpSRQ29q/view?usp=sharing\n",
        "# !gdown 1NkCNecLpFG3i7bo3Vl9RQSwzBpSRQ29q -O gsv.zip\n",
        "# !unzip /content/gsv.zip -d /content\n",
        "# import shutil\n",
        "# shutil.rmtree('/content/google_street_view/meta_data', ignore_errors=True) # delete the meta_data folder\n",
        "\n",
        "# # clip cleaned\n",
        "# # https://drive.google.com/file/d/1-xcHyVAMeTkY7SUdUyQRVAn_FFXPSDGB/view?usp=share_link\n",
        "# !gdown 1-xcHyVAMeTkY7SUdUyQRVAn_FFXPSDGB -O gsv.zip\n",
        "# !unzip /content/gsv.zip -d /\n",
        "# !rm -R /content/gsv/.ipynb_checkpoints\n",
        "# !rm -R /content/gsv/01/.ipynb_checkpoints\n",
        "# !rm -R /content/gsv/02/.ipynb_checkpoints\n",
        "# !rm -R /content/gsv/03/.ipynb_checkpoints\n",
        "# !rm -R /content/gsv/04/.ipynb_checkpoints\n",
        "# !rm -R /content/gsv/05/.ipynb_checkpoints\n",
        "# !rm -R /content/gsv/06/.ipynb_checkpoints\n",
        "\n",
        "# # gsv 70k\n",
        "# # https://drive.google.com/file/d/1-7ZC29k4VxXQkpnOuLfj7Ag_SFTM4LV8/view?usp=share_link\n",
        "# !gdown 1-7ZC29k4VxXQkpnOuLfj7Ag_SFTM4LV8 -O gsv.zip\n",
        "# !unzip /content/gsv.zip -d /\n",
        "\n",
        "# # # !ls\n",
        "# !ls -a /content/gsv70k\n",
        "# !rm -R /content/gsv70k/.ipynb_checkpoints\n",
        "# # # !rm -R /content/gsv/06/.ipynb_checkpoints\n",
        "\n",
        "# # 70k+gmap\n",
        "# # https://drive.google.com/file/d/1-CZp7TbhJLeRQpbKQCyT8ofGg89Yt137/view?usp=sharing\n",
        "!gdown 1-CZp7TbhJLeRQpbKQCyT8ofGg89Yt137 -O gsv.zip\n",
        "!unzip /content/gsv.zip -d /\n",
        "!rm -R /content/gsv70kg/.ipynb_checkpoints\n",
        "!rm -R /content/gsv70kg/01/.ipynb_checkpoints\n",
        "!rm -R /content/gsv70kg/02/.ipynb_checkpoints\n",
        "!rm -R /content/gsv70kg/03/.ipynb_checkpoints\n",
        "!rm -R /content/gsv70kg/04/.ipynb_checkpoints\n",
        "!rm -R /content/gsv70kg/05/.ipynb_checkpoints\n",
        "!rm -R /content/gsv70kg/06/.ipynb_checkpoints\n",
        "\n",
        "# # https://bestasoff.medium.com/how-to-fine-tune-very-large-model-if-it-doesnt-fit-on-your-gpu-3561e50859af\n",
        "!pip install bitsandbytes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "hEUffQ24mkRY"
      },
      "outputs": [],
      "source": [
        "# @title torch augment\n",
        "# https://github.com/facebookresearch/vicreg/blob/main/augmentations.py\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "class TrainTransform(object):\n",
        "    def __init__(self):\n",
        "        self.transform = transforms.Compose([\n",
        "                transforms.RandomPerspective(distortion_scale=0.3, p=0.5), # me\n",
        "                transforms.RandomResizedCrop((400,640), scale=(0.7, 1.0), ratio=(0.8, 1.25), interpolation=transforms.InterpolationMode.BICUBIC),\n",
        "                transforms.RandomHorizontalFlip(p=0.5), # 0.5\n",
        "                transforms.RandomApply([transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1)], p=0.8,), # brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1)], p=0.8\n",
        "                transforms.RandomGrayscale(p=0.2), # 0.2\n",
        "                # transforms.RandomChoice(transforms.ColorJitter , transforms.RandomGrayscale(p=1.)\n",
        "                transforms.RandomApply([transforms.GaussianBlur(kernel_size=5, sigma=(0.1, 2.0)),], p=1.0),\n",
        "                # transforms.RandomSolarize(threshold=130, p=0.5)\n",
        "                transforms.RandomErasing(p=0.5, scale=(0.1, 0.11), ratio=(1,1), value=0, inplace=True), # default p=0.5, scale=(0.02, 0.33), ratio=(0.3, 3.3), value=0, inplace=False\n",
        "                # transforms.ToTensor(), # ToTensored at dataset level, no need to ToTensor again\n",
        "                # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), # normalised at dataset level. default 0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225\n",
        "            ])\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        dims = len(sample.shape)\n",
        "        if dims==3: x1 = self.transform(sample) # same transforms per minibatch\n",
        "        elif dims==4: x1 = transforms.Lambda(lambda x: torch.stack([self.transform(x_) for x_ in x]))(sample) # diff transforms per img in minibatch\n",
        "        return x1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pqw5n--6WYEG",
        "outputId": "64ef0f10-d3b6-4eee-8041-33c664b7feb7",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ],
      "source": [
        "# @title data weighted\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# dataset has PILImage images of range [0, 1], transform them to Tensors of normalized range [-1, 1]\n",
        "# transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
        "# transform = transforms.Compose(transforms.ToTensor())\n",
        "\n",
        "# dir='/content/gsv'\n",
        "dir='/content/gsv70kg'\n",
        "\n",
        "# data = datasets.ImageFolder(dir, transform=transform)\n",
        "data = datasets.ImageFolder(dir, transform=None)\n",
        "torch.manual_seed(0)\n",
        "\n",
        "# split data manually so that can work with weighted random sampler\n",
        "# train_data, test_data = torch.utils.data.random_split(data, [.85,.15])\n",
        "# https://www.scaler.com/topics/pytorch/how-to-split-a-torch-dataset/\n",
        "data_size = len(data)\n",
        "indices = np.arange(data_size)\n",
        "np.random.shuffle(indices)\n",
        "split_index = int(np.floor(0.9 * data_size))\n",
        "train_idx, test_idx = indices[:split_index], indices[split_index:]\n",
        "train_data = torch.utils.data.Subset(data, train_idx)\n",
        "test_data = torch.utils.data.Subset(data, test_idx)\n",
        "targets = np.array(data.targets)\n",
        "train_targets = targets[train_idx]\n",
        "test_targets = targets[test_idx]\n",
        "\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "class DatasetWrap(Dataset):\n",
        "    def __init__(self, dataset, transform=None):\n",
        "        super(DatasetWrap, self).__init__()\n",
        "        self.dataset = dataset\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        x, y = self.dataset[index]\n",
        "        if self.transform:\n",
        "            x = self.transform(x)\n",
        "        return x, y\n",
        "\n",
        "# dataset wrapper in order to apply transforms to train data only\n",
        "# train_data = DatasetWrap(train_data, TrainTransform()) # apply data augmentation to train dataset only\n",
        "train_data = DatasetWrap(train_data, transform) # apply transform during training to use gpu\n",
        "test_data = DatasetWrap(test_data, transform)\n",
        "\n",
        "# use batch size 16 for resnet 152/ vit with grad accumulation\n",
        "# can use batch size 64 for inception v3 without grad accumulation?\n",
        "batch_size = 64 # 64/16\n",
        "num_batches=int(np.ceil(len(train_data)/batch_size))\n",
        "\n",
        "# oversampling\n",
        "# https://stackoverflow.com/questions/62319228/number-of-instances-per-class-in-pytorch-dataset\n",
        "from collections import Counter\n",
        "class_count = torch.tensor(list(Counter(data.targets).values()))\n",
        "weights=1./class_count\n",
        "# weights=sum(class_count)/class_count\n",
        "# print(weights)\n",
        "# https://pytorch.org/docs/stable/data.html#torch.utils.data.WeightedRandomSampler\n",
        "\n",
        "train_weight = weights[train_targets]\n",
        "test_weight = weights[test_targets]\n",
        "# train_sampler = torch.utils.data.WeightedRandomSampler(train_weight, len(train_weight))\n",
        "# test_sampler = torch.utils.data.WeightedRandomSampler(test_weight, len(test_weight))\n",
        "train_sampler = torch.utils.data.WeightedRandomSampler(train_weight, 70000)\n",
        "test_sampler = torch.utils.data.WeightedRandomSampler(test_weight, 7000)\n",
        "train_loader = torch.utils.data.DataLoader(train_data, sampler=train_sampler, batch_size=batch_size, num_workers=4, pin_memory=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, sampler=test_sampler, batch_size=batch_size, num_workers=4, pin_memory=True)\n",
        "data, train_data, test_data = None, None, None\n",
        "\n",
        "# test oversampling: occurence of each class should be roughly equal\n",
        "# c=0\n",
        "# print(len(test_loader))\n",
        "# # for batch, (x, y) in enumerate(train_loader):\n",
        "# for batch, (x, y) in enumerate(test_loader):\n",
        "#     print(torch.bincount(y)) # torch count number of elements with value in tensor\n",
        "#     c+=1\n",
        "#     if c>5: break\n",
        "\n",
        "# display img from torch tensor\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "# dataiter = iter(train_loader)\n",
        "# images, labels = next(dataiter)\n",
        "# imshow(torchvision.utils.make_grid(images,nrow=4))\n",
        "# dataiter=None\n",
        "# print(labels)\n",
        "\n",
        "# dataiter = iter(test_loader)\n",
        "# images, labels = next(dataiter)\n",
        "# imshow(torchvision.utils.make_grid(images,nrow=4))\n",
        "# dataiter=None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7oYDr8kuA5Bl"
      },
      "outputs": [],
      "source": [
        "# @title resnet\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import datasets, models, transforms\n",
        "# # https://pytorch.org/vision/0.12/models.html#id10\n",
        "model = models.resnet152(pretrained=True) # 18 34 50 101 152\n",
        "\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Sequential(\n",
        "    # nn.Linear(num_ftrs, 6, bias=False), # og\n",
        "    nn.Linear(num_ftrs, 128), nn.ReLU(), nn.Linear(128, 6),\n",
        "    nn.Softmax(dim=1),\n",
        "    )\n",
        "\n",
        "# # model.mods = [module for k, module in model._modules.items()]\n",
        "# # modules = [module for k, module in model._modules.items()]\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# model = model.to(device)\n",
        "# model = torch.compile(model.to(device))\n",
        "# model = torch.compile(model.to(device),mode='reduce-overhead')\n",
        "model = torch.compile(model.to(device),mode='max-autotune')\n",
        "\n",
        "# resnet152 batchsize16 gradacc gradckpt lr3e-6,3e-5\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "rJw9_Ort2Sek"
      },
      "outputs": [],
      "source": [
        "# @title vit\n",
        "# https://arxiv.org/pdf/2010.11929.pdf\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "# https://pytorch.org/vision/main/models/vision_transformer.html\n",
        "# https://pytorch.org/vision/main/models/generated/torchvision.models.vit_b_16.html#torchvision.models.vit_b_16\n",
        "# model = models.vit_l_16(weights='DEFAULT') # small vit_b_16 vit_b_32 vit_l_16 vit_l_32 vit_h_14 big\n",
        "# # VisionTransformer(image_size, patch_size, num_layers, num_heads, hidden_dim, mlp_dim)\n",
        "# num_ftrs = model.heads.head.in_features\n",
        "# # num_ftrs = model.heads[-1].in_features\n",
        "# model.heads = nn.Sequential(\n",
        "#     # nn.Dropout(0.2),\n",
        "#     nn.Linear(num_ftrs, 6, bias=False),\n",
        "#     nn.Softmax(dim=1),\n",
        "#     )\n",
        "\n",
        "\n",
        "!pip install timm\n",
        "# https://github.com/huggingface/pytorch-image-models/issues/908\n",
        "import timm\n",
        "# model = timm.create_model('vit_base_patch16_224', pretrained=True)\n",
        "model = timm.create_model('vit_base_patch16_224', img_size=(400, 640), pretrained=True)\n",
        "# [print(x) for x in timm.list_models('vit*',pretrained=True)]\n",
        "# https://huggingface.co/google/vit-base-patch16-224\n",
        "# https://github.com/huggingface/pytorch-image-models/blob/main/timm/models/vision_transformer.py\n",
        "# vit_base_patch16_224 compile,no ckpt # patch_size=16, embed_dim=768, depth=12, num_heads=12\n",
        "# vit_base_patch16_384\n",
        "# vit_large_patch16_224 explodesgpu # patch_size=16, embed_dim=1024, depth=24, num_heads=16\n",
        "# vit_large_patch16_384 # patch_size=16, embed_dim=1024, depth=24, num_heads=16\n",
        "\n",
        "# or fine tune huge\n",
        "# vit_large_patch14_224 # patch_size=16, embed_dim=1024, depth=24, num_heads=16\n",
        "# vit_large_patch16_384\n",
        "\n",
        "\n",
        "num_ftrs = model.head.in_features\n",
        "model.head = nn.Sequential(\n",
        "    # nn.Dropout(0.2),\n",
        "    nn.Linear(num_ftrs, 6, bias=False),\n",
        "    nn.Softmax(dim=1),\n",
        "    )\n",
        "# model.set_grad_checkpointing()\n",
        "\n",
        "# print(model.patch_embed.grid_size) # (25, 40)\n",
        "# print(model.pos_embed.shape) # [1, 1001, 768]\n",
        "# https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py\n",
        "\n",
        "# print(model)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# model = model.to(device)\n",
        "# model = torch.compile(model.to(device))\n",
        "# model = torch.compile(model.to(device),mode='reduce-overhead')\n",
        "model = torch.compile(model.to(device),mode='max-autotune')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-oKYpG8n2fBI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71323f78-cba4-4fee-fab8-b8992ff2263f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ],
      "source": [
        "# @title inception\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "# # https://pytorch.org/vision/0.12/models.html#id10\n",
        "model = models.inception_v3(pretrained=True)\n",
        "# https://discuss.pytorch.org/t/inception-v3-is-not-working-very-well/38296/16\n",
        "# https://colab.research.google.com/github/CaoCharles/Deep-Learning-with-PyTorch/blob/master/2_Inception.ipynb\n",
        "model.aux_logits = False\n",
        "num_ftrs = model.fc.in_features # 2048\n",
        "model.fc = nn.Sequential(\n",
        "    nn.Linear(num_ftrs, 6), # og: (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
        "    # nn.Linear(num_ftrs, 128), nn.ReLU(), nn.Linear(128, 6),\n",
        "    nn.Softmax(dim=1),\n",
        "    )\n",
        "# pytorch \"inception\" v3 \"gradient checkpointing\" https://github.com/jianweif/OptimalGradCheckpointing\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "# model = torch.compile(model.to(device))\n",
        "# model = torch.compile(model.to(device),mode='max-autotune')\n",
        "\n",
        "# inception batchsize64 nogradacc nogradckpt lr3e-6,3e-5\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "vEZCFg5YSS9J"
      },
      "outputs": [],
      "source": [
        "# @title try\n",
        "\n",
        "# # check model's input and output dimensions are correct\n",
        "# X = torch.rand(64, 3, 32, 32, device=device)\n",
        "X = torch.rand(16, 3, 400, 640, device=device)\n",
        "# X = torch.rand(16, 3, 224, 224, device=device)\n",
        "model.eval()\n",
        "\n",
        "# 224x224\n",
        "# 16x16 / 32x32 patch\n",
        "# -> 14x14=196 7x7=49 seq length\n",
        "# 400x640 -> 25x40=1000 seq length\n",
        "\n",
        "\n",
        "logits = model(X)\n",
        "\n",
        "# modules = [module for k, module in model._modules.items()]\n",
        "# for i,x in enumerate(modules):\n",
        "#     print(i,x)\n",
        "\n",
        "# logits = checkpoint_sequential(functions=modules, segments=1, input=X)\n",
        "\n",
        "print(logits.shape)\n",
        "# print(logits[0])\n",
        "# print(logits[0].argmax(1))\n",
        "\n",
        "pred_probab = nn.Softmax(dim=1)(logits)\n",
        "y_pred = pred_probab.argmax(1)\n",
        "print(y_pred)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fsealXK3OPQa"
      },
      "outputs": [],
      "source": [
        "# @title train test function\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "# https://github.com/prigoyal/pytorch_memonger/blob/master/models/optimized/resnet_new.py\n",
        "from torch.utils.checkpoint import checkpoint, checkpoint_sequential\n",
        "\n",
        "trs=TrainTransform()\n",
        "# train function with automatic mixed precision\n",
        "def strain(dataloader, model, loss_fn, optimizer, scheduler=None, verbose=True):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    loss_list = []\n",
        "    for batch, (x, y) in enumerate(dataloader):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        with torch.cuda.amp.autocast(): # automatic mixed percision\n",
        "            x = trs(x) # image augmentation during train time to use gpu\n",
        "            pred = model(x) # default\n",
        "\n",
        "            # modules = [module for k, module in model._modules.items()]\n",
        "            # pred = checkpoint_sequential(functions=modules, segments=1, input=x) # gradient checkpointing for resnet and inception only\n",
        "            # # # pred = checkpoint_sequential(functions=model.mods, segments=1, input=x)\n",
        "\n",
        "            loss = loss_fn(pred, y) # /4 to scale by gradient accumulation_steps? no?\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        # if ((batch + 1) % 4 == 0) or (batch + 1 == len(dataloader)): # gradient accumulation\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        optimizer.zero_grad()\n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "\n",
        "        loss_list.append(loss.item())\n",
        "        # loss.backward()\n",
        "        # optimizer.step()\n",
        "        # if (batch) % (size//(10* len(x))) == 0:\n",
        "        # if batch % ((dataloader.sampler.num_samples/dataloader.batch_size)//10) == 0:\n",
        "        # loss, current = loss.item(), batch * len(x)\n",
        "        loss, current = loss.item()/len(y), batch * len(x)\n",
        "        # if verbose: print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "        if verbose: print(f\"loss: {loss:>7f} [{current:>5d}/{dataloader.sampler.num_samples:>5d}]\")\n",
        "    return loss_list\n",
        "\n",
        "def test(dataloader, model, loss_fn, verbose=True):\n",
        "    # size = len(dataloader.dataset)\n",
        "    # num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            x, y = X.to(device), y.to(device)\n",
        "            pred = model(x)\n",
        "            loss = loss_fn(pred, y)\n",
        "            # predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    # test_loss /= num_batches\n",
        "    # correct /= size\n",
        "    test_loss /= dataloader.sampler.num_samples\n",
        "    correct /= dataloader.sampler.num_samples\n",
        "    # if verbose: print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "    if verbose: print(f\"Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f}\")\n",
        "    return correct, test_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iuumbm2SB_lX",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title LR range test\n",
        "# gives insight into good LR range to use. \n",
        "# for accurate results, be sure to use a new model for range test; \n",
        "# also reset the model before training bec range test destroys the model\n",
        "# 1cycle super convergencehttps://arxiv.org/pdf/1708.07120.pdf\n",
        "# # cyclic lr https://arxiv.org/pdf/1506.01186.pdf\n",
        "# Note the learning rate value when the accuracy starts to\n",
        "# increase and when the accuracy slows, becomes ragged, or starts to fall\n",
        "\n",
        "# one training run of the network for a few epochs\n",
        "# pth='/content/lr.pth'\n",
        "# torch.save(model.state_dict(), pth) # save temporary model for lr finding\n",
        "# model.load_state_dict(torch.load(\"lr.pth\"))\n",
        "torch.autograd.set_detect_anomaly(True)\n",
        "epochs=1\n",
        "min_lr= 1e-6\n",
        "max_lr= 1e-3 # 1e-2\n",
        "# 152: 1e-7 - 1e-4      result 3e-7 - 3e-6\n",
        "# inception: 1e-7 - 1e1      result 3e-7 - 3e-6\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "# optimizer = torch.optim.AdamW(model.parameters(), lr=start_lr, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01)\n",
        "# optimizer = torch.optim.SGD(model.parameters(), lr=start_lr, momentum=0.9)\n",
        "import bitsandbytes as bnb\n",
        "optimizer = bnb.optim.AdamW(model.parameters(), lr=min_lr, betas=(0.9, 0.999), optim_bits=8)\n",
        "\n",
        "num_batches=len(test_loader)\n",
        "# num_batches=len(train_loader)\n",
        "\n",
        "total_steps=int(num_batches*epochs)\n",
        "# total_steps=int(np.ceil(num_batches/4)*epochs)\n",
        "\n",
        "# min_lr* gamma^total_steps = max_lr\n",
        "gamma = np.exp(np.log(max_lr/min_lr)/total_steps) # for scheduler step every optimizer step\n",
        "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=gamma) # 0.75(20)-0.9(100)\n",
        "train_lst, test_lst=[],[]\n",
        "lr_list=np.ones(total_steps)*min_lr*gamma**np.arange(total_steps)\n",
        "# gamma = np.exp(np.log(max_lr/min_lr)/(total_steps*4)) # total_steps*4 bec grad accumulation, loss step 4x per lr step\n",
        "# lr_list=np.ones(total_steps*4)*min_lr*gamma**np.arange(total_steps*4)\n",
        "\n",
        "\n",
        "for i in range(epochs):\n",
        "    train_ls = strain(test_loader, model, loss_fn, optimizer, scheduler)    \n",
        "    # train_ls = strain(train_loader, model, loss_fn, optimizer, scheduler)    \n",
        "    train_lst.extend(train_ls)\n",
        "\n",
        "# https://stackoverflow.com/a/53472966/13359815\n",
        "from scipy.ndimage.filters import gaussian_filter1d\n",
        "train_lstsm = gaussian_filter1d(train_lst, sigma=30)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(lr_list, train_lst)\n",
        "plt.plot(lr_list, train_lstsm)\n",
        "plt.xscale('log')\n",
        "# plt.plot(train_lst)\n",
        "# plt.plot(np.linspace(0,len(train_lst),len(test_lst)), test_lst)\n",
        "plt.show()\n",
        "plt.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDBEk-l-Oxjn",
        "outputId": "3b3034b6-fb55-4a7f-c412-823a768b2635"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===================================BUG REPORT===================================\n",
            "Welcome to bitsandbytes. For bug reports, please run\n",
            "\n",
            "python -m bitsandbytes\n",
            "\n",
            " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
            "================================================================================\n",
            "bin /usr/local/lib/python3.10/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so\n",
            "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
            "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0\n",
            "CUDA SETUP: Highest compute capability among GPUs detected: 7.5\n",
            "CUDA SETUP: Detected CUDA version 118\n",
            "CUDA SETUP: Loading binary /usr/local/lib/python3.10/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/lib64-nvidia did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/sys/fs/cgroup/memory.events /var/colab/cgroup/jupyter-children/memory.events')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('8013'), PosixPath('//172.28.0.1'), PosixPath('http')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//colab.research.google.com/tun/m/cc48301118ce562b961b3c22d803539adc1e0c19/gpu-t4-s-ot4vyr04ve9t --tunnel_background_save_delay=10s --tunnel_periodic_background_save_frequency=30m0s --enable_output_coalescing=true --output_coalescing_required=true'), PosixPath('--logtostderr --listen_host=172.28.0.12 --target_host=172.28.0.12 --tunnel_background_save_url=https')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/env/python')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//ipykernel.pylab.backend_inline'), PosixPath('module')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0'), PosixPath('/usr/local/cuda/lib64/libcudart.so')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
            "Either way, this might cause trouble in the future:\n",
            "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
            "  warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "3.0000000000000018e-06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 0.028013 [    0/70000]\n",
            "loss: 0.027990 [   64/70000]\n",
            "loss: 0.027891 [  128/70000]\n",
            "loss: 0.028031 [  192/70000]\n",
            "loss: 0.027995 [  256/70000]\n",
            "loss: 0.027862 [  320/70000]\n",
            "loss: 0.027917 [  384/70000]\n",
            "loss: 0.027954 [  448/70000]\n",
            "loss: 0.027939 [  512/70000]\n",
            "loss: 0.027912 [  576/70000]\n",
            "loss: 0.027858 [  640/70000]\n",
            "loss: 0.028022 [  704/70000]\n",
            "loss: 0.028003 [  768/70000]\n",
            "loss: 0.027945 [  832/70000]\n",
            "loss: 0.028147 [  896/70000]\n",
            "loss: 0.027986 [  960/70000]\n",
            "loss: 0.027887 [ 1024/70000]\n",
            "loss: 0.028239 [ 1088/70000]\n",
            "loss: 0.027936 [ 1152/70000]\n",
            "loss: 0.028028 [ 1216/70000]\n",
            "loss: 0.028006 [ 1280/70000]\n",
            "loss: 0.027841 [ 1344/70000]\n",
            "loss: 0.027944 [ 1408/70000]\n",
            "loss: 0.027977 [ 1472/70000]\n",
            "loss: 0.027873 [ 1536/70000]\n",
            "loss: 0.027786 [ 1600/70000]\n",
            "loss: 0.027971 [ 1664/70000]\n",
            "loss: 0.028081 [ 1728/70000]\n",
            "loss: 0.028049 [ 1792/70000]\n",
            "loss: 0.027893 [ 1856/70000]\n",
            "loss: 0.028045 [ 1920/70000]\n",
            "loss: 0.028024 [ 1984/70000]\n",
            "loss: 0.027953 [ 2048/70000]\n",
            "loss: 0.028036 [ 2112/70000]\n",
            "loss: 0.027940 [ 2176/70000]\n",
            "loss: 0.027980 [ 2240/70000]\n",
            "loss: 0.027909 [ 2304/70000]\n",
            "loss: 0.027929 [ 2368/70000]\n",
            "loss: 0.027884 [ 2432/70000]\n",
            "loss: 0.028045 [ 2496/70000]\n",
            "loss: 0.027822 [ 2560/70000]\n",
            "loss: 0.027984 [ 2624/70000]\n",
            "loss: 0.028082 [ 2688/70000]\n",
            "loss: 0.027889 [ 2752/70000]\n",
            "loss: 0.027959 [ 2816/70000]\n",
            "loss: 0.028118 [ 2880/70000]\n",
            "loss: 0.028119 [ 2944/70000]\n",
            "loss: 0.028147 [ 3008/70000]\n",
            "loss: 0.027847 [ 3072/70000]\n",
            "loss: 0.027968 [ 3136/70000]\n",
            "loss: 0.027963 [ 3200/70000]\n",
            "loss: 0.027939 [ 3264/70000]\n",
            "loss: 0.027922 [ 3328/70000]\n",
            "loss: 0.027972 [ 3392/70000]\n",
            "loss: 0.027853 [ 3456/70000]\n",
            "loss: 0.027924 [ 3520/70000]\n",
            "loss: 0.027902 [ 3584/70000]\n",
            "loss: 0.028023 [ 3648/70000]\n",
            "loss: 0.027953 [ 3712/70000]\n",
            "loss: 0.028012 [ 3776/70000]\n",
            "loss: 0.027896 [ 3840/70000]\n",
            "loss: 0.027912 [ 3904/70000]\n",
            "loss: 0.027929 [ 3968/70000]\n",
            "loss: 0.028026 [ 4032/70000]\n",
            "loss: 0.027937 [ 4096/70000]\n",
            "loss: 0.027931 [ 4160/70000]\n",
            "loss: 0.027993 [ 4224/70000]\n",
            "loss: 0.028109 [ 4288/70000]\n",
            "loss: 0.027766 [ 4352/70000]\n",
            "loss: 0.027917 [ 4416/70000]\n",
            "loss: 0.028125 [ 4480/70000]\n",
            "loss: 0.027838 [ 4544/70000]\n",
            "loss: 0.027932 [ 4608/70000]\n",
            "loss: 0.027927 [ 4672/70000]\n",
            "loss: 0.027972 [ 4736/70000]\n",
            "loss: 0.027809 [ 4800/70000]\n",
            "loss: 0.027881 [ 4864/70000]\n",
            "loss: 0.027820 [ 4928/70000]\n",
            "loss: 0.027855 [ 4992/70000]\n",
            "loss: 0.027935 [ 5056/70000]\n",
            "loss: 0.027923 [ 5120/70000]\n",
            "loss: 0.027798 [ 5184/70000]\n",
            "loss: 0.027868 [ 5248/70000]\n",
            "loss: 0.027849 [ 5312/70000]\n",
            "loss: 0.027958 [ 5376/70000]\n",
            "loss: 0.027867 [ 5440/70000]\n",
            "loss: 0.027734 [ 5504/70000]\n",
            "loss: 0.027875 [ 5568/70000]\n",
            "loss: 0.027744 [ 5632/70000]\n",
            "loss: 0.027809 [ 5696/70000]\n",
            "loss: 0.027797 [ 5760/70000]\n",
            "loss: 0.028005 [ 5824/70000]\n",
            "loss: 0.027926 [ 5888/70000]\n",
            "loss: 0.027863 [ 5952/70000]\n",
            "loss: 0.027727 [ 6016/70000]\n",
            "loss: 0.027844 [ 6080/70000]\n",
            "loss: 0.027661 [ 6144/70000]\n",
            "loss: 0.028010 [ 6208/70000]\n",
            "loss: 0.027682 [ 6272/70000]\n",
            "loss: 0.027874 [ 6336/70000]\n",
            "loss: 0.027698 [ 6400/70000]\n",
            "loss: 0.027906 [ 6464/70000]\n",
            "loss: 0.027651 [ 6528/70000]\n",
            "loss: 0.027917 [ 6592/70000]\n",
            "loss: 0.027566 [ 6656/70000]\n",
            "loss: 0.027861 [ 6720/70000]\n",
            "loss: 0.027687 [ 6784/70000]\n",
            "loss: 0.027980 [ 6848/70000]\n",
            "loss: 0.027801 [ 6912/70000]\n",
            "loss: 0.027779 [ 6976/70000]\n",
            "loss: 0.027745 [ 7040/70000]\n",
            "loss: 0.027680 [ 7104/70000]\n",
            "loss: 0.027703 [ 7168/70000]\n",
            "loss: 0.027871 [ 7232/70000]\n",
            "loss: 0.027809 [ 7296/70000]\n",
            "loss: 0.027901 [ 7360/70000]\n",
            "loss: 0.027673 [ 7424/70000]\n",
            "loss: 0.027778 [ 7488/70000]\n",
            "loss: 0.027759 [ 7552/70000]\n",
            "loss: 0.027835 [ 7616/70000]\n",
            "loss: 0.027758 [ 7680/70000]\n",
            "loss: 0.027778 [ 7744/70000]\n",
            "loss: 0.027585 [ 7808/70000]\n",
            "loss: 0.027606 [ 7872/70000]\n",
            "loss: 0.027699 [ 7936/70000]\n",
            "loss: 0.027500 [ 8000/70000]\n",
            "loss: 0.027634 [ 8064/70000]\n",
            "loss: 0.027654 [ 8128/70000]\n",
            "loss: 0.027607 [ 8192/70000]\n",
            "loss: 0.027347 [ 8256/70000]\n",
            "loss: 0.027544 [ 8320/70000]\n",
            "loss: 0.027585 [ 8384/70000]\n",
            "loss: 0.027483 [ 8448/70000]\n",
            "loss: 0.027680 [ 8512/70000]\n",
            "loss: 0.027638 [ 8576/70000]\n",
            "loss: 0.027717 [ 8640/70000]\n",
            "loss: 0.027673 [ 8704/70000]\n",
            "loss: 0.027602 [ 8768/70000]\n",
            "loss: 0.027312 [ 8832/70000]\n",
            "loss: 0.027734 [ 8896/70000]\n",
            "loss: 0.027221 [ 8960/70000]\n",
            "loss: 0.027431 [ 9024/70000]\n",
            "loss: 0.027325 [ 9088/70000]\n",
            "loss: 0.027375 [ 9152/70000]\n",
            "loss: 0.027482 [ 9216/70000]\n",
            "loss: 0.027654 [ 9280/70000]\n",
            "loss: 0.027562 [ 9344/70000]\n",
            "loss: 0.027526 [ 9408/70000]\n",
            "loss: 0.027473 [ 9472/70000]\n",
            "loss: 0.027098 [ 9536/70000]\n",
            "loss: 0.027284 [ 9600/70000]\n",
            "loss: 0.027562 [ 9664/70000]\n",
            "loss: 0.027236 [ 9728/70000]\n",
            "loss: 0.027416 [ 9792/70000]\n",
            "loss: 0.027383 [ 9856/70000]\n",
            "loss: 0.027339 [ 9920/70000]\n",
            "loss: 0.027290 [ 9984/70000]\n",
            "loss: 0.026949 [10048/70000]\n",
            "loss: 0.027088 [10112/70000]\n",
            "loss: 0.027336 [10176/70000]\n",
            "loss: 0.027432 [10240/70000]\n",
            "loss: 0.027081 [10304/70000]\n",
            "loss: 0.027077 [10368/70000]\n",
            "loss: 0.027413 [10432/70000]\n",
            "loss: 0.027104 [10496/70000]\n",
            "loss: 0.026962 [10560/70000]\n",
            "loss: 0.027265 [10624/70000]\n",
            "loss: 0.027462 [10688/70000]\n",
            "loss: 0.026858 [10752/70000]\n",
            "loss: 0.026738 [10816/70000]\n",
            "loss: 0.027020 [10880/70000]\n",
            "loss: 0.026771 [10944/70000]\n",
            "loss: 0.027104 [11008/70000]\n",
            "loss: 0.027275 [11072/70000]\n",
            "loss: 0.026931 [11136/70000]\n",
            "loss: 0.027005 [11200/70000]\n",
            "loss: 0.026566 [11264/70000]\n",
            "loss: 0.026139 [11328/70000]\n",
            "loss: 0.026737 [11392/70000]\n",
            "loss: 0.025940 [11456/70000]\n",
            "loss: 0.026862 [11520/70000]\n",
            "loss: 0.027561 [11584/70000]\n",
            "loss: 0.027046 [11648/70000]\n",
            "loss: 0.026071 [11712/70000]\n",
            "loss: 0.026713 [11776/70000]\n",
            "loss: 0.026426 [11840/70000]\n",
            "loss: 0.026996 [11904/70000]\n",
            "loss: 0.026924 [11968/70000]\n",
            "loss: 0.027304 [12032/70000]\n",
            "loss: 0.026128 [12096/70000]\n",
            "loss: 0.026216 [12160/70000]\n",
            "loss: 0.026292 [12224/70000]\n",
            "loss: 0.026933 [12288/70000]\n",
            "loss: 0.027381 [12352/70000]\n",
            "loss: 0.026207 [12416/70000]\n",
            "loss: 0.026680 [12480/70000]\n",
            "loss: 0.026436 [12544/70000]\n",
            "loss: 0.026826 [12608/70000]\n",
            "loss: 0.025981 [12672/70000]\n",
            "loss: 0.027308 [12736/70000]\n",
            "loss: 0.026993 [12800/70000]\n",
            "loss: 0.026398 [12864/70000]\n",
            "loss: 0.026562 [12928/70000]\n",
            "loss: 0.026368 [12992/70000]\n",
            "loss: 0.026386 [13056/70000]\n",
            "loss: 0.027408 [13120/70000]\n",
            "loss: 0.026529 [13184/70000]\n",
            "loss: 0.027256 [13248/70000]\n",
            "loss: 0.026384 [13312/70000]\n",
            "loss: 0.026406 [13376/70000]\n",
            "loss: 0.026609 [13440/70000]\n",
            "loss: 0.025744 [13504/70000]\n",
            "loss: 0.026281 [13568/70000]\n",
            "loss: 0.026431 [13632/70000]\n",
            "loss: 0.026530 [13696/70000]\n",
            "loss: 0.026046 [13760/70000]\n",
            "loss: 0.026776 [13824/70000]\n",
            "loss: 0.026259 [13888/70000]\n",
            "loss: 0.026547 [13952/70000]\n",
            "loss: 0.026398 [14016/70000]\n",
            "loss: 0.025960 [14080/70000]\n",
            "loss: 0.026385 [14144/70000]\n",
            "loss: 0.025614 [14208/70000]\n",
            "loss: 0.026582 [14272/70000]\n",
            "loss: 0.026230 [14336/70000]\n",
            "loss: 0.025857 [14400/70000]\n",
            "loss: 0.026862 [14464/70000]\n",
            "loss: 0.026279 [14528/70000]\n",
            "loss: 0.026498 [14592/70000]\n",
            "loss: 0.026487 [14656/70000]\n",
            "loss: 0.025577 [14720/70000]\n",
            "loss: 0.025993 [14784/70000]\n",
            "loss: 0.026109 [14848/70000]\n",
            "loss: 0.026069 [14912/70000]\n",
            "loss: 0.026574 [14976/70000]\n",
            "loss: 0.026600 [15040/70000]\n",
            "loss: 0.025875 [15104/70000]\n",
            "loss: 0.026717 [15168/70000]\n",
            "loss: 0.026653 [15232/70000]\n",
            "loss: 0.025824 [15296/70000]\n",
            "loss: 0.025589 [15360/70000]\n",
            "loss: 0.025665 [15424/70000]\n",
            "loss: 0.026016 [15488/70000]\n",
            "loss: 0.026307 [15552/70000]\n",
            "loss: 0.025766 [15616/70000]\n",
            "loss: 0.026292 [15680/70000]\n",
            "loss: 0.026225 [15744/70000]\n",
            "loss: 0.026580 [15808/70000]\n",
            "loss: 0.026336 [15872/70000]\n",
            "loss: 0.025335 [15936/70000]\n",
            "loss: 0.025665 [16000/70000]\n",
            "loss: 0.026197 [16064/70000]\n",
            "loss: 0.024721 [16128/70000]\n",
            "loss: 0.026034 [16192/70000]\n",
            "loss: 0.025735 [16256/70000]\n",
            "loss: 0.025931 [16320/70000]\n",
            "loss: 0.026844 [16384/70000]\n",
            "loss: 0.026084 [16448/70000]\n",
            "loss: 0.025912 [16512/70000]\n",
            "loss: 0.025789 [16576/70000]\n",
            "loss: 0.024953 [16640/70000]\n",
            "loss: 0.026153 [16704/70000]\n",
            "loss: 0.024784 [16768/70000]\n",
            "loss: 0.025721 [16832/70000]\n",
            "loss: 0.025424 [16896/70000]\n",
            "loss: 0.025777 [16960/70000]\n",
            "loss: 0.026389 [17024/70000]\n",
            "loss: 0.025650 [17088/70000]\n",
            "loss: 0.025482 [17152/70000]\n",
            "loss: 0.025051 [17216/70000]\n",
            "loss: 0.025374 [17280/70000]\n",
            "loss: 0.025727 [17344/70000]\n",
            "loss: 0.025579 [17408/70000]\n",
            "loss: 0.024857 [17472/70000]\n",
            "loss: 0.025768 [17536/70000]\n",
            "loss: 0.025103 [17600/70000]\n",
            "loss: 0.024961 [17664/70000]\n",
            "loss: 0.026067 [17728/70000]\n",
            "loss: 0.025496 [17792/70000]\n",
            "loss: 0.025614 [17856/70000]\n",
            "loss: 0.025603 [17920/70000]\n",
            "loss: 0.025421 [17984/70000]\n",
            "loss: 0.025584 [18048/70000]\n",
            "loss: 0.025162 [18112/70000]\n",
            "loss: 0.025627 [18176/70000]\n",
            "loss: 0.026671 [18240/70000]\n",
            "loss: 0.026024 [18304/70000]\n",
            "loss: 0.026172 [18368/70000]\n",
            "loss: 0.027373 [18432/70000]\n",
            "loss: 0.025555 [18496/70000]\n",
            "loss: 0.025852 [18560/70000]\n",
            "loss: 0.024450 [18624/70000]\n",
            "loss: 0.024779 [18688/70000]\n",
            "loss: 0.024764 [18752/70000]\n",
            "loss: 0.025312 [18816/70000]\n",
            "loss: 0.024876 [18880/70000]\n",
            "loss: 0.026235 [18944/70000]\n",
            "loss: 0.026368 [19008/70000]\n",
            "loss: 0.025197 [19072/70000]\n",
            "loss: 0.026060 [19136/70000]\n",
            "loss: 0.025018 [19200/70000]\n",
            "loss: 0.025297 [19264/70000]\n",
            "loss: 0.025508 [19328/70000]\n",
            "loss: 0.026056 [19392/70000]\n",
            "loss: 0.025519 [19456/70000]\n",
            "loss: 0.026026 [19520/70000]\n",
            "loss: 0.024843 [19584/70000]\n",
            "loss: 0.025146 [19648/70000]\n",
            "loss: 0.024734 [19712/70000]\n",
            "loss: 0.024205 [19776/70000]\n",
            "loss: 0.024947 [19840/70000]\n",
            "loss: 0.026342 [19904/70000]\n",
            "loss: 0.025167 [19968/70000]\n",
            "loss: 0.025635 [20032/70000]\n",
            "loss: 0.025129 [20096/70000]\n",
            "loss: 0.025583 [20160/70000]\n",
            "loss: 0.024971 [20224/70000]\n",
            "loss: 0.025061 [20288/70000]\n",
            "loss: 0.024109 [20352/70000]\n",
            "loss: 0.026945 [20416/70000]\n",
            "loss: 0.025377 [20480/70000]\n",
            "loss: 0.025978 [20544/70000]\n",
            "loss: 0.025112 [20608/70000]\n",
            "loss: 0.024418 [20672/70000]\n",
            "loss: 0.024396 [20736/70000]\n",
            "loss: 0.024927 [20800/70000]\n",
            "loss: 0.024204 [20864/70000]\n",
            "loss: 0.025281 [20928/70000]\n",
            "loss: 0.024428 [20992/70000]\n",
            "loss: 0.025003 [21056/70000]\n",
            "loss: 0.025999 [21120/70000]\n",
            "loss: 0.025561 [21184/70000]\n",
            "loss: 0.024410 [21248/70000]\n",
            "loss: 0.025016 [21312/70000]\n",
            "loss: 0.024961 [21376/70000]\n",
            "loss: 0.024708 [21440/70000]\n",
            "loss: 0.025203 [21504/70000]\n",
            "loss: 0.025759 [21568/70000]\n",
            "loss: 0.025091 [21632/70000]\n",
            "loss: 0.025077 [21696/70000]\n",
            "loss: 0.025488 [21760/70000]\n",
            "loss: 0.024660 [21824/70000]\n",
            "loss: 0.025899 [21888/70000]\n",
            "loss: 0.024178 [21952/70000]\n",
            "loss: 0.026277 [22016/70000]\n",
            "loss: 0.024755 [22080/70000]\n",
            "loss: 0.025263 [22144/70000]\n",
            "loss: 0.025574 [22208/70000]\n",
            "loss: 0.024523 [22272/70000]\n",
            "loss: 0.025545 [22336/70000]\n",
            "loss: 0.024838 [22400/70000]\n",
            "loss: 0.024911 [22464/70000]\n",
            "loss: 0.024263 [22528/70000]\n",
            "loss: 0.024693 [22592/70000]\n",
            "loss: 0.025460 [22656/70000]\n",
            "loss: 0.024537 [22720/70000]\n",
            "loss: 0.024887 [22784/70000]\n",
            "loss: 0.023846 [22848/70000]\n",
            "loss: 0.025025 [22912/70000]\n",
            "loss: 0.024513 [22976/70000]\n",
            "loss: 0.025718 [23040/70000]\n",
            "loss: 0.024682 [23104/70000]\n",
            "loss: 0.025490 [23168/70000]\n",
            "loss: 0.023850 [23232/70000]\n",
            "loss: 0.024563 [23296/70000]\n",
            "loss: 0.024309 [23360/70000]\n",
            "loss: 0.023317 [23424/70000]\n",
            "loss: 0.025575 [23488/70000]\n",
            "loss: 0.025595 [23552/70000]\n",
            "loss: 0.025013 [23616/70000]\n",
            "loss: 0.024228 [23680/70000]\n",
            "loss: 0.024994 [23744/70000]\n",
            "loss: 0.024467 [23808/70000]\n",
            "loss: 0.024616 [23872/70000]\n",
            "loss: 0.025282 [23936/70000]\n",
            "loss: 0.025011 [24000/70000]\n",
            "loss: 0.026209 [24064/70000]\n",
            "loss: 0.025236 [24128/70000]\n",
            "loss: 0.025341 [24192/70000]\n",
            "loss: 0.025437 [24256/70000]\n",
            "loss: 0.024668 [24320/70000]\n",
            "loss: 0.024910 [24384/70000]\n",
            "loss: 0.024880 [24448/70000]\n",
            "loss: 0.026451 [24512/70000]\n",
            "loss: 0.025232 [24576/70000]\n",
            "loss: 0.026066 [24640/70000]\n",
            "loss: 0.023707 [24704/70000]\n",
            "loss: 0.024297 [24768/70000]\n",
            "loss: 0.024558 [24832/70000]\n",
            "loss: 0.024358 [24896/70000]\n",
            "loss: 0.025300 [24960/70000]\n",
            "loss: 0.024636 [25024/70000]\n",
            "loss: 0.025096 [25088/70000]\n",
            "loss: 0.024846 [25152/70000]\n",
            "loss: 0.024856 [25216/70000]\n",
            "loss: 0.024040 [25280/70000]\n",
            "loss: 0.023392 [25344/70000]\n",
            "loss: 0.023466 [25408/70000]\n",
            "loss: 0.025057 [25472/70000]\n",
            "loss: 0.024099 [25536/70000]\n",
            "loss: 0.025814 [25600/70000]\n",
            "loss: 0.024612 [25664/70000]\n",
            "loss: 0.024130 [25728/70000]\n",
            "loss: 0.025249 [25792/70000]\n",
            "loss: 0.024417 [25856/70000]\n",
            "loss: 0.024210 [25920/70000]\n",
            "loss: 0.025883 [25984/70000]\n",
            "loss: 0.024376 [26048/70000]\n",
            "loss: 0.024954 [26112/70000]\n",
            "loss: 0.023846 [26176/70000]\n",
            "loss: 0.024227 [26240/70000]\n",
            "loss: 0.025179 [26304/70000]\n",
            "loss: 0.025213 [26368/70000]\n",
            "loss: 0.025267 [26432/70000]\n",
            "loss: 0.024297 [26496/70000]\n",
            "loss: 0.025672 [26560/70000]\n",
            "loss: 0.025299 [26624/70000]\n",
            "loss: 0.024289 [26688/70000]\n",
            "loss: 0.025017 [26752/70000]\n",
            "loss: 0.026118 [26816/70000]\n",
            "loss: 0.024877 [26880/70000]\n",
            "loss: 0.024106 [26944/70000]\n",
            "loss: 0.024944 [27008/70000]\n",
            "loss: 0.023716 [27072/70000]\n",
            "loss: 0.024495 [27136/70000]\n",
            "loss: 0.025396 [27200/70000]\n",
            "loss: 0.024446 [27264/70000]\n",
            "loss: 0.023876 [27328/70000]\n",
            "loss: 0.024347 [27392/70000]\n",
            "loss: 0.025663 [27456/70000]\n",
            "loss: 0.023919 [27520/70000]\n",
            "loss: 0.025736 [27584/70000]\n",
            "loss: 0.024948 [27648/70000]\n",
            "loss: 0.023138 [27712/70000]\n",
            "loss: 0.024657 [27776/70000]\n",
            "loss: 0.024390 [27840/70000]\n",
            "loss: 0.023409 [27904/70000]\n",
            "loss: 0.025290 [27968/70000]\n",
            "loss: 0.025856 [28032/70000]\n",
            "loss: 0.025697 [28096/70000]\n",
            "loss: 0.024912 [28160/70000]\n",
            "loss: 0.024302 [28224/70000]\n",
            "loss: 0.025601 [28288/70000]\n",
            "loss: 0.025029 [28352/70000]\n",
            "loss: 0.024966 [28416/70000]\n",
            "loss: 0.024909 [28480/70000]\n",
            "loss: 0.024732 [28544/70000]\n",
            "loss: 0.025582 [28608/70000]\n",
            "loss: 0.024435 [28672/70000]\n",
            "loss: 0.023504 [28736/70000]\n",
            "loss: 0.024934 [28800/70000]\n",
            "loss: 0.024898 [28864/70000]\n",
            "loss: 0.024138 [28928/70000]\n",
            "loss: 0.024793 [28992/70000]\n",
            "loss: 0.025620 [29056/70000]\n",
            "loss: 0.024582 [29120/70000]\n",
            "loss: 0.024214 [29184/70000]\n",
            "loss: 0.025297 [29248/70000]\n",
            "loss: 0.025151 [29312/70000]\n",
            "loss: 0.024865 [29376/70000]\n",
            "loss: 0.025621 [29440/70000]\n",
            "loss: 0.024124 [29504/70000]\n",
            "loss: 0.025364 [29568/70000]\n",
            "loss: 0.024790 [29632/70000]\n",
            "loss: 0.025423 [29696/70000]\n",
            "loss: 0.024772 [29760/70000]\n",
            "loss: 0.024983 [29824/70000]\n",
            "loss: 0.024751 [29888/70000]\n",
            "loss: 0.024181 [29952/70000]\n",
            "loss: 0.024141 [30016/70000]\n",
            "loss: 0.024612 [30080/70000]\n",
            "loss: 0.024805 [30144/70000]\n",
            "loss: 0.024678 [30208/70000]\n",
            "loss: 0.024211 [30272/70000]\n",
            "loss: 0.022734 [30336/70000]\n",
            "loss: 0.024136 [30400/70000]\n",
            "loss: 0.024811 [30464/70000]\n",
            "loss: 0.024517 [30528/70000]\n",
            "loss: 0.024776 [30592/70000]\n",
            "loss: 0.025318 [30656/70000]\n",
            "loss: 0.025383 [30720/70000]\n",
            "loss: 0.025083 [30784/70000]\n",
            "loss: 0.024442 [30848/70000]\n",
            "loss: 0.025124 [30912/70000]\n",
            "loss: 0.024070 [30976/70000]\n",
            "loss: 0.024150 [31040/70000]\n",
            "loss: 0.023757 [31104/70000]\n",
            "loss: 0.025439 [31168/70000]\n",
            "loss: 0.025812 [31232/70000]\n",
            "loss: 0.025290 [31296/70000]\n",
            "loss: 0.024930 [31360/70000]\n",
            "loss: 0.026000 [31424/70000]\n",
            "loss: 0.024512 [31488/70000]\n",
            "loss: 0.024175 [31552/70000]\n",
            "loss: 0.023156 [31616/70000]\n",
            "loss: 0.022606 [31680/70000]\n",
            "loss: 0.023887 [31744/70000]\n",
            "loss: 0.024781 [31808/70000]\n",
            "loss: 0.024772 [31872/70000]\n",
            "loss: 0.024298 [31936/70000]\n",
            "loss: 0.024940 [32000/70000]\n",
            "loss: 0.024653 [32064/70000]\n",
            "loss: 0.023495 [32128/70000]\n",
            "loss: 0.024611 [32192/70000]\n",
            "loss: 0.024770 [32256/70000]\n",
            "loss: 0.024929 [32320/70000]\n",
            "loss: 0.025146 [32384/70000]\n",
            "loss: 0.025156 [32448/70000]\n",
            "loss: 0.023847 [32512/70000]\n",
            "loss: 0.024304 [32576/70000]\n",
            "loss: 0.024292 [32640/70000]\n",
            "loss: 0.024356 [32704/70000]\n",
            "loss: 0.024649 [32768/70000]\n",
            "loss: 0.024034 [32832/70000]\n",
            "loss: 0.024537 [32896/70000]\n",
            "loss: 0.024552 [32960/70000]\n",
            "loss: 0.024238 [33024/70000]\n",
            "loss: 0.024620 [33088/70000]\n",
            "loss: 0.023830 [33152/70000]\n",
            "loss: 0.024376 [33216/70000]\n",
            "loss: 0.024575 [33280/70000]\n",
            "loss: 0.024662 [33344/70000]\n",
            "loss: 0.023586 [33408/70000]\n",
            "loss: 0.025775 [33472/70000]\n",
            "loss: 0.023683 [33536/70000]\n",
            "loss: 0.024132 [33600/70000]\n",
            "loss: 0.024784 [33664/70000]\n",
            "loss: 0.024173 [33728/70000]\n",
            "loss: 0.024620 [33792/70000]\n",
            "loss: 0.024787 [33856/70000]\n",
            "loss: 0.023167 [33920/70000]\n",
            "loss: 0.024272 [33984/70000]\n",
            "loss: 0.025868 [34048/70000]\n",
            "loss: 0.023659 [34112/70000]\n",
            "loss: 0.024411 [34176/70000]\n",
            "loss: 0.023876 [34240/70000]\n",
            "loss: 0.024452 [34304/70000]\n",
            "loss: 0.023873 [34368/70000]\n",
            "loss: 0.026084 [34432/70000]\n",
            "loss: 0.024792 [34496/70000]\n",
            "loss: 0.024055 [34560/70000]\n",
            "loss: 0.023723 [34624/70000]\n",
            "loss: 0.023661 [34688/70000]\n",
            "loss: 0.024755 [34752/70000]\n",
            "loss: 0.023220 [34816/70000]\n",
            "loss: 0.023616 [34880/70000]\n",
            "loss: 0.023905 [34944/70000]\n",
            "loss: 0.024279 [35008/70000]\n",
            "loss: 0.022898 [35072/70000]\n",
            "loss: 0.023657 [35136/70000]\n",
            "loss: 0.023620 [35200/70000]\n",
            "loss: 0.024247 [35264/70000]\n",
            "loss: 0.023140 [35328/70000]\n",
            "loss: 0.023127 [35392/70000]\n",
            "loss: 0.024596 [35456/70000]\n",
            "loss: 0.024114 [35520/70000]\n",
            "loss: 0.022544 [35584/70000]\n",
            "loss: 0.023695 [35648/70000]\n",
            "loss: 0.023650 [35712/70000]\n",
            "loss: 0.024596 [35776/70000]\n",
            "loss: 0.025264 [35840/70000]\n",
            "loss: 0.024727 [35904/70000]\n",
            "loss: 0.024123 [35968/70000]\n",
            "loss: 0.023695 [36032/70000]\n",
            "loss: 0.023481 [36096/70000]\n",
            "loss: 0.024143 [36160/70000]\n",
            "loss: 0.023637 [36224/70000]\n",
            "loss: 0.024434 [36288/70000]\n",
            "loss: 0.024918 [36352/70000]\n",
            "loss: 0.024844 [36416/70000]\n",
            "loss: 0.023964 [36480/70000]\n",
            "loss: 0.024066 [36544/70000]\n",
            "loss: 0.024004 [36608/70000]\n",
            "loss: 0.023765 [36672/70000]\n",
            "loss: 0.024395 [36736/70000]\n",
            "loss: 0.024733 [36800/70000]\n",
            "loss: 0.023942 [36864/70000]\n",
            "loss: 0.023087 [36928/70000]\n",
            "loss: 0.024509 [36992/70000]\n",
            "loss: 0.024113 [37056/70000]\n",
            "loss: 0.024929 [37120/70000]\n",
            "loss: 0.023939 [37184/70000]\n",
            "loss: 0.023875 [37248/70000]\n",
            "loss: 0.024143 [37312/70000]\n",
            "loss: 0.023755 [37376/70000]\n",
            "loss: 0.024374 [37440/70000]\n",
            "loss: 0.023625 [37504/70000]\n",
            "loss: 0.024843 [37568/70000]\n",
            "loss: 0.023890 [37632/70000]\n",
            "loss: 0.023844 [37696/70000]\n",
            "loss: 0.025833 [37760/70000]\n",
            "loss: 0.024821 [37824/70000]\n",
            "loss: 0.024052 [37888/70000]\n",
            "loss: 0.024306 [37952/70000]\n",
            "loss: 0.024064 [38016/70000]\n",
            "loss: 0.025264 [38080/70000]\n",
            "loss: 0.024715 [38144/70000]\n",
            "loss: 0.023284 [38208/70000]\n",
            "loss: 0.023939 [38272/70000]\n",
            "loss: 0.024138 [38336/70000]\n",
            "loss: 0.023367 [38400/70000]\n",
            "loss: 0.024018 [38464/70000]\n",
            "loss: 0.024097 [38528/70000]\n",
            "loss: 0.023479 [38592/70000]\n",
            "loss: 0.023818 [38656/70000]\n",
            "loss: 0.023392 [38720/70000]\n",
            "loss: 0.023171 [38784/70000]\n",
            "loss: 0.024966 [38848/70000]\n",
            "loss: 0.025584 [38912/70000]\n",
            "loss: 0.023852 [38976/70000]\n",
            "loss: 0.024445 [39040/70000]\n",
            "loss: 0.024307 [39104/70000]\n",
            "loss: 0.023152 [39168/70000]\n",
            "loss: 0.022707 [39232/70000]\n",
            "loss: 0.024593 [39296/70000]\n",
            "loss: 0.023047 [39360/70000]\n",
            "loss: 0.022917 [39424/70000]\n",
            "loss: 0.024348 [39488/70000]\n",
            "loss: 0.023767 [39552/70000]\n",
            "loss: 0.023336 [39616/70000]\n",
            "loss: 0.024572 [39680/70000]\n",
            "loss: 0.023858 [39744/70000]\n",
            "loss: 0.023393 [39808/70000]\n",
            "loss: 0.023285 [39872/70000]\n",
            "loss: 0.023435 [39936/70000]\n",
            "loss: 0.024277 [40000/70000]\n",
            "loss: 0.024202 [40064/70000]\n",
            "loss: 0.023591 [40128/70000]\n",
            "loss: 0.025064 [40192/70000]\n",
            "loss: 0.023878 [40256/70000]\n",
            "loss: 0.024818 [40320/70000]\n",
            "loss: 0.022744 [40384/70000]\n",
            "loss: 0.025362 [40448/70000]\n",
            "loss: 0.025476 [40512/70000]\n",
            "loss: 0.022054 [40576/70000]\n",
            "loss: 0.024463 [40640/70000]\n",
            "loss: 0.025072 [40704/70000]\n",
            "loss: 0.024614 [40768/70000]\n",
            "loss: 0.022948 [40832/70000]\n",
            "loss: 0.025024 [40896/70000]\n",
            "loss: 0.024952 [40960/70000]\n",
            "loss: 0.023561 [41024/70000]\n",
            "loss: 0.024186 [41088/70000]\n",
            "loss: 0.024049 [41152/70000]\n",
            "loss: 0.022999 [41216/70000]\n",
            "loss: 0.024014 [41280/70000]\n",
            "loss: 0.023671 [41344/70000]\n",
            "loss: 0.024116 [41408/70000]\n",
            "loss: 0.023626 [41472/70000]\n",
            "loss: 0.024643 [41536/70000]\n",
            "loss: 0.024684 [41600/70000]\n",
            "loss: 0.023212 [41664/70000]\n",
            "loss: 0.022436 [41728/70000]\n",
            "loss: 0.024553 [41792/70000]\n",
            "loss: 0.024627 [41856/70000]\n",
            "loss: 0.024474 [41920/70000]\n",
            "loss: 0.025372 [41984/70000]\n",
            "loss: 0.023794 [42048/70000]\n",
            "loss: 0.024398 [42112/70000]\n",
            "loss: 0.025319 [42176/70000]\n",
            "loss: 0.024326 [42240/70000]\n",
            "loss: 0.025496 [42304/70000]\n",
            "loss: 0.024745 [42368/70000]\n",
            "loss: 0.025679 [42432/70000]\n",
            "loss: 0.023426 [42496/70000]\n",
            "loss: 0.024817 [42560/70000]\n",
            "loss: 0.024079 [42624/70000]\n",
            "loss: 0.023136 [42688/70000]\n",
            "loss: 0.024172 [42752/70000]\n",
            "loss: 0.023393 [42816/70000]\n",
            "loss: 0.023959 [42880/70000]\n",
            "loss: 0.024454 [42944/70000]\n",
            "loss: 0.022717 [43008/70000]\n",
            "loss: 0.025546 [43072/70000]\n",
            "loss: 0.025116 [43136/70000]\n",
            "loss: 0.024025 [43200/70000]\n",
            "loss: 0.024005 [43264/70000]\n",
            "loss: 0.023965 [43328/70000]\n",
            "loss: 0.024444 [43392/70000]\n",
            "loss: 0.023365 [43456/70000]\n",
            "loss: 0.024151 [43520/70000]\n",
            "loss: 0.024981 [43584/70000]\n",
            "loss: 0.022499 [43648/70000]\n",
            "loss: 0.023879 [43712/70000]\n",
            "loss: 0.025421 [43776/70000]\n",
            "loss: 0.023657 [43840/70000]\n",
            "loss: 0.025212 [43904/70000]\n",
            "loss: 0.023619 [43968/70000]\n",
            "loss: 0.023541 [44032/70000]\n",
            "loss: 0.023375 [44096/70000]\n",
            "loss: 0.024391 [44160/70000]\n",
            "loss: 0.024599 [44224/70000]\n",
            "loss: 0.023884 [44288/70000]\n",
            "loss: 0.023904 [44352/70000]\n",
            "loss: 0.023062 [44416/70000]\n",
            "loss: 0.023400 [44480/70000]\n",
            "loss: 0.023041 [44544/70000]\n",
            "loss: 0.025109 [44608/70000]\n",
            "loss: 0.023318 [44672/70000]\n",
            "loss: 0.024296 [44736/70000]\n",
            "loss: 0.023744 [44800/70000]\n",
            "loss: 0.023797 [44864/70000]\n",
            "loss: 0.024935 [44928/70000]\n",
            "loss: 0.024190 [44992/70000]\n",
            "loss: 0.025157 [45056/70000]\n",
            "loss: 0.024126 [45120/70000]\n",
            "loss: 0.022954 [45184/70000]\n",
            "loss: 0.024707 [45248/70000]\n",
            "loss: 0.024581 [45312/70000]\n",
            "loss: 0.025342 [45376/70000]\n",
            "loss: 0.024763 [45440/70000]\n",
            "loss: 0.024713 [45504/70000]\n",
            "loss: 0.023880 [45568/70000]\n",
            "loss: 0.022703 [45632/70000]\n",
            "loss: 0.022757 [45696/70000]\n",
            "loss: 0.022266 [45760/70000]\n",
            "loss: 0.025460 [45824/70000]\n",
            "loss: 0.023798 [45888/70000]\n",
            "loss: 0.023851 [45952/70000]\n",
            "loss: 0.023901 [46016/70000]\n",
            "loss: 0.024542 [46080/70000]\n",
            "loss: 0.023444 [46144/70000]\n",
            "loss: 0.023685 [46208/70000]\n",
            "loss: 0.022872 [46272/70000]\n",
            "loss: 0.023907 [46336/70000]\n",
            "loss: 0.025140 [46400/70000]\n",
            "loss: 0.024756 [46464/70000]\n",
            "loss: 0.024024 [46528/70000]\n",
            "loss: 0.023466 [46592/70000]\n",
            "loss: 0.025011 [46656/70000]\n",
            "loss: 0.023714 [46720/70000]\n",
            "loss: 0.024110 [46784/70000]\n",
            "loss: 0.024918 [46848/70000]\n",
            "loss: 0.023800 [46912/70000]\n",
            "loss: 0.023979 [46976/70000]\n",
            "loss: 0.024082 [47040/70000]\n",
            "loss: 0.023537 [47104/70000]\n",
            "loss: 0.024248 [47168/70000]\n",
            "loss: 0.023517 [47232/70000]\n",
            "loss: 0.023438 [47296/70000]\n",
            "loss: 0.024626 [47360/70000]\n",
            "loss: 0.024526 [47424/70000]\n",
            "loss: 0.024756 [47488/70000]\n",
            "loss: 0.024825 [47552/70000]\n",
            "loss: 0.024579 [47616/70000]\n",
            "loss: 0.023478 [47680/70000]\n",
            "loss: 0.022149 [47744/70000]\n",
            "loss: 0.023388 [47808/70000]\n",
            "loss: 0.023809 [47872/70000]\n",
            "loss: 0.024490 [47936/70000]\n",
            "loss: 0.024322 [48000/70000]\n",
            "loss: 0.024002 [48064/70000]\n",
            "loss: 0.024319 [48128/70000]\n",
            "loss: 0.023084 [48192/70000]\n",
            "loss: 0.022806 [48256/70000]\n",
            "loss: 0.024191 [48320/70000]\n",
            "loss: 0.024169 [48384/70000]\n",
            "loss: 0.023698 [48448/70000]\n",
            "loss: 0.023096 [48512/70000]\n",
            "loss: 0.024313 [48576/70000]\n",
            "loss: 0.024523 [48640/70000]\n",
            "loss: 0.022926 [48704/70000]\n",
            "loss: 0.024177 [48768/70000]\n",
            "loss: 0.025207 [48832/70000]\n",
            "loss: 0.024368 [48896/70000]\n",
            "loss: 0.023360 [48960/70000]\n",
            "loss: 0.024662 [49024/70000]\n",
            "loss: 0.023934 [49088/70000]\n",
            "loss: 0.025154 [49152/70000]\n",
            "loss: 0.024270 [49216/70000]\n",
            "loss: 0.024047 [49280/70000]\n",
            "loss: 0.023983 [49344/70000]\n",
            "loss: 0.023401 [49408/70000]\n",
            "loss: 0.022953 [49472/70000]\n",
            "loss: 0.023698 [49536/70000]\n",
            "loss: 0.024633 [49600/70000]\n",
            "loss: 0.024444 [49664/70000]\n",
            "loss: 0.024313 [49728/70000]\n",
            "loss: 0.023524 [49792/70000]\n",
            "loss: 0.023408 [49856/70000]\n",
            "loss: 0.024143 [49920/70000]\n",
            "loss: 0.024274 [49984/70000]\n",
            "loss: 0.023821 [50048/70000]\n",
            "loss: 0.023818 [50112/70000]\n",
            "loss: 0.023778 [50176/70000]\n",
            "loss: 0.024662 [50240/70000]\n",
            "loss: 0.024556 [50304/70000]\n",
            "loss: 0.024996 [50368/70000]\n",
            "loss: 0.023928 [50432/70000]\n",
            "loss: 0.024088 [50496/70000]\n",
            "loss: 0.022062 [50560/70000]\n",
            "loss: 0.023939 [50624/70000]\n",
            "loss: 0.024363 [50688/70000]\n",
            "loss: 0.024146 [50752/70000]\n",
            "loss: 0.023157 [50816/70000]\n",
            "loss: 0.023791 [50880/70000]\n",
            "loss: 0.023119 [50944/70000]\n",
            "loss: 0.026042 [51008/70000]\n",
            "loss: 0.023786 [51072/70000]\n",
            "loss: 0.023059 [51136/70000]\n",
            "loss: 0.024373 [51200/70000]\n",
            "loss: 0.024846 [51264/70000]\n",
            "loss: 0.023400 [51328/70000]\n",
            "loss: 0.024017 [51392/70000]\n",
            "loss: 0.023835 [51456/70000]\n",
            "loss: 0.025195 [51520/70000]\n",
            "loss: 0.023906 [51584/70000]\n",
            "loss: 0.024784 [51648/70000]\n",
            "loss: 0.022977 [51712/70000]\n",
            "loss: 0.024000 [51776/70000]\n",
            "loss: 0.025084 [51840/70000]\n",
            "loss: 0.024848 [51904/70000]\n",
            "loss: 0.023030 [51968/70000]\n",
            "loss: 0.023169 [52032/70000]\n",
            "loss: 0.023444 [52096/70000]\n",
            "loss: 0.023317 [52160/70000]\n",
            "loss: 0.024627 [52224/70000]\n",
            "loss: 0.023774 [52288/70000]\n",
            "loss: 0.023725 [52352/70000]\n",
            "loss: 0.025411 [52416/70000]\n",
            "loss: 0.023663 [52480/70000]\n",
            "loss: 0.024699 [52544/70000]\n",
            "loss: 0.022406 [52608/70000]\n",
            "loss: 0.024046 [52672/70000]\n",
            "loss: 0.024071 [52736/70000]\n",
            "loss: 0.023779 [52800/70000]\n",
            "loss: 0.023538 [52864/70000]\n",
            "loss: 0.023308 [52928/70000]\n",
            "loss: 0.024580 [52992/70000]\n",
            "loss: 0.023910 [53056/70000]\n",
            "loss: 0.024121 [53120/70000]\n",
            "loss: 0.025305 [53184/70000]\n",
            "loss: 0.025590 [53248/70000]\n",
            "loss: 0.024667 [53312/70000]\n",
            "loss: 0.023647 [53376/70000]\n",
            "loss: 0.024547 [53440/70000]\n",
            "loss: 0.023528 [53504/70000]\n",
            "loss: 0.024585 [53568/70000]\n",
            "loss: 0.023845 [53632/70000]\n",
            "loss: 0.024545 [53696/70000]\n",
            "loss: 0.025195 [53760/70000]\n",
            "loss: 0.023304 [53824/70000]\n",
            "loss: 0.023509 [53888/70000]\n",
            "loss: 0.024578 [53952/70000]\n",
            "loss: 0.024026 [54016/70000]\n",
            "loss: 0.022880 [54080/70000]\n",
            "loss: 0.024826 [54144/70000]\n",
            "loss: 0.024435 [54208/70000]\n",
            "loss: 0.023241 [54272/70000]\n",
            "loss: 0.023119 [54336/70000]\n",
            "loss: 0.024217 [54400/70000]\n",
            "loss: 0.022094 [54464/70000]\n",
            "loss: 0.023707 [54528/70000]\n",
            "loss: 0.023531 [54592/70000]\n",
            "loss: 0.025393 [54656/70000]\n",
            "loss: 0.024020 [54720/70000]\n",
            "loss: 0.024062 [54784/70000]\n",
            "loss: 0.025639 [54848/70000]\n",
            "loss: 0.025691 [54912/70000]\n",
            "loss: 0.024040 [54976/70000]\n",
            "loss: 0.024412 [55040/70000]\n",
            "loss: 0.024671 [55104/70000]\n",
            "loss: 0.024714 [55168/70000]\n",
            "loss: 0.024639 [55232/70000]\n",
            "loss: 0.023891 [55296/70000]\n",
            "loss: 0.024324 [55360/70000]\n",
            "loss: 0.024890 [55424/70000]\n",
            "loss: 0.024518 [55488/70000]\n",
            "loss: 0.023597 [55552/70000]\n",
            "loss: 0.024194 [55616/70000]\n",
            "loss: 0.023103 [55680/70000]\n",
            "loss: 0.024521 [55744/70000]\n",
            "loss: 0.023321 [55808/70000]\n",
            "loss: 0.023713 [55872/70000]\n",
            "loss: 0.023298 [55936/70000]\n",
            "loss: 0.023647 [56000/70000]\n",
            "loss: 0.023866 [56064/70000]\n",
            "loss: 0.024788 [56128/70000]\n",
            "loss: 0.023341 [56192/70000]\n",
            "loss: 0.022571 [56256/70000]\n",
            "loss: 0.023890 [56320/70000]\n",
            "loss: 0.023903 [56384/70000]\n",
            "loss: 0.023110 [56448/70000]\n",
            "loss: 0.023424 [56512/70000]\n",
            "loss: 0.024388 [56576/70000]\n",
            "loss: 0.024259 [56640/70000]\n",
            "loss: 0.022796 [56704/70000]\n",
            "loss: 0.025633 [56768/70000]\n",
            "loss: 0.026071 [56832/70000]\n",
            "loss: 0.023654 [56896/70000]\n",
            "loss: 0.024181 [56960/70000]\n",
            "loss: 0.024463 [57024/70000]\n",
            "loss: 0.023068 [57088/70000]\n",
            "loss: 0.024019 [57152/70000]\n",
            "loss: 0.023880 [57216/70000]\n",
            "loss: 0.025058 [57280/70000]\n",
            "loss: 0.024322 [57344/70000]\n",
            "loss: 0.025249 [57408/70000]\n",
            "loss: 0.022515 [57472/70000]\n",
            "loss: 0.023548 [57536/70000]\n",
            "loss: 0.023125 [57600/70000]\n",
            "loss: 0.025675 [57664/70000]\n",
            "loss: 0.024365 [57728/70000]\n",
            "loss: 0.024655 [57792/70000]\n",
            "loss: 0.025197 [57856/70000]\n",
            "loss: 0.023378 [57920/70000]\n",
            "loss: 0.024079 [57984/70000]\n",
            "loss: 0.024017 [58048/70000]\n",
            "loss: 0.024847 [58112/70000]\n",
            "loss: 0.023953 [58176/70000]\n",
            "loss: 0.024164 [58240/70000]\n",
            "loss: 0.024438 [58304/70000]\n",
            "loss: 0.024088 [58368/70000]\n",
            "loss: 0.025081 [58432/70000]\n",
            "loss: 0.024303 [58496/70000]\n",
            "loss: 0.022536 [58560/70000]\n",
            "loss: 0.024139 [58624/70000]\n",
            "loss: 0.024171 [58688/70000]\n",
            "loss: 0.024290 [58752/70000]\n",
            "loss: 0.023641 [58816/70000]\n",
            "loss: 0.023193 [58880/70000]\n",
            "loss: 0.023396 [58944/70000]\n",
            "loss: 0.025267 [59008/70000]\n",
            "loss: 0.024699 [59072/70000]\n",
            "loss: 0.022756 [59136/70000]\n",
            "loss: 0.024645 [59200/70000]\n",
            "loss: 0.024447 [59264/70000]\n",
            "loss: 0.024336 [59328/70000]\n",
            "loss: 0.024844 [59392/70000]\n",
            "loss: 0.025102 [59456/70000]\n",
            "loss: 0.022675 [59520/70000]\n",
            "loss: 0.023368 [59584/70000]\n",
            "loss: 0.023300 [59648/70000]\n",
            "loss: 0.022734 [59712/70000]\n",
            "loss: 0.024429 [59776/70000]\n",
            "loss: 0.023719 [59840/70000]\n",
            "loss: 0.024493 [59904/70000]\n",
            "loss: 0.024248 [59968/70000]\n",
            "loss: 0.024304 [60032/70000]\n",
            "loss: 0.022777 [60096/70000]\n",
            "loss: 0.023373 [60160/70000]\n",
            "loss: 0.023817 [60224/70000]\n",
            "loss: 0.023627 [60288/70000]\n",
            "loss: 0.023265 [60352/70000]\n",
            "loss: 0.023218 [60416/70000]\n",
            "loss: 0.023389 [60480/70000]\n",
            "loss: 0.024079 [60544/70000]\n",
            "loss: 0.023749 [60608/70000]\n",
            "loss: 0.025453 [60672/70000]\n",
            "loss: 0.025181 [60736/70000]\n",
            "loss: 0.024989 [60800/70000]\n",
            "loss: 0.024073 [60864/70000]\n",
            "loss: 0.024840 [60928/70000]\n",
            "loss: 0.023508 [60992/70000]\n",
            "loss: 0.024180 [61056/70000]\n",
            "loss: 0.024178 [61120/70000]\n",
            "loss: 0.023984 [61184/70000]\n",
            "loss: 0.022572 [61248/70000]\n",
            "loss: 0.025330 [61312/70000]\n",
            "loss: 0.024472 [61376/70000]\n",
            "loss: 0.024391 [61440/70000]\n",
            "loss: 0.023754 [61504/70000]\n",
            "loss: 0.023831 [61568/70000]\n",
            "loss: 0.023985 [61632/70000]\n",
            "loss: 0.023869 [61696/70000]\n",
            "loss: 0.025197 [61760/70000]\n",
            "loss: 0.023237 [61824/70000]\n",
            "loss: 0.025000 [61888/70000]\n",
            "loss: 0.024116 [61952/70000]\n",
            "loss: 0.024255 [62016/70000]\n",
            "loss: 0.025074 [62080/70000]\n",
            "loss: 0.024039 [62144/70000]\n",
            "loss: 0.022181 [62208/70000]\n",
            "loss: 0.024482 [62272/70000]\n",
            "loss: 0.023980 [62336/70000]\n",
            "loss: 0.024738 [62400/70000]\n",
            "loss: 0.023043 [62464/70000]\n",
            "loss: 0.023780 [62528/70000]\n",
            "loss: 0.023986 [62592/70000]\n",
            "loss: 0.024303 [62656/70000]\n",
            "loss: 0.024064 [62720/70000]\n",
            "loss: 0.024354 [62784/70000]\n",
            "loss: 0.026384 [62848/70000]\n",
            "loss: 0.023505 [62912/70000]\n",
            "loss: 0.024449 [62976/70000]\n",
            "loss: 0.024085 [63040/70000]\n",
            "loss: 0.024112 [63104/70000]\n",
            "loss: 0.023335 [63168/70000]\n",
            "loss: 0.024003 [63232/70000]\n",
            "loss: 0.023679 [63296/70000]\n",
            "loss: 0.022952 [63360/70000]\n",
            "loss: 0.023827 [63424/70000]\n",
            "loss: 0.024648 [63488/70000]\n",
            "loss: 0.024867 [63552/70000]\n",
            "loss: 0.024517 [63616/70000]\n",
            "loss: 0.024965 [63680/70000]\n",
            "loss: 0.024663 [63744/70000]\n",
            "loss: 0.023084 [63808/70000]\n",
            "loss: 0.023048 [63872/70000]\n",
            "loss: 0.024452 [63936/70000]\n",
            "loss: 0.023846 [64000/70000]\n",
            "loss: 0.023959 [64064/70000]\n",
            "loss: 0.023836 [64128/70000]\n",
            "loss: 0.023900 [64192/70000]\n",
            "loss: 0.023665 [64256/70000]\n",
            "loss: 0.023740 [64320/70000]\n",
            "loss: 0.024844 [64384/70000]\n",
            "loss: 0.024933 [64448/70000]\n",
            "loss: 0.023789 [64512/70000]\n",
            "loss: 0.024246 [64576/70000]\n",
            "loss: 0.024377 [64640/70000]\n",
            "loss: 0.024655 [64704/70000]\n",
            "loss: 0.022965 [64768/70000]\n",
            "loss: 0.022924 [64832/70000]\n",
            "loss: 0.025220 [64896/70000]\n",
            "loss: 0.024495 [64960/70000]\n",
            "loss: 0.023204 [65024/70000]\n",
            "loss: 0.025276 [65088/70000]\n",
            "loss: 0.023806 [65152/70000]\n",
            "loss: 0.025570 [65216/70000]\n",
            "loss: 0.023374 [65280/70000]\n",
            "loss: 0.025303 [65344/70000]\n",
            "loss: 0.024053 [65408/70000]\n",
            "loss: 0.023612 [65472/70000]\n",
            "loss: 0.023644 [65536/70000]\n",
            "loss: 0.023842 [65600/70000]\n",
            "loss: 0.024766 [65664/70000]\n",
            "loss: 0.023774 [65728/70000]\n",
            "loss: 0.024089 [65792/70000]\n",
            "loss: 0.024193 [65856/70000]\n",
            "loss: 0.025118 [65920/70000]\n",
            "loss: 0.024007 [65984/70000]\n",
            "loss: 0.023604 [66048/70000]\n",
            "loss: 0.024539 [66112/70000]\n",
            "loss: 0.023747 [66176/70000]\n",
            "loss: 0.024538 [66240/70000]\n",
            "loss: 0.023297 [66304/70000]\n",
            "loss: 0.025119 [66368/70000]\n",
            "loss: 0.023664 [66432/70000]\n",
            "loss: 0.023983 [66496/70000]\n",
            "loss: 0.024585 [66560/70000]\n",
            "loss: 0.023542 [66624/70000]\n",
            "loss: 0.023907 [66688/70000]\n",
            "loss: 0.023656 [66752/70000]\n",
            "loss: 0.024298 [66816/70000]\n",
            "loss: 0.023538 [66880/70000]\n",
            "loss: 0.023455 [66944/70000]\n",
            "loss: 0.023336 [67008/70000]\n",
            "loss: 0.024583 [67072/70000]\n",
            "loss: 0.022733 [67136/70000]\n",
            "loss: 0.024387 [67200/70000]\n",
            "loss: 0.023401 [67264/70000]\n",
            "loss: 0.025158 [67328/70000]\n",
            "loss: 0.024006 [67392/70000]\n",
            "loss: 0.023715 [67456/70000]\n",
            "loss: 0.023970 [67520/70000]\n",
            "loss: 0.025391 [67584/70000]\n",
            "loss: 0.024217 [67648/70000]\n",
            "loss: 0.024373 [67712/70000]\n",
            "loss: 0.024476 [67776/70000]\n",
            "loss: 0.022678 [67840/70000]\n",
            "loss: 0.024450 [67904/70000]\n",
            "loss: 0.022748 [67968/70000]\n",
            "loss: 0.023759 [68032/70000]\n",
            "loss: 0.024666 [68096/70000]\n",
            "loss: 0.024269 [68160/70000]\n",
            "loss: 0.024498 [68224/70000]\n",
            "loss: 0.025423 [68288/70000]\n",
            "loss: 0.024945 [68352/70000]\n",
            "loss: 0.023914 [68416/70000]\n",
            "loss: 0.024780 [68480/70000]\n",
            "loss: 0.023983 [68544/70000]\n",
            "loss: 0.023016 [68608/70000]\n",
            "loss: 0.024374 [68672/70000]\n",
            "loss: 0.023920 [68736/70000]\n",
            "loss: 0.023147 [68800/70000]\n",
            "loss: 0.022184 [68864/70000]\n",
            "loss: 0.024606 [68928/70000]\n",
            "loss: 0.023753 [68992/70000]\n",
            "loss: 0.024871 [69056/70000]\n",
            "loss: 0.023633 [69120/70000]\n",
            "loss: 0.024201 [69184/70000]\n",
            "loss: 0.024078 [69248/70000]\n",
            "loss: 0.023710 [69312/70000]\n",
            "loss: 0.024364 [69376/70000]\n",
            "loss: 0.024393 [69440/70000]\n",
            "loss: 0.024356 [69504/70000]\n",
            "loss: 0.024007 [69568/70000]\n",
            "loss: 0.023253 [69632/70000]\n",
            "loss: 0.024137 [69696/70000]\n",
            "loss: 0.023717 [69760/70000]\n",
            "loss: 0.022102 [69824/70000]\n",
            "loss: 0.023184 [69888/70000]\n",
            "loss: 0.031500 [52464/70000]\n",
            "Accuracy: 44.8%, Avg loss: 0.024790\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "1.4159898419866535e-05\n",
            "loss: 0.024197 [    0/70000]\n",
            "loss: 0.024447 [   64/70000]\n",
            "loss: 0.024454 [  128/70000]\n",
            "loss: 0.025472 [  192/70000]\n",
            "loss: 0.024275 [  256/70000]\n",
            "loss: 0.023416 [  320/70000]\n",
            "loss: 0.024920 [  384/70000]\n",
            "loss: 0.024788 [  448/70000]\n",
            "loss: 0.023966 [  512/70000]\n",
            "loss: 0.025431 [  576/70000]\n",
            "loss: 0.024515 [  640/70000]\n",
            "loss: 0.022081 [  704/70000]\n",
            "loss: 0.024292 [  768/70000]\n",
            "loss: 0.023111 [  832/70000]\n",
            "loss: 0.023653 [  896/70000]\n",
            "loss: 0.024002 [  960/70000]\n",
            "loss: 0.024105 [ 1024/70000]\n",
            "loss: 0.024592 [ 1088/70000]\n",
            "loss: 0.025264 [ 1152/70000]\n",
            "loss: 0.024339 [ 1216/70000]\n",
            "loss: 0.023130 [ 1280/70000]\n",
            "loss: 0.023788 [ 1344/70000]\n",
            "loss: 0.022617 [ 1408/70000]\n",
            "loss: 0.023703 [ 1472/70000]\n",
            "loss: 0.024463 [ 1536/70000]\n",
            "loss: 0.022913 [ 1600/70000]\n",
            "loss: 0.023357 [ 1664/70000]\n",
            "loss: 0.023698 [ 1728/70000]\n",
            "loss: 0.022622 [ 1792/70000]\n",
            "loss: 0.023663 [ 1856/70000]\n",
            "loss: 0.025023 [ 1920/70000]\n",
            "loss: 0.025355 [ 1984/70000]\n",
            "loss: 0.024607 [ 2048/70000]\n",
            "loss: 0.023700 [ 2112/70000]\n",
            "loss: 0.024114 [ 2176/70000]\n",
            "loss: 0.024160 [ 2240/70000]\n",
            "loss: 0.024103 [ 2304/70000]\n",
            "loss: 0.023828 [ 2368/70000]\n",
            "loss: 0.022456 [ 2432/70000]\n",
            "loss: 0.024656 [ 2496/70000]\n",
            "loss: 0.022316 [ 2560/70000]\n",
            "loss: 0.024474 [ 2624/70000]\n",
            "loss: 0.024835 [ 2688/70000]\n",
            "loss: 0.023935 [ 2752/70000]\n",
            "loss: 0.023412 [ 2816/70000]\n",
            "loss: 0.022879 [ 2880/70000]\n",
            "loss: 0.023233 [ 2944/70000]\n",
            "loss: 0.023775 [ 3008/70000]\n",
            "loss: 0.022994 [ 3072/70000]\n",
            "loss: 0.023688 [ 3136/70000]\n",
            "loss: 0.024145 [ 3200/70000]\n",
            "loss: 0.022433 [ 3264/70000]\n",
            "loss: 0.025148 [ 3328/70000]\n",
            "loss: 0.023676 [ 3392/70000]\n",
            "loss: 0.023100 [ 3456/70000]\n",
            "loss: 0.023722 [ 3520/70000]\n",
            "loss: 0.023771 [ 3584/70000]\n",
            "loss: 0.024094 [ 3648/70000]\n",
            "loss: 0.023802 [ 3712/70000]\n",
            "loss: 0.024357 [ 3776/70000]\n",
            "loss: 0.025008 [ 3840/70000]\n",
            "loss: 0.023236 [ 3904/70000]\n",
            "loss: 0.022757 [ 3968/70000]\n",
            "loss: 0.023409 [ 4032/70000]\n",
            "loss: 0.023155 [ 4096/70000]\n",
            "loss: 0.022869 [ 4160/70000]\n",
            "loss: 0.022977 [ 4224/70000]\n",
            "loss: 0.023230 [ 4288/70000]\n",
            "loss: 0.023373 [ 4352/70000]\n",
            "loss: 0.022522 [ 4416/70000]\n",
            "loss: 0.023381 [ 4480/70000]\n",
            "loss: 0.023911 [ 4544/70000]\n",
            "loss: 0.024085 [ 4608/70000]\n",
            "loss: 0.023615 [ 4672/70000]\n",
            "loss: 0.024195 [ 4736/70000]\n",
            "loss: 0.023604 [ 4800/70000]\n",
            "loss: 0.025670 [ 4864/70000]\n",
            "loss: 0.023714 [ 4928/70000]\n",
            "loss: 0.022806 [ 4992/70000]\n",
            "loss: 0.024557 [ 5056/70000]\n",
            "loss: 0.024326 [ 5120/70000]\n",
            "loss: 0.023321 [ 5184/70000]\n",
            "loss: 0.023351 [ 5248/70000]\n",
            "loss: 0.024446 [ 5312/70000]\n",
            "loss: 0.025718 [ 5376/70000]\n",
            "loss: 0.023145 [ 5440/70000]\n",
            "loss: 0.023216 [ 5504/70000]\n",
            "loss: 0.024317 [ 5568/70000]\n",
            "loss: 0.025725 [ 5632/70000]\n",
            "loss: 0.023208 [ 5696/70000]\n",
            "loss: 0.025041 [ 5760/70000]\n",
            "loss: 0.023155 [ 5824/70000]\n",
            "loss: 0.024040 [ 5888/70000]\n",
            "loss: 0.022543 [ 5952/70000]\n",
            "loss: 0.024701 [ 6016/70000]\n",
            "loss: 0.024960 [ 6080/70000]\n",
            "loss: 0.025648 [ 6144/70000]\n",
            "loss: 0.024424 [ 6208/70000]\n",
            "loss: 0.024291 [ 6272/70000]\n",
            "loss: 0.024949 [ 6336/70000]\n",
            "loss: 0.023555 [ 6400/70000]\n",
            "loss: 0.024304 [ 6464/70000]\n",
            "loss: 0.025474 [ 6528/70000]\n",
            "loss: 0.024536 [ 6592/70000]\n",
            "loss: 0.025070 [ 6656/70000]\n",
            "loss: 0.024104 [ 6720/70000]\n",
            "loss: 0.023034 [ 6784/70000]\n",
            "loss: 0.024203 [ 6848/70000]\n",
            "loss: 0.024718 [ 6912/70000]\n",
            "loss: 0.023082 [ 6976/70000]\n",
            "loss: 0.024814 [ 7040/70000]\n",
            "loss: 0.025237 [ 7104/70000]\n",
            "loss: 0.025796 [ 7168/70000]\n",
            "loss: 0.024126 [ 7232/70000]\n",
            "loss: 0.023978 [ 7296/70000]\n",
            "loss: 0.024307 [ 7360/70000]\n",
            "loss: 0.022231 [ 7424/70000]\n",
            "loss: 0.023930 [ 7488/70000]\n",
            "loss: 0.025201 [ 7552/70000]\n",
            "loss: 0.024941 [ 7616/70000]\n",
            "loss: 0.024761 [ 7680/70000]\n",
            "loss: 0.024832 [ 7744/70000]\n",
            "loss: 0.024921 [ 7808/70000]\n",
            "loss: 0.022853 [ 7872/70000]\n",
            "loss: 0.023786 [ 7936/70000]\n",
            "loss: 0.024021 [ 8000/70000]\n",
            "loss: 0.025286 [ 8064/70000]\n",
            "loss: 0.024066 [ 8128/70000]\n",
            "loss: 0.024831 [ 8192/70000]\n",
            "loss: 0.023373 [ 8256/70000]\n",
            "loss: 0.024053 [ 8320/70000]\n",
            "loss: 0.023954 [ 8384/70000]\n",
            "loss: 0.023271 [ 8448/70000]\n",
            "loss: 0.023109 [ 8512/70000]\n",
            "loss: 0.023051 [ 8576/70000]\n",
            "loss: 0.022913 [ 8640/70000]\n",
            "loss: 0.024265 [ 8704/70000]\n",
            "loss: 0.024130 [ 8768/70000]\n",
            "loss: 0.023520 [ 8832/70000]\n",
            "loss: 0.023953 [ 8896/70000]\n",
            "loss: 0.025014 [ 8960/70000]\n",
            "loss: 0.024334 [ 9024/70000]\n",
            "loss: 0.024409 [ 9088/70000]\n",
            "loss: 0.023437 [ 9152/70000]\n",
            "loss: 0.024049 [ 9216/70000]\n",
            "loss: 0.024243 [ 9280/70000]\n",
            "loss: 0.025206 [ 9344/70000]\n",
            "loss: 0.024570 [ 9408/70000]\n",
            "loss: 0.024130 [ 9472/70000]\n",
            "loss: 0.023337 [ 9536/70000]\n",
            "loss: 0.024276 [ 9600/70000]\n",
            "loss: 0.023550 [ 9664/70000]\n",
            "loss: 0.024946 [ 9728/70000]\n",
            "loss: 0.024981 [ 9792/70000]\n",
            "loss: 0.024315 [ 9856/70000]\n",
            "loss: 0.022344 [ 9920/70000]\n",
            "loss: 0.023821 [ 9984/70000]\n",
            "loss: 0.024935 [10048/70000]\n",
            "loss: 0.024183 [10112/70000]\n",
            "loss: 0.025181 [10176/70000]\n",
            "loss: 0.023840 [10240/70000]\n",
            "loss: 0.024728 [10304/70000]\n",
            "loss: 0.022795 [10368/70000]\n",
            "loss: 0.024081 [10432/70000]\n",
            "loss: 0.023554 [10496/70000]\n",
            "loss: 0.023467 [10560/70000]\n",
            "loss: 0.024065 [10624/70000]\n",
            "loss: 0.023891 [10688/70000]\n",
            "loss: 0.023667 [10752/70000]\n",
            "loss: 0.022517 [10816/70000]\n",
            "loss: 0.024970 [10880/70000]\n",
            "loss: 0.025016 [10944/70000]\n",
            "loss: 0.025157 [11008/70000]\n",
            "loss: 0.025597 [11072/70000]\n",
            "loss: 0.025190 [11136/70000]\n",
            "loss: 0.025892 [11200/70000]\n",
            "loss: 0.024072 [11264/70000]\n",
            "loss: 0.024737 [11328/70000]\n",
            "loss: 0.025563 [11392/70000]\n",
            "loss: 0.023955 [11456/70000]\n",
            "loss: 0.025085 [11520/70000]\n",
            "loss: 0.024406 [11584/70000]\n",
            "loss: 0.023621 [11648/70000]\n",
            "loss: 0.025599 [11712/70000]\n",
            "loss: 0.023460 [11776/70000]\n",
            "loss: 0.025232 [11840/70000]\n",
            "loss: 0.024325 [11904/70000]\n",
            "loss: 0.024660 [11968/70000]\n",
            "loss: 0.023246 [12032/70000]\n",
            "loss: 0.024040 [12096/70000]\n",
            "loss: 0.023620 [12160/70000]\n",
            "loss: 0.024545 [12224/70000]\n",
            "loss: 0.026208 [12288/70000]\n",
            "loss: 0.024231 [12352/70000]\n",
            "loss: 0.024847 [12416/70000]\n",
            "loss: 0.024368 [12480/70000]\n",
            "loss: 0.024144 [12544/70000]\n",
            "loss: 0.023354 [12608/70000]\n",
            "loss: 0.025430 [12672/70000]\n",
            "loss: 0.023418 [12736/70000]\n",
            "loss: 0.024889 [12800/70000]\n",
            "loss: 0.024350 [12864/70000]\n",
            "loss: 0.024331 [12928/70000]\n",
            "loss: 0.024621 [12992/70000]\n",
            "loss: 0.024890 [13056/70000]\n",
            "loss: 0.025132 [13120/70000]\n",
            "loss: 0.024917 [13184/70000]\n",
            "loss: 0.024359 [13248/70000]\n",
            "loss: 0.023993 [13312/70000]\n",
            "loss: 0.023904 [13376/70000]\n",
            "loss: 0.024407 [13440/70000]\n",
            "loss: 0.024821 [13504/70000]\n",
            "loss: 0.024020 [13568/70000]\n",
            "loss: 0.024182 [13632/70000]\n",
            "loss: 0.023818 [13696/70000]\n",
            "loss: 0.024131 [13760/70000]\n",
            "loss: 0.024563 [13824/70000]\n",
            "loss: 0.022621 [13888/70000]\n",
            "loss: 0.023949 [13952/70000]\n",
            "loss: 0.024848 [14016/70000]\n",
            "loss: 0.023737 [14080/70000]\n",
            "loss: 0.024962 [14144/70000]\n",
            "loss: 0.023903 [14208/70000]\n",
            "loss: 0.024649 [14272/70000]\n",
            "loss: 0.024094 [14336/70000]\n",
            "loss: 0.023494 [14400/70000]\n",
            "loss: 0.023954 [14464/70000]\n",
            "loss: 0.024983 [14528/70000]\n",
            "loss: 0.021813 [14592/70000]\n",
            "loss: 0.025108 [14656/70000]\n",
            "loss: 0.024074 [14720/70000]\n",
            "loss: 0.025375 [14784/70000]\n",
            "loss: 0.023180 [14848/70000]\n",
            "loss: 0.023383 [14912/70000]\n",
            "loss: 0.024196 [14976/70000]\n",
            "loss: 0.024628 [15040/70000]\n",
            "loss: 0.024591 [15104/70000]\n",
            "loss: 0.024497 [15168/70000]\n",
            "loss: 0.023890 [15232/70000]\n",
            "loss: 0.023741 [15296/70000]\n",
            "loss: 0.023722 [15360/70000]\n",
            "loss: 0.024779 [15424/70000]\n",
            "loss: 0.025153 [15488/70000]\n",
            "loss: 0.025226 [15552/70000]\n",
            "loss: 0.024412 [15616/70000]\n",
            "loss: 0.023724 [15680/70000]\n",
            "loss: 0.023798 [15744/70000]\n",
            "loss: 0.025570 [15808/70000]\n",
            "loss: 0.022705 [15872/70000]\n",
            "loss: 0.023985 [15936/70000]\n",
            "loss: 0.025104 [16000/70000]\n",
            "loss: 0.024582 [16064/70000]\n",
            "loss: 0.023352 [16128/70000]\n",
            "loss: 0.023915 [16192/70000]\n",
            "loss: 0.022816 [16256/70000]\n",
            "loss: 0.023901 [16320/70000]\n",
            "loss: 0.024512 [16384/70000]\n",
            "loss: 0.024979 [16448/70000]\n",
            "loss: 0.023688 [16512/70000]\n",
            "loss: 0.025861 [16576/70000]\n",
            "loss: 0.024406 [16640/70000]\n",
            "loss: 0.024515 [16704/70000]\n",
            "loss: 0.025384 [16768/70000]\n",
            "loss: 0.023961 [16832/70000]\n",
            "loss: 0.023140 [16896/70000]\n",
            "loss: 0.026478 [16960/70000]\n",
            "loss: 0.025446 [17024/70000]\n",
            "loss: 0.024434 [17088/70000]\n",
            "loss: 0.023753 [17152/70000]\n",
            "loss: 0.024603 [17216/70000]\n",
            "loss: 0.022132 [17280/70000]\n",
            "loss: 0.025870 [17344/70000]\n",
            "loss: 0.024160 [17408/70000]\n",
            "loss: 0.024820 [17472/70000]\n",
            "loss: 0.023128 [17536/70000]\n",
            "loss: 0.023647 [17600/70000]\n",
            "loss: 0.023751 [17664/70000]\n",
            "loss: 0.023882 [17728/70000]\n",
            "loss: 0.025707 [17792/70000]\n",
            "loss: 0.023943 [17856/70000]\n",
            "loss: 0.023732 [17920/70000]\n",
            "loss: 0.023841 [17984/70000]\n",
            "loss: 0.024613 [18048/70000]\n",
            "loss: 0.022928 [18112/70000]\n",
            "loss: 0.023009 [18176/70000]\n",
            "loss: 0.023480 [18240/70000]\n",
            "loss: 0.023966 [18304/70000]\n",
            "loss: 0.023649 [18368/70000]\n",
            "loss: 0.022422 [18432/70000]\n",
            "loss: 0.024557 [18496/70000]\n",
            "loss: 0.024243 [18560/70000]\n",
            "loss: 0.024929 [18624/70000]\n",
            "loss: 0.023217 [18688/70000]\n",
            "loss: 0.024098 [18752/70000]\n",
            "loss: 0.023566 [18816/70000]\n",
            "loss: 0.023595 [18880/70000]\n",
            "loss: 0.024162 [18944/70000]\n",
            "loss: 0.024237 [19008/70000]\n",
            "loss: 0.024728 [19072/70000]\n",
            "loss: 0.024828 [19136/70000]\n",
            "loss: 0.025465 [19200/70000]\n",
            "loss: 0.024453 [19264/70000]\n",
            "loss: 0.024435 [19328/70000]\n",
            "loss: 0.023508 [19392/70000]\n",
            "loss: 0.024580 [19456/70000]\n",
            "loss: 0.024624 [19520/70000]\n",
            "loss: 0.022974 [19584/70000]\n",
            "loss: 0.024651 [19648/70000]\n",
            "loss: 0.024753 [19712/70000]\n",
            "loss: 0.024898 [19776/70000]\n",
            "loss: 0.024000 [19840/70000]\n",
            "loss: 0.025012 [19904/70000]\n",
            "loss: 0.023477 [19968/70000]\n",
            "loss: 0.023550 [20032/70000]\n",
            "loss: 0.025052 [20096/70000]\n",
            "loss: 0.023038 [20160/70000]\n",
            "loss: 0.022951 [20224/70000]\n",
            "loss: 0.024707 [20288/70000]\n",
            "loss: 0.025406 [20352/70000]\n",
            "loss: 0.024108 [20416/70000]\n",
            "loss: 0.024789 [20480/70000]\n",
            "loss: 0.024132 [20544/70000]\n",
            "loss: 0.023215 [20608/70000]\n",
            "loss: 0.024864 [20672/70000]\n",
            "loss: 0.023857 [20736/70000]\n",
            "loss: 0.025648 [20800/70000]\n",
            "loss: 0.024930 [20864/70000]\n",
            "loss: 0.024887 [20928/70000]\n",
            "loss: 0.024660 [20992/70000]\n",
            "loss: 0.025241 [21056/70000]\n",
            "loss: 0.024945 [21120/70000]\n",
            "loss: 0.025034 [21184/70000]\n",
            "loss: 0.024230 [21248/70000]\n",
            "loss: 0.023441 [21312/70000]\n",
            "loss: 0.025044 [21376/70000]\n",
            "loss: 0.024394 [21440/70000]\n",
            "loss: 0.023036 [21504/70000]\n",
            "loss: 0.024375 [21568/70000]\n",
            "loss: 0.026082 [21632/70000]\n",
            "loss: 0.023186 [21696/70000]\n",
            "loss: 0.024396 [21760/70000]\n",
            "loss: 0.023769 [21824/70000]\n",
            "loss: 0.026461 [21888/70000]\n",
            "loss: 0.025026 [21952/70000]\n",
            "loss: 0.024270 [22016/70000]\n",
            "loss: 0.024069 [22080/70000]\n",
            "loss: 0.024022 [22144/70000]\n",
            "loss: 0.023541 [22208/70000]\n",
            "loss: 0.024471 [22272/70000]\n",
            "loss: 0.023494 [22336/70000]\n",
            "loss: 0.024584 [22400/70000]\n",
            "loss: 0.025479 [22464/70000]\n",
            "loss: 0.025388 [22528/70000]\n",
            "loss: 0.024810 [22592/70000]\n",
            "loss: 0.023846 [22656/70000]\n",
            "loss: 0.022171 [22720/70000]\n",
            "loss: 0.024716 [22784/70000]\n",
            "loss: 0.023913 [22848/70000]\n",
            "loss: 0.024529 [22912/70000]\n",
            "loss: 0.024280 [22976/70000]\n",
            "loss: 0.025083 [23040/70000]\n",
            "loss: 0.022708 [23104/70000]\n",
            "loss: 0.025125 [23168/70000]\n",
            "loss: 0.025021 [23232/70000]\n",
            "loss: 0.024734 [23296/70000]\n",
            "loss: 0.023900 [23360/70000]\n",
            "loss: 0.023892 [23424/70000]\n",
            "loss: 0.024465 [23488/70000]\n",
            "loss: 0.024925 [23552/70000]\n",
            "loss: 0.023753 [23616/70000]\n",
            "loss: 0.025270 [23680/70000]\n",
            "loss: 0.023953 [23744/70000]\n",
            "loss: 0.023957 [23808/70000]\n",
            "loss: 0.024343 [23872/70000]\n",
            "loss: 0.024615 [23936/70000]\n",
            "loss: 0.024705 [24000/70000]\n",
            "loss: 0.023940 [24064/70000]\n",
            "loss: 0.024011 [24128/70000]\n",
            "loss: 0.023314 [24192/70000]\n",
            "loss: 0.024306 [24256/70000]\n",
            "loss: 0.023268 [24320/70000]\n",
            "loss: 0.023434 [24384/70000]\n",
            "loss: 0.024326 [24448/70000]\n",
            "loss: 0.023753 [24512/70000]\n",
            "loss: 0.023357 [24576/70000]\n",
            "loss: 0.023928 [24640/70000]\n",
            "loss: 0.024401 [24704/70000]\n",
            "loss: 0.023489 [24768/70000]\n",
            "loss: 0.025589 [24832/70000]\n",
            "loss: 0.023454 [24896/70000]\n",
            "loss: 0.024216 [24960/70000]\n",
            "loss: 0.023567 [25024/70000]\n",
            "loss: 0.023367 [25088/70000]\n",
            "loss: 0.023540 [25152/70000]\n",
            "loss: 0.024807 [25216/70000]\n",
            "loss: 0.023015 [25280/70000]\n",
            "loss: 0.024742 [25344/70000]\n",
            "loss: 0.025343 [25408/70000]\n",
            "loss: 0.024784 [25472/70000]\n",
            "loss: 0.024495 [25536/70000]\n",
            "loss: 0.023745 [25600/70000]\n",
            "loss: 0.023723 [25664/70000]\n",
            "loss: 0.025214 [25728/70000]\n",
            "loss: 0.024045 [25792/70000]\n",
            "loss: 0.024599 [25856/70000]\n",
            "loss: 0.022989 [25920/70000]\n",
            "loss: 0.026118 [25984/70000]\n",
            "loss: 0.024174 [26048/70000]\n",
            "loss: 0.023939 [26112/70000]\n",
            "loss: 0.024219 [26176/70000]\n",
            "loss: 0.024767 [26240/70000]\n",
            "loss: 0.023320 [26304/70000]\n",
            "loss: 0.023958 [26368/70000]\n",
            "loss: 0.025152 [26432/70000]\n",
            "loss: 0.024017 [26496/70000]\n",
            "loss: 0.024315 [26560/70000]\n",
            "loss: 0.024121 [26624/70000]\n",
            "loss: 0.023134 [26688/70000]\n",
            "loss: 0.025339 [26752/70000]\n",
            "loss: 0.025265 [26816/70000]\n",
            "loss: 0.023276 [26880/70000]\n",
            "loss: 0.024610 [26944/70000]\n",
            "loss: 0.024304 [27008/70000]\n",
            "loss: 0.023925 [27072/70000]\n",
            "loss: 0.023617 [27136/70000]\n",
            "loss: 0.024652 [27200/70000]\n",
            "loss: 0.022772 [27264/70000]\n",
            "loss: 0.024731 [27328/70000]\n",
            "loss: 0.025140 [27392/70000]\n",
            "loss: 0.024257 [27456/70000]\n",
            "loss: 0.024808 [27520/70000]\n",
            "loss: 0.023305 [27584/70000]\n",
            "loss: 0.024553 [27648/70000]\n",
            "loss: 0.023904 [27712/70000]\n",
            "loss: 0.024003 [27776/70000]\n",
            "loss: 0.024557 [27840/70000]\n",
            "loss: 0.024589 [27904/70000]\n",
            "loss: 0.024275 [27968/70000]\n",
            "loss: 0.023990 [28032/70000]\n",
            "loss: 0.025574 [28096/70000]\n",
            "loss: 0.026579 [28160/70000]\n",
            "loss: 0.024838 [28224/70000]\n",
            "loss: 0.022728 [28288/70000]\n",
            "loss: 0.024538 [28352/70000]\n",
            "loss: 0.023890 [28416/70000]\n",
            "loss: 0.024424 [28480/70000]\n",
            "loss: 0.023919 [28544/70000]\n",
            "loss: 0.023406 [28608/70000]\n",
            "loss: 0.023140 [28672/70000]\n",
            "loss: 0.023886 [28736/70000]\n",
            "loss: 0.024441 [28800/70000]\n",
            "loss: 0.024198 [28864/70000]\n",
            "loss: 0.025367 [28928/70000]\n",
            "loss: 0.023704 [28992/70000]\n",
            "loss: 0.024769 [29056/70000]\n",
            "loss: 0.023256 [29120/70000]\n",
            "loss: 0.024036 [29184/70000]\n",
            "loss: 0.024034 [29248/70000]\n",
            "loss: 0.024108 [29312/70000]\n",
            "loss: 0.025204 [29376/70000]\n",
            "loss: 0.023428 [29440/70000]\n",
            "loss: 0.024631 [29504/70000]\n",
            "loss: 0.024945 [29568/70000]\n",
            "loss: 0.024380 [29632/70000]\n",
            "loss: 0.023396 [29696/70000]\n",
            "loss: 0.023657 [29760/70000]\n",
            "loss: 0.023551 [29824/70000]\n",
            "loss: 0.024048 [29888/70000]\n",
            "loss: 0.024708 [29952/70000]\n",
            "loss: 0.024375 [30016/70000]\n",
            "loss: 0.023996 [30080/70000]\n",
            "loss: 0.023256 [30144/70000]\n",
            "loss: 0.024866 [30208/70000]\n",
            "loss: 0.024550 [30272/70000]\n",
            "loss: 0.024480 [30336/70000]\n",
            "loss: 0.024403 [30400/70000]\n",
            "loss: 0.024028 [30464/70000]\n",
            "loss: 0.023518 [30528/70000]\n",
            "loss: 0.024131 [30592/70000]\n",
            "loss: 0.024480 [30656/70000]\n",
            "loss: 0.024302 [30720/70000]\n",
            "loss: 0.023378 [30784/70000]\n",
            "loss: 0.024678 [30848/70000]\n",
            "loss: 0.023967 [30912/70000]\n",
            "loss: 0.022943 [30976/70000]\n",
            "loss: 0.024525 [31040/70000]\n",
            "loss: 0.022700 [31104/70000]\n",
            "loss: 0.023843 [31168/70000]\n",
            "loss: 0.025266 [31232/70000]\n",
            "loss: 0.025174 [31296/70000]\n",
            "loss: 0.023570 [31360/70000]\n",
            "loss: 0.023892 [31424/70000]\n",
            "loss: 0.024276 [31488/70000]\n",
            "loss: 0.025666 [31552/70000]\n",
            "loss: 0.024350 [31616/70000]\n",
            "loss: 0.022200 [31680/70000]\n",
            "loss: 0.023833 [31744/70000]\n",
            "loss: 0.024575 [31808/70000]\n",
            "loss: 0.022898 [31872/70000]\n",
            "loss: 0.024038 [31936/70000]\n",
            "loss: 0.025374 [32000/70000]\n",
            "loss: 0.024527 [32064/70000]\n",
            "loss: 0.022728 [32128/70000]\n",
            "loss: 0.023829 [32192/70000]\n",
            "loss: 0.024461 [32256/70000]\n",
            "loss: 0.023653 [32320/70000]\n",
            "loss: 0.024612 [32384/70000]\n",
            "loss: 0.023963 [32448/70000]\n",
            "loss: 0.024324 [32512/70000]\n",
            "loss: 0.024810 [32576/70000]\n",
            "loss: 0.023481 [32640/70000]\n",
            "loss: 0.023894 [32704/70000]\n",
            "loss: 0.023780 [32768/70000]\n",
            "loss: 0.023793 [32832/70000]\n",
            "loss: 0.024926 [32896/70000]\n",
            "loss: 0.025559 [32960/70000]\n",
            "loss: 0.023927 [33024/70000]\n",
            "loss: 0.024382 [33088/70000]\n",
            "loss: 0.025049 [33152/70000]\n",
            "loss: 0.024880 [33216/70000]\n",
            "loss: 0.025240 [33280/70000]\n",
            "loss: 0.023737 [33344/70000]\n",
            "loss: 0.024190 [33408/70000]\n",
            "loss: 0.022686 [33472/70000]\n",
            "loss: 0.023469 [33536/70000]\n",
            "loss: 0.023440 [33600/70000]\n",
            "loss: 0.024717 [33664/70000]\n",
            "loss: 0.022895 [33728/70000]\n",
            "loss: 0.024378 [33792/70000]\n",
            "loss: 0.025385 [33856/70000]\n",
            "loss: 0.022633 [33920/70000]\n",
            "loss: 0.025784 [33984/70000]\n",
            "loss: 0.023388 [34048/70000]\n",
            "loss: 0.023605 [34112/70000]\n",
            "loss: 0.024384 [34176/70000]\n",
            "loss: 0.023973 [34240/70000]\n",
            "loss: 0.024249 [34304/70000]\n",
            "loss: 0.025383 [34368/70000]\n",
            "loss: 0.024382 [34432/70000]\n",
            "loss: 0.024366 [34496/70000]\n",
            "loss: 0.023967 [34560/70000]\n",
            "loss: 0.022880 [34624/70000]\n",
            "loss: 0.024860 [34688/70000]\n",
            "loss: 0.023880 [34752/70000]\n",
            "loss: 0.026323 [34816/70000]\n",
            "loss: 0.025249 [34880/70000]\n",
            "loss: 0.023813 [34944/70000]\n",
            "loss: 0.023843 [35008/70000]\n",
            "loss: 0.023133 [35072/70000]\n",
            "loss: 0.025953 [35136/70000]\n",
            "loss: 0.024839 [35200/70000]\n",
            "loss: 0.024887 [35264/70000]\n",
            "loss: 0.023375 [35328/70000]\n",
            "loss: 0.024688 [35392/70000]\n",
            "loss: 0.023993 [35456/70000]\n",
            "loss: 0.022735 [35520/70000]\n",
            "loss: 0.023597 [35584/70000]\n",
            "loss: 0.024317 [35648/70000]\n",
            "loss: 0.025380 [35712/70000]\n",
            "loss: 0.024326 [35776/70000]\n",
            "loss: 0.024426 [35840/70000]\n",
            "loss: 0.024348 [35904/70000]\n",
            "loss: 0.024242 [35968/70000]\n",
            "loss: 0.025170 [36032/70000]\n",
            "loss: 0.025740 [36096/70000]\n",
            "loss: 0.025093 [36160/70000]\n",
            "loss: 0.024158 [36224/70000]\n",
            "loss: 0.024411 [36288/70000]\n",
            "loss: 0.024803 [36352/70000]\n",
            "loss: 0.023672 [36416/70000]\n",
            "loss: 0.026359 [36480/70000]\n",
            "loss: 0.024538 [36544/70000]\n",
            "loss: 0.024281 [36608/70000]\n",
            "loss: 0.025964 [36672/70000]\n",
            "loss: 0.025452 [36736/70000]\n",
            "loss: 0.024533 [36800/70000]\n",
            "loss: 0.025625 [36864/70000]\n",
            "loss: 0.024259 [36928/70000]\n",
            "loss: 0.025148 [36992/70000]\n",
            "loss: 0.023766 [37056/70000]\n",
            "loss: 0.023978 [37120/70000]\n",
            "loss: 0.024259 [37184/70000]\n",
            "loss: 0.025807 [37248/70000]\n",
            "loss: 0.024875 [37312/70000]\n",
            "loss: 0.023795 [37376/70000]\n",
            "loss: 0.023855 [37440/70000]\n",
            "loss: 0.024190 [37504/70000]\n",
            "loss: 0.025559 [37568/70000]\n",
            "loss: 0.024339 [37632/70000]\n",
            "loss: 0.024227 [37696/70000]\n",
            "loss: 0.025114 [37760/70000]\n",
            "loss: 0.024509 [37824/70000]\n",
            "loss: 0.024579 [37888/70000]\n",
            "loss: 0.024449 [37952/70000]\n",
            "loss: 0.025733 [38016/70000]\n",
            "loss: 0.024485 [38080/70000]\n",
            "loss: 0.025032 [38144/70000]\n",
            "loss: 0.024412 [38208/70000]\n",
            "loss: 0.024334 [38272/70000]\n",
            "loss: 0.026240 [38336/70000]\n",
            "loss: 0.024425 [38400/70000]\n",
            "loss: 0.024710 [38464/70000]\n",
            "loss: 0.024791 [38528/70000]\n",
            "loss: 0.025013 [38592/70000]\n",
            "loss: 0.024606 [38656/70000]\n",
            "loss: 0.023963 [38720/70000]\n",
            "loss: 0.024164 [38784/70000]\n",
            "loss: 0.024560 [38848/70000]\n",
            "loss: 0.025621 [38912/70000]\n",
            "loss: 0.025239 [38976/70000]\n",
            "loss: 0.023488 [39040/70000]\n",
            "loss: 0.023697 [39104/70000]\n",
            "loss: 0.023447 [39168/70000]\n",
            "loss: 0.025251 [39232/70000]\n",
            "loss: 0.024720 [39296/70000]\n",
            "loss: 0.025214 [39360/70000]\n",
            "loss: 0.024461 [39424/70000]\n",
            "loss: 0.024869 [39488/70000]\n",
            "loss: 0.023627 [39552/70000]\n",
            "loss: 0.025676 [39616/70000]\n",
            "loss: 0.023615 [39680/70000]\n",
            "loss: 0.024765 [39744/70000]\n",
            "loss: 0.024646 [39808/70000]\n",
            "loss: 0.024360 [39872/70000]\n",
            "loss: 0.025624 [39936/70000]\n",
            "loss: 0.025677 [40000/70000]\n",
            "loss: 0.026433 [40064/70000]\n",
            "loss: 0.022927 [40128/70000]\n",
            "loss: 0.022811 [40192/70000]\n",
            "loss: 0.024653 [40256/70000]\n",
            "loss: 0.023865 [40320/70000]\n",
            "loss: 0.025292 [40384/70000]\n",
            "loss: 0.023716 [40448/70000]\n",
            "loss: 0.025105 [40512/70000]\n",
            "loss: 0.024286 [40576/70000]\n",
            "loss: 0.023123 [40640/70000]\n",
            "loss: 0.024509 [40704/70000]\n",
            "loss: 0.023738 [40768/70000]\n",
            "loss: 0.024386 [40832/70000]\n",
            "loss: 0.024154 [40896/70000]\n",
            "loss: 0.026102 [40960/70000]\n",
            "loss: 0.023786 [41024/70000]\n",
            "loss: 0.023236 [41088/70000]\n",
            "loss: 0.023255 [41152/70000]\n",
            "loss: 0.025057 [41216/70000]\n",
            "loss: 0.025514 [41280/70000]\n",
            "loss: 0.024598 [41344/70000]\n",
            "loss: 0.025590 [41408/70000]\n",
            "loss: 0.023872 [41472/70000]\n",
            "loss: 0.024149 [41536/70000]\n",
            "loss: 0.025909 [41600/70000]\n",
            "loss: 0.025338 [41664/70000]\n",
            "loss: 0.025679 [41728/70000]\n",
            "loss: 0.026211 [41792/70000]\n",
            "loss: 0.023648 [41856/70000]\n",
            "loss: 0.024141 [41920/70000]\n",
            "loss: 0.024231 [41984/70000]\n",
            "loss: 0.025500 [42048/70000]\n",
            "loss: 0.023277 [42112/70000]\n",
            "loss: 0.024397 [42176/70000]\n",
            "loss: 0.022948 [42240/70000]\n",
            "loss: 0.024142 [42304/70000]\n",
            "loss: 0.023640 [42368/70000]\n",
            "loss: 0.023695 [42432/70000]\n",
            "loss: 0.024346 [42496/70000]\n",
            "loss: 0.024505 [42560/70000]\n",
            "loss: 0.023270 [42624/70000]\n",
            "loss: 0.025769 [42688/70000]\n",
            "loss: 0.024294 [42752/70000]\n",
            "loss: 0.024074 [42816/70000]\n",
            "loss: 0.024822 [42880/70000]\n",
            "loss: 0.026519 [42944/70000]\n",
            "loss: 0.023922 [43008/70000]\n",
            "loss: 0.024211 [43072/70000]\n",
            "loss: 0.023499 [43136/70000]\n",
            "loss: 0.025467 [43200/70000]\n",
            "loss: 0.026026 [43264/70000]\n",
            "loss: 0.025384 [43328/70000]\n",
            "loss: 0.024016 [43392/70000]\n",
            "loss: 0.024329 [43456/70000]\n",
            "loss: 0.024831 [43520/70000]\n",
            "loss: 0.024892 [43584/70000]\n",
            "loss: 0.025437 [43648/70000]\n",
            "loss: 0.025102 [43712/70000]\n",
            "loss: 0.025151 [43776/70000]\n",
            "loss: 0.024778 [43840/70000]\n",
            "loss: 0.025116 [43904/70000]\n",
            "loss: 0.024432 [43968/70000]\n",
            "loss: 0.025390 [44032/70000]\n",
            "loss: 0.023190 [44096/70000]\n",
            "loss: 0.024654 [44160/70000]\n",
            "loss: 0.023393 [44224/70000]\n",
            "loss: 0.023384 [44288/70000]\n",
            "loss: 0.024538 [44352/70000]\n",
            "loss: 0.024155 [44416/70000]\n",
            "loss: 0.023624 [44480/70000]\n",
            "loss: 0.023996 [44544/70000]\n",
            "loss: 0.023370 [44608/70000]\n",
            "loss: 0.024824 [44672/70000]\n",
            "loss: 0.025317 [44736/70000]\n",
            "loss: 0.023398 [44800/70000]\n",
            "loss: 0.025751 [44864/70000]\n",
            "loss: 0.026401 [44928/70000]\n",
            "loss: 0.026679 [44992/70000]\n",
            "loss: 0.025182 [45056/70000]\n",
            "loss: 0.024735 [45120/70000]\n",
            "loss: 0.025351 [45184/70000]\n",
            "loss: 0.023816 [45248/70000]\n",
            "loss: 0.024808 [45312/70000]\n",
            "loss: 0.025199 [45376/70000]\n",
            "loss: 0.023400 [45440/70000]\n",
            "loss: 0.024664 [45504/70000]\n",
            "loss: 0.023773 [45568/70000]\n",
            "loss: 0.023639 [45632/70000]\n",
            "loss: 0.024958 [45696/70000]\n",
            "loss: 0.024613 [45760/70000]\n",
            "loss: 0.024610 [45824/70000]\n",
            "loss: 0.024987 [45888/70000]\n",
            "loss: 0.022871 [45952/70000]\n",
            "loss: 0.024627 [46016/70000]\n",
            "loss: 0.024711 [46080/70000]\n",
            "loss: 0.026245 [46144/70000]\n",
            "loss: 0.022206 [46208/70000]\n",
            "loss: 0.025298 [46272/70000]\n",
            "loss: 0.025786 [46336/70000]\n",
            "loss: 0.024233 [46400/70000]\n",
            "loss: 0.024392 [46464/70000]\n",
            "loss: 0.026258 [46528/70000]\n",
            "loss: 0.026011 [46592/70000]\n",
            "loss: 0.023795 [46656/70000]\n",
            "loss: 0.024268 [46720/70000]\n",
            "loss: 0.025670 [46784/70000]\n",
            "loss: 0.024047 [46848/70000]\n",
            "loss: 0.025033 [46912/70000]\n",
            "loss: 0.026174 [46976/70000]\n",
            "loss: 0.023918 [47040/70000]\n",
            "loss: 0.023526 [47104/70000]\n",
            "loss: 0.024368 [47168/70000]\n",
            "loss: 0.024117 [47232/70000]\n",
            "loss: 0.025145 [47296/70000]\n",
            "loss: 0.025123 [47360/70000]\n",
            "loss: 0.023726 [47424/70000]\n",
            "loss: 0.022745 [47488/70000]\n",
            "loss: 0.022787 [47552/70000]\n",
            "loss: 0.024371 [47616/70000]\n",
            "loss: 0.023477 [47680/70000]\n",
            "loss: 0.023720 [47744/70000]\n",
            "loss: 0.024626 [47808/70000]\n",
            "loss: 0.024309 [47872/70000]\n",
            "loss: 0.024797 [47936/70000]\n",
            "loss: 0.026027 [48000/70000]\n",
            "loss: 0.025452 [48064/70000]\n",
            "loss: 0.025005 [48128/70000]\n",
            "loss: 0.024908 [48192/70000]\n",
            "loss: 0.025614 [48256/70000]\n",
            "loss: 0.024417 [48320/70000]\n",
            "loss: 0.025754 [48384/70000]\n",
            "loss: 0.024404 [48448/70000]\n",
            "loss: 0.023392 [48512/70000]\n",
            "loss: 0.024825 [48576/70000]\n",
            "loss: 0.025558 [48640/70000]\n",
            "loss: 0.024672 [48704/70000]\n",
            "loss: 0.025314 [48768/70000]\n",
            "loss: 0.025522 [48832/70000]\n",
            "loss: 0.025522 [48896/70000]\n",
            "loss: 0.026116 [48960/70000]\n",
            "loss: 0.024471 [49024/70000]\n",
            "loss: 0.023994 [49088/70000]\n",
            "loss: 0.024002 [49152/70000]\n",
            "loss: 0.023873 [49216/70000]\n",
            "loss: 0.025168 [49280/70000]\n",
            "loss: 0.025473 [49344/70000]\n",
            "loss: 0.025358 [49408/70000]\n",
            "loss: 0.024166 [49472/70000]\n",
            "loss: 0.023946 [49536/70000]\n",
            "loss: 0.024844 [49600/70000]\n",
            "loss: 0.024218 [49664/70000]\n",
            "loss: 0.024530 [49728/70000]\n",
            "loss: 0.024955 [49792/70000]\n",
            "loss: 0.024607 [49856/70000]\n",
            "loss: 0.025831 [49920/70000]\n",
            "loss: 0.024051 [49984/70000]\n",
            "loss: 0.023212 [50048/70000]\n",
            "loss: 0.024893 [50112/70000]\n",
            "loss: 0.024765 [50176/70000]\n",
            "loss: 0.024188 [50240/70000]\n",
            "loss: 0.025104 [50304/70000]\n",
            "loss: 0.024316 [50368/70000]\n",
            "loss: 0.025751 [50432/70000]\n",
            "loss: 0.025170 [50496/70000]\n",
            "loss: 0.023530 [50560/70000]\n",
            "loss: 0.024570 [50624/70000]\n",
            "loss: 0.024336 [50688/70000]\n",
            "loss: 0.024504 [50752/70000]\n",
            "loss: 0.025155 [50816/70000]\n",
            "loss: 0.024299 [50880/70000]\n",
            "loss: 0.024458 [50944/70000]\n",
            "loss: 0.023424 [51008/70000]\n",
            "loss: 0.024989 [51072/70000]\n",
            "loss: 0.024670 [51136/70000]\n",
            "loss: 0.023593 [51200/70000]\n",
            "loss: 0.024739 [51264/70000]\n",
            "loss: 0.023877 [51328/70000]\n",
            "loss: 0.024651 [51392/70000]\n",
            "loss: 0.024819 [51456/70000]\n",
            "loss: 0.023617 [51520/70000]\n",
            "loss: 0.022353 [51584/70000]\n",
            "loss: 0.025416 [51648/70000]\n",
            "loss: 0.023583 [51712/70000]\n",
            "loss: 0.024506 [51776/70000]\n",
            "loss: 0.024842 [51840/70000]\n",
            "loss: 0.025760 [51904/70000]\n",
            "loss: 0.023825 [51968/70000]\n",
            "loss: 0.025875 [52032/70000]\n",
            "loss: 0.024942 [52096/70000]\n",
            "loss: 0.023080 [52160/70000]\n",
            "loss: 0.023422 [52224/70000]\n",
            "loss: 0.024016 [52288/70000]\n",
            "loss: 0.025457 [52352/70000]\n",
            "loss: 0.024195 [52416/70000]\n",
            "loss: 0.025107 [52480/70000]\n",
            "loss: 0.023725 [52544/70000]\n",
            "loss: 0.023677 [52608/70000]\n",
            "loss: 0.023940 [52672/70000]\n",
            "loss: 0.024439 [52736/70000]\n",
            "loss: 0.025543 [52800/70000]\n",
            "loss: 0.023394 [52864/70000]\n",
            "loss: 0.024691 [52928/70000]\n",
            "loss: 0.023715 [52992/70000]\n",
            "loss: 0.023429 [53056/70000]\n",
            "loss: 0.026124 [53120/70000]\n",
            "loss: 0.023907 [53184/70000]\n",
            "loss: 0.026427 [53248/70000]\n",
            "loss: 0.024241 [53312/70000]\n",
            "loss: 0.025985 [53376/70000]\n",
            "loss: 0.022872 [53440/70000]\n",
            "loss: 0.025090 [53504/70000]\n",
            "loss: 0.025731 [53568/70000]\n",
            "loss: 0.026195 [53632/70000]\n",
            "loss: 0.023981 [53696/70000]\n",
            "loss: 0.025280 [53760/70000]\n",
            "loss: 0.025296 [53824/70000]\n",
            "loss: 0.025081 [53888/70000]\n",
            "loss: 0.023703 [53952/70000]\n",
            "loss: 0.023628 [54016/70000]\n",
            "loss: 0.025367 [54080/70000]\n",
            "loss: 0.026545 [54144/70000]\n",
            "loss: 0.023558 [54208/70000]\n",
            "loss: 0.024904 [54272/70000]\n",
            "loss: 0.024422 [54336/70000]\n",
            "loss: 0.024734 [54400/70000]\n",
            "loss: 0.024360 [54464/70000]\n",
            "loss: 0.023493 [54528/70000]\n",
            "loss: 0.025435 [54592/70000]\n",
            "loss: 0.023589 [54656/70000]\n",
            "loss: 0.024693 [54720/70000]\n",
            "loss: 0.023479 [54784/70000]\n",
            "loss: 0.024167 [54848/70000]\n",
            "loss: 0.023269 [54912/70000]\n",
            "loss: 0.023243 [54976/70000]\n",
            "loss: 0.025012 [55040/70000]\n",
            "loss: 0.025160 [55104/70000]\n",
            "loss: 0.025365 [55168/70000]\n",
            "loss: 0.023829 [55232/70000]\n",
            "loss: 0.023282 [55296/70000]\n",
            "loss: 0.024112 [55360/70000]\n",
            "loss: 0.023893 [55424/70000]\n",
            "loss: 0.024283 [55488/70000]\n",
            "loss: 0.025503 [55552/70000]\n",
            "loss: 0.024810 [55616/70000]\n",
            "loss: 0.024484 [55680/70000]\n",
            "loss: 0.023089 [55744/70000]\n",
            "loss: 0.024753 [55808/70000]\n",
            "loss: 0.025329 [55872/70000]\n",
            "loss: 0.024084 [55936/70000]\n",
            "loss: 0.025513 [56000/70000]\n",
            "loss: 0.024315 [56064/70000]\n",
            "loss: 0.023776 [56128/70000]\n",
            "loss: 0.024600 [56192/70000]\n",
            "loss: 0.023733 [56256/70000]\n",
            "loss: 0.024607 [56320/70000]\n",
            "loss: 0.025397 [56384/70000]\n",
            "loss: 0.024079 [56448/70000]\n",
            "loss: 0.024380 [56512/70000]\n",
            "loss: 0.024798 [56576/70000]\n",
            "loss: 0.024883 [56640/70000]\n",
            "loss: 0.024161 [56704/70000]\n",
            "loss: 0.025391 [56768/70000]\n",
            "loss: 0.024891 [56832/70000]\n",
            "loss: 0.023376 [56896/70000]\n",
            "loss: 0.023704 [56960/70000]\n",
            "loss: 0.024706 [57024/70000]\n",
            "loss: 0.024495 [57088/70000]\n",
            "loss: 0.026172 [57152/70000]\n",
            "loss: 0.024753 [57216/70000]\n",
            "loss: 0.024938 [57280/70000]\n",
            "loss: 0.021929 [57344/70000]\n",
            "loss: 0.024897 [57408/70000]\n",
            "loss: 0.025764 [57472/70000]\n",
            "loss: 0.024153 [57536/70000]\n",
            "loss: 0.023235 [57600/70000]\n",
            "loss: 0.026560 [57664/70000]\n",
            "loss: 0.023613 [57728/70000]\n",
            "loss: 0.025687 [57792/70000]\n",
            "loss: 0.025530 [57856/70000]\n",
            "loss: 0.026744 [57920/70000]\n",
            "loss: 0.024704 [57984/70000]\n",
            "loss: 0.025726 [58048/70000]\n",
            "loss: 0.024936 [58112/70000]\n",
            "loss: 0.023781 [58176/70000]\n",
            "loss: 0.024375 [58240/70000]\n",
            "loss: 0.024896 [58304/70000]\n",
            "loss: 0.025118 [58368/70000]\n",
            "loss: 0.023941 [58432/70000]\n",
            "loss: 0.025441 [58496/70000]\n",
            "loss: 0.025402 [58560/70000]\n",
            "loss: 0.024488 [58624/70000]\n",
            "loss: 0.024102 [58688/70000]\n",
            "loss: 0.023307 [58752/70000]\n",
            "loss: 0.025515 [58816/70000]\n",
            "loss: 0.026165 [58880/70000]\n",
            "loss: 0.024315 [58944/70000]\n",
            "loss: 0.024028 [59008/70000]\n",
            "loss: 0.024541 [59072/70000]\n",
            "loss: 0.023720 [59136/70000]\n",
            "loss: 0.024225 [59200/70000]\n",
            "loss: 0.023140 [59264/70000]\n",
            "loss: 0.025267 [59328/70000]\n",
            "loss: 0.024150 [59392/70000]\n",
            "loss: 0.023125 [59456/70000]\n",
            "loss: 0.023945 [59520/70000]\n",
            "loss: 0.025083 [59584/70000]\n",
            "loss: 0.024028 [59648/70000]\n",
            "loss: 0.022945 [59712/70000]\n",
            "loss: 0.024597 [59776/70000]\n",
            "loss: 0.023870 [59840/70000]\n",
            "loss: 0.023726 [59904/70000]\n",
            "loss: 0.024966 [59968/70000]\n",
            "loss: 0.024833 [60032/70000]\n",
            "loss: 0.024556 [60096/70000]\n",
            "loss: 0.025023 [60160/70000]\n",
            "loss: 0.022804 [60224/70000]\n",
            "loss: 0.025744 [60288/70000]\n",
            "loss: 0.024137 [60352/70000]\n",
            "loss: 0.024454 [60416/70000]\n",
            "loss: 0.023727 [60480/70000]\n",
            "loss: 0.023847 [60544/70000]\n",
            "loss: 0.024238 [60608/70000]\n",
            "loss: 0.024028 [60672/70000]\n",
            "loss: 0.024809 [60736/70000]\n",
            "loss: 0.025242 [60800/70000]\n",
            "loss: 0.024590 [60864/70000]\n",
            "loss: 0.023211 [60928/70000]\n",
            "loss: 0.024165 [60992/70000]\n",
            "loss: 0.025010 [61056/70000]\n",
            "loss: 0.024985 [61120/70000]\n",
            "loss: 0.024332 [61184/70000]\n",
            "loss: 0.025145 [61248/70000]\n",
            "loss: 0.023906 [61312/70000]\n",
            "loss: 0.025349 [61376/70000]\n",
            "loss: 0.024328 [61440/70000]\n",
            "loss: 0.023182 [61504/70000]\n",
            "loss: 0.024438 [61568/70000]\n",
            "loss: 0.023265 [61632/70000]\n",
            "loss: 0.024523 [61696/70000]\n",
            "loss: 0.025397 [61760/70000]\n",
            "loss: 0.023260 [61824/70000]\n",
            "loss: 0.025365 [61888/70000]\n",
            "loss: 0.023716 [61952/70000]\n",
            "loss: 0.024267 [62016/70000]\n",
            "loss: 0.024322 [62080/70000]\n",
            "loss: 0.023964 [62144/70000]\n",
            "loss: 0.025575 [62208/70000]\n",
            "loss: 0.025613 [62272/70000]\n",
            "loss: 0.023965 [62336/70000]\n",
            "loss: 0.025660 [62400/70000]\n",
            "loss: 0.025075 [62464/70000]\n",
            "loss: 0.025664 [62528/70000]\n",
            "loss: 0.024357 [62592/70000]\n",
            "loss: 0.024336 [62656/70000]\n",
            "loss: 0.024405 [62720/70000]\n",
            "loss: 0.024450 [62784/70000]\n",
            "loss: 0.025293 [62848/70000]\n",
            "loss: 0.026166 [62912/70000]\n",
            "loss: 0.025253 [62976/70000]\n",
            "loss: 0.026146 [63040/70000]\n",
            "loss: 0.025932 [63104/70000]\n",
            "loss: 0.026331 [63168/70000]\n",
            "loss: 0.025984 [63232/70000]\n",
            "loss: 0.025180 [63296/70000]\n",
            "loss: 0.022974 [63360/70000]\n",
            "loss: 0.024161 [63424/70000]\n",
            "loss: 0.023448 [63488/70000]\n",
            "loss: 0.024398 [63552/70000]\n",
            "loss: 0.024658 [63616/70000]\n",
            "loss: 0.023327 [63680/70000]\n",
            "loss: 0.023338 [63744/70000]\n",
            "loss: 0.025078 [63808/70000]\n",
            "loss: 0.024726 [63872/70000]\n",
            "loss: 0.023544 [63936/70000]\n",
            "loss: 0.023789 [64000/70000]\n",
            "loss: 0.024286 [64064/70000]\n",
            "loss: 0.024882 [64128/70000]\n",
            "loss: 0.024072 [64192/70000]\n",
            "loss: 0.025421 [64256/70000]\n",
            "loss: 0.023496 [64320/70000]\n",
            "loss: 0.024475 [64384/70000]\n",
            "loss: 0.025337 [64448/70000]\n",
            "loss: 0.025852 [64512/70000]\n",
            "loss: 0.024530 [64576/70000]\n",
            "loss: 0.025447 [64640/70000]\n",
            "loss: 0.024243 [64704/70000]\n",
            "loss: 0.024756 [64768/70000]\n",
            "loss: 0.023891 [64832/70000]\n",
            "loss: 0.024945 [64896/70000]\n",
            "loss: 0.024227 [64960/70000]\n",
            "loss: 0.024261 [65024/70000]\n",
            "loss: 0.024984 [65088/70000]\n",
            "loss: 0.023369 [65152/70000]\n",
            "loss: 0.024403 [65216/70000]\n",
            "loss: 0.025480 [65280/70000]\n",
            "loss: 0.024807 [65344/70000]\n",
            "loss: 0.024972 [65408/70000]\n",
            "loss: 0.024818 [65472/70000]\n",
            "loss: 0.024923 [65536/70000]\n",
            "loss: 0.023286 [65600/70000]\n",
            "loss: 0.024670 [65664/70000]\n",
            "loss: 0.024913 [65728/70000]\n",
            "loss: 0.023759 [65792/70000]\n",
            "loss: 0.024579 [65856/70000]\n",
            "loss: 0.024418 [65920/70000]\n",
            "loss: 0.024275 [65984/70000]\n",
            "loss: 0.023474 [66048/70000]\n",
            "loss: 0.025881 [66112/70000]\n",
            "loss: 0.024756 [66176/70000]\n",
            "loss: 0.025131 [66240/70000]\n",
            "loss: 0.025276 [66304/70000]\n",
            "loss: 0.024037 [66368/70000]\n",
            "loss: 0.025812 [66432/70000]\n",
            "loss: 0.022292 [66496/70000]\n",
            "loss: 0.025778 [66560/70000]\n",
            "loss: 0.026084 [66624/70000]\n",
            "loss: 0.025757 [66688/70000]\n",
            "loss: 0.024639 [66752/70000]\n",
            "loss: 0.024165 [66816/70000]\n",
            "loss: 0.025116 [66880/70000]\n",
            "loss: 0.024679 [66944/70000]\n",
            "loss: 0.023723 [67008/70000]\n",
            "loss: 0.024481 [67072/70000]\n",
            "loss: 0.025792 [67136/70000]\n",
            "loss: 0.023191 [67200/70000]\n",
            "loss: 0.025119 [67264/70000]\n",
            "loss: 0.023588 [67328/70000]\n",
            "loss: 0.023509 [67392/70000]\n",
            "loss: 0.024613 [67456/70000]\n",
            "loss: 0.024634 [67520/70000]\n",
            "loss: 0.025431 [67584/70000]\n",
            "loss: 0.024984 [67648/70000]\n",
            "loss: 0.024350 [67712/70000]\n",
            "loss: 0.023861 [67776/70000]\n",
            "loss: 0.023955 [67840/70000]\n",
            "loss: 0.023510 [67904/70000]\n",
            "loss: 0.022587 [67968/70000]\n",
            "loss: 0.023819 [68032/70000]\n",
            "loss: 0.023741 [68096/70000]\n",
            "loss: 0.024503 [68160/70000]\n",
            "loss: 0.023749 [68224/70000]\n",
            "loss: 0.023698 [68288/70000]\n",
            "loss: 0.024253 [68352/70000]\n",
            "loss: 0.025314 [68416/70000]\n",
            "loss: 0.025343 [68480/70000]\n",
            "loss: 0.024438 [68544/70000]\n",
            "loss: 0.024273 [68608/70000]\n",
            "loss: 0.024005 [68672/70000]\n",
            "loss: 0.026084 [68736/70000]\n",
            "loss: 0.024669 [68800/70000]\n",
            "loss: 0.025344 [68864/70000]\n",
            "loss: 0.024673 [68928/70000]\n",
            "loss: 0.024513 [68992/70000]\n",
            "loss: 0.023482 [69056/70000]\n",
            "loss: 0.025132 [69120/70000]\n",
            "loss: 0.024577 [69184/70000]\n",
            "loss: 0.025422 [69248/70000]\n",
            "loss: 0.023660 [69312/70000]\n",
            "loss: 0.024817 [69376/70000]\n",
            "loss: 0.024574 [69440/70000]\n",
            "loss: 0.023584 [69504/70000]\n",
            "loss: 0.023954 [69568/70000]\n",
            "loss: 0.022952 [69632/70000]\n",
            "loss: 0.023332 [69696/70000]\n",
            "loss: 0.024556 [69760/70000]\n",
            "loss: 0.024853 [69824/70000]\n",
            "loss: 0.024578 [69888/70000]\n",
            "loss: 0.033984 [52464/70000]\n",
            "Accuracy: 43.9%, Avg loss: 0.025094\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "2.9188729569578794e-05\n",
            "loss: 0.024351 [    0/70000]\n",
            "loss: 0.024428 [   64/70000]\n",
            "loss: 0.025361 [  128/70000]\n",
            "loss: 0.024045 [  192/70000]\n",
            "loss: 0.023860 [  256/70000]\n",
            "loss: 0.024109 [  320/70000]\n",
            "loss: 0.024860 [  384/70000]\n",
            "loss: 0.024262 [  448/70000]\n",
            "loss: 0.023977 [  512/70000]\n",
            "loss: 0.025308 [  576/70000]\n",
            "loss: 0.024448 [  640/70000]\n",
            "loss: 0.024969 [  704/70000]\n",
            "loss: 0.025088 [  768/70000]\n",
            "loss: 0.025436 [  832/70000]\n",
            "loss: 0.023908 [  896/70000]\n",
            "loss: 0.024318 [  960/70000]\n",
            "loss: 0.024785 [ 1024/70000]\n",
            "loss: 0.023911 [ 1088/70000]\n",
            "loss: 0.026198 [ 1152/70000]\n",
            "loss: 0.024726 [ 1216/70000]\n",
            "loss: 0.023841 [ 1280/70000]\n",
            "loss: 0.024122 [ 1344/70000]\n",
            "loss: 0.025097 [ 1408/70000]\n",
            "loss: 0.024666 [ 1472/70000]\n",
            "loss: 0.024058 [ 1536/70000]\n",
            "loss: 0.025423 [ 1600/70000]\n",
            "loss: 0.023657 [ 1664/70000]\n",
            "loss: 0.024016 [ 1728/70000]\n",
            "loss: 0.025683 [ 1792/70000]\n",
            "loss: 0.023946 [ 1856/70000]\n",
            "loss: 0.024964 [ 1920/70000]\n",
            "loss: 0.025276 [ 1984/70000]\n",
            "loss: 0.023582 [ 2048/70000]\n",
            "loss: 0.023545 [ 2112/70000]\n",
            "loss: 0.025186 [ 2176/70000]\n",
            "loss: 0.023421 [ 2240/70000]\n",
            "loss: 0.025660 [ 2304/70000]\n",
            "loss: 0.027692 [ 2368/70000]\n",
            "loss: 0.024970 [ 2432/70000]\n",
            "loss: 0.025316 [ 2496/70000]\n",
            "loss: 0.025123 [ 2560/70000]\n",
            "loss: 0.025887 [ 2624/70000]\n",
            "loss: 0.025712 [ 2688/70000]\n",
            "loss: 0.025428 [ 2752/70000]\n",
            "loss: 0.025106 [ 2816/70000]\n",
            "loss: 0.024553 [ 2880/70000]\n",
            "loss: 0.023561 [ 2944/70000]\n",
            "loss: 0.024212 [ 3008/70000]\n",
            "loss: 0.024429 [ 3072/70000]\n",
            "loss: 0.023955 [ 3136/70000]\n",
            "loss: 0.026143 [ 3200/70000]\n",
            "loss: 0.024464 [ 3264/70000]\n",
            "loss: 0.024425 [ 3328/70000]\n",
            "loss: 0.023930 [ 3392/70000]\n",
            "loss: 0.024753 [ 3456/70000]\n",
            "loss: 0.024785 [ 3520/70000]\n",
            "loss: 0.024881 [ 3584/70000]\n",
            "loss: 0.023742 [ 3648/70000]\n",
            "loss: 0.025719 [ 3712/70000]\n",
            "loss: 0.025157 [ 3776/70000]\n",
            "loss: 0.024210 [ 3840/70000]\n",
            "loss: 0.024148 [ 3904/70000]\n",
            "loss: 0.025581 [ 3968/70000]\n",
            "loss: 0.024643 [ 4032/70000]\n",
            "loss: 0.023657 [ 4096/70000]\n",
            "loss: 0.024891 [ 4160/70000]\n",
            "loss: 0.024506 [ 4224/70000]\n",
            "loss: 0.022665 [ 4288/70000]\n",
            "loss: 0.024900 [ 4352/70000]\n",
            "loss: 0.024435 [ 4416/70000]\n",
            "loss: 0.025016 [ 4480/70000]\n",
            "loss: 0.024725 [ 4544/70000]\n",
            "loss: 0.025862 [ 4608/70000]\n",
            "loss: 0.024341 [ 4672/70000]\n",
            "loss: 0.025239 [ 4736/70000]\n",
            "loss: 0.023540 [ 4800/70000]\n",
            "loss: 0.023856 [ 4864/70000]\n",
            "loss: 0.025941 [ 4928/70000]\n",
            "loss: 0.024585 [ 4992/70000]\n",
            "loss: 0.024381 [ 5056/70000]\n",
            "loss: 0.023451 [ 5120/70000]\n",
            "loss: 0.024761 [ 5184/70000]\n",
            "loss: 0.024931 [ 5248/70000]\n",
            "loss: 0.024118 [ 5312/70000]\n",
            "loss: 0.024842 [ 5376/70000]\n",
            "loss: 0.024221 [ 5440/70000]\n",
            "loss: 0.024624 [ 5504/70000]\n",
            "loss: 0.024186 [ 5568/70000]\n",
            "loss: 0.024721 [ 5632/70000]\n",
            "loss: 0.025104 [ 5696/70000]\n",
            "loss: 0.023613 [ 5760/70000]\n",
            "loss: 0.024514 [ 5824/70000]\n",
            "loss: 0.025865 [ 5888/70000]\n",
            "loss: 0.024777 [ 5952/70000]\n",
            "loss: 0.023319 [ 6016/70000]\n",
            "loss: 0.025274 [ 6080/70000]\n",
            "loss: 0.025431 [ 6144/70000]\n",
            "loss: 0.024323 [ 6208/70000]\n",
            "loss: 0.025407 [ 6272/70000]\n",
            "loss: 0.024411 [ 6336/70000]\n",
            "loss: 0.025046 [ 6400/70000]\n",
            "loss: 0.024598 [ 6464/70000]\n",
            "loss: 0.024426 [ 6528/70000]\n",
            "loss: 0.024735 [ 6592/70000]\n",
            "loss: 0.025451 [ 6656/70000]\n",
            "loss: 0.023982 [ 6720/70000]\n",
            "loss: 0.023420 [ 6784/70000]\n",
            "loss: 0.023751 [ 6848/70000]\n",
            "loss: 0.026326 [ 6912/70000]\n",
            "loss: 0.024576 [ 6976/70000]\n",
            "loss: 0.025048 [ 7040/70000]\n",
            "loss: 0.022638 [ 7104/70000]\n",
            "loss: 0.024450 [ 7168/70000]\n",
            "loss: 0.023306 [ 7232/70000]\n",
            "loss: 0.024003 [ 7296/70000]\n",
            "loss: 0.025048 [ 7360/70000]\n",
            "loss: 0.024186 [ 7424/70000]\n",
            "loss: 0.024215 [ 7488/70000]\n",
            "loss: 0.025137 [ 7552/70000]\n",
            "loss: 0.024503 [ 7616/70000]\n",
            "loss: 0.023266 [ 7680/70000]\n",
            "loss: 0.023574 [ 7744/70000]\n",
            "loss: 0.023414 [ 7808/70000]\n",
            "loss: 0.024575 [ 7872/70000]\n",
            "loss: 0.025029 [ 7936/70000]\n",
            "loss: 0.024390 [ 8000/70000]\n",
            "loss: 0.024813 [ 8064/70000]\n",
            "loss: 0.025544 [ 8128/70000]\n",
            "loss: 0.024866 [ 8192/70000]\n",
            "loss: 0.024664 [ 8256/70000]\n",
            "loss: 0.025626 [ 8320/70000]\n",
            "loss: 0.023729 [ 8384/70000]\n",
            "loss: 0.024652 [ 8448/70000]\n",
            "loss: 0.023690 [ 8512/70000]\n",
            "loss: 0.023682 [ 8576/70000]\n",
            "loss: 0.024556 [ 8640/70000]\n",
            "loss: 0.024637 [ 8704/70000]\n",
            "loss: 0.024140 [ 8768/70000]\n",
            "loss: 0.025925 [ 8832/70000]\n",
            "loss: 0.023665 [ 8896/70000]\n",
            "loss: 0.025069 [ 8960/70000]\n",
            "loss: 0.026376 [ 9024/70000]\n",
            "loss: 0.024663 [ 9088/70000]\n",
            "loss: 0.024216 [ 9152/70000]\n",
            "loss: 0.024555 [ 9216/70000]\n",
            "loss: 0.025343 [ 9280/70000]\n",
            "loss: 0.023170 [ 9344/70000]\n",
            "loss: 0.023590 [ 9408/70000]\n",
            "loss: 0.023129 [ 9472/70000]\n",
            "loss: 0.024356 [ 9536/70000]\n",
            "loss: 0.023408 [ 9600/70000]\n",
            "loss: 0.024685 [ 9664/70000]\n",
            "loss: 0.025240 [ 9728/70000]\n",
            "loss: 0.025187 [ 9792/70000]\n",
            "loss: 0.022644 [ 9856/70000]\n",
            "loss: 0.024945 [ 9920/70000]\n",
            "loss: 0.025109 [ 9984/70000]\n",
            "loss: 0.023022 [10048/70000]\n",
            "loss: 0.024305 [10112/70000]\n",
            "loss: 0.024127 [10176/70000]\n",
            "loss: 0.022491 [10240/70000]\n",
            "loss: 0.023736 [10304/70000]\n",
            "loss: 0.025567 [10368/70000]\n",
            "loss: 0.022940 [10432/70000]\n",
            "loss: 0.024612 [10496/70000]\n",
            "loss: 0.024808 [10560/70000]\n",
            "loss: 0.024435 [10624/70000]\n",
            "loss: 0.024503 [10688/70000]\n",
            "loss: 0.024637 [10752/70000]\n",
            "loss: 0.023721 [10816/70000]\n",
            "loss: 0.025328 [10880/70000]\n",
            "loss: 0.023241 [10944/70000]\n",
            "loss: 0.025155 [11008/70000]\n",
            "loss: 0.024164 [11072/70000]\n",
            "loss: 0.025469 [11136/70000]\n",
            "loss: 0.023963 [11200/70000]\n",
            "loss: 0.025360 [11264/70000]\n",
            "loss: 0.024035 [11328/70000]\n",
            "loss: 0.024149 [11392/70000]\n",
            "loss: 0.023587 [11456/70000]\n",
            "loss: 0.022595 [11520/70000]\n",
            "loss: 0.023510 [11584/70000]\n",
            "loss: 0.024867 [11648/70000]\n",
            "loss: 0.024202 [11712/70000]\n",
            "loss: 0.024264 [11776/70000]\n",
            "loss: 0.024656 [11840/70000]\n",
            "loss: 0.025725 [11904/70000]\n",
            "loss: 0.023738 [11968/70000]\n",
            "loss: 0.024624 [12032/70000]\n",
            "loss: 0.025102 [12096/70000]\n",
            "loss: 0.022661 [12160/70000]\n",
            "loss: 0.023511 [12224/70000]\n",
            "loss: 0.024267 [12288/70000]\n",
            "loss: 0.024031 [12352/70000]\n",
            "loss: 0.023170 [12416/70000]\n",
            "loss: 0.023565 [12480/70000]\n",
            "loss: 0.024210 [12544/70000]\n",
            "loss: 0.025870 [12608/70000]\n",
            "loss: 0.025551 [12672/70000]\n",
            "loss: 0.025197 [12736/70000]\n",
            "loss: 0.023991 [12800/70000]\n",
            "loss: 0.025926 [12864/70000]\n",
            "loss: 0.025585 [12928/70000]\n",
            "loss: 0.023583 [12992/70000]\n",
            "loss: 0.024492 [13056/70000]\n",
            "loss: 0.025049 [13120/70000]\n",
            "loss: 0.025897 [13184/70000]\n",
            "loss: 0.023731 [13248/70000]\n",
            "loss: 0.024410 [13312/70000]\n",
            "loss: 0.023006 [13376/70000]\n",
            "loss: 0.023987 [13440/70000]\n",
            "loss: 0.024803 [13504/70000]\n",
            "loss: 0.024576 [13568/70000]\n",
            "loss: 0.024362 [13632/70000]\n",
            "loss: 0.025173 [13696/70000]\n",
            "loss: 0.023973 [13760/70000]\n",
            "loss: 0.025141 [13824/70000]\n",
            "loss: 0.023632 [13888/70000]\n",
            "loss: 0.024627 [13952/70000]\n",
            "loss: 0.024848 [14016/70000]\n",
            "loss: 0.024043 [14080/70000]\n",
            "loss: 0.024104 [14144/70000]\n",
            "loss: 0.024143 [14208/70000]\n",
            "loss: 0.023474 [14272/70000]\n",
            "loss: 0.024104 [14336/70000]\n",
            "loss: 0.024313 [14400/70000]\n",
            "loss: 0.022996 [14464/70000]\n",
            "loss: 0.024304 [14528/70000]\n",
            "loss: 0.024109 [14592/70000]\n",
            "loss: 0.024302 [14656/70000]\n",
            "loss: 0.023692 [14720/70000]\n",
            "loss: 0.021950 [14784/70000]\n",
            "loss: 0.025125 [14848/70000]\n",
            "loss: 0.024375 [14912/70000]\n",
            "loss: 0.025271 [14976/70000]\n",
            "loss: 0.024731 [15040/70000]\n",
            "loss: 0.023974 [15104/70000]\n",
            "loss: 0.023818 [15168/70000]\n",
            "loss: 0.023995 [15232/70000]\n",
            "loss: 0.025083 [15296/70000]\n",
            "loss: 0.023856 [15360/70000]\n",
            "loss: 0.023114 [15424/70000]\n",
            "loss: 0.024064 [15488/70000]\n",
            "loss: 0.023382 [15552/70000]\n",
            "loss: 0.025144 [15616/70000]\n",
            "loss: 0.024527 [15680/70000]\n",
            "loss: 0.022489 [15744/70000]\n",
            "loss: 0.024285 [15808/70000]\n",
            "loss: 0.023851 [15872/70000]\n",
            "loss: 0.024170 [15936/70000]\n",
            "loss: 0.023954 [16000/70000]\n",
            "loss: 0.025531 [16064/70000]\n",
            "loss: 0.024309 [16128/70000]\n",
            "loss: 0.023336 [16192/70000]\n",
            "loss: 0.024195 [16256/70000]\n",
            "loss: 0.024206 [16320/70000]\n",
            "loss: 0.023710 [16384/70000]\n",
            "loss: 0.024782 [16448/70000]\n",
            "loss: 0.023869 [16512/70000]\n",
            "loss: 0.024153 [16576/70000]\n",
            "loss: 0.022655 [16640/70000]\n",
            "loss: 0.023520 [16704/70000]\n",
            "loss: 0.024658 [16768/70000]\n",
            "loss: 0.025010 [16832/70000]\n",
            "loss: 0.026339 [16896/70000]\n",
            "loss: 0.025087 [16960/70000]\n",
            "loss: 0.025480 [17024/70000]\n",
            "loss: 0.026164 [17088/70000]\n",
            "loss: 0.026453 [17152/70000]\n",
            "loss: 0.025754 [17216/70000]\n",
            "loss: 0.023796 [17280/70000]\n",
            "loss: 0.024260 [17344/70000]\n",
            "loss: 0.022758 [17408/70000]\n",
            "loss: 0.024195 [17472/70000]\n",
            "loss: 0.022564 [17536/70000]\n",
            "loss: 0.025275 [17600/70000]\n",
            "loss: 0.023523 [17664/70000]\n",
            "loss: 0.024252 [17728/70000]\n",
            "loss: 0.023732 [17792/70000]\n",
            "loss: 0.023865 [17856/70000]\n",
            "loss: 0.024010 [17920/70000]\n",
            "loss: 0.023406 [17984/70000]\n",
            "loss: 0.025086 [18048/70000]\n",
            "loss: 0.025441 [18112/70000]\n",
            "loss: 0.023999 [18176/70000]\n"
          ]
        }
      ],
      "source": [
        "# @title wwwwwwwww\n",
        "import time\n",
        "start = time.time()\n",
        "acc_lst, train_lst, test_lst=[],[],[]\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# base_lr, max_lr = 3e-7, 3e-6 # resnet\n",
        "base_lr, max_lr = 3e-6, 3e-5 # inception\n",
        "# base_lr, max_lr = 1e-5, 1e-2 # vit\n",
        "# end_lr, start_lr = 1e-5, 1e-3 # 0.0001,0.1\n",
        "tp=0\n",
        "epochs = 5 #5 20\n",
        "# optimizer = torch.optim.SGD(model.parameters(), lr = start_lr, momentum=0.9)\n",
        "# optimizer = torch.optim.AdamW(model.parameters(), lr=base_lr, betas=(0.9, 0.999), eps=1e-08, weight_decay=3e-6)\n",
        "import bitsandbytes as bnb # 8bit optimizer\n",
        "optimizer = bnb.optim.AdamW(model.parameters(), lr=base_lr, betas=(0.9, 0.999), optim_bits=8)\n",
        "# optimizer = bnb.optim.AdamW(model.parameters(), lr=1e-5, betas=(0.9, 0.999), optim_bits=8)\n",
        "# 152 1e-5\n",
        "# cnn 3e-4\n",
        "\n",
        "div_factor = max_lr/base_lr\n",
        "num_batches=len(train_loader)\n",
        "total_steps=int(num_batches*epochs)+1 # +1 to excluse uptick at the end of onecycle\n",
        "# total_steps=int(np.ceil(num_batches/4)*epochs +1) # /4 for when using grad accumulation\n",
        "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=max_lr, total_steps=total_steps, pct_start=0.45, anneal_strategy='cos', cycle_momentum=True, base_momentum=0.85, max_momentum=0.95, div_factor=div_factor, final_div_factor=100.0, three_phase=True,)\n",
        "# scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=max_lr, epochs=epochs, steps_per_epoch=num_batches, pct_start=0.45, anneal_strategy='cos', cycle_momentum=True, base_momentum=0.85, max_momentum=0.95, div_factor=div_factor, final_div_factor=10000.0, three_phase=True,)\n",
        "# gamma = np.exp(np.log(end_lr/start_lr)/(num_batches*epochs)) # for scheduler step every optimizer step\n",
        "# scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=gamma) # 0.75(20)-0.9(100)\n",
        "\n",
        "# pth='/content/inception2.pth'\n",
        "pth='/content/drive/MyDrive/frame/inception1.pth'\n",
        "pth='/content/drive/MyDrive/frame/vit.pth'\n",
        "\n",
        "# to continue training\n",
        "# tp, modelsd, optimsd, schedsd = torch.load(pth).values()\n",
        "# model.load_state_dict(modelsd)\n",
        "# optimizer.load_state_dict(optimsd)\n",
        "# scheduler.load_state_dict(schedsd)\n",
        "\n",
        "\n",
        "for t in range(tp,epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    lr=optimizer.param_groups[0][\"lr\"]\n",
        "    print(lr)\n",
        "    train_ls = strain(train_loader, model, loss_fn, optimizer, scheduler)\n",
        "    correct, test_loss = test(test_loader, model, loss_fn)\n",
        "    train_lst.extend(train_ls)\n",
        "    test_lst.append(test_loss)\n",
        "    acc_lst.append(correct)\n",
        "\n",
        "    checkpoint = { # https://discuss.pytorch.org/t/saving-model-and-optimiser-and-scheduler/52030\n",
        "    'epoch': t,\n",
        "    'model': model.state_dict(),\n",
        "    'optimizer': optimizer.state_dict(),\n",
        "    'lr_sched': scheduler.state_dict()}\n",
        "    torch.save(checkpoint, pth)\n",
        "    # torch.save(model.state_dict(), pth)\n",
        "\n",
        "# print(\"Done!\")\n",
        "\n",
        "# end = time.time()\n",
        "# print(\"time: \",end - start)\n",
        "\n",
        "# print(len(train_lst), len(test_lst))\n",
        "# import matplotlib.pyplot as plt\n",
        "# plt.plot(train_lst)\n",
        "# plt.plot(np.linspace(0,len(train_lst),len(test_lst)), test_lst)\n",
        "# plt.show()\n",
        "# plt.plot(acc_lst)\n",
        "# plt.show()\n",
        "# plt.close()\n",
        "\n",
        "\n",
        "# torch.save(model.state_dict(), \"model.pth\")\n",
        "\n",
        "# resnet 18, 60/61 38.4%, 528s\n",
        "# resnet 18, 58/61 39.8%, 523s\n",
        "# resnet 18 compile , 58/61 40.4%, 555s\n",
        "# resnet 18 compile augment , 58/61 36.4%, 1941 # augment on cpu, takes longer\n",
        "# resnet 18 augment lr3e-4:3e-3, 58/61 37.7%, 1863s\n",
        "# resnet 18 augment 10epoch lr1e-5:3e-4, 58/61 33.5%, 3387s\n",
        "# resnet 18 compile lr1e-5:3e-4, 58/61 35.0%, 493s\n",
        "# resnet 18 compile scratch lr1e-5:1e-3, 58/61 26.8%, 475s\n",
        "# resnet 18 compile lr1e-5:1e-3, 55/61 47.3%, 480s\n",
        "# resnet 18 compile lr1e-5:1e-3, 52/61 51.7% 503s\n",
        "# resnet 18 compile lr1e-5:1e-3, unfreeze 51.0%, 550s\n",
        "# resnet 18 compile lr1e-5:1e-4, unfreeze 52.7%, 518s\n",
        "# resnet 34 compile lr1e-5:1e-4, unfreeze bitsadamW batch16*4\n",
        "# resnet 152 compileoverhead lr3e-7:3e-6, bitsadamW batch16*4 ckpt 53.8%, 2066s\n",
        "# resnet 152 from53.8% augment+cutout lr3e-7:3e-6, 53.8%, 2088s\n",
        "# resnet 152 comile augment+cutout lr1e-5 /4 1epoch 48.3%, 454s\n",
        "# resnet 152 comile augment+cutout lr1e-5 1epoch 48.4%, 446s\n",
        "# resnet 152 comilemaxautotue augment+cutout lr1e-5 1epoch 47.7%, 448\n",
        "# resnet 152 clipclean comilemax augment+cutout lr1e-5 10epoch 45.1%, 1585 *2\n",
        "# resnet 152 clipclean comilemax augmax lr1e-6:3e-5 20epoch \n",
        "# resnet 152 clipclean compilemax lr3e-7:1e-5,  \n",
        "# resnet 152 70k augment compilemax lr3e-7:3e-6 5epoch, 65.4% 11585s*5/3=19300s = 5h20m\n",
        "\n",
        "\n",
        "# vit b16 lr1e-5 5epochs 41.3%, 466s # 4.4ram, 5.5vram\n",
        "# vit l16 lr1e-5 5epochs # 32.0%, 1242s 4.5ram, 8.0vram\n",
        "# vit l16 lr3e-7;1e-5 5epochs # 45.4%, 1315s 4.5ram, 8.0vram\n",
        "# vit l32 lr1e-5 5epochs # .ram, .vram\n",
        "# vit_large_patch16_384\n",
        "# vit_base_patch16_224 maxcompile nockpt lr3e-7;1e-5 5epochs # 45.2%, 2272s 5.3ram, 11.0vram\n",
        "# vit_base_patch16_224 maxcompile nockpt gradacc batch16 1e-5, 1e-2\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# inception\n",
        "# inception og 10kclean 1e-7, 3e-5 batch64 nockpt 5epochs 0.020348  51.4%, 0.024312 20m; 1st 40.5% max 52.1\n",
        "# inception hid 10kclean 1e-7, 3e-5 batch64 nockpt 1/5epochs 0.019635 53.6%, 0.024120 1st 32.8%, max 53.6%\n",
        "# inception og 10kclean 1e-7, 1e-4 batch64 nockpt 2/10epochs 0.020348  0.020923 39.9%, 0.026105\n",
        "# inception og 70kg 1e-7 nope\n",
        "# inception og 70kg 3e-6 3e-5 \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pytorch release gpu ram after training\n",
        "# https://discuss.pytorch.org/t/free-all-gpu-memory-used-in-between-runs/168202/2\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "correct, test_loss = test(test_loader, model, loss_fn)\n",
        "train_lst.extend(train_ls)\n",
        "test_lst.append(test_loss)\n",
        "acc_lst.append(correct)\n",
        "\n",
        "checkpoint = { # https://discuss.pytorch.org/t/saving-model-and-optimiser-and-scheduler/52030\n",
        "'epoch': t,\n",
        "'model': model.state_dict(),\n",
        "'optimizer': optimizer.state_dict(),\n",
        "'lr_sched': scheduler.state_dict()}\n",
        "torch.save(checkpoint, 'checkpoint.pth')\n"
      ],
      "metadata": {
        "id": "ayRePbr_rq9F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "REiP7-nvhc4s",
        "outputId": "379c5e19-6b94-49a2-d28d-229a3df3be89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# @title save\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# # pth='/content/drive/MyDrive/frame/vit.pth'\n",
        "# pth='/content/drive/MyDrive/frame/res55.pth'\n",
        "# pth='/content/drive/MyDrive/frame/resnet152.pth'\n",
        "# pth='/content/drive/MyDrive/frame/inception.pth'\n",
        "# pth='/content/inception2.pth'\n",
        "\n",
        "# torch.save(model.state_dict(), pth)\n",
        "# model.load_state_dict(torch.load(pth))\n",
        "# # model.load_state_dict(torch.load(\"model.pth\"))\n",
        "\n",
        "# !gdown 1---4fdFbOUBTrS-VP5Va6pKowfgoU2UN -O inception2.pth\n",
        "# !gdown 1visTNvWmnuV7jAm2TBiAIIrNjbOAi1Fv -O resnet152.pth\n",
        "\n",
        "# # t, modelsd, optimsd, scheduler = torch.load('/content/drive/MyDrive/frame/resnet152.pth').values()\n",
        "# t, modelsd, optimsd, scheduler = torch.load('/content/resnet152.pth').values()\n",
        "# model.load_state_dict(modelsd)\n",
        "# # optimizer.load_state_dict(optimsd)\n",
        "\n",
        "# # matt152 # https://drive.google.com/file/d/1MQ0xLfHbio458uEVbn2VyMpD3bij2A4J/view?usp=sharing\n",
        "# !gdown 1MQ0xLfHbio458uEVbn2VyMpD3bij2A4J -O res152.pth\n",
        "# model.load_state_dict(torch.load(\"res152.pth\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A57FKq_YGadX",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title trash\n",
        "# dir='/content/google_street_view'\n",
        "\n",
        "# # data = datasets.ImageFolder(dir, transform=transform)\n",
        "# data = datasets.ImageFolder(dir, transform=None)\n",
        "# torch.manual_seed(0)\n",
        "# train_data, test_data = torch.utils.data.random_split(data, [.85,.15])\n",
        "\n",
        "# # train_data = DatasetWrap(train_data, TrainTransform()) # apply data augmentation to train dataset only\n",
        "# train_data = DatasetWrap(train_data, transform)\n",
        "# test_data = DatasetWrap(test_data, transform)\n",
        "\n",
        "\n",
        "# batch_size = 16 # 64\n",
        "# num_batches=int(np.ceil(len(train_data)/batch_size))\n",
        "\n",
        "# train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
        "# test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
        "# # data, train_data, test_data = None, None, None\n",
        "\n",
        "\n",
        "# dataiter = iter(train_loader)\n",
        "# images, labels = next(dataiter)\n",
        "# imshow(torchvision.utils.make_grid(images))\n",
        "# dataiter=None\n",
        "\n",
        "\n",
        "%matplotlib inline \n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# dataiter = iter(train_data)\n",
        "\n",
        "\n",
        "# img,label = next(iter(sample_ds))\n",
        "i=5\n",
        "# print(len(test_data))\n",
        "# img,label=test_data[i]\n",
        "img,label=sample_ds[i]\n",
        "print(img.shape)\n",
        "# print(type(img))\n",
        "model.eval()\n",
        "pred=model(img.unsqueeze(0).to(device))\n",
        "pred_probab = nn.Softmax(dim=1)(pred)\n",
        "y_pred = pred_probab.argmax(1)\n",
        "print(pred_probab)\n",
        "print(\"pred: \",y_pred.item())\n",
        "# print(img)\n",
        "# image=images[0]\n",
        "\n",
        "print(\"actual: \",label)\n",
        "plt.imshow(np.transpose(img, (1, 2, 0)))\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q_3Cttn1qHcc",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title trash\n",
        "\n",
        "model.eval()\n",
        "count=0\n",
        "i=138\n",
        "rong_lst=[]\n",
        "\n",
        "while count<20:\n",
        "    img,label=test_data[i]\n",
        "    pred=model(img.unsqueeze(0).to(device))\n",
        "    pred_probab = nn.Softmax(dim=1)(pred)\n",
        "    y_pred = pred_probab.argmax(1)\n",
        "    if y_pred.item() != label:\n",
        "        print(\"pred: \",y_pred.item(),\", actual: \",label)\n",
        "        # plt.imshow(np.transpose(img, (1, 2, 0)))\n",
        "        # plt.show()\n",
        "        imshow(img)\n",
        "        rong_lst.append(img)\n",
        "        count+=1\n",
        "    i+=1\n",
        "\n",
        "\n",
        "# 20/137 wrong\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OJs2bVL0rXnG"
      },
      "outputs": [],
      "source": [
        "print(i)\n",
        "# imshow(torchvision.utils.make_grid(images,nrow=4))\n",
        "# print(torch.stack(rong_lst).shape)\n",
        "# print(len(rong_lst))\n",
        "# print(rong_lst[0].shape)\n",
        "imshow(torchvision.utils.make_grid(torch.stack(rong_lst),nrow=4))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}