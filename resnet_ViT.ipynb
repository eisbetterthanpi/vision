{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eisbetterthanpi/vision/blob/main/resnet_ViT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "U9HjjsoPSTCx"
      },
      "outputs": [],
      "source": [
        "# @title from drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive') # mount to google drive\n",
        "!unzip /content/drive/MyDrive/extra/iras/google_street_view.zip -d /content # unzip google_street_view.zip\n",
        "import shutil\n",
        "shutil.rmtree('/content/google_street_view/meta_data', ignore_errors=True) # delete the meta_data folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2kQGpnynh2oQ",
        "outputId": "24655d91-7edd-43eb-fad1-9db002c798d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "rm: cannot remove '/content/gsv/06/.ipynb_checkpoints': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "# @title download\n",
        "# # https://drive.google.com/file/d/1NkCNecLpFG3i7bo3Vl9RQSwzBpSRQ29q/view?usp=sharing\n",
        "# !gdown 1NkCNecLpFG3i7bo3Vl9RQSwzBpSRQ29q -O gsv.zip\n",
        "# !unzip /content/gsv.zip -d /content\n",
        "# import shutil\n",
        "# shutil.rmtree('/content/google_street_view/meta_data', ignore_errors=True) # delete the meta_data folder\n",
        "\n",
        "# clip cleaned\n",
        "# https://drive.google.com/file/d/1-xcHyVAMeTkY7SUdUyQRVAn_FFXPSDGB/view?usp=share_link\n",
        "!gdown 1-xcHyVAMeTkY7SUdUyQRVAn_FFXPSDGB -O gsv.zip\n",
        "!unzip /content/gsv.zip -d /\n",
        "# !ls\n",
        "# !ls -a /content/gsv\n",
        "!rm -R /content/gsv/.ipynb_checkpoints\n",
        "# !rm -R /content/gsv/06/.ipynb_checkpoints\n",
        "\n",
        "# https://bestasoff.medium.com/how-to-fine-tune-very-large-model-if-it-doesnt-fit-on-your-gpu-3561e50859af\n",
        "!pip install bitsandbytes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "hEUffQ24mkRY"
      },
      "outputs": [],
      "source": [
        "# @title augmentations\n",
        "# https://github.com/facebookresearch/vicreg/blob/main/augmentations.py\n",
        "\n",
        "from PIL import ImageOps, ImageFilter\n",
        "import numpy as np\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.transforms import InterpolationMode\n",
        "\n",
        "class GaussianBlur(object):\n",
        "    def __init__(self, p):\n",
        "        self.p = p\n",
        "    def __call__(self, img):\n",
        "        if np.random.rand() < self.p:\n",
        "            sigma = np.random.rand() * 1.9 + 0.1\n",
        "            # return img.filter(ImageFilter.GaussianBlur(sigma))\n",
        "            return transforms.GaussianBlur(kernel_size=5, sigma=sigma)(img)\n",
        "        else:\n",
        "            return img\n",
        "\n",
        "class Solarization(object):\n",
        "    def __init__(self, p):\n",
        "        self.p = p\n",
        "    def __call__(self, img):\n",
        "        if np.random.rand() < self.p:\n",
        "            return ImageOps.solarize(img)\n",
        "        else:\n",
        "            return img\n",
        "\n",
        "class Cutout(object):\n",
        "    def __init__(self, p=0.5, mask_size=140, mask_color=(0, 0, 0)):\n",
        "        self.p = p\n",
        "        self.mask_size = mask_size\n",
        "        self.mask_color = mask_color\n",
        "        self.mask_size_half = self.mask_size // 2\n",
        "        # offset = 1 if mask_size % 2 == 0 else 0\n",
        "    def __call__(self, img):\n",
        "        if np.random.rand() > self.p:\n",
        "            return img\n",
        "        else:\n",
        "            img = np.asarray(img).copy()\n",
        "            h, w = img.shape[:2] # 480x600\n",
        "            cx = np.random.randint(0, w)\n",
        "            cy = np.random.randint(0, h)\n",
        "            xmin, xmax = cx - self.mask_size_half, cx + self.mask_size_half\n",
        "            ymin, ymax = cy - self.mask_size_half, cy + self.mask_size_half\n",
        "            xmin, xmax = max(0, xmin), min(w, xmax)\n",
        "            ymin, ymax = max(0, ymin), min(h, ymax)\n",
        "            img[ymin:ymax, xmin:xmax] = self.mask_color\n",
        "            return img\n",
        "\n",
        "\n",
        "class TrainTransform(object):\n",
        "    def __init__(self):\n",
        "        self.transform = transforms.Compose([\n",
        "                transforms.RandomPerspective(distortion_scale=0.3, p=0.5), # me\n",
        "                # transforms.RandomResizedCrop(224, interpolation=InterpolationMode.BICUBIC),#224\n",
        "                # transforms.RandomResizedCrop((400,640), interpolation=InterpolationMode.BICUBIC),#224\n",
        "                # transforms.RandomResizedCrop((400,640), scale=(0.8, 1.0), ratio=(0.75, 1.3333333333333333), interpolation=InterpolationMode.BICUBIC),\n",
        "                transforms.RandomResizedCrop((400,640), scale=(0.7, 1.0), ratio=(0.8, 1.25), interpolation=InterpolationMode.BICUBIC),\n",
        "                transforms.RandomHorizontalFlip(p=0.5), # 0.5\n",
        "                transforms.RandomApply([transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1)], p=0.8,), # brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1)], p=0.8\n",
        "                transforms.RandomGrayscale(p=0.2), # 0.2\n",
        "                GaussianBlur(p=1.0), # 1.0\n",
        "                # Solarization(p=0.0), # 0.0\n",
        "                Cutout(p=1.0),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), # 0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225\n",
        "            ])\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        x1 = self.transform(sample)\n",
        "        # x2 = transforms.ToTensor()(sample)\n",
        "        return x1\n",
        "# https://pytorch.org/vision/main/auto_examples/plot_transforms.html#sphx-glr-auto-examples-plot-transforms-py\n",
        "\n",
        "\n",
        "\n",
        "# https://github.com/hysts/pytorch_cutout/blob/master/dataloader.py\n",
        "# https://arxiv.org/pdf/1708.04552.pdf\n",
        "# size of the cutout region is a more important hyperparameter than the shape\n",
        "# randomly select a pixel coordinate within the image as a center point and then place the cutout mask around that location.\n",
        "# alternative randomly apply cutout constrained within the image region, but with 50% probability so that the network sometimes receives unmodified images\n",
        "def cutout(p=0.5, mask_size=16, mask_color=(0, 0, 0)):\n",
        "    mask_size_half = mask_size // 2\n",
        "    # offset = 1 if mask_size % 2 == 0 else 0\n",
        "    def _cutout(image):\n",
        "        image = np.asarray(image).copy()\n",
        "        if np.random.random() > p:\n",
        "            return image\n",
        "        h, w = image.shape[:2]\n",
        "        cx = np.random.randint(0, w)\n",
        "        cy = np.random.randint(0, h)\n",
        "        xmin, xmax = cx - mask_size_half, cx + mask_size_half\n",
        "        ymin, ymax = cy - mask_size_half, cy + mask_size_half\n",
        "        xmin, xmax = max(0, xmin), min(w, xmax)\n",
        "        ymin, ymax = max(0, ymin), min(h, ymax)\n",
        "        image[ymin:ymax, xmin:xmax] = mask_color\n",
        "        return image\n",
        "    return _cutout\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LxACli7GdyGq",
        "outputId": "332ceb64-3de0-48fd-a34f-43e0bbb454bb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ],
      "source": [
        "# @title data\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# dataset has PILImage images of range [0, 1], transform them to Tensors of normalized range [-1, 1]\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "# transform = transforms.Compose([transforms.ToTensor(), transforms.Resize((224,224)), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "# transform = transforms.Compose([transforms.ToTensor(), transforms.Resize((224,224)), transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
        "# transform = transforms.Compose(transforms.ToTensor())\n",
        "\n",
        "# dir='/content/google_street_view'\n",
        "dir='/content/gsv'\n",
        "\n",
        "# data = datasets.ImageFolder(dir, transform=transform)\n",
        "data = datasets.ImageFolder(dir, transform=None)\n",
        "torch.manual_seed(0)\n",
        "train_data, test_data = torch.utils.data.random_split(data, [.85,.15])\n",
        "\n",
        "\n",
        "\n",
        "# # https://medium.com/analytics-vidhya/augment-your-data-easily-with-pytorch-313f5808fc8b\n",
        "# from torch.utils.data import TensorDataset, DataLoader, WeightedRandomSampler\n",
        "\n",
        "# from tqdm import tqdm\n",
        "# all_label_ids = torch.tensor ([label for label in df['labels']], dtype=torch.long) 5\n",
        "# train_data = TensorDataset (all_label_ids)\n",
        "\n",
        "# BATCH_SIZE = 100 # We should have 90 observation in class 0, 10 observation in class 1\n",
        "\n",
        "# # Class weighting\n",
        "# labels_unique, counts = np. unique (df['labels' ], return_counts=True)\n",
        "# print(\"Unique labels: {}\".format(labels_unique))\n",
        "# class_weights = [sum(counts) / c for c in counts] # [#{class_0}, #{class_1}] 14 # Assign weight to each input sample\n",
        "# example_weights = [class_weights [e] for e in df['labels']]\n",
        "# sampler WeightedRandomSampler (example_weights, len (df['labels']))\n",
        "# train_dataloader = DataLoader (train_data, sampler-sampler, batch_size=BATCH_SIZE) 18\n",
        "# # Build your batches of sentences based on your augmented dataset\n",
        "# arr_batch = []\n",
        "# for step, batch in enumerate (tqdm(train_dataloader, desc=\"Iteration\")):\n",
        "# batch = tuple(t.to('cpu') for t in batch)\n",
        "# label_ids = batch\n",
        "# arr_batch.append(label_ids[0].detach().cpu().numpy())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "class DatasetWrap(Dataset):\n",
        "    def __init__(self, dataset, transform=None):\n",
        "        super(DatasetWrap, self).__init__()\n",
        "        self.dataset = dataset\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        x, y = self.dataset[index]\n",
        "        if self.transform:\n",
        "            x = self.transform(x)\n",
        "        return x, y\n",
        "train_data = DatasetWrap(train_data, TrainTransform()) # apply data augmentation to train dataset only\n",
        "# train_data = DatasetWrap(train_data, transform)\n",
        "test_data = DatasetWrap(test_data, transform)\n",
        "\n",
        "\n",
        "batch_size = 16 # 64\n",
        "num_batches=int(np.ceil(len(train_data)/batch_size))\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
        "data, train_data, test_data = None, None, None\n",
        "\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "# dataiter = iter(train_loader)\n",
        "# images, labels = next(dataiter)\n",
        "# imshow(torchvision.utils.make_grid(images,nrow=4))\n",
        "# dataiter=None\n",
        "\n",
        "# dataiter = iter(test_loader)\n",
        "# images, labels = next(dataiter)\n",
        "# imshow(torchvision.utils.make_grid(images,nrow=4))\n",
        "# dataiter=None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KhnHtBenpNuD"
      },
      "outputs": [],
      "source": [
        "dataiter = iter(train_loader)\n",
        "images, labels = next(dataiter)\n",
        "print(images.shape)\n",
        "# imshow(torchvision.utils.make_grid(images))\n",
        "dataiter=None\n",
        "\n",
        "dataiter = iter(test_loader)\n",
        "images, labels = next(dataiter)\n",
        "print(images.shape)\n",
        "# imshow(torchvision.utils.make_grid(images))\n",
        "dataiter=None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uD-pTPWcyD6j"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline \n",
        "# images, labels = next(dataiter)\n",
        "\n",
        "# from matplotlib import pyplot as plt\n",
        "image=images[0]\n",
        "plt.imshow(np.transpose(image, (1, 2, 0)))\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7oYDr8kuA5Bl",
        "outputId": "1075d7aa-969e-49d8-bccb-13474c05b8ab"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet152_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet152_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ],
      "source": [
        "# @title resnet\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import datasets, models, transforms\n",
        "# # https://pytorch.org/vision/0.12/models.html#id10\n",
        "model = models.resnet152(pretrained=True) # 18 34 50 101 152\n",
        "\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Sequential(\n",
        "    nn.Linear(num_ftrs, 6, bias=False),\n",
        "    nn.Softmax(dim=1),\n",
        "    )\n",
        "# # model.mods = [module for k, module in model._modules.items()]\n",
        "# # modules = [module for k, module in model._modules.items()]\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# model = model.to(device)\n",
        "# model = torch.compile(model.to(device))\n",
        "# model = torch.compile(model.to(device),mode='reduce-overhead')\n",
        "model = torch.compile(model.to(device),mode='max-autotune')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "rJw9_Ort2Sek"
      },
      "outputs": [],
      "source": [
        "# @title vit\n",
        "# https://arxiv.org/pdf/2010.11929.pdf\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "# https://pytorch.org/vision/main/models/vision_transformer.html\n",
        "# https://pytorch.org/vision/main/models/generated/torchvision.models.vit_b_16.html#torchvision.models.vit_b_16\n",
        "# model = models.vit_l_16(weights='DEFAULT') # small vit_b_16 vit_b_32 vit_l_16 vit_l_32 vit_h_14 big\n",
        "# # VisionTransformer(image_size, patch_size, num_layers, num_heads, hidden_dim, mlp_dim)\n",
        "# num_ftrs = model.heads.head.in_features\n",
        "# # num_ftrs = model.heads[-1].in_features\n",
        "# model.heads = nn.Sequential(\n",
        "#     # nn.Dropout(0.2),\n",
        "#     nn.Linear(num_ftrs, 6, bias=False),\n",
        "#     nn.Softmax(dim=1),\n",
        "#     )\n",
        "\n",
        "\n",
        "!pip install timm\n",
        "# https://github.com/huggingface/pytorch-image-models/issues/908\n",
        "import timm\n",
        "# model = timm.create_model('vit_base_patch16_224', pretrained=True)\n",
        "model = timm.create_model('vit_base_patch16_224', img_size=(400, 640), pretrained=True)\n",
        "# [print(x) for x in timm.list_models('vit*',pretrained=True)]\n",
        "# https://huggingface.co/google/vit-base-patch16-224\n",
        "# https://github.com/huggingface/pytorch-image-models/blob/main/timm/models/vision_transformer.py\n",
        "# vit_base_patch16_224 compile,no ckpt # patch_size=16, embed_dim=768, depth=12, num_heads=12\n",
        "# vit_base_patch16_384\n",
        "# vit_large_patch16_224 explodesgpu # patch_size=16, embed_dim=1024, depth=24, num_heads=16\n",
        "# vit_large_patch16_384 # patch_size=16, embed_dim=1024, depth=24, num_heads=16\n",
        "\n",
        "# or fine tune huge\n",
        "# vit_large_patch14_224 # patch_size=16, embed_dim=1024, depth=24, num_heads=16\n",
        "# vit_large_patch16_384\n",
        "\n",
        "\n",
        "num_ftrs = model.head.in_features\n",
        "model.head = nn.Sequential(\n",
        "    # nn.Dropout(0.2),\n",
        "    nn.Linear(num_ftrs, 6, bias=False),\n",
        "    nn.Softmax(dim=1),\n",
        "    )\n",
        "# model.set_grad_checkpointing()\n",
        "\n",
        "# print(model.patch_embed.grid_size) # (25, 40)\n",
        "# print(model.pos_embed.shape) # [1, 1001, 768]\n",
        "# https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py\n",
        "\n",
        "# print(model)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# model = model.to(device)\n",
        "# model = torch.compile(model.to(device))\n",
        "# model = torch.compile(model.to(device),mode='reduce-overhead')\n",
        "model = torch.compile(model.to(device),mode='max-autotune')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "-oKYpG8n2fBI"
      },
      "outputs": [],
      "source": [
        "# @title inception\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "# # https://pytorch.org/vision/0.12/models.html#id10\n",
        "model = models.inception_v3(pretrained=True)\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Sequential(\n",
        "    nn.Linear(num_ftrs, 6, bias=False),\n",
        "    nn.Softmax(dim=1),\n",
        "    )\n",
        "# # model.mods = [module for k, module in model._modules.items()]\n",
        "# # modules = [module for k, module in model._modules.items()]\n",
        "\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# model = model.to(device)\n",
        "# model = torch.compile(model.to(device))\n",
        "# model = torch.compile(model.to(device),mode='reduce-overhead')\n",
        "model = torch.compile(model.to(device),mode='max-autotune')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "vEZCFg5YSS9J"
      },
      "outputs": [],
      "source": [
        "# @title try\n",
        "\n",
        "# # check model's input and output dimensions are correct\n",
        "# X = torch.rand(64, 3, 32, 32, device=device)\n",
        "X = torch.rand(16, 3, 400, 640, device=device)\n",
        "# X = torch.rand(16, 3, 224, 224, device=device)\n",
        "model.eval()\n",
        "\n",
        "# 224x224\n",
        "# 16x16 / 32x32 patch\n",
        "# -> 14x14=196 7x7=49 seq length\n",
        "# 400x640 -> 25x40=1000 seq length\n",
        "\n",
        "\n",
        "logits = model(X)\n",
        "\n",
        "# modules = [module for k, module in model._modules.items()]\n",
        "# for i,x in enumerate(modules):\n",
        "#     print(i,x)\n",
        "\n",
        "# logits = checkpoint_sequential(functions=modules, segments=1, input=X)\n",
        "\n",
        "print(logits.shape)\n",
        "# print(logits[0])\n",
        "# print(logits[0].argmax(1))\n",
        "\n",
        "pred_probab = nn.Softmax(dim=1)(logits)\n",
        "y_pred = pred_probab.argmax(1)\n",
        "print(y_pred)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "fsealXK3OPQa"
      },
      "outputs": [],
      "source": [
        "# @title train test function\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "# https://github.com/prigoyal/pytorch_memonger/blob/master/models/optimized/resnet_new.py\n",
        "from torch.utils.checkpoint import checkpoint, checkpoint_sequential\n",
        "\n",
        "# train function with automatic mixed precision\n",
        "def strain(dataloader, model, loss_fn, optimizer, scheduler=None, verbose=True):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    loss_list = []\n",
        "    for batch, (x, y) in enumerate(dataloader):\n",
        "        sx, sy = x.to(device), y.to(device)\n",
        "        with torch.cuda.amp.autocast(): # automatic mixed percision\n",
        "            # pred = model(sx) # default\n",
        "\n",
        "            # pred,_ = model(sx) # inception\n",
        "\n",
        "            modules = [module for k, module in model._modules.items()]\n",
        "            pred = checkpoint_sequential(functions=modules, segments=1, input=sx) # gradient checkpointing for resnet and inception only\n",
        "            # # pred = checkpoint_sequential(functions=model.mods, segments=1, input=sx)\n",
        "\n",
        "            loss = loss_fn(pred, sy) # /4 to scale by gradient accumulation_steps\n",
        "        \n",
        "        scaler.scale(loss).backward()\n",
        "        if ((batch + 1) % 4 == 0) or (batch + 1 == len(dataloader)): # gradient accumulation\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "\n",
        "        loss_list.append(loss.item())\n",
        "        # loss.backward()\n",
        "        # optimizer.step()\n",
        "        # print(\"strai\",batch, size, len(x)) # 0 6 1\n",
        "        if (batch) % (size//(10* len(x))) == 0:\n",
        "            loss, current = loss.item(), batch * len(x)\n",
        "            if verbose: print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "    return loss_list\n",
        "\n",
        "def test(dataloader, model, loss_fn, verbose=True):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            x, y = X.to(device), y.to(device)\n",
        "            pred = model(x)\n",
        "            loss = loss_fn(pred, y)\n",
        "            # predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    # if verbose: print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "    if verbose: print(f\"Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f}\")\n",
        "    return correct, test_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2wXgUMmBAqqF",
        "outputId": "4d29c2af-f9b4-46dd-d857-689f39b202b3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ],
      "source": [
        "# @title lrfinder data\n",
        "import numpy as np\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "# transform = transforms.Compose([transforms.ToTensor(), transforms.Resize((224,224)), transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
        "\n",
        "dir='/content/gsv'\n",
        "\n",
        "# data = datasets.ImageFolder(dir, transform=transforms.ToTensor())\n",
        "data = datasets.ImageFolder(dir, transform=transform)\n",
        "torch.manual_seed(0)\n",
        "train_data, test_data = torch.utils.data.random_split(data, [.85,.15])\n",
        "# print(len(train_data))\n",
        "# print(len(test_data))\n",
        "\n",
        "a=0.10 # 0.2 # proportion of train data to train on\n",
        "train_sets,_ = torch.utils.data.random_split(train_data, [a, 1-a])\n",
        "test_sets,_ = torch.utils.data.random_split(test_data, [a, 1-a])\n",
        "\n",
        "batch_size=16#64\n",
        "num_batchess=int(np.ceil(len(train_data)/batch_size))\n",
        "train_loaders = torch.utils.data.DataLoader(train_sets, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
        "test_loaders = torch.utils.data.DataLoader(test_sets, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
        "data, train_data, test_data = None, None, None\n",
        "\n",
        "pth='/content/lr.pth'\n",
        "torch.save(model.state_dict(), pth) # save temporary model for lr finding\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "-AZwzcM_48Rr"
      },
      "outputs": [],
      "source": [
        "# @title lrfinder\n",
        "\n",
        "# https://towardsdatascience.com/super-convergence-with-just-pytorch-c223c0fc1e51\n",
        "# https://github.com/davidtvs/pytorch-lr-finder\n",
        "\n",
        "# from fastai.vision.all import *\n",
        "# # https://docs.fast.ai/learner.html#learner\n",
        "# # learn = cnn_learner(train_loaders, model, loss_fn)\n",
        "# learn = vision_learner(train_loaders, model, loss_fn)\n",
        "# # vision_learner\n",
        "# # https://docs.fast.ai/tutorial.vision.html\n",
        "# learn.lr_find()\n",
        "# # https://pypi.org/project/torch-lr-finder/\n",
        "\n",
        "\n",
        "# # https://arxiv.org/pdf/1506.01186.pdf\n",
        "# LR range test\n",
        "# Note the learning rate value when the accuracy starts to\n",
        "# increase and when the accuracy slows, becomes ragged, or starts to fall\n",
        "\n",
        "# https://github.com/davidtvs/pytorch-lr-finder/blob/master/torch_lr_finder/lr_finder.py\n",
        "\n",
        "num_iter=10 #20 # num of lr data points\n",
        "start_lr= 1e-6 # 1e-6\n",
        "end_lr= 1e-3 # 1e-2\n",
        "# 152: 1e-7 - 1e-4\n",
        "\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "# optimizer = torch.optim.AdamW(model.parameters(), lr=start_lr, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01)\n",
        "# optimizer = torch.optim.SGD(model.parameters(), lr=start_lr, momentum=0.9)\n",
        "\n",
        "import bitsandbytes as bnb\n",
        "optimizer = bnb.optim.AdamW(model.parameters(), lr=start_lr, betas=(0.9, 0.999), optim_bits=8)\n",
        "\n",
        "\n",
        "# def lrfinder(model, optimizer, loss_fn, train_loaders, test_loaders, start_lr=1e-7, end_lr=1e-2, num_iter=10, train_batches=100, test_batches=20):\n",
        "acc_list=[]\n",
        "lr_list=[]\n",
        "test_list=[]\n",
        "for i in range(num_iter):\n",
        "    model.load_state_dict(torch.load(\"lr.pth\"))\n",
        "    lr=scheduler.get_last_lr()[0]\n",
        "    # lr=scheduler.get_lr()[0]\n",
        "    # lr=optimizer.param_groups[0][\"lr\"]\n",
        "    print(i,\": lr = \",lr)\n",
        "    lr_list.append(lr)\n",
        "\n",
        "    loss_list = strain(train_loaders, model, loss_fn, optimizer)\n",
        "    # train(train_loaders, model1, loss_fn, optimizer)\n",
        "    \n",
        "    scheduler.step()\n",
        "    accuracy, test_loss = test(test_loaders, model, loss_fn)\n",
        "    acc_list.append(accuracy)\n",
        "    test_list.append(test_loss)\n",
        "    # if torch.isnan(torch.tensor(test_loss)): return lr_list,acc_list,test_list\n",
        "    if torch.isnan(torch.tensor(loss_list[-1])): return lr_list,acc_list,test_list\n",
        "    # return lr_list,acc_list,test_list\n",
        "\n",
        "\n",
        "# (end_lr/start_lr)=gamma**(num_iter)\n",
        "gamma = np.exp(np.log(end_lr/start_lr)/num_iter)\n",
        "# print(\"gamma\",gamma)\n",
        "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=gamma)\n",
        "\n",
        "\n",
        "# lrfinder(model, optimizer, loss_fn, train_loaders, test_loaders, start_lr=0.0001, end_lr=0.1, num_iter=20, train_batches=100, test_batches=20):\n",
        "# lr_list,acc_list = lrfinder(model, optimizer, loss_fn)#, train_loaders, end_lr, num_iter)\n",
        "# lr_list,acc_list,test_list = lrfinder(model, optimizer, loss_fn, train_loaders, test_loaders)# end_lr, num_iter)\n",
        "lr_list,acc_list,test_list = lrfinder(model, optimizer, loss_fn, train_loaders, test_loaders, start_lr, end_lr, num_iter)\n",
        "import matplotlib.pyplot as plt\n",
        "# plt.plot(lr_list, acc_list)\n",
        "plt.plot(lr_list, test_list)\n",
        "plt.xscale('log')\n",
        "plt.show()\n",
        "plt.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install bitsandbytes\n",
        "import torch\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "gD70Qv-wJE38"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title schedulerme\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "min_lr, max_lr = 1e-5, 1e-3 #0.5#\n",
        "# optimizer = torch.optim.SGD(model.parameters(), lr = 1e-1, momentum=0.9)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=min_lr, betas=(0.9, 0.999), eps=1e-08, weight_decay=3e-6)\n",
        "\n",
        "# print(optimizer.param_groups[0][\"betas\"])\n",
        "\n",
        "total_steps=97\n",
        "\n",
        "\n",
        "class schedulerme():\n",
        "    def __init__(self, optimizer, total_steps, min_lr, max_lr):\n",
        "        self.optimizer = optimizer\n",
        "        self.total_steps = total_steps\n",
        "        self.min_lr = min_lr\n",
        "        self.max_lr = max_lr\n",
        "        self.lr_schedule = np.linspace(min_lr, max_lr, total_steps)\n",
        "        # self.mm_schedule = np.linspace(max_mm, min_mm, total_steps)\n",
        "        self.step_count = 0\n",
        "        # if 'momentum' in self.optimizer.defaults: self.mb='momentum'\n",
        "        # elif 'betas' in self.optimizer.defaults: self.mb='betas'\n",
        "\n",
        "    def step(self, n=None):\n",
        "        self.optimizer.param_groups[0][\"lr\"]=self.lr_schedule[self.step_count]\n",
        "        # if self.mb=='momentum': self.optimizer.param_groups[0][\"momentum\"]=self.mm_schedule[self.step_count]\n",
        "        # elif self.mb=='betas': #https://github.com/pytorch/pytorch/blob/master/torch/optim/lr_scheduler.py\n",
        "        #     b1, b2 = self.optimizer.param_groups[0][\"betas\"] #https://pytorch.org/docs/stable/_modules/torch/optim/lr_scheduler.html#OneCycleLR\n",
        "        #     self.optimizer.param_groups[0][\"betas\"] = (self.mm_schedule[self.step_count],b2)\n",
        "        self.step_count+=1\n",
        "\n",
        "\n",
        "scheduler=schedulerme(optimizer,total_steps, min_lr, max_lr)\n",
        "\n",
        "lr_list=[]\n",
        "mm_list=[]\n",
        "mm1_list,mm2_list=[],[]\n",
        "for x in range(total_steps):\n",
        "# for x in range(num_iter):\n",
        "    scheduler.step()\n",
        "    lr=optimizer.param_groups[0][\"lr\"]\n",
        "    lr_list.append(lr)\n",
        "    # mm=optimizer.param_groups[0][\"momentum\"]\n",
        "    # mm=optimizer.param_groups[0][\"betas\"][0]\n",
        "    # mm_list.append(mm)\n",
        "\n",
        "    # mm1,mm2=optimizer.param_groups[0][\"betas\"]\n",
        "    # mm1_list.append(mm1)\n",
        "    # mm2_list.append(mm2)\n",
        "\n",
        "# print(mm_list)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(lr_list)\n",
        "# plt.yscale('log')\n",
        "plt.show()\n",
        "# plt.plot(mm1_list)\n",
        "# plt.yscale('log')\n",
        "# plt.show()\n",
        "# plt.plot(mm2_list)\n",
        "# plt.yscale('log')\n",
        "# plt.show()\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "iPyNb1mtKMyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title LR range test\n",
        "# 1cycle super convergencehttps://arxiv.org/pdf/1708.07120.pdf\n",
        "# # cyclic lr https://arxiv.org/pdf/1506.01186.pdf\n",
        "# Note the learning rate value when the accuracy starts to\n",
        "# increase and when the accuracy slows, becomes ragged, or starts to fall\n",
        "\n",
        "# one training run of the network for a few epochs\n",
        "\n",
        "num_iter=40 #20 # num of lr data points\n",
        "min_lr= 1e-6 # 1e-6\n",
        "max_lr= 1e-3 # 1e-2\n",
        "# 152: 1e-7 - 1e-4\n",
        "\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "# optimizer = torch.optim.AdamW(model.parameters(), lr=start_lr, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01)\n",
        "# optimizer = torch.optim.SGD(model.parameters(), lr=start_lr, momentum=0.9)\n",
        "import bitsandbytes as bnb\n",
        "model=nn.Linear(1,1)\n",
        "optimizer = bnb.optim.AdamW(model.parameters(), lr=start_lr, betas=(0.9, 0.999), optim_bits=8)\n",
        "\n",
        "total_steps=num_batchess*num_iter\n",
        "# https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
        "# https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.ConstantLR.html\n",
        "scheduler=schedulerme(optimizer,total_steps, min_lr, max_lr)\n",
        "\n",
        "\n",
        "# def lrfinder(model, optimizer, loss_fn, train_loaders, test_loaders, start_lr=1e-7, end_lr=1e-2, num_iter=10, train_batches=100, test_batches=20):\n",
        "acc_list=[]\n",
        "lr_list=[]\n",
        "test_list=[]\n",
        "for i in range(num_iter):\n",
        "    # model.load_state_dict(torch.load(\"lr.pth\"))\n",
        "    lr=scheduler.get_last_lr()[0]\n",
        "    # lr=scheduler.get_lr()[0]\n",
        "    # lr=optimizer.param_groups[0][\"lr\"]\n",
        "    for j in range(5):\n",
        "        scheduler.step()\n",
        "    print(i,\": lr = \",lr)\n",
        "    lr_list.append(lr)\n",
        "\n",
        "    # loss_list = strain(train_loaders, model, loss_fn, optimizer)\n",
        "    # # train(train_loaders, model1, loss_fn, optimizer)\n",
        "    \n",
        "    # scheduler.step()\n",
        "    # accuracy, test_loss = test(test_loaders, model, loss_fn)\n",
        "    # acc_list.append(accuracy)\n",
        "    # test_list.append(test_loss)\n",
        "    # # if torch.isnan(torch.tensor(test_loss)): return lr_list,acc_list,test_list\n",
        "    # if torch.isnan(torch.tensor(loss_list[-1])): return lr_list,acc_list,test_list\n",
        "    # # return lr_list,acc_list,test_list\n",
        "\n",
        "\n",
        "\n",
        "# lrfinder(model, optimizer, loss_fn, train_loaders, test_loaders, start_lr=0.0001, end_lr=0.1, num_iter=20, train_batches=100, test_batches=20):\n",
        "# lr_list,acc_list = lrfinder(model, optimizer, loss_fn)#, train_loaders, end_lr, num_iter)\n",
        "# lr_list,acc_list,test_list = lrfinder(model, optimizer, loss_fn, train_loaders, test_loaders)# end_lr, num_iter)\n",
        "# lr_list,acc_list,test_list = lrfinder(model, optimizer, loss_fn, train_loaders, test_loaders, start_lr, end_lr, num_iter)\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(lr_list, acc_list)\n",
        "# plt.plot(lr_list, test_list)\n",
        "plt.show()\n",
        "plt.close()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "id": "iuumbm2SB_lX",
        "outputId": "aa08db19-cbf6-47a7-a6db-6a83c9021cb9"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-5ca84393e0ff>\u001b[0m in \u001b[0;36m<cell line: 22>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbnb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdamW\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbetas\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.999\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptim_bits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mtotal_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_batchess\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnum_iter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;31m# https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.ConstantLR.html\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'num_batchess' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(lr_list, acc_list)\n",
        "# plt.plot(lr_list, test_list)\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "metadata": {
        "id": "JNPnbEllGse4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDBEk-l-Oxjn",
        "outputId": "47391b03-2675-41bb-8467-473425cb364e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "1.0000000000000006e-06\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss: 1.706699  [    0/ 7010]\n",
            "loss: 1.736223  [  688/ 7010]\n",
            "loss: 1.711139  [ 1376/ 7010]\n",
            "loss: 1.695090  [ 2064/ 7010]\n",
            "loss: 1.719945  [ 2752/ 7010]\n",
            "loss: 1.759832  [ 3440/ 7010]\n",
            "loss: 1.674960  [ 4128/ 7010]\n",
            "loss: 1.642776  [ 4816/ 7010]\n",
            "loss: 1.674703  [ 5504/ 7010]\n",
            "loss: 1.667141  [ 6192/ 7010]\n",
            "loss: 1.661900  [ 6880/ 7010]\n",
            "Accuracy: 36.7%, Avg loss: 1.665805\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "1.8748953099107182e-06\n",
            "loss: 1.616740  [    0/ 7010]\n",
            "loss: 1.586149  [  688/ 7010]\n",
            "loss: 1.755171  [ 1376/ 7010]\n",
            "loss: 1.725706  [ 2064/ 7010]\n",
            "loss: 1.584388  [ 2752/ 7010]\n",
            "loss: 1.554109  [ 3440/ 7010]\n",
            "loss: 1.765398  [ 4128/ 7010]\n",
            "loss: 1.614059  [ 4816/ 7010]\n",
            "loss: 1.526439  [ 5504/ 7010]\n",
            "loss: 1.569088  [ 6192/ 7010]\n",
            "loss: 1.522087  [ 6880/ 7010]\n",
            "Accuracy: 42.4%, Avg loss: 1.607692\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "4.3940030598768375e-06\n",
            "loss: 1.649625  [    0/ 7010]\n",
            "loss: 1.683177  [  688/ 7010]\n",
            "loss: 1.667177  [ 1376/ 7010]\n",
            "loss: 1.614658  [ 2064/ 7010]\n",
            "loss: 1.530823  [ 2752/ 7010]\n",
            "loss: 1.580185  [ 3440/ 7010]\n",
            "loss: 1.478112  [ 4128/ 7010]\n",
            "loss: 1.651748  [ 4816/ 7010]\n",
            "loss: 1.516813  [ 5504/ 7010]\n",
            "loss: 1.509030  [ 6192/ 7010]\n",
            "loss: 1.727274  [ 6880/ 7010]\n",
            "Accuracy: 38.5%, Avg loss: 1.637599\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "8.253329380159727e-06\n",
            "loss: 1.450007  [    0/ 7010]\n",
            "loss: 1.667924  [  688/ 7010]\n",
            "loss: 1.607634  [ 1376/ 7010]\n",
            "loss: 1.652923  [ 2064/ 7010]\n",
            "loss: 1.683354  [ 2752/ 7010]\n",
            "loss: 1.413603  [ 3440/ 7010]\n",
            "loss: 1.764606  [ 4128/ 7010]\n",
            "loss: 1.549587  [ 4816/ 7010]\n",
            "loss: 1.688178  [ 5504/ 7010]\n",
            "loss: 1.461853  [ 6192/ 7010]\n",
            "loss: 1.650944  [ 6880/ 7010]\n",
            "Accuracy: 40.8%, Avg loss: 1.616932\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "1.2987149236686207e-05\n",
            "loss: 1.439973  [    0/ 7010]\n",
            "loss: 1.604637  [  688/ 7010]\n",
            "loss: 1.471885  [ 1376/ 7010]\n",
            "loss: 1.747067  [ 2064/ 7010]\n",
            "loss: 1.540770  [ 2752/ 7010]\n",
            "loss: 1.498304  [ 3440/ 7010]\n",
            "loss: 1.406035  [ 4128/ 7010]\n",
            "loss: 1.463576  [ 4816/ 7010]\n",
            "loss: 1.424063  [ 5504/ 7010]\n",
            "loss: 1.637410  [ 6192/ 7010]\n",
            "loss: 1.530801  [ 6880/ 7010]\n",
            "Accuracy: 33.3%, Avg loss: 1.689378\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "1.8024207899740794e-05\n",
            "loss: 1.454050  [    0/ 7010]\n",
            "loss: 1.749167  [  688/ 7010]\n",
            "loss: 1.556434  [ 1376/ 7010]\n",
            "loss: 1.497733  [ 2064/ 7010]\n",
            "loss: 1.554673  [ 2752/ 7010]\n",
            "loss: 1.538104  [ 3440/ 7010]\n",
            "loss: 1.580036  [ 4128/ 7010]\n",
            "loss: 1.721261  [ 4816/ 7010]\n",
            "loss: 1.433066  [ 5504/ 7010]\n",
            "loss: 1.584156  [ 6192/ 7010]\n",
            "loss: 1.621935  [ 6880/ 7010]\n",
            "Accuracy: 34.3%, Avg loss: 1.673010\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "2.2756657231385346e-05\n",
            "loss: 1.739909  [    0/ 7010]\n",
            "loss: 1.679392  [  688/ 7010]\n",
            "loss: 1.690814  [ 1376/ 7010]\n",
            "loss: 1.479228  [ 2064/ 7010]\n",
            "loss: 1.703189  [ 2752/ 7010]\n",
            "loss: 1.545673  [ 3440/ 7010]\n",
            "loss: 1.651534  [ 4128/ 7010]\n",
            "loss: 1.679362  [ 4816/ 7010]\n",
            "loss: 1.733294  [ 5504/ 7010]\n",
            "loss: 1.635791  [ 6192/ 7010]\n",
            "loss: 1.736187  [ 6880/ 7010]\n",
            "Accuracy: 36.7%, Avg loss: 1.648908\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "2.6613407890289376e-05\n",
            "loss: 1.538138  [    0/ 7010]\n",
            "loss: 1.559064  [  688/ 7010]\n",
            "loss: 1.772042  [ 1376/ 7010]\n",
            "loss: 1.820359  [ 2064/ 7010]\n",
            "loss: 1.673925  [ 2752/ 7010]\n",
            "loss: 1.528253  [ 3440/ 7010]\n",
            "loss: 1.710416  [ 4128/ 7010]\n",
            "loss: 1.554635  [ 4816/ 7010]\n",
            "loss: 1.858817  [ 5504/ 7010]\n",
            "loss: 1.602046  [ 6192/ 7010]\n",
            "loss: 1.529510  [ 6880/ 7010]\n",
            "Accuracy: 34.1%, Avg loss: 1.680568\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "2.9129045660870797e-05\n",
            "loss: 1.548409  [    0/ 7010]\n"
          ]
        }
      ],
      "source": [
        "# @title wwwwwwwww\n",
        "import time\n",
        "start = time.time()\n",
        "acc_lst, train_lst, test_lst=[],[],[]\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "base_lr, max_lr = 1e-6, 3e-5 # 1e-5, 1e-3 #0.5#\n",
        "# base_lr, max_lr = 3e-6, 3e-4\n",
        "end_lr, start_lr = 1e-5, 1e-3 # 0.0001,0.1\n",
        "\n",
        "# print(num_batches)\n",
        "epochs = 20 #5 20\n",
        "# (1e-5/1e-1)=gamma**(num_batches*epochs)\n",
        "# gamma = np.exp(np.log(1e-3/1e-1)/epochs) # for scheduler step every epoch\n",
        "gamma = np.exp(np.log(end_lr/start_lr)/(num_batches*epochs)) # for scheduler step every optimizer step\n",
        "# print(gamma)\n",
        "# optimizer = torch.optim.SGD(model.parameters(), lr = start_lr, momentum=0.9)\n",
        "# optimizer = torch.optim.AdamW(model.parameters(), lr=base_lr, betas=(0.9, 0.999), eps=1e-08, weight_decay=3e-6)\n",
        "import bitsandbytes as bnb # 8bit optimizer\n",
        "# optimizer = bnb.optim.AdamW(model.parameters(), lr=base_lr, betas=(0.9, 0.999), optim_bits=8)\n",
        "optimizer = bnb.optim.AdamW(model.parameters(), lr=1e-5, betas=(0.9, 0.999), optim_bits=8)\n",
        "# 152 1e-5\n",
        "# cnn 3e-4\n",
        "\n",
        "div_factor = max_lr/base_lr\n",
        "# scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=max_lr, total_steps=num_iter, pct_start=0.45, anneal_strategy='cos', cycle_momentum=True, base_momentum=0.85, max_momentum=0.95, div_factor=div_factor, final_div_factor=10000.0, three_phase=True,)\n",
        "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=max_lr, epochs=epochs, steps_per_epoch=num_batches, pct_start=0.45, anneal_strategy='cos', cycle_momentum=True, base_momentum=0.85, max_momentum=0.95, div_factor=div_factor, final_div_factor=10000.0, three_phase=True,)\n",
        "# optimizer = torch.optim.AdamW(model.parameters(), lr=start_lr, betas=(0.9, 0.999), eps=1e-08, weight_decay=3e-6)\n",
        "# scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=gamma) # 0.75(20)-0.9(100)\n",
        "total_steps=num_batches*epochs\n",
        "\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    lr=optimizer.param_groups[0][\"lr\"]\n",
        "    print(lr)\n",
        "    train_ls = strain(train_loader, model, loss_fn, optimizer, scheduler)\n",
        "    # train_ls = strain(train_loader, model, loss_fn, optimizer)\n",
        "    correct, test_loss = test(test_loader, model, loss_fn)\n",
        "    # scheduler.step()\n",
        "    train_lst.extend(train_ls)\n",
        "    test_lst.append(test_loss)\n",
        "    acc_lst.append(correct)\n",
        "print(\"Done!\")\n",
        "\n",
        "end = time.time()\n",
        "print(\"time: \",end - start)\n",
        "\n",
        "print(len(train_lst), len(test_lst))\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(train_lst)\n",
        "plt.plot(np.linspace(0,len(train_lst),len(test_lst)), test_lst)\n",
        "plt.show()\n",
        "plt.plot(acc_lst)\n",
        "plt.show()\n",
        "plt.close()\n",
        "\n",
        "\n",
        "torch.save(model.state_dict(), \"model.pth\")\n",
        "\n",
        "# model = NeuralNetwork().to(device)\n",
        "# model.load_state_dict(torch.load(\"model.pth\"))\n",
        "\n",
        "# resnet 18, 60/61 38.4%, 528s\n",
        "# resnet 18, 58/61 39.8%, 523s\n",
        "# resnet 18 compile , 58/61 40.4%, 555s\n",
        "# resnet 18 compile augment , 58/61 36.4%, 1941 # augment on cpu, takes longer\n",
        "# resnet 18 augment lr3e-4:3e-3, 58/61 37.7%, 1863s\n",
        "# resnet 18 augment 10epoch lr1e-5:3e-4, 58/61 33.5%, 3387s\n",
        "# resnet 18 compile lr1e-5:3e-4, 58/61 35.0%, 493s\n",
        "# resnet 18 compile scratch lr1e-5:1e-3, 58/61 26.8%, 475s\n",
        "# resnet 18 compile lr1e-5:1e-3, 55/61 47.3%, 480s\n",
        "# resnet 18 compile lr1e-5:1e-3, 52/61 51.7% 503s\n",
        "# resnet 18 compile lr1e-5:1e-3, unfreeze 51.0%, 550s\n",
        "# resnet 18 compile lr1e-5:1e-4, unfreeze 52.7%, 518s\n",
        "# resnet 34 compile lr1e-5:1e-4, unfreeze bitsadamW batch16*4\n",
        "# resnet 152 compileoverhead lr3e-7:3e-6, bitsadamW batch16*4 ckpt 53.8%, 2066\n",
        "# resnet 152 from53.8% augment+cutout lr3e-7:3e-6, 53.8%, 2088\n",
        "# resnet 152 comile augment+cutout lr1e-5 /4 1epoch 48.3%, 454s\n",
        "# resnet 152 comile augment+cutout lr1e-5 1epoch 48.4%, 446s\n",
        "# resnet 152 comilemaxautotue augment+cutout lr1e-5 1epoch 47.7%, 448\n",
        "# resnet 152 clipclean comilemax augment+cutout lr1e-5 10epoch 45.1%, 1585 *2\n",
        "# resnet 152 clipclean comilemax augmax lr1e-6:3e-5 20epoch \n",
        "\n",
        "\n",
        "# vit b16 lr1e-5 5epochs 41.3%, 466s # 4.4ram, 5.5vram\n",
        "# vit l16 lr1e-5 5epochs # 32.0%, 1242s 4.5ram, 8.0vram\n",
        "# vit l16 lr3e-7;1e-5 5epochs # 45.4%, 1315s 4.5ram, 8.0vram\n",
        "# vit l32 lr1e-5 5epochs # .ram, .vram\n",
        "# vit_large_patch16_384\n",
        "# vit_base_patch16_224 maxcompile nockpt lr3e-7;1e-5 5epochs # 45.2%, 2272s 5.3ram, 11.0vram\n",
        "\n",
        "# inception\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "REiP7-nvhc4s",
        "outputId": "d98122db-2954-45db-d2e5-5598941976de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# @title save\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# pth='/content/drive/MyDrive/frame/vit.pth'\n",
        "# pth='/content/drive/MyDrive/frame/res1e-5.pth'\n",
        "# pth='/content/drive/MyDrive/frame/res2e-5.pth'\n",
        "\n",
        "# torch.save(model.state_dict(), pth)\n",
        "# model.load_state_dict(torch.load(pth))\n",
        "# model.load_state_dict(torch.load(\"model.pth\"))\n",
        "\n",
        "# # matt152 # https://drive.google.com/file/d/1MQ0xLfHbio458uEVbn2VyMpD3bij2A4J/view?usp=sharing\n",
        "# !gdown 1MQ0xLfHbio458uEVbn2VyMpD3bij2A4J -O res152.pth\n",
        "# model.load_state_dict(torch.load(\"res152.pth\"))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A57FKq_YGadX"
      },
      "outputs": [],
      "source": [
        "# dir='/content/google_street_view'\n",
        "\n",
        "# # data = datasets.ImageFolder(dir, transform=transform)\n",
        "# data = datasets.ImageFolder(dir, transform=None)\n",
        "# torch.manual_seed(0)\n",
        "# train_data, test_data = torch.utils.data.random_split(data, [.85,.15])\n",
        "\n",
        "# # train_data = DatasetWrap(train_data, TrainTransform()) # apply data augmentation to train dataset only\n",
        "# train_data = DatasetWrap(train_data, transform)\n",
        "# test_data = DatasetWrap(test_data, transform)\n",
        "\n",
        "\n",
        "# batch_size = 16 # 64\n",
        "# num_batches=int(np.ceil(len(train_data)/batch_size))\n",
        "\n",
        "# train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
        "# test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
        "# # data, train_data, test_data = None, None, None\n",
        "\n",
        "\n",
        "# dataiter = iter(train_loader)\n",
        "# images, labels = next(dataiter)\n",
        "# imshow(torchvision.utils.make_grid(images))\n",
        "# dataiter=None\n",
        "\n",
        "\n",
        "%matplotlib inline \n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# dataiter = iter(train_data)\n",
        "\n",
        "\n",
        "# img,label = next(iter(sample_ds))\n",
        "i=5\n",
        "# print(len(test_data))\n",
        "# img,label=test_data[i]\n",
        "img,label=sample_ds[i]\n",
        "print(img.shape)\n",
        "# print(type(img))\n",
        "model.eval()\n",
        "pred=model(img.unsqueeze(0).to(device))\n",
        "pred_probab = nn.Softmax(dim=1)(pred)\n",
        "y_pred = pred_probab.argmax(1)\n",
        "print(pred_probab)\n",
        "print(\"pred: \",y_pred.item())\n",
        "# print(img)\n",
        "# image=images[0]\n",
        "\n",
        "print(\"actual: \",label)\n",
        "plt.imshow(np.transpose(img, (1, 2, 0)))\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q_3Cttn1qHcc"
      },
      "outputs": [],
      "source": [
        "\n",
        "model.eval()\n",
        "count=0\n",
        "i=138\n",
        "rong_lst=[]\n",
        "\n",
        "while count<20:\n",
        "    img,label=test_data[i]\n",
        "    pred=model(img.unsqueeze(0).to(device))\n",
        "    pred_probab = nn.Softmax(dim=1)(pred)\n",
        "    y_pred = pred_probab.argmax(1)\n",
        "    if y_pred.item() != label:\n",
        "        print(\"pred: \",y_pred.item(),\", actual: \",label)\n",
        "        # plt.imshow(np.transpose(img, (1, 2, 0)))\n",
        "        # plt.show()\n",
        "        imshow(img)\n",
        "        rong_lst.append(img)\n",
        "        count+=1\n",
        "    i+=1\n",
        "\n",
        "\n",
        "# 20/137 wrong\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OJs2bVL0rXnG"
      },
      "outputs": [],
      "source": [
        "print(i)\n",
        "# imshow(torchvision.utils.make_grid(images,nrow=4))\n",
        "# print(torch.stack(rong_lst).shape)\n",
        "# print(len(rong_lst))\n",
        "# print(rong_lst[0].shape)\n",
        "imshow(torchvision.utils.make_grid(torch.stack(rong_lst),nrow=4))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "nLvM4QvEkCw6"
      },
      "outputs": [],
      "source": [
        "# @title trash\n",
        "# a = []\n",
        "# while(1):\n",
        "#     a.append(1)\n",
        "\n",
        "# gc.collect() # Python thing\n",
        "\n",
        "# torch.cuda.empty_cache() # PyTorch thing\n",
        "\n",
        "# device = cuda.get_current_device() \n",
        "# device.reset()\n",
        "\n",
        "# clear gpu memory\n",
        "from numba import cuda\n",
        "cuda.select_device(0)\n",
        "cuda.close()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}