{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eisbetterthanpi/vision/blob/main/resnet_ViT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## resnet_ViT"
      ],
      "metadata": {
        "id": "Cac8nXhrnCsy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "2kQGpnynh2oQ"
      },
      "outputs": [],
      "source": [
        "# @title download\n",
        "# original 10k\n",
        "# # https://drive.google.com/file/d/1NkCNecLpFG3i7bo3Vl9RQSwzBpSRQ29q/view?usp=sharing\n",
        "# !gdown 1NkCNecLpFG3i7bo3Vl9RQSwzBpSRQ29q -O gsv.zip\n",
        "# !unzip /content/gsv.zip -d /content\n",
        "# import shutil\n",
        "# shutil.rmtree('/content/google_street_view/meta_data', ignore_errors=True) # delete the meta_data folder\n",
        "\n",
        "# # clip cleaned\n",
        "# # https://drive.google.com/file/d/1-xcHyVAMeTkY7SUdUyQRVAn_FFXPSDGB/view?usp=share_link\n",
        "# !gdown 1-xcHyVAMeTkY7SUdUyQRVAn_FFXPSDGB -O gsv.zip\n",
        "# !unzip /content/gsv.zip -d /\n",
        "# !rm -R /content/gsv/.ipynb_checkpoints\n",
        "# !rm -R /content/gsv/01/.ipynb_checkpoints\n",
        "# !rm -R /content/gsv/02/.ipynb_checkpoints\n",
        "# !rm -R /content/gsv/03/.ipynb_checkpoints\n",
        "# !rm -R /content/gsv/04/.ipynb_checkpoints\n",
        "# !rm -R /content/gsv/05/.ipynb_checkpoints\n",
        "# !rm -R /content/gsv/06/.ipynb_checkpoints\n",
        "\n",
        "# # gsv 70k\n",
        "# # https://drive.google.com/file/d/1-7ZC29k4VxXQkpnOuLfj7Ag_SFTM4LV8/view?usp=share_link\n",
        "# !gdown 1-7ZC29k4VxXQkpnOuLfj7Ag_SFTM4LV8 -O gsv.zip\n",
        "# !unzip /content/gsv.zip -d /\n",
        "\n",
        "# # # !ls\n",
        "# !ls -a /content/gsv70k\n",
        "# !rm -R /content/gsv70k/.ipynb_checkpoints\n",
        "# # # !rm -R /content/gsv/06/.ipynb_checkpoints\n",
        "\n",
        "# # 70k+gmap\n",
        "# # https://drive.google.com/file/d/1-CZp7TbhJLeRQpbKQCyT8ofGg89Yt137/view?usp=sharing\n",
        "!gdown 1-CZp7TbhJLeRQpbKQCyT8ofGg89Yt137 -O gsv.zip\n",
        "!unzip /content/gsv.zip -d /\n",
        "!rm -R /content/gsv70kg/.ipynb_checkpoints\n",
        "!rm -R /content/gsv70kg/01/.ipynb_checkpoints\n",
        "!rm -R /content/gsv70kg/02/.ipynb_checkpoints\n",
        "!rm -R /content/gsv70kg/03/.ipynb_checkpoints\n",
        "!rm -R /content/gsv70kg/04/.ipynb_checkpoints\n",
        "!rm -R /content/gsv70kg/05/.ipynb_checkpoints\n",
        "!rm -R /content/gsv70kg/06/.ipynb_checkpoints\n",
        "\n",
        "# # https://bestasoff.medium.com/how-to-fine-tune-very-large-model-if-it-doesnt-fit-on-your-gpu-3561e50859af\n",
        "!pip install bitsandbytes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "hEUffQ24mkRY",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title torch augment\n",
        "# https://github.com/facebookresearch/vicreg/blob/main/augmentations.py\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "class TrainTransform(object):\n",
        "    def __init__(self):\n",
        "        # self.transform = transforms.RandomApply([transforms.Compose([\n",
        "        self.transform = transforms.Compose([\n",
        "                transforms.RandomPerspective(distortion_scale=0.3, p=0.5), # me\n",
        "                transforms.RandomResizedCrop((400,640), scale=(0.7, 1.0), ratio=(0.8, 1.25), interpolation=transforms.InterpolationMode.BICUBIC),\n",
        "                transforms.RandomHorizontalFlip(p=0.5), # 0.5\n",
        "                transforms.Lambda(lambda x : torch.clamp(x, 0., 1.)), # clamp else ColorJitter will return nan https://discuss.pytorch.org/t/input-is-nan-after-transformation/125455/6\n",
        "                transforms.RandomApply([transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1)], p=0.8,), # brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1)], p=0.8\n",
        "                transforms.RandomGrayscale(p=0.2), # 0.2\n",
        "                # # transforms.RandomChoice(transforms.ColorJitter , transforms.RandomGrayscale(p=1.)\n",
        "                transforms.RandomApply([transforms.GaussianBlur(kernel_size=5, sigma=(0.1, 2.0)),], p=1.0),\n",
        "                # # transforms.RandomSolarize(threshold=130, p=0.5)\n",
        "                # transforms.RandomErasing(p=1., scale=(0.1, 0.11), ratio=(1,1), value=(0.485, 0.456, 0.406)),\n",
        "                transforms.RandomErasing(p=1., scale=(0.1, 0.11), ratio=(1,1), value=(0.5, 0.5, 0.5)),\n",
        "                # transforms.ToTensor(), # ToTensored at dataset level, no need to ToTensor again\n",
        "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), # normalised at dataset level. default 0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225\n",
        "                ])\n",
        "            # ], p=1.)\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        dims = len(sample.shape)\n",
        "        if dims==3: x1 = self.transform(sample) # same transforms per minibatch\n",
        "        elif dims==4: x1 = transforms.Lambda(lambda x: torch.stack([self.transform(x_) for x_ in x]))(sample) # diff transforms per img in minibatch\n",
        "        # x1 = self.transform(sample)\n",
        "        return x1\n",
        "\n",
        "trs=TrainTransform()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "yjix6KhUqxY1"
      },
      "outputs": [],
      "source": [
        "# @title data (old)\n",
        "# without oversampling\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
        "\n",
        "dir='/content/gsv70kg'\n",
        "\n",
        "data = datasets.ImageFolder(dir, transform=transform)\n",
        "# data = datasets.ImageFolder(dir, transform=None)\n",
        "torch.manual_seed(0)\n",
        "train_data, test_data = torch.utils.data.random_split(data, [.9,.1])\n",
        "\n",
        "batch_size = 16 # 64\n",
        "num_batches=int(np.ceil(len(train_data)/batch_size))\n",
        "\n",
        "# train_loader = torch.utils.data.DataLoader(train_data, sampler=train_sampler, batch_size=batch_size, num_workers=4, pin_memory=True)\n",
        "# test_loader = torch.utils.data.DataLoader(test_data, sampler=test_sampler, batch_size=batch_size, num_workers=4, pin_memory=True)\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, num_workers=4, pin_memory=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, num_workers=4, pin_memory=True)\n",
        "del data, train_data, test_data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Pqw5n--6WYEG",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title data weighted\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# dataset has PILImage images of range [0, 1], transform them to Tensors of normalized range [-1, 1]\n",
        "# transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
        "# transform = transforms.Compose(transforms.ToTensor())\n",
        "\n",
        "dir='/content/gsv70kg'\n",
        "\n",
        "# data = datasets.ImageFolder(dir, transform=transform)\n",
        "data = datasets.ImageFolder(dir, transform=None)\n",
        "# torch.manual_seed(0)\n",
        "\n",
        "# random.seed(0)\n",
        "np.random.seed(0)\n",
        "torch.manual_seed(0)\n",
        "torch.cuda.manual_seed_all(0)\n",
        "\n",
        "\n",
        "# split data manually so that can work with weighted random sampler\n",
        "# train_data, test_data = torch.utils.data.random_split(data, [.85,.15])\n",
        "# https://www.scaler.com/topics/pytorch/how-to-split-a-torch-dataset/\n",
        "data_size = len(data)\n",
        "indices = np.arange(data_size)\n",
        "np.random.shuffle(indices)\n",
        "split_index = int(np.floor(0.9 * data_size))\n",
        "# split_index = int(np.floor(1. * data_size))\n",
        "train_idx, test_idx = indices[:split_index], indices[split_index:]\n",
        "# train_idx, test_idx = indices[:split_index], indices[split_index:split_index*2]\n",
        "train_data = torch.utils.data.Subset(data, train_idx)\n",
        "# train_data, _ = torch.utils.data.random_split(train_data, [.1,.9])\n",
        "test_data = torch.utils.data.Subset(data, test_idx)\n",
        "targets = np.array(data.targets)\n",
        "train_targets = targets[train_idx]\n",
        "test_targets = targets[test_idx]\n",
        "\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "class DatasetWrap(Dataset):\n",
        "    def __init__(self, dataset, transform=None):\n",
        "        super(DatasetWrap, self).__init__()\n",
        "        self.dataset = dataset\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        x, y = self.dataset[index]\n",
        "        if self.transform:\n",
        "            x = self.transform(x)\n",
        "        return x, y\n",
        "\n",
        "# dataset wrapper in order to apply transforms to train data only\n",
        "# train_data = DatasetWrap(train_data, TrainTransform()) # apply data augmentation to train dataset only\n",
        "# train_data = DatasetWrap(train_data, transform) # apply transform during training to use gpu\n",
        "train_data = DatasetWrap(train_data, transforms.ToTensor()) # apply transform during training to use gpu\n",
        "test_data = DatasetWrap(test_data, transform)\n",
        "\n",
        "# use batch size 16 for resnet 152/ vit with grad accumulation\n",
        "# can use batch size 64 for inception v3 without grad accumulation?\n",
        "batch_size = 16 # 64/16\n",
        "grad_acc = 4\n",
        "num_batches=int(np.ceil(len(train_data)/batch_size))\n",
        "\n",
        "# oversampling\n",
        "# https://stackoverflow.com/questions/62319228/number-of-instances-per-class-in-pytorch-dataset\n",
        "from collections import Counter\n",
        "class_count = torch.tensor(list(Counter(data.targets).values()))\n",
        "weights=1./class_count\n",
        "# weights=sum(class_count)/class_count\n",
        "# print(weights)\n",
        "# https://pytorch.org/docs/stable/data.html#torch.utils.data.WeightedRandomSampler\n",
        "\n",
        "train_weight = weights[train_targets]\n",
        "test_weight = weights[test_targets]\n",
        "train_sampler = torch.utils.data.WeightedRandomSampler(train_weight, len(train_weight))\n",
        "test_sampler = torch.utils.data.WeightedRandomSampler(test_weight, len(test_weight))\n",
        "# train_loader = torch.utils.data.DataLoader(train_data, sampler=train_sampler, batch_size=batch_size, num_workers=4, pin_memory=True)\n",
        "# test_loader = torch.utils.data.DataLoader(test_data, sampler=test_sampler, batch_size=batch_size, num_workers=4, pin_memory=True)\n",
        "train_loader = torch.utils.data.DataLoader(train_data, sampler=train_sampler, batch_size=batch_size, pin_memory=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, sampler=test_sampler, batch_size=batch_size, pin_memory=True)\n",
        "del data, train_data, test_data\n",
        "\n",
        "\n",
        "# test oversampling: occurence of each class should be roughly equal\n",
        "# c=0\n",
        "# print(len(test_loader))\n",
        "# # for batch, (x, y) in enumerate(train_loader):\n",
        "# for batch, (x, y) in enumerate(test_loader):\n",
        "#     print(torch.bincount(y)) # torch count number of elements with value in tensor\n",
        "#     c+=1\n",
        "#     if c>5: break\n",
        "\n",
        "# import matplotlib\n",
        "# matplotlib.rcParams['figure.dpi'] = 300\n",
        "# def imshow(img): # display img from torch tensor\n",
        "#     img = img / 2 + 0.5  # unnormalize\n",
        "#     plt.axis('off')\n",
        "#     npimg = img.numpy()\n",
        "#     plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "#     plt.show()\n",
        "\n",
        "# trainiter = iter(train_loader)\n",
        "# images, labels = next(trainiter)\n",
        "# images=trs(images)\n",
        "# imshow(torchvision.utils.make_grid(images,nrow=4))\n",
        "# del trainiter\n",
        "\n",
        "# print(labels)\n",
        "\n",
        "# testiter = iter(test_loader)\n",
        "# images, labels = next(testiter)\n",
        "# imshow(torchvision.utils.make_grid(images,nrow=4))\n",
        "# del testiter\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "cellView": "form",
        "id": "7oYDr8kuA5Bl"
      },
      "outputs": [],
      "source": [
        "# @title resnet\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "# # https://pytorch.org/vision/0.12/models.html#id10\n",
        "model = models.resnet152(weights='DEFAULT') # 18 34 50 101 152\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Sequential( # og (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
        "    nn.Linear(num_ftrs, 6, bias=False),\n",
        "    nn.Softmax(dim=1),\n",
        "    )\n",
        "\n",
        "# print(model)\n",
        "\n",
        "# # model.mods = [module for k, module in model._modules.items()]\n",
        "# # modules = [module for k, module in model._modules.items()]\n",
        "\n",
        "# torch._dynamo.config.suppress_errors = True\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# model = model.to(device)\n",
        "model = torch.compile(model.to(device))\n",
        "# model = torch.compile(model.to(device),mode='reduce-overhead')\n",
        "# model = torch.compile(model.to(device),mode='max-autotune') # max needs grad ckpt\n",
        "\n",
        "# resnet152 batch16 compile gradacc4 nogradckpt lr3e-6,3e-5\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "rJw9_Ort2Sek"
      },
      "outputs": [],
      "source": [
        "# @title vit\n",
        "# https://arxiv.org/pdf/2010.11929.pdf\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "# https://pytorch.org/vision/main/models/vision_transformer.html\n",
        "# https://pytorch.org/vision/main/models/generated/torchvision.models.vit_b_16.html#torchvision.models.vit_b_16\n",
        "# model = models.vit_l_16(weights='DEFAULT') # small vit_b_16 vit_b_32 vit_l_16 vit_l_32 vit_h_14 big\n",
        "# # VisionTransformer(image_size, patch_size, num_layers, num_heads, hidden_dim, mlp_dim)\n",
        "# num_ftrs = model.heads.head.in_features\n",
        "# # num_ftrs = model.heads[-1].in_features\n",
        "# model.heads = nn.Sequential(\n",
        "#     # nn.Dropout(0.2),\n",
        "#     nn.Linear(num_ftrs, 6, bias=False),\n",
        "#     nn.Softmax(dim=1),\n",
        "#     )\n",
        "\n",
        "\n",
        "!pip install timm\n",
        "# https://github.com/huggingface/pytorch-image-models/issues/908\n",
        "import timm\n",
        "# model = timm.create_model('vit_base_patch16_224', pretrained=True)\n",
        "model = timm.create_model('vit_base_patch16_224', img_size=(400, 640), pretrained=True)\n",
        "# [print(x) for x in timm.list_models('vit*',pretrained=True)]\n",
        "# https://huggingface.co/google/vit-base-patch16-224\n",
        "# https://github.com/huggingface/pytorch-image-models/blob/main/timm/models/vision_transformer.py\n",
        "# vit_base_patch16_224 compile,no ckpt # patch_size=16, embed_dim=768, depth=12, num_heads=12\n",
        "# vit_base_patch16_384\n",
        "# vit_large_patch16_224 explodesgpu # patch_size=16, embed_dim=1024, depth=24, num_heads=16\n",
        "# vit_large_patch16_384 # patch_size=16, embed_dim=1024, depth=24, num_heads=16\n",
        "\n",
        "# or fine tune huge\n",
        "# vit_large_patch14_224 # patch_size=16, embed_dim=1024, depth=24, num_heads=16\n",
        "# vit_large_patch16_384\n",
        "\n",
        "\n",
        "num_ftrs = model.head.in_features\n",
        "model.head = nn.Sequential(\n",
        "    nn.Linear(num_ftrs, 6, bias=False),\n",
        "    nn.Softmax(dim=1),\n",
        "    )\n",
        "# model.set_grad_checkpointing()\n",
        "\n",
        "# print(model.patch_embed.grid_size) # (25, 40)\n",
        "# print(model.pos_embed.shape) # [1, 1001, 768]\n",
        "# https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py\n",
        "\n",
        "# print(model)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# model = model.to(device)\n",
        "model = torch.compile(model.to(device))\n",
        "# model = torch.compile(model.to(device),mode='reduce-overhead')\n",
        "# model = torch.compile(model.to(device),mode='max-autotune')\n",
        "\n",
        "# vit_base_patch16_224 batch16 maxcompile nockpt gradacc lr1e-5,1e-4\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "-oKYpG8n2fBI"
      },
      "outputs": [],
      "source": [
        "# @title inception\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "# # https://pytorch.org/vision/0.12/models.html#id10\n",
        "model = models.inception_v3(pretrained=True)\n",
        "# https://discuss.pytorch.org/t/inception-v3-is-not-working-very-well/38296/16\n",
        "# https://colab.research.google.com/github/CaoCharles/Deep-Learning-with-PyTorch/blob/master/2_Inception.ipynb\n",
        "model.aux_logits = False\n",
        "num_ftrs = model.fc.in_features # 2048\n",
        "model.fc = nn.Sequential( # og: (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
        "    nn.Linear(num_ftrs, 6, bias=False),\n",
        "    nn.Softmax(dim=1),\n",
        "    )\n",
        "# pytorch \"inception\" v3 \"gradient checkpointing\" https://github.com/jianweif/OptimalGradCheckpointing\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# model = model.to(device)\n",
        "model = torch.compile(model.to(device))\n",
        "# model = torch.compile(model.to(device),mode='max-autotune')\n",
        "\n",
        "# inception batch64 compile nogradacc nogradckpt lr3e-6,3e-5\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "vEZCFg5YSS9J"
      },
      "outputs": [],
      "source": [
        "# @title try\n",
        "\n",
        "# # check model's input and output dimensions are correct\n",
        "# X = torch.rand(64, 3, 32, 32, device=device)\n",
        "X = torch.rand(64, 3, 400, 640, device=device)\n",
        "# X = torch.rand(16, 3, 224, 224, device=device)\n",
        "model.train()\n",
        "\n",
        "# 224x224\n",
        "# 16x16 / 32x32 patch\n",
        "# -> 14x14=196 7x7=49 seq length\n",
        "# 400x640 -> 25x40=1000 seq length\n",
        "\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "with torch.cuda.amp.autocast():\n",
        "    logits = model(X)\n",
        "\n",
        "# modules = [module for k, module in model._modules.items()]\n",
        "# for i,x in enumerate(modules):\n",
        "#     print(i,x)\n",
        "\n",
        "# logits = checkpoint_sequential(functions=modules, segments=1, input=X)\n",
        "\n",
        "print(logits.shape)\n",
        "# print(logits[0])\n",
        "# print(logits[0].argmax(1))\n",
        "\n",
        "# pred_probab = nn.Softmax(dim=1)(logits)\n",
        "# y_pred = pred_probab.argmax(1)\n",
        "# print(y_pred)\n",
        "# del X, logits\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()\n"
      ],
      "metadata": {
        "id": "97phwgdcdJ85"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uT9m-J1BUWyz",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title wandb\n",
        "# https://docs.wandb.ai/quickstart\n",
        "!pip install wandb\n",
        "import wandb\n",
        "wandb.login() # 487a2109e55dce4e13fc70681781de9f50f27be7\n",
        "run = wandb.init(\n",
        "    project=\"resnet50teacher\",\n",
        "    config={\n",
        "        \"model\": \"resnet\",\n",
        "        # \"model\": \"vit\",\n",
        "        # \"model\": \"inception\",\n",
        "        # \"optim\": \"adamw\",\n",
        "        # \"learning_rate\": lr,\n",
        "        # \"epochs\": epochs,\n",
        "    })\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "fsealXK3OPQa",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title train test function\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "# https://github.com/prigoyal/pytorch_memonger/blob/master/models/optimized/resnet_new.py\n",
        "from torch.utils.checkpoint import checkpoint, checkpoint_sequential\n",
        "\n",
        "trs=TrainTransform() # for image augmentation during train time\n",
        "# train function with automatic mixed precision\n",
        "def strain(dataloader, model, loss_fn, optimizer, scheduler=None, verbose=True):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    loss_list = []\n",
        "    for batch, (x, y) in enumerate(dataloader):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        with torch.cuda.amp.autocast(): # automatic mixed percision\n",
        "            x = trs(x) # image augmentation during train time to use gpu\n",
        "            pred = model(x) # default\n",
        "            # modules = [module for k, module in model._modules.items()]\n",
        "            # pred = checkpoint_sequential(functions=modules, segments=1, input=x) # gradient checkpointing for resnet and inception only\n",
        "            # # pred = checkpoint_sequential(functions=model.mods, segments=1, input=x)\n",
        "            # print(\"train\",pred[0])\n",
        "            # print((pred.argmax(1)==y).sum().item(), pred.argmax(1).tolist(), y.tolist())\n",
        "            loss = loss_fn(pred, y)\n",
        "        scaler.scale(loss).backward()\n",
        "        if ((batch + 1) % grad_acc == 0) or (batch + 1 == len(dataloader)): # gradient accumulation\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            optimizer.zero_grad()\n",
        "            if scheduler is not None:\n",
        "                scheduler.step()\n",
        "                # print(\"### lr: \", optimizer.param_groups[0][\"lr\"])\n",
        "\n",
        "        # print(model.state_dict()['_orig_mod.bn1.running_mean'][0])\n",
        "        train_loss = loss.item()/len(y)\n",
        "        loss_list.append(train_loss)\n",
        "        try: wandb.log({\"train loss\": train_loss})\n",
        "        except: pass\n",
        "        if batch % (size//(10* len(y))) == 0:\n",
        "            # loss, current = loss.item(), batch * len(x)\n",
        "            # loss, current = loss.item()/len(y), batch * len(x)\n",
        "            current = batch * len(x)\n",
        "            if verbose: print(f\"loss: {train_loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "    return loss_list\n",
        "\n",
        "\n",
        "def test(dataloader, model, loss_fn, verbose=True):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in dataloader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            pred = model(x)\n",
        "            # print(pred[0])\n",
        "            loss = loss_fn(pred, y)\n",
        "            # predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            # print((pred.argmax(1)==y).sum().item(), pred.argmax(1).tolist(), y.tolist())\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    # test_loss /= num_batches\n",
        "    test_loss /= size\n",
        "    correct /= size\n",
        "    # should not use weighted rand sampler for test?\n",
        "    try: wandb.log({\"test loss\": test_loss})\n",
        "    except: pass\n",
        "    if verbose: print(f\"Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f}\")\n",
        "    return correct, test_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "a5CHCIMo7ZC5"
      },
      "outputs": [],
      "source": [
        "# @title Lamb\n",
        "# lamb optimizer (optional)\n",
        "# its like adamw but with warmup-like properties built in\n",
        "# https://github.com/cybertronai/pytorch-lamb/blob/master/pytorch_lamb/lamb.py\n",
        "import torch\n",
        "from torch.optim import Optimizer\n",
        "\n",
        "class Lamb(Optimizer):\n",
        "    \"\"\"Large Batch Optimization for Deep Learning: Training BERT in 76 minutes: https://arxiv.org/abs/1904.00962\n",
        "        adam (bool, optional): always use trust ratio = 1, which turns this into Adam\"\"\"\n",
        "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-6, weight_decay=0, adam=False): # eps=1e-8, weight_decay=0\n",
        "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n",
        "        self.adam = adam\n",
        "        super(Lamb, self).__init__(params, defaults)\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        loss = None\n",
        "        if closure is not None: loss = closure() # closure (callable, optional): A closure that reevaluates the model and returns the loss.\n",
        "        for group in self.param_groups:\n",
        "            for p in group['params']:\n",
        "                if p.grad is None: continue\n",
        "                grad = p.grad.data\n",
        "                if grad.is_sparse: raise RuntimeError('Lamb does not support sparse gradients, consider SparseAdam instad.')\n",
        "                state = self.state[p] # State initialization\n",
        "                if len(state) == 0:\n",
        "                    state['step'] = 0\n",
        "                    state['exp_avg'] = torch.zeros_like(p.data) # Exponential moving average of gradient values\n",
        "                    state['exp_avg_sq'] = torch.zeros_like(p.data) # Exponential moving average of squared gradient values\n",
        "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
        "                beta1, beta2 = group['betas']\n",
        "                state['step'] += 1\n",
        "\n",
        "                # Decay the first and second moment running average coefficient\n",
        "                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1) # m_t\n",
        "                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2) # v_t\n",
        "\n",
        "                # Paper v3 does not use debiasing.\n",
        "                # bias_correction1 = 1 - beta1 ** state['step']\n",
        "                # bias_correction2 = 1 - beta2 ** state['step']\n",
        "                # Apply bias to lr to avoid broadcast.\n",
        "                step_size = group['lr'] # * math.sqrt(bias_correction2) / bias_correction1\n",
        "                weight_norm = p.data.pow(2).sum().sqrt().clamp(0, 10)\n",
        "                adam_step = exp_avg / exp_avg_sq.sqrt().add(group['eps'])\n",
        "                if group['weight_decay'] != 0: adam_step.add_(p.data, alpha=group['weight_decay'])\n",
        "                adam_norm = adam_step.pow(2).sum().sqrt()\n",
        "                if weight_norm == 0 or adam_norm == 0: trust_ratio = 1\n",
        "                else: trust_ratio = weight_norm / adam_norm\n",
        "                state['weight_norm'] = weight_norm\n",
        "                state['adam_norm'] = adam_norm\n",
        "                state['trust_ratio'] = trust_ratio\n",
        "                if self.adam: trust_ratio = 1\n",
        "                p.data.add_(adam_step, alpha=-step_size * trust_ratio)\n",
        "        return loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iuumbm2SB_lX",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title LR range test\n",
        "# gives insight into good LR range to use.\n",
        "# for accurate results, be sure to use a new model for range test;\n",
        "# also reset the model before training bec range test destroys the model!!!\n",
        "# 1cycle super convergencehttps://arxiv.org/pdf/1708.07120.pdf\n",
        "# # cyclic lr https://arxiv.org/pdf/1506.01186.pdf\n",
        "# Note the learning rate value when the accuracy starts to\n",
        "# increase and when the accuracy slows, becomes ragged, or starts to fall\n",
        "\n",
        "# one training run of the network for a few epochs\n",
        "\n",
        "# torch.autograd.set_detect_anomaly(True)\n",
        "epochs=1\n",
        "min_lr= 1e-7\n",
        "max_lr= 1e-2 # 1e-2\n",
        "# 152: 1e-7 - 1e-4      result 3e-7 - 3e-6\n",
        "# inception: 1e-7 - 1e1      result 3e-7 - 3e-6\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "# optimizer = torch.optim.AdamW(model.parameters(), lr=min_lr, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=min_lr, momentum=0.9)\n",
        "# optimizer = torch.optim.SGD(model.parameters(), lr=min_lr, momentum=0.)\n",
        "# import bitsandbytes as bnb\n",
        "# optimizer = bnb.optim.AdamW(model.parameters(), lr=min_lr, betas=(0.9, 0.999), optim_bits=8)\n",
        "# optimizer = Lamb(model.parameters(), lr=min_lr, betas=(0.9, 0.999), eps=1e-08, weight_decay=3e-6)\n",
        "\n",
        "num_batches=len(test_loader)\n",
        "# num_batches=len(train_loader)\n",
        "\n",
        "# total_steps=int(num_batches*epochs)\n",
        "total_steps=int(np.ceil(num_batches/grad_acc)*epochs) # for grad accumulation\n",
        "\n",
        "# min_lr* gamma^total_steps = max_lr\n",
        "gamma = np.exp(np.log(max_lr/min_lr)/total_steps)\n",
        "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=gamma) # 0.75(20)-0.9(100)\n",
        "\n",
        "gamma = np.exp(np.log(max_lr/min_lr)/(total_steps*grad_acc))\n",
        "# lr_list=np.ones(total_steps)*min_lr*gamma**np.arange(total_steps)\n",
        "lr_list = min_lr*gamma**np.arange(total_steps*grad_acc+1-grad_acc)\n",
        "\n",
        "train_lst, test_lst=[],[]\n",
        "# use test_loader so that the range test is shorter\n",
        "# can do that since we are going to reset the model later anyways\n",
        "\n",
        "print(\"lr: \", optimizer.param_groups[0][\"lr\"])\n",
        "for i in range(epochs):\n",
        "    # train_ls = train(test_loader, model, loss_fn, optimizer, scheduler)\n",
        "    train_ls = strain(test_loader, model, loss_fn, optimizer, scheduler)\n",
        "    # train_ls = strain(train_loader, model, loss_fn, optimizer, scheduler)\n",
        "    train_lst.extend(train_ls)\n",
        "print(optimizer.param_groups[0][\"lr\"])\n",
        "\n",
        "# https://stackoverflow.com/a/53472966/13359815\n",
        "from scipy.ndimage.filters import gaussian_filter1d\n",
        "train_lstsm = gaussian_filter1d(train_lst, sigma=30)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(lr_list[:-1], train_lst[:-1])\n",
        "plt.plot(lr_list[:-1], train_lstsm[:-1])\n",
        "plt.xscale('log')\n",
        "# plt.plot(train_lst)\n",
        "# plt.plot(np.linspace(0,len(train_lst),len(test_lst)), test_lst)\n",
        "plt.show()\n",
        "plt.close()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "wkBp7WjC8LLA"
      },
      "outputs": [],
      "source": [
        "# @title wwwwwwwww\n",
        "acc_lst, train_lst, test_lst=[],[],[]\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# base_lr, max_lr = 3e-6, 3e-5 # resnet152?\n",
        "base_lr, max_lr = 3e-7, 3e-6 # resnet152 batch16 gradacc4\n",
        "# base_lr, max_lr = 3e-5, 3e-4 # resnet50 batch32 gradacc2\n",
        "# base_lr, max_lr = 1e-5, 3e-4 # resnet18 batch128/64 gradacc1\n",
        "# base_lr, max_lr = 1e-6, 1e-5 # vit\n",
        "# base_lr, max_lr = 3e-6, 3e-5 # inception\n",
        "# end_lr, start_lr = 1e-5, 1e-3 # 0.0001,0.1\n",
        "tp=0\n",
        "epochs = 8 #5 20\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = base_lr, momentum=0.9)\n",
        "# optimizer = torch.optim.AdamW(model.parameters(), lr=base_lr, betas=(0.9, 0.999), eps=1e-08, weight_decay=3e-6)\n",
        "# import bitsandbytes as bnb # 8bit optimizer\n",
        "# optimizer = bnb.optim.AdamW(model.parameters(), lr=base_lr, betas=(0.9, 0.999), optim_bits=8)\n",
        "# optimizer = bnb.optim.AdamW(model.parameters(), lr=3e-6, betas=(0.9, 0.999), optim_bits=8)\n",
        "# optimizer = bnb.optim.AdamW(model.parameters(), lr=3e-5, betas=(0.9, 0.999), optim_bits=8)\n",
        "# optimizer = Lamb(model.parameters(), lr=base_lr, betas=(0.9, 0.999), eps=1e-08, weight_decay=3e-6)\n",
        "# optimizer = Lamb(model.parameters(), lr=3e-6, betas=(0.9, 0.999), eps=1e-08, weight_decay=3e-6)\n",
        "# 152 1e-5\n",
        "# cnn 3e-4\n",
        "\n",
        "div_factor = max_lr/base_lr\n",
        "num_batches=len(train_loader)\n",
        "# total_steps=int(num_batches*epochs)+1 # +1 to excluse uptick at the end of onecycle\n",
        "total_steps=int(np.ceil(num_batches/grad_acc)*epochs +1) # /4 for when using grad accumulation\n",
        "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=max_lr, total_steps=total_steps, pct_start=0.45, anneal_strategy='cos', cycle_momentum=True, base_momentum=0.85, max_momentum=0.95, div_factor=div_factor, final_div_factor=100.0, three_phase=True,)\n",
        "# scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=max_lr, epochs=epochs, steps_per_epoch=num_batches, pct_start=0.45, anneal_strategy='cos', cycle_momentum=True, base_momentum=0.85, max_momentum=0.95, div_factor=div_factor, final_div_factor=10000.0, three_phase=True,)\n",
        "# gamma = np.exp(np.log(end_lr/start_lr)/(num_batches*epochs)) # for scheduler step every optimizer step\n",
        "# scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=gamma) # 0.75(20)-0.9(100)\n",
        "\n",
        "# pth='/content/res15270kg.pth'\n",
        "# pth='/content/drive/MyDrive/frame/res15236.pth' # B\n",
        "# pth='/content/drive/MyDrive/frame/res15270kold.pth' # M\n",
        "# pth='/content/drive/MyDrive/frame/res152trackf.pth' # B\n",
        "# pth='/content/drive/MyDrive/frame/res152plus.pth' # B\n",
        "# pth='/content/res152adamw71.pth' # ty\n",
        "# pth='/content/drive/MyDrive/frame/res152Teacher_onecycle.pth' # M\n",
        "# pth='/content/drive/MyDrive/frame/res18Teacher.pth' # A\n",
        "# pth='/content/drive/MyDrive/frame/res50Teacher.pth' # B/\n",
        "pth='/content/drive/MyDrive/frame/resnet1e1sgd.pth' # M\n",
        "\n",
        "\n",
        "# pth='/content/vit.pth'\n",
        "# pth='/content/drive/MyDrive/frame/vit.pth'\n",
        "# pth='/content/drive/MyDrive/frame/vit3736.pth'\n",
        "# pth='/content/drive/MyDrive/frame/vitadamw.pth' # M\n",
        "\n",
        "# pth='/content/drive/MyDrive/frame/inception.pth'\n",
        "# pth='/content/inception2.pth'\n",
        "# pth='/content/inception.pth'\n",
        "\n",
        "\n",
        "# # # to continue training\n",
        "# tp, modelsd, optimsd, schedsd = torch.load(pth).values()\n",
        "# # tp, modelsd, optimsd = torch.load(pth).values()\n",
        "# # model.load_state_dict(modelsd)\n",
        "# model.load_state_dict(modelsd,strict=False)\n",
        "# optimizer.load_state_dict(optimsd)\n",
        "# scheduler.load_state_dict(schedsd)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "REiP7-nvhc4s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee50819b-9c3c-4dbe-d055-7f1199a81452",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# @title save\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# # pth='/content/drive/MyDrive/frame/vit.pth'\n",
        "# pth='/content/drive/MyDrive/frame/res55.pth'\n",
        "# pth='/content/drive/MyDrive/frame/resnet152.pth'\n",
        "# pth='/content/drive/MyDrive/frame/res152373605aug.pth'\n",
        "# pth='/content/res152373605aug.pth' # B\n",
        "# pth='/content/drive/MyDrive/frame/res152lamb12.pth' # ty\n",
        "# pth='/content/drive/MyDrive/frame/res1522.pth' # A\n",
        "# pth='/content/drive/MyDrive/frame/res1522do.pth' # A\n",
        "# pth='/content/drive/MyDrive/frame/res152fol6470kg.pth' # Ty B\n",
        "\n",
        "# pth='/content/drive/MyDrive/frame/inception.pth'\n",
        "# pth='/content/inception2.pth'\n",
        "\n",
        "# pth='/content/res15270kg.pth'\n",
        "\n",
        "# pth='/content/drive/MyDrive/frame/vit3736.pth'\n",
        "\n",
        "\n",
        "# tp, modelsd, optimsd, schedsd = torch.load(pth).values()\n",
        "# model.load_state_dict(modelsd)\n",
        "# # optimizer.load_state_dict(optimsd)\n",
        "# # scheduler.load_state_dict(schedsd)\n",
        "\n",
        "# pth='/content/drive/MyDrive/frame/res152Teacher3e-6.pth' # M\n",
        "# pth='/content/drive/MyDrive/frame/res152TeacherSGD.pth' # B\n",
        "\n",
        "# checkpoint = { # https://discuss.pytorch.org/t/saving-model-and-optimiser-and-scheduler/52030\n",
        "# 'epoch': 0,\n",
        "# 'model': model.state_dict(),\n",
        "# 'optimizer': optimizer.state_dict(),\n",
        "# 'lr_sched': scheduler.state_dict()\n",
        "# }\n",
        "# torch.save(checkpoint, pth)\n",
        "\n",
        "\n",
        "\n",
        "# pth='/content/model.pth'\n",
        "# torch.save(model.state_dict(), pth)\n",
        "# model.load_state_dict(torch.load(pth))\n",
        "# # model.load_state_dict(torch.load(\"model.pth\"))\n",
        "\n",
        "# !gdown 1---4fdFbOUBTrS-VP5Va6pKowfgoU2UN -O inception2.pth\n",
        "# !gdown 1visTNvWmnuV7jAm2TBiAIIrNjbOAi1Fv -O resnet152.pth\n",
        "# !gdown 1-3oA1cKxgw4cfrqx079h_MKtMqRT1UFq -O res152lamb12.pth # S\n",
        "# !gdown 1cxu8Qq4-FoH3W3nQdRHT5mwe0xOxZ0Kb -O res152adamw.pth # M\n",
        "# !gdown 1ysJfdsvwMiWbCdkvFHwNqAUnJTtm6KbT -O res152adamw71.pth # ty\n",
        "# !gdown 1VaPxGoaLjmt7K9VHi0FWbJ5efEZTLhwd -O res18teacher.pth # A\n",
        "\n",
        "# !gdown 1NBxqdTfVjNC37R47rc79j_cl23Fd93v3 -O res50teacher.pth # B\n",
        "# !gdown 1LAK8lr1_lpd7l48Z-NiB-o5JRa9k6C2V -O res50teacher.pth # M\n",
        "\n",
        "\n",
        "# !gdown 1qsbDQgnPgWLIwdwsb6MKJnxAf5SRTFz5 -O inception.pth # Ty 9jun\n",
        "# !gdown 1-032red_AZ4nABCXmYTvJN4s31xGctAC -O inception1615.pth # Ty\n",
        "\n",
        "# !gdown 1SAC0TW-KNJcFqVb4kA79sQb859eT-wME -O vit.pth # A 9jun\n",
        "# !gdown 1-0-GTDs5vEWnezoo-vUVagHi_niATwrN -O vit3736.pth # A 9jun\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# # t, modelsd, optimsd, scheduler = torch.load('/content/drive/MyDrive/frame/resnet152.pth').values()\n",
        "# t, modelsd, optimsd, scheduler = torch.load('/content/resnet152.pth').values()\n",
        "# model.load_state_dict(modelsd)\n",
        "# # optimizer.load_state_dict(optimsd)\n",
        "\n",
        "# # matt152 # https://drive.google.com/file/d/1MQ0xLfHbio458uEVbn2VyMpD3bij2A4J/view?usp=sharing\n",
        "# !gdown 1MQ0xLfHbio458uEVbn2VyMpD3bij2A4J -O res152.pth\n",
        "# model.load_state_dict(torch.load(\"res152.pth\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kDBEk-l-Oxjn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b36be69a-c10f-40e7-e509-26ab05e38402"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "0.1\n",
            "loss: 0.112065  [    0/61144]\n",
            "loss: 0.113565  [ 6112/61144]\n",
            "loss: 0.109091  [12224/61144]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import time\n",
        "start = time.time()\n",
        "\n",
        "# optimizer = bnb.optim.AdamW(model.parameters(), lr=3e-5, betas=(0.9, 0.999), optim_bits=8)\n",
        "# optimizer = bnb.optim.AdamW(model.parameters(), lr=1e-7, betas=(0.9, 0.999), optim_bits=8)\n",
        "# optimizer = bnb.optim.AdamW(model.parameters(), lr=0, betas=(0.9, 0.999), optim_bits=8)\n",
        "# optimizer = Lamb(model.parameters(), lr=3e-5, betas=(0.9, 0.999), eps=1e-08, weight_decay=3e-6)\n",
        "# optimizer = torch.optim.SGD(model.parameters(), lr=3e-6, momentum=0.9)\n",
        "# optimizer.param_groups[0][\"lr\"]=0\n",
        "# optimizer.param_groups[0][\"lr\"]=3e-6\n",
        "optimizer.param_groups[0][\"lr\"]=3e-3\n",
        "# optimizer.param_groups[0][\"lr\"]=1e-2\n",
        "# optimizer.param_groups[0][\"lr\"]=3e-2\n",
        "# optimizer.param_groups[0][\"lr\"]=1e-1\n",
        "# resnet152 3e-3 - 1e-2     3e-2 - 1e-1?\n",
        "# vit 3e-5 - 1e-4           3e-5 - 1e-4?\n",
        "\n",
        "# scheduler = torch.optim.lr_scheduler.PolynomialLR(optimizer, total_iters=int(np.ceil(num_batches/4)*5), power=1.0)\n",
        "# pth='/content/drive/MyDrive/frame/res152Tn.pth' # M\n",
        "# pth='/content/drive/MyDrive/frame/res50Teacher.pth' # B/M\n",
        "\n",
        "# scheduler = PolynomialLR(optimizer, total_iters=4, power=1.0)\n",
        "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=10**(-1/2))\n",
        "# for epoch in range(5):\n",
        "#     scheduler.step()\n",
        "# for t in range(0,epochs):\n",
        "for t in range(8):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    lr=optimizer.param_groups[0][\"lr\"]\n",
        "    print(lr)\n",
        "    # train_ls = strain(train_loader, model, loss_fn, optimizer, scheduler)\n",
        "    train_ls = strain(train_loader, model, loss_fn, optimizer)\n",
        "    correct, test_loss = test(test_loader, model, loss_fn)\n",
        "    # train_lst.extend(train_ls)\n",
        "    # test_lst.append(test_loss)\n",
        "    # acc_lst.append(correct)\n",
        "\n",
        "    checkpoint = { # https://discuss.pytorch.org/t/saving-model-and-optimiser-and-scheduler/52030\n",
        "    'epoch': t+1,\n",
        "    'model': model.state_dict(),\n",
        "    'optimizer': optimizer.state_dict(),\n",
        "    'lr_sched': scheduler.state_dict()}\n",
        "    torch.save(checkpoint, pth)\n",
        "    # torch.save(model.state_dict(), pth)\n",
        "    end = time.time()\n",
        "    print(\"time: \",end - start)\n",
        "    start = end\n",
        "\n",
        "# print(\"Done!\")\n",
        "\n",
        "\n",
        "# print(len(train_lst), len(test_lst))\n",
        "# import matplotlib.pyplot as plt\n",
        "# plt.plot(train_lst)\n",
        "# plt.plot(np.linspace(0,len(train_lst),len(test_lst)), test_lst)\n",
        "# plt.show()\n",
        "# plt.plot(acc_lst)\n",
        "# plt.show()\n",
        "# plt.close()\n",
        "\n",
        "\n",
        "# torch.save(model.state_dict(), \"model.pth\")\n",
        "\n",
        "# resnet 18, 60/61 38.4%, 528s\n",
        "# resnet 18, 58/61 39.8%, 523s\n",
        "# resnet 18 compile , 58/61 40.4%, 555s\n",
        "# resnet 18 compile augment , 58/61 36.4%, 1941 # augment on cpu, takes longer\n",
        "# resnet 18 augment lr3e-4:3e-3, 58/61 37.7%, 1863s\n",
        "# resnet 18 augment 10epoch lr1e-5:3e-4, 58/61 33.5%, 3387s\n",
        "# resnet 18 compile lr1e-5:3e-4, 58/61 35.0%, 493s\n",
        "# resnet 18 compile scratch lr1e-5:1e-3, 58/61 26.8%, 475s\n",
        "# resnet 18 compile lr1e-5:1e-3, 55/61 47.3%, 480s\n",
        "# resnet 18 compile lr1e-5:1e-3, 52/61 51.7% 503s\n",
        "# resnet 18 compile lr1e-5:1e-3, unfreeze 51.0%, 550s\n",
        "# resnet 18 compile lr1e-5:1e-4, unfreeze 52.7%, 518s\n",
        "# resnet 34 compile lr1e-5:1e-4, unfreeze bitsadamW batch16*4\n",
        "# resnet 152 compileoverhead lr3e-7:3e-6, bitsadamW batch16*4 ckpt 53.8%, 2066s\n",
        "# resnet 152 from53.8% augment+cutout lr3e-7:3e-6, 53.8%, 2088s\n",
        "# resnet 152 comile augment+cutout lr1e-5 /4 1epoch 48.3%, 454s\n",
        "# resnet 152 comile augment+cutout lr1e-5 1epoch 48.4%, 446s\n",
        "# resnet 152 comilemaxautotue augment+cutout lr1e-5 1epoch 47.7%, 448\n",
        "# resnet 152 clipclean comilemax augment+cutout lr1e-5 10epoch 45.1%, 1585 *2\n",
        "# resnet 152 clipclean comilemax augmax lr1e-6:3e-5 20epoch\n",
        "# resnet 152 clipclean compilemax lr3e-7:1e-5,\n",
        "# resnet 152 70k augment compilemax lr3e-7:3e-6 5epoch, 65.4% 11585s*5/3=19300s = 5h20m\n",
        "\n",
        "# resnet 152 70k augment compile adamw const lr3e-6, 59.8%\n",
        "# resnet 152 70k augment compile adamw const3e-6 stepdown\n",
        "\n",
        "\n",
        "\n",
        "# resnet 152 70k augment compile lamb lr1e-2 not learning\n",
        "# resnet 152 70k augment compile lamb lr3e-3 not learning\n",
        "# resnet 152 70k augment compile lamb lr1e-3:1e-2 5epoch,\n",
        "# 2/5epochwarmup 55.0%\n",
        "\n",
        "# resnet 152 70kg augment1 cut1,-1 compilemax adamw lr3e-7:3e-6 1/5epoch 31.1% ty\n",
        "# resnet 152 70k augment1 cut1,-1 compilemax adamw lr3e-7:3e-6 1/5epoch 28.1% A\n",
        "\n",
        "# resnet 152 70k dataold augment1 cut1,0 compilemax adamw lr3e-7:3e-6 M nantest\n",
        "# resnet 152 70kg augment1 cut1,0 compilemax adamw lr3e-7:3e-6 B\n",
        "# resnet 152 70k augment1 from64.5 cut(mean) compilemax adamw lr3e-7:3e-6 4/5epochs 58.1% res152fol64 A 61.5% res152kplus M\n",
        "# resnet 152 70kg augment1 from64.5 cut(mean) compilemax adamw lr3e-7:3e-6 5epochs+ 63.8% res152fol64kg res152plus B\n",
        "# 73.6% 67% 66.1%\n",
        "# 0 1e5 nan\n",
        "# 0 3e6\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# vit b16 lr1e-5 5epochs 41.3%, 466s # 4.4ram, 5.5vram\n",
        "# vit l16 lr1e-5 5epochs # 32.0%, 1242s 4.5ram, 8.0vram\n",
        "# vit l16 lr3e-7;1e-5 5epochs # 45.4%, 1315s 4.5ram, 8.0vram\n",
        "# vit l32 lr1e-5 5epochs # .ram, .vram\n",
        "# vit_large_patch16_384\n",
        "# vit_base_patch16_224 maxcompile nockpt lr3e-7;1e-5 5epochs # 45.2%, 2272s 5.3ram, 11.0vram\n",
        "# vit_base_patch16_224 maxcompile nockpt gradacc batch16 1e-5, 1e-2 explode\n",
        "# vit_base_patch16_224 maxcompile nockpt gradacc batch16 1e-6, 1e-5 low acc, test nan\n",
        "# vit_base_patch16_224 maxcompile nockpt gradacc batch16 3e-7, 3e-6 4/5 epoch50%\n",
        "# vit_base_patch16_224 maxcompile nockpt gradacc batch16 1e-6, 1e-5 0.5aug\n",
        "# vit_base_patch16_224 maxcompile nockpt gradacc batch16 aug1 noblur Lamb lr1e-5 vitadamw B\n",
        "# 1h per epoch\n",
        "# 0 1e5 nan\n",
        "# 0 1e6\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# inception\n",
        "# inception og 10kclean 1e-7, 3e-5 batch64 nockpt 5epochs 0.020348  51.4%, 0.024312 20m; 1st 40.5% max 52.1\n",
        "# inception hid 10kclean 1e-7, 3e-5 batch64 nockpt 1/5epochs 0.019635 53.6%, 0.024120 1st 32.8%, max 53.6%\n",
        "# inception og 10kclean 1e-7, 1e-4 batch64 nockpt 2/10epochs 0.020348  0.020923 39.9%, 0.026105\n",
        "# inception og 70kg 1e-7 nope\n",
        "# inception og 70kg 3e-6 3e-5 54.8%\n",
        "# inception og 70kg 1e-6, 1e-5\n",
        "# inception og 70kg 3e-6 3e-5 0.5aug\n",
        "# adamw 3e-5 A 51.8% 53.3%\n",
        "# inception 70kg 3e-5 S\n",
        "# inception 70kg 3e-5 3e 61.0% 30m per epoch\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8mzr-IlDjSQU"
      },
      "outputs": [],
      "source": [
        "\n",
        "def deactivate_batchnorm(m):\n",
        "    if isinstance(m, nn.BatchNorm2d):\n",
        "        m.reset_parameters()\n",
        "        m.eval()\n",
        "        with torch.no_grad():\n",
        "            m.weight.fill_(1.0)\n",
        "            m.bias.zero_()\n",
        "\n",
        "model.apply(deactivate_batchnorm)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PFsnIRVz7Xp5",
        "outputId": "6fbbd27d-0ef4-467b-8bc3-128ec311a6d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 45.6%, Avg loss: 0.049405\n",
            "0.4562689511024758 0.049405341554549245\n"
          ]
        }
      ],
      "source": [
        "# correct, test_loss = test(test_loader, model, loss_fn)\n",
        "correct, test_loss = test(train_loader, model, loss_fn) # dont forget to normalise train data\n",
        "print(correct, test_loss)\n",
        "# res152 70 Accuracy: 69.5%, Avg loss: 0.084133 ; trainloader:Accuracy: 68.6%, Avg loss: 0.084691\n",
        "\n",
        "# # https://discuss.pytorch.org/t/model-predictions-changing-with-no-grad-and-eval/126543/2\n",
        "# self.drop_layer = nn.Dropout(p = .5)\n",
        "# out = F.dropout(x, training=self.training)\n",
        "# # https://discuss.pytorch.org/t/trained-resnet-doesnt-work-in-eval-mode-behaves-strangely/121242/8\n",
        "# self.bn = torch.nn.BatchNorm2d(input_features,track_running_stats=False)\n",
        "\n",
        "# res50 Accuracy: 36.3%, Avg loss: 0.052037 Accuracy: 45.6%, Avg loss: 0.049405\n",
        "# res18 Accuracy: 62.3%, Avg loss: 0.044281\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "GWt4B_YoCxKB"
      },
      "outputs": [],
      "source": [
        "# @title confusion matrix\n",
        "# for plotting confusoin matrix\n",
        "# left vertical axis is actual, bottom horizontal axis is predicted\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "def confusion(dataloader, model, loss_fn, verbose=True):\n",
        "    model.eval()\n",
        "    y_true, y_pred = torch.empty(0, device=device), torch.empty(0, device=device)\n",
        "    for x, y in dataloader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        with torch.no_grad():\n",
        "            pred = model(x)\n",
        "        # print((pred.argmax(1)==y).sum().item(), pred.argmax(1).tolist(), y.tolist())\n",
        "        y_pred = torch.cat((y_pred, pred.argmax(1)), 0)\n",
        "        y_true = torch.cat((y_true, y), 0)\n",
        "    return y_true, y_pred\n",
        "\n",
        "y_true, y_pred = confusion(test_loader, model, loss_fn)\n",
        "\n",
        "y_true, y_pred = y_true.cpu(), y_pred.cpu()\n",
        "cm = confusion_matrix(y_true=y_true, y_pred=y_pred)\n",
        "cmn = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "fig, ax = plt.subplots(figsize=(5,5))\n",
        "sns.heatmap(cmn, annot=True, fmt='.2f', xticklabels=['1','2','3','4','5','6',], yticklabels=['1','2','3','4','5','6',])\n",
        "plt.ylabel('Actual')\n",
        "plt.xlabel('Predicted')\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "icF25Bbpp6gR",
        "outputId": "2162156b-9172-4160-fe5d-024c0ee8ad5f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "38767 67938 57.062321528452415\n"
          ]
        }
      ],
      "source": [
        "# @title collect unconfident preds\n",
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
        "\n",
        "t=0\n",
        "for cls in range(1,7):\n",
        "    img_dir = '/content/gsv70kg/0'+str(cls)\n",
        "    for filename in os.listdir(img_dir):\n",
        "        img_file=os.path.join(img_dir, filename)\n",
        "        image = Image.open(img_file).convert(\"RGB\")\n",
        "        with torch.no_grad():\n",
        "            with torch.cuda.amp.autocast(): # automatic mixed percision\n",
        "                x = transform(image).to(device).unsqueeze(0)\n",
        "                y = torch.tensor(cls-1, device=device)\n",
        "                pred = model(x)\n",
        "\n",
        "        p = pred.max(1)\n",
        "        # s.append((p, img_file, y.item(), pred.argmax(1)))\n",
        "        s.append((p, img_file))\n",
        "\n",
        "        t+=1\n",
        "        # if t >=5: break\n",
        "\n",
        "\n",
        "s.sort() # lowest confidence to highest confidence\n",
        "\n",
        "sa=[x[0] for x in s] # p\n",
        "sb=[x[1] for x in s] # img_file\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "matplotlib.rcParams['figure.dpi'] = 300\n",
        "\n",
        "def imshow(img):\n",
        "    # img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.axis('off')\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "from torchvision import datasets, transforms\n",
        "c=3*64\n",
        "images = [Image.open(img_file).convert(\"RGB\") for img_file in sb[c:c+64]]\n",
        "images = [transforms.ToTensor()(im) for im in images]\n",
        "imshow(torchvision.utils.make_grid(images,nrow=8))\n",
        "\n",
        "\n",
        "# print(type(sa[0]))\n",
        "# print(sa[0].values.item())\n",
        "\n",
        "# for i,x in enumerate(sa[:256]):\n",
        "#     # print(x)\n",
        "#     print(i, x.values.item(), sb[i])\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y-Cjw51AedI7",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title collect misclassified\n",
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
        "\n",
        "rongs=[[[] for i in range(6)] for j in range(6)]\n",
        "\n",
        "\n",
        "t=0\n",
        "c=0\n",
        "for cls in range(1,7):\n",
        "    img_dir = '/content/gsv70kg/0'+str(cls)\n",
        "    for filename in os.listdir(img_dir):\n",
        "        # name = os.path.splitext(filename)[0]\n",
        "        img_file=os.path.join(img_dir, filename)\n",
        "        image = Image.open(img_file).convert(\"RGB\")\n",
        "        with torch.no_grad():\n",
        "            with torch.cuda.amp.autocast(): # automatic mixed percision\n",
        "                x = transform(image).to(device).unsqueeze(0)\n",
        "                y = torch.tensor(cls-1, device=device)\n",
        "                pred = model(x)\n",
        "\n",
        "        p = pred.argmax(1)\n",
        "        # print(\"p, y\", p, y)\n",
        "        if p != y:\n",
        "            # print(filename)\n",
        "            c+=1\n",
        "            rongs[y][p].append(filename)\n",
        "\n",
        "        t+=1\n",
        "        # if t >=5: break\n",
        "\n",
        "print(c,t,c*100/t) # correct, total, accuracy\n",
        "# 38767 67938 57.062321528452415\n",
        "\n",
        "# print(rongs)\n",
        "[print([len(s) for s in l]) for l in rongs]\n",
        "\n",
        "\n",
        "# display misclassified images\n",
        "\n",
        "# def imshow(img):\n",
        "#     img = img / 2 + 0.5  # unnormalize\n",
        "#     npimg = img.numpy()\n",
        "    # plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    # plt.show()\n",
        "\n",
        "\n",
        "# combi = torch.combinations(torch.arange(0,6), with_replacement=True)\n",
        "combi = [[i,j] for i in range(6) for j in range(6)]\n",
        "# print(combi)\n",
        "\n",
        "for i, j in combi:\n",
        "    print(\"actual: \", i, \"predicted: \", j)\n",
        "    c=0\n",
        "    images=torch.empty(0)\n",
        "    plt.figure(figsize=(16, 16))\n",
        "    plt.axis('off')\n",
        "    for filename in rongs[i][j]:\n",
        "    # for filename in rongs[5][0]:\n",
        "        # print(filename)\n",
        "        cls=filename[1]\n",
        "        img_dir = '/content/gsv70kg/0'+str(cls)\n",
        "        img_file=os.path.join(img_dir, filename)\n",
        "        image = Image.open(img_file)#.convert(\"RGB\")\n",
        "        image = transform(image).unsqueeze(0)\n",
        "        images=torch.cat((images,image),0)\n",
        "\n",
        "        c+=1\n",
        "        if c>15: break\n",
        "    try: imshow(torchvision.utils.make_grid(images,nrow=4))\n",
        "    except: continue\n",
        "# except: pass\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ayRePbr_rq9F"
      },
      "outputs": [],
      "source": [
        "# pytorch release gpu ram after training\n",
        "# https://discuss.pytorch.org/t/free-all-gpu-memory-used-in-between-runs/168202/2\n",
        "torch.cuda.empty_cache()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title plot lr scheduler\n",
        "lr_lst=[]\n",
        "total_steps=100\n",
        "base_lr, max_lr = 3e-5, 3e-4 # resnet50 batch32 gradacc2\n",
        "\n",
        "optimizer = bnb.optim.AdamW(model.parameters(), lr=base_lr, betas=(0.9, 0.999), optim_bits=8)\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=max_lr, total_steps=total_steps, pct_start=0.45, anneal_strategy='cos', cycle_momentum=True, base_momentum=0.85, max_momentum=0.95, div_factor=div_factor, final_div_factor=100.0, three_phase=True,)\n",
        "\n",
        "for t in range(total_steps):\n",
        "    lr=optimizer.param_groups[0][\"lr\"]\n",
        "    lr_lst.append(lr)\n",
        "    scheduler.step()\n",
        "plt.plot(lr_lst)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "fCYDnCuD3UWo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}