{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eisbetterthanpi/vision/blob/main/resnet_ViT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "2kQGpnynh2oQ"
      },
      "outputs": [],
      "source": [
        "# @title download\n",
        "# original 10k\n",
        "# # https://drive.google.com/file/d/1NkCNecLpFG3i7bo3Vl9RQSwzBpSRQ29q/view?usp=sharing\n",
        "# !gdown 1NkCNecLpFG3i7bo3Vl9RQSwzBpSRQ29q -O gsv.zip\n",
        "# !unzip /content/gsv.zip -d /content\n",
        "# import shutil\n",
        "# shutil.rmtree('/content/google_street_view/meta_data', ignore_errors=True) # delete the meta_data folder\n",
        "\n",
        "# # clip cleaned\n",
        "# # https://drive.google.com/file/d/1-xcHyVAMeTkY7SUdUyQRVAn_FFXPSDGB/view?usp=share_link\n",
        "# !gdown 1-xcHyVAMeTkY7SUdUyQRVAn_FFXPSDGB -O gsv.zip\n",
        "# !unzip /content/gsv.zip -d /\n",
        "# !rm -R /content/gsv/.ipynb_checkpoints\n",
        "# !rm -R /content/gsv/01/.ipynb_checkpoints\n",
        "# !rm -R /content/gsv/02/.ipynb_checkpoints\n",
        "# !rm -R /content/gsv/03/.ipynb_checkpoints\n",
        "# !rm -R /content/gsv/04/.ipynb_checkpoints\n",
        "# !rm -R /content/gsv/05/.ipynb_checkpoints\n",
        "# !rm -R /content/gsv/06/.ipynb_checkpoints\n",
        "\n",
        "# # gsv 70k\n",
        "# # https://drive.google.com/file/d/1-7ZC29k4VxXQkpnOuLfj7Ag_SFTM4LV8/view?usp=share_link\n",
        "# !gdown 1-7ZC29k4VxXQkpnOuLfj7Ag_SFTM4LV8 -O gsv.zip\n",
        "# !unzip /content/gsv.zip -d /\n",
        "\n",
        "# # # !ls\n",
        "# !ls -a /content/gsv70k\n",
        "# !rm -R /content/gsv70k/.ipynb_checkpoints\n",
        "# # # !rm -R /content/gsv/06/.ipynb_checkpoints\n",
        "\n",
        "# # 70k+gmap\n",
        "# # https://drive.google.com/file/d/1-CZp7TbhJLeRQpbKQCyT8ofGg89Yt137/view?usp=sharing\n",
        "!gdown 1-CZp7TbhJLeRQpbKQCyT8ofGg89Yt137 -O gsv.zip\n",
        "!unzip /content/gsv.zip -d /\n",
        "!rm -R /content/gsv70kg/.ipynb_checkpoints\n",
        "!rm -R /content/gsv70kg/01/.ipynb_checkpoints\n",
        "!rm -R /content/gsv70kg/02/.ipynb_checkpoints\n",
        "!rm -R /content/gsv70kg/03/.ipynb_checkpoints\n",
        "!rm -R /content/gsv70kg/04/.ipynb_checkpoints\n",
        "!rm -R /content/gsv70kg/05/.ipynb_checkpoints\n",
        "!rm -R /content/gsv70kg/06/.ipynb_checkpoints\n",
        "\n",
        "# # https://bestasoff.medium.com/how-to-fine-tune-very-large-model-if-it-doesnt-fit-on-your-gpu-3561e50859af\n",
        "!pip install bitsandbytes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "hEUffQ24mkRY"
      },
      "outputs": [],
      "source": [
        "# @title torch augment\n",
        "# https://github.com/facebookresearch/vicreg/blob/main/augmentations.py\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "class TrainTransform(object):\n",
        "    def __init__(self):\n",
        "        self.transform = transforms.RandomApply([transforms.Compose([\n",
        "        # self.transform = transforms.Compose([\n",
        "                transforms.RandomPerspective(distortion_scale=0.3, p=0.5), # me\n",
        "                transforms.RandomResizedCrop((400,640), scale=(0.7, 1.0), ratio=(0.8, 1.25), interpolation=transforms.InterpolationMode.BICUBIC),\n",
        "                transforms.RandomHorizontalFlip(p=0.5), # 0.5\n",
        "                transforms.RandomApply([transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1)], p=0.8,), # brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1)], p=0.8\n",
        "                transforms.RandomGrayscale(p=0.2), # 0.2\n",
        "                # transforms.RandomChoice(transforms.ColorJitter , transforms.RandomGrayscale(p=1.)\n",
        "                transforms.RandomApply([transforms.GaussianBlur(kernel_size=5, sigma=(0.1, 2.0)),], p=1.0),\n",
        "                # transforms.RandomSolarize(threshold=130, p=0.5)\n",
        "                transforms.RandomErasing(p=0.5, scale=(0.1, 0.11), ratio=(1,1), value=0, inplace=True), # default p=0.5, scale=(0.02, 0.33), ratio=(0.3, 3.3), value=0, inplace=False\n",
        "                # transforms.ToTensor(), # ToTensored at dataset level, no need to ToTensor again\n",
        "                # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), # normalised at dataset level. default 0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225\n",
        "                ])\n",
        "            ], p=0.5)\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        dims = len(sample.shape)\n",
        "        if dims==3: x1 = self.transform(sample) # same transforms per minibatch\n",
        "        elif dims==4: x1 = transforms.Lambda(lambda x: torch.stack([self.transform(x_) for x_ in x]))(sample) # diff transforms per img in minibatch\n",
        "        return x1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pqw5n--6WYEG",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title data weighted\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# dataset has PILImage images of range [0, 1], transform them to Tensors of normalized range [-1, 1]\n",
        "# transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
        "# transform = transforms.Compose(transforms.ToTensor())\n",
        "\n",
        "# dir='/content/gsv'\n",
        "dir='/content/gsv70kg'\n",
        "\n",
        "# data = datasets.ImageFolder(dir, transform=transform)\n",
        "data = datasets.ImageFolder(dir, transform=None)\n",
        "torch.manual_seed(0)\n",
        "\n",
        "# split data manually so that can work with weighted random sampler\n",
        "# train_data, test_data = torch.utils.data.random_split(data, [.85,.15])\n",
        "# https://www.scaler.com/topics/pytorch/how-to-split-a-torch-dataset/\n",
        "data_size = len(data)\n",
        "indices = np.arange(data_size)\n",
        "np.random.shuffle(indices)\n",
        "split_index = int(np.floor(0.9 * data_size))\n",
        "train_idx, test_idx = indices[:split_index], indices[split_index:]\n",
        "train_data = torch.utils.data.Subset(data, train_idx)\n",
        "test_data = torch.utils.data.Subset(data, test_idx)\n",
        "targets = np.array(data.targets)\n",
        "train_targets = targets[train_idx]\n",
        "test_targets = targets[test_idx]\n",
        "\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "class DatasetWrap(Dataset):\n",
        "    def __init__(self, dataset, transform=None):\n",
        "        super(DatasetWrap, self).__init__()\n",
        "        self.dataset = dataset\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        x, y = self.dataset[index]\n",
        "        if self.transform:\n",
        "            x = self.transform(x)\n",
        "        return x, y\n",
        "\n",
        "# dataset wrapper in order to apply transforms to train data only\n",
        "# train_data = DatasetWrap(train_data, TrainTransform()) # apply data augmentation to train dataset only\n",
        "train_data = DatasetWrap(train_data, transform) # apply transform during training to use gpu\n",
        "test_data = DatasetWrap(test_data, transform)\n",
        "\n",
        "# use batch size 16 for resnet 152/ vit with grad accumulation\n",
        "# can use batch size 64 for inception v3 without grad accumulation?\n",
        "batch_size = 16 # 64/16\n",
        "num_batches=int(np.ceil(len(train_data)/batch_size))\n",
        "\n",
        "# oversampling\n",
        "# https://stackoverflow.com/questions/62319228/number-of-instances-per-class-in-pytorch-dataset\n",
        "from collections import Counter\n",
        "class_count = torch.tensor(list(Counter(data.targets).values()))\n",
        "weights=1./class_count\n",
        "# weights=sum(class_count)/class_count\n",
        "# print(weights)\n",
        "# https://pytorch.org/docs/stable/data.html#torch.utils.data.WeightedRandomSampler\n",
        "\n",
        "train_weight = weights[train_targets]\n",
        "test_weight = weights[test_targets]\n",
        "# train_sampler = torch.utils.data.WeightedRandomSampler(train_weight, len(train_weight))\n",
        "# test_sampler = torch.utils.data.WeightedRandomSampler(test_weight, len(test_weight))\n",
        "train_sampler = torch.utils.data.WeightedRandomSampler(train_weight, 70000)\n",
        "test_sampler = torch.utils.data.WeightedRandomSampler(test_weight, 7000)\n",
        "train_loader = torch.utils.data.DataLoader(train_data, sampler=train_sampler, batch_size=batch_size, num_workers=4, pin_memory=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, sampler=test_sampler, batch_size=batch_size, num_workers=4, pin_memory=True)\n",
        "data, train_data, test_data = None, None, None\n",
        "\n",
        "# test oversampling: occurence of each class should be roughly equal\n",
        "# c=0\n",
        "# print(len(test_loader))\n",
        "# # for batch, (x, y) in enumerate(train_loader):\n",
        "# for batch, (x, y) in enumerate(test_loader):\n",
        "#     print(torch.bincount(y)) # torch count number of elements with value in tensor\n",
        "#     c+=1\n",
        "#     if c>5: break\n",
        "\n",
        "# display img from torch tensor\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "# dataiter = iter(train_loader)\n",
        "# images, labels = next(dataiter)\n",
        "# imshow(torchvision.utils.make_grid(images,nrow=4))\n",
        "# dataiter=None\n",
        "# print(labels)\n",
        "\n",
        "# dataiter = iter(test_loader)\n",
        "# images, labels = next(dataiter)\n",
        "# imshow(torchvision.utils.make_grid(images,nrow=4))\n",
        "# dataiter=None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7oYDr8kuA5Bl"
      },
      "outputs": [],
      "source": [
        "# @title resnet\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "# # https://pytorch.org/vision/0.12/models.html#id10\n",
        "model = models.resnet152(weights='DEFAULT') # 18 34 50 101 152\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Sequential( # og (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
        "    nn.Linear(num_ftrs, 6),\n",
        "    nn.Softmax(dim=1),\n",
        "    )\n",
        "\n",
        "# # model.mods = [module for k, module in model._modules.items()]\n",
        "# # modules = [module for k, module in model._modules.items()]\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# model = model.to(device)\n",
        "model = torch.compile(model.to(device))\n",
        "# model = torch.compile(model.to(device),mode='reduce-overhead')\n",
        "# model = torch.compile(model.to(device),mode='max-autotune')\n",
        "\n",
        "# resnet152 batch16 compile gradacc nogradckpt lr3e-6,3e-5\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rJw9_Ort2Sek"
      },
      "outputs": [],
      "source": [
        "# @title vit\n",
        "# https://arxiv.org/pdf/2010.11929.pdf\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "# https://pytorch.org/vision/main/models/vision_transformer.html\n",
        "# https://pytorch.org/vision/main/models/generated/torchvision.models.vit_b_16.html#torchvision.models.vit_b_16\n",
        "# model = models.vit_l_16(weights='DEFAULT') # small vit_b_16 vit_b_32 vit_l_16 vit_l_32 vit_h_14 big\n",
        "# # VisionTransformer(image_size, patch_size, num_layers, num_heads, hidden_dim, mlp_dim)\n",
        "# num_ftrs = model.heads.head.in_features\n",
        "# # num_ftrs = model.heads[-1].in_features\n",
        "# model.heads = nn.Sequential(\n",
        "#     # nn.Dropout(0.2),\n",
        "#     nn.Linear(num_ftrs, 6, bias=False),\n",
        "#     nn.Softmax(dim=1),\n",
        "#     )\n",
        "\n",
        "\n",
        "!pip install timm\n",
        "# https://github.com/huggingface/pytorch-image-models/issues/908\n",
        "import timm\n",
        "# model = timm.create_model('vit_base_patch16_224', pretrained=True)\n",
        "model = timm.create_model('vit_base_patch16_224', img_size=(400, 640), pretrained=True)\n",
        "# [print(x) for x in timm.list_models('vit*',pretrained=True)]\n",
        "# https://huggingface.co/google/vit-base-patch16-224\n",
        "# https://github.com/huggingface/pytorch-image-models/blob/main/timm/models/vision_transformer.py\n",
        "# vit_base_patch16_224 compile,no ckpt # patch_size=16, embed_dim=768, depth=12, num_heads=12\n",
        "# vit_base_patch16_384\n",
        "# vit_large_patch16_224 explodesgpu # patch_size=16, embed_dim=1024, depth=24, num_heads=16\n",
        "# vit_large_patch16_384 # patch_size=16, embed_dim=1024, depth=24, num_heads=16\n",
        "\n",
        "# or fine tune huge\n",
        "# vit_large_patch14_224 # patch_size=16, embed_dim=1024, depth=24, num_heads=16\n",
        "# vit_large_patch16_384\n",
        "\n",
        "\n",
        "num_ftrs = model.head.in_features\n",
        "model.head = nn.Sequential(\n",
        "    nn.Linear(num_ftrs, 6),\n",
        "    nn.Softmax(dim=1),\n",
        "    )\n",
        "# model.set_grad_checkpointing()\n",
        "\n",
        "# print(model.patch_embed.grid_size) # (25, 40)\n",
        "# print(model.pos_embed.shape) # [1, 1001, 768]\n",
        "# https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py\n",
        "\n",
        "# print(model)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# model = model.to(device)\n",
        "model = torch.compile(model.to(device))\n",
        "# model = torch.compile(model.to(device),mode='reduce-overhead')\n",
        "# model = torch.compile(model.to(device),mode='max-autotune')\n",
        "\n",
        "# vit_base_patch16_224 batch16 maxcompile nockpt gradacc lr1e-5,1e-4\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-oKYpG8n2fBI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48769e4d-fe25-4a0e-da67-97b0091208a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/inception_v3_google-0cc3c7bd.pth\" to /root/.cache/torch/hub/checkpoints/inception_v3_google-0cc3c7bd.pth\n",
            "100%|██████████| 104M/104M [00:00<00:00, 208MB/s]\n"
          ]
        }
      ],
      "source": [
        "# @title inception\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "# # https://pytorch.org/vision/0.12/models.html#id10\n",
        "model = models.inception_v3(pretrained=True)\n",
        "# https://discuss.pytorch.org/t/inception-v3-is-not-working-very-well/38296/16\n",
        "# https://colab.research.google.com/github/CaoCharles/Deep-Learning-with-PyTorch/blob/master/2_Inception.ipynb\n",
        "model.aux_logits = False\n",
        "num_ftrs = model.fc.in_features # 2048\n",
        "model.fc = nn.Sequential(\n",
        "    nn.Linear(num_ftrs, 6), # og: (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
        "    # nn.Linear(num_ftrs, 128), nn.ReLU(), nn.Linear(128, 6),\n",
        "    nn.Softmax(dim=1),\n",
        "    )\n",
        "# pytorch \"inception\" v3 \"gradient checkpointing\" https://github.com/jianweif/OptimalGradCheckpointing\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# model = model.to(device)\n",
        "model = torch.compile(model.to(device))\n",
        "# model = torch.compile(model.to(device),mode='max-autotune')\n",
        "\n",
        "# inception batch64 compile nogradacc nogradckpt lr3e-6,3e-5\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "vEZCFg5YSS9J"
      },
      "outputs": [],
      "source": [
        "# @title try\n",
        "\n",
        "# # check model's input and output dimensions are correct\n",
        "# X = torch.rand(64, 3, 32, 32, device=device)\n",
        "X = torch.rand(16, 3, 400, 640, device=device)\n",
        "# X = torch.rand(16, 3, 224, 224, device=device)\n",
        "model.eval()\n",
        "\n",
        "# 224x224\n",
        "# 16x16 / 32x32 patch\n",
        "# -> 14x14=196 7x7=49 seq length\n",
        "# 400x640 -> 25x40=1000 seq length\n",
        "\n",
        "\n",
        "logits = model(X)\n",
        "\n",
        "# modules = [module for k, module in model._modules.items()]\n",
        "# for i,x in enumerate(modules):\n",
        "#     print(i,x)\n",
        "\n",
        "# logits = checkpoint_sequential(functions=modules, segments=1, input=X)\n",
        "\n",
        "print(logits.shape)\n",
        "# print(logits[0])\n",
        "# print(logits[0].argmax(1))\n",
        "\n",
        "pred_probab = nn.Softmax(dim=1)(logits)\n",
        "y_pred = pred_probab.argmax(1)\n",
        "print(y_pred)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fsealXK3OPQa",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title train test function\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "# https://github.com/prigoyal/pytorch_memonger/blob/master/models/optimized/resnet_new.py\n",
        "from torch.utils.checkpoint import checkpoint, checkpoint_sequential\n",
        "\n",
        "trs=TrainTransform()\n",
        "# train function with automatic mixed precision\n",
        "def strain(dataloader, model, loss_fn, optimizer, scheduler=None, verbose=True):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    loss_list = []\n",
        "    for batch, (x, y) in enumerate(dataloader):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        with torch.cuda.amp.autocast(): # automatic mixed percision\n",
        "            x = trs(x) # image augmentation during train time to use gpu\n",
        "            pred = model(x) # default\n",
        "\n",
        "            # modules = [module for k, module in model._modules.items()]\n",
        "            # pred = checkpoint_sequential(functions=modules, segments=1, input=x) # gradient checkpointing for resnet and inception only\n",
        "            # # # pred = checkpoint_sequential(functions=model.mods, segments=1, input=x)\n",
        "\n",
        "            loss = loss_fn(pred, y) # /4 to scale by gradient accumulation_steps? no?\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        if ((batch + 1) % 4 == 0) or (batch + 1 == len(dataloader)): # gradient accumulation\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            optimizer.zero_grad()\n",
        "            if scheduler is not None:\n",
        "                scheduler.step()\n",
        "\n",
        "        loss_list.append(loss.item())\n",
        "        # loss.backward()\n",
        "        # optimizer.step()\n",
        "        # if (batch) % (size//(10* len(x))) == 0:\n",
        "        # if batch % ((dataloader.sampler.num_samples/dataloader.batch_size)//10) == 0:\n",
        "        # loss, current = loss.item(), batch * len(x)\n",
        "        loss, current = loss.item()/len(y), batch * len(x)\n",
        "        # if verbose: print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "        if verbose: print(f\"loss: {loss:>7f} [{current:>5d}/{dataloader.sampler.num_samples:>5d}]\")\n",
        "    return loss_list\n",
        "\n",
        "def test(dataloader, model, loss_fn, verbose=True):\n",
        "    # size = len(dataloader.dataset)\n",
        "    # num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            x, y = X.to(device), y.to(device)\n",
        "            pred = model(x)\n",
        "            loss = loss_fn(pred, y)\n",
        "            # predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    # test_loss /= num_batches\n",
        "    # correct /= size\n",
        "    test_loss /= dataloader.sampler.num_samples\n",
        "    correct /= dataloader.sampler.num_samples\n",
        "    # if verbose: print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "    if verbose: print(f\"Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f}\")\n",
        "    return correct, test_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iuumbm2SB_lX",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title LR range test\n",
        "# gives insight into good LR range to use. \n",
        "# for accurate results, be sure to use a new model for range test; \n",
        "# also reset the model before training bec range test destroys the model\n",
        "# 1cycle super convergencehttps://arxiv.org/pdf/1708.07120.pdf\n",
        "# # cyclic lr https://arxiv.org/pdf/1506.01186.pdf\n",
        "# Note the learning rate value when the accuracy starts to\n",
        "# increase and when the accuracy slows, becomes ragged, or starts to fall\n",
        "\n",
        "# one training run of the network for a few epochs\n",
        "# pth='/content/lr.pth'\n",
        "# torch.save(model.state_dict(), pth) # save temporary model for lr finding\n",
        "# model.load_state_dict(torch.load(\"lr.pth\"))\n",
        "torch.autograd.set_detect_anomaly(True)\n",
        "epochs=1\n",
        "min_lr= 1e-6\n",
        "max_lr= 1e-3 # 1e-2\n",
        "# 152: 1e-7 - 1e-4      result 3e-7 - 3e-6\n",
        "# inception: 1e-7 - 1e1      result 3e-7 - 3e-6\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "# optimizer = torch.optim.AdamW(model.parameters(), lr=start_lr, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01)\n",
        "# optimizer = torch.optim.SGD(model.parameters(), lr=start_lr, momentum=0.9)\n",
        "import bitsandbytes as bnb\n",
        "optimizer = bnb.optim.AdamW(model.parameters(), lr=min_lr, betas=(0.9, 0.999), optim_bits=8)\n",
        "\n",
        "num_batches=len(test_loader)\n",
        "# num_batches=len(train_loader)\n",
        "\n",
        "total_steps=int(num_batches*epochs)\n",
        "# total_steps=int(np.ceil(num_batches/4)*epochs)\n",
        "\n",
        "# min_lr* gamma^total_steps = max_lr\n",
        "gamma = np.exp(np.log(max_lr/min_lr)/total_steps) # for scheduler step every optimizer step\n",
        "lr_list=np.ones(total_steps)*min_lr*gamma**np.arange(total_steps)\n",
        "# gamma = np.exp(np.log(max_lr/min_lr)/(total_steps*4)) # total_steps*4 bec grad accumulation, loss step 4x per lr step\n",
        "# lr_list=np.ones(total_steps*4)*min_lr*gamma**np.arange(total_steps*4)\n",
        "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=gamma) # 0.75(20)-0.9(100)\n",
        "train_lst, test_lst=[],[]\n",
        "\n",
        "\n",
        "for i in range(epochs):\n",
        "    train_ls = strain(test_loader, model, loss_fn, optimizer, scheduler)    \n",
        "    # train_ls = strain(train_loader, model, loss_fn, optimizer, scheduler)    \n",
        "    train_lst.extend(train_ls)\n",
        "\n",
        "# https://stackoverflow.com/a/53472966/13359815\n",
        "from scipy.ndimage.filters import gaussian_filter1d\n",
        "train_lstsm = gaussian_filter1d(train_lst, sigma=30)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(lr_list, train_lst)\n",
        "plt.plot(lr_list, train_lstsm)\n",
        "plt.xscale('log')\n",
        "# plt.plot(train_lst)\n",
        "# plt.plot(np.linspace(0,len(train_lst),len(test_lst)), test_lst)\n",
        "plt.show()\n",
        "plt.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title wwwwwwwww\n",
        "acc_lst, train_lst, test_lst=[],[],[]\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "base_lr, max_lr = 3e-7, 3e-6 # resnet\n",
        "# base_lr, max_lr = 1e-5, 1e-2 # vit\n",
        "# base_lr, max_lr = 3e-6, 3e-5 # inception\n",
        "# end_lr, start_lr = 1e-5, 1e-3 # 0.0001,0.1\n",
        "tp=0\n",
        "epochs = 5 #5 20\n",
        "# optimizer = torch.optim.SGD(model.parameters(), lr = start_lr, momentum=0.9)\n",
        "# optimizer = torch.optim.AdamW(model.parameters(), lr=base_lr, betas=(0.9, 0.999), eps=1e-08, weight_decay=3e-6)\n",
        "import bitsandbytes as bnb # 8bit optimizer\n",
        "optimizer = bnb.optim.AdamW(model.parameters(), lr=base_lr, betas=(0.9, 0.999), optim_bits=8)\n",
        "# optimizer = bnb.optim.AdamW(model.parameters(), lr=1e-5, betas=(0.9, 0.999), optim_bits=8)\n",
        "# 152 1e-5\n",
        "# cnn 3e-4\n",
        "\n",
        "div_factor = max_lr/base_lr\n",
        "num_batches=len(train_loader)\n",
        "# total_steps=int(num_batches*epochs)+1 # +1 to excluse uptick at the end of onecycle\n",
        "total_steps=int(np.ceil(num_batches/4)*epochs +1) # /4 for when using grad accumulation\n",
        "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=max_lr, total_steps=total_steps, pct_start=0.45, anneal_strategy='cos', cycle_momentum=True, base_momentum=0.85, max_momentum=0.95, div_factor=div_factor, final_div_factor=100.0, three_phase=True,)\n",
        "# scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=max_lr, epochs=epochs, steps_per_epoch=num_batches, pct_start=0.45, anneal_strategy='cos', cycle_momentum=True, base_momentum=0.85, max_momentum=0.95, div_factor=div_factor, final_div_factor=10000.0, three_phase=True,)\n",
        "# gamma = np.exp(np.log(end_lr/start_lr)/(num_batches*epochs)) # for scheduler step every optimizer step\n",
        "# scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=gamma) # 0.75(20)-0.9(100)\n",
        "\n",
        "# pth='/content/res15270kg.pth'\n",
        "pth='/content/res15270kg05aug.pth' # sps\n",
        "\n",
        "# pth='/content/drive/MyDrive/frame/vit.pth'\n",
        "# pth='/content/drive/MyDrive/frame/vit3736.pth'\n",
        "pth='/content/drive/MyDrive/frame/vit161508aug.pth' # A\n",
        "\n",
        "# pth='/content/drive/MyDrive/frame/inception1.pth'\n",
        "pth='/content/drive/MyDrive/frame/inception363505aug.pth' # ty\n",
        "\n",
        "\n",
        "# # to continue training\n",
        "# tp, modelsd, optimsd, schedsd = torch.load(pth).values()\n",
        "# model.load_state_dict(modelsd)\n",
        "# optimizer.load_state_dict(optimsd)\n",
        "# scheduler.load_state_dict(schedsd)\n",
        "\n"
      ],
      "metadata": {
        "id": "wkBp7WjC8LLA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDBEk-l-Oxjn",
        "outputId": "9dcc01f1-0f59-40c9-eb70-272e9371d1f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "3.000000000000001e-07\n",
            "loss: 0.087659 [    0/70000]\n",
            "loss: 0.085325 [   16/70000]\n",
            "loss: 0.089948 [   32/70000]\n",
            "loss: 0.094672 [   48/70000]\n",
            "loss: 0.083321 [   64/70000]\n",
            "loss: 0.095851 [   80/70000]\n",
            "loss: 0.095427 [   96/70000]\n",
            "loss: 0.072946 [  112/70000]\n",
            "loss: 0.084744 [  128/70000]\n",
            "loss: 0.088063 [  144/70000]\n",
            "loss: 0.082541 [  160/70000]\n",
            "loss: 0.085507 [  176/70000]\n",
            "loss: 0.089179 [  192/70000]\n",
            "loss: 0.082221 [  208/70000]\n",
            "loss: 0.092031 [  224/70000]\n",
            "loss: 0.092860 [  240/70000]\n",
            "loss: 0.081482 [  256/70000]\n",
            "loss: 0.090427 [  272/70000]\n",
            "loss: 0.087398 [  288/70000]\n",
            "loss: 0.086880 [  304/70000]\n",
            "loss: 0.077834 [  320/70000]\n",
            "loss: 0.082740 [  336/70000]\n",
            "loss: 0.093418 [  352/70000]\n",
            "loss: 0.092308 [  368/70000]\n",
            "loss: 0.087848 [  384/70000]\n",
            "loss: 0.084858 [  400/70000]\n",
            "loss: 0.074539 [  416/70000]\n",
            "loss: 0.109645 [  432/70000]\n",
            "loss: 0.086421 [  448/70000]\n",
            "loss: 0.084082 [  464/70000]\n",
            "loss: 0.090399 [  480/70000]\n",
            "loss: 0.100385 [  496/70000]\n",
            "loss: 0.074906 [  512/70000]\n",
            "loss: 0.092828 [  528/70000]\n",
            "loss: 0.082959 [  544/70000]\n",
            "loss: 0.094566 [  560/70000]\n",
            "loss: 0.096780 [  576/70000]\n",
            "loss: 0.095334 [  592/70000]\n",
            "loss: 0.085787 [  608/70000]\n",
            "loss: 0.099179 [  624/70000]\n",
            "loss: 0.101331 [  640/70000]\n",
            "loss: 0.084508 [  656/70000]\n",
            "loss: 0.087710 [  672/70000]\n",
            "loss: 0.089592 [  688/70000]\n",
            "loss: 0.099069 [  704/70000]\n",
            "loss: 0.083562 [  720/70000]\n",
            "loss: 0.084521 [  736/70000]\n",
            "loss: 0.094622 [  752/70000]\n",
            "loss: 0.082918 [  768/70000]\n",
            "loss: 0.092362 [  784/70000]\n",
            "loss: 0.088288 [  800/70000]\n",
            "loss: 0.089013 [  816/70000]\n",
            "loss: 0.086600 [  832/70000]\n",
            "loss: 0.089200 [  848/70000]\n",
            "loss: 0.089592 [  864/70000]\n",
            "loss: 0.081708 [  880/70000]\n",
            "loss: 0.073932 [  896/70000]\n",
            "loss: 0.087550 [  912/70000]\n",
            "loss: 0.082538 [  928/70000]\n",
            "loss: 0.084543 [  944/70000]\n",
            "loss: 0.082597 [  960/70000]\n",
            "loss: 0.087768 [  976/70000]\n",
            "loss: 0.095192 [  992/70000]\n",
            "loss: 0.083898 [ 1008/70000]\n",
            "loss: 0.088134 [ 1024/70000]\n",
            "loss: 0.075612 [ 1040/70000]\n",
            "loss: 0.085742 [ 1056/70000]\n",
            "loss: 0.089306 [ 1072/70000]\n",
            "loss: 0.082940 [ 1088/70000]\n",
            "loss: 0.074088 [ 1104/70000]\n",
            "loss: 0.092317 [ 1120/70000]\n",
            "loss: 0.087719 [ 1136/70000]\n",
            "loss: 0.092679 [ 1152/70000]\n",
            "loss: 0.096374 [ 1168/70000]\n",
            "loss: 0.088435 [ 1184/70000]\n",
            "loss: 0.079105 [ 1200/70000]\n",
            "loss: 0.081719 [ 1216/70000]\n",
            "loss: 0.094816 [ 1232/70000]\n",
            "loss: 0.081811 [ 1248/70000]\n",
            "loss: 0.091706 [ 1264/70000]\n",
            "loss: 0.093458 [ 1280/70000]\n",
            "loss: 0.088671 [ 1296/70000]\n",
            "loss: 0.074691 [ 1312/70000]\n",
            "loss: 0.078567 [ 1328/70000]\n",
            "loss: 0.088895 [ 1344/70000]\n",
            "loss: 0.078244 [ 1360/70000]\n",
            "loss: 0.097496 [ 1376/70000]\n",
            "loss: 0.082381 [ 1392/70000]\n",
            "loss: 0.083689 [ 1408/70000]\n",
            "loss: 0.089848 [ 1424/70000]\n",
            "loss: 0.088182 [ 1440/70000]\n",
            "loss: 0.085729 [ 1456/70000]\n",
            "loss: 0.086591 [ 1472/70000]\n",
            "loss: 0.093220 [ 1488/70000]\n",
            "loss: 0.085429 [ 1504/70000]\n",
            "loss: 0.101355 [ 1520/70000]\n",
            "loss: 0.094801 [ 1536/70000]\n",
            "loss: 0.085161 [ 1552/70000]\n",
            "loss: 0.092041 [ 1568/70000]\n",
            "loss: 0.095579 [ 1584/70000]\n",
            "loss: 0.081332 [ 1600/70000]\n",
            "loss: 0.091298 [ 1616/70000]\n",
            "loss: 0.093359 [ 1632/70000]\n",
            "loss: 0.095408 [ 1648/70000]\n",
            "loss: 0.082576 [ 1664/70000]\n",
            "loss: 0.081986 [ 1680/70000]\n",
            "loss: 0.084029 [ 1696/70000]\n",
            "loss: 0.084947 [ 1712/70000]\n",
            "loss: 0.093539 [ 1728/70000]\n",
            "loss: 0.093824 [ 1744/70000]\n",
            "loss: 0.087320 [ 1760/70000]\n",
            "loss: 0.085012 [ 1776/70000]\n",
            "loss: 0.084948 [ 1792/70000]\n",
            "loss: 0.098101 [ 1808/70000]\n",
            "loss: 0.084063 [ 1824/70000]\n",
            "loss: 0.089157 [ 1840/70000]\n",
            "loss: 0.096726 [ 1856/70000]\n",
            "loss: 0.084666 [ 1872/70000]\n",
            "loss: 0.098941 [ 1888/70000]\n",
            "loss: 0.088005 [ 1904/70000]\n",
            "loss: 0.089590 [ 1920/70000]\n",
            "loss: 0.083454 [ 1936/70000]\n",
            "loss: 0.085066 [ 1952/70000]\n",
            "loss: 0.085380 [ 1968/70000]\n",
            "loss: 0.090695 [ 1984/70000]\n",
            "loss: 0.081804 [ 2000/70000]\n",
            "loss: 0.085734 [ 2016/70000]\n",
            "loss: 0.095146 [ 2032/70000]\n",
            "loss: 0.086731 [ 2048/70000]\n",
            "loss: 0.083898 [ 2064/70000]\n",
            "loss: 0.088709 [ 2080/70000]\n",
            "loss: 0.097601 [ 2096/70000]\n",
            "loss: 0.084159 [ 2112/70000]\n",
            "loss: 0.086631 [ 2128/70000]\n",
            "loss: 0.083261 [ 2144/70000]\n",
            "loss: 0.099355 [ 2160/70000]\n",
            "loss: 0.087873 [ 2176/70000]\n",
            "loss: 0.084229 [ 2192/70000]\n",
            "loss: 0.094638 [ 2208/70000]\n",
            "loss: 0.088626 [ 2224/70000]\n",
            "loss: 0.086752 [ 2240/70000]\n",
            "loss: 0.094699 [ 2256/70000]\n",
            "loss: 0.087873 [ 2272/70000]\n",
            "loss: 0.098859 [ 2288/70000]\n",
            "loss: 0.087829 [ 2304/70000]\n",
            "loss: 0.079931 [ 2320/70000]\n",
            "loss: 0.088123 [ 2336/70000]\n",
            "loss: 0.099835 [ 2352/70000]\n",
            "loss: 0.112784 [ 2368/70000]\n",
            "loss: 0.086466 [ 2384/70000]\n",
            "loss: 0.083482 [ 2400/70000]\n",
            "loss: 0.089144 [ 2416/70000]\n",
            "loss: 0.086187 [ 2432/70000]\n",
            "loss: 0.078193 [ 2448/70000]\n",
            "loss: 0.083942 [ 2464/70000]\n",
            "loss: 0.084275 [ 2480/70000]\n",
            "loss: 0.091008 [ 2496/70000]\n",
            "loss: 0.100032 [ 2512/70000]\n",
            "loss: 0.078010 [ 2528/70000]\n",
            "loss: 0.079335 [ 2544/70000]\n",
            "loss: 0.086013 [ 2560/70000]\n",
            "loss: 0.082356 [ 2576/70000]\n",
            "loss: 0.092364 [ 2592/70000]\n",
            "loss: 0.094608 [ 2608/70000]\n",
            "loss: 0.092458 [ 2624/70000]\n",
            "loss: 0.096690 [ 2640/70000]\n",
            "loss: 0.081591 [ 2656/70000]\n",
            "loss: 0.080564 [ 2672/70000]\n",
            "loss: 0.082126 [ 2688/70000]\n",
            "loss: 0.080000 [ 2704/70000]\n",
            "loss: 0.078443 [ 2720/70000]\n",
            "loss: 0.081068 [ 2736/70000]\n",
            "loss: 0.079962 [ 2752/70000]\n",
            "loss: 0.095979 [ 2768/70000]\n",
            "loss: 0.097375 [ 2784/70000]\n",
            "loss: 0.080664 [ 2800/70000]\n",
            "loss: 0.092955 [ 2816/70000]\n",
            "loss: 0.082571 [ 2832/70000]\n",
            "loss: 0.075504 [ 2848/70000]\n",
            "loss: 0.086678 [ 2864/70000]\n",
            "loss: 0.085186 [ 2880/70000]\n",
            "loss: 0.079333 [ 2896/70000]\n",
            "loss: 0.090124 [ 2912/70000]\n",
            "loss: 0.093078 [ 2928/70000]\n",
            "loss: 0.089701 [ 2944/70000]\n",
            "loss: 0.090092 [ 2960/70000]\n",
            "loss: 0.086967 [ 2976/70000]\n",
            "loss: 0.100952 [ 2992/70000]\n",
            "loss: 0.088233 [ 3008/70000]\n",
            "loss: 0.086657 [ 3024/70000]\n",
            "loss: 0.090253 [ 3040/70000]\n",
            "loss: 0.082553 [ 3056/70000]\n",
            "loss: 0.079129 [ 3072/70000]\n",
            "loss: 0.095327 [ 3088/70000]\n",
            "loss: 0.087105 [ 3104/70000]\n",
            "loss: 0.088264 [ 3120/70000]\n",
            "loss: 0.089732 [ 3136/70000]\n",
            "loss: 0.086154 [ 3152/70000]\n",
            "loss: 0.073842 [ 3168/70000]\n",
            "loss: 0.079165 [ 3184/70000]\n",
            "loss: 0.085984 [ 3200/70000]\n",
            "loss: 0.093002 [ 3216/70000]\n",
            "loss: 0.092979 [ 3232/70000]\n",
            "loss: 0.084535 [ 3248/70000]\n",
            "loss: 0.089531 [ 3264/70000]\n",
            "loss: 0.085920 [ 3280/70000]\n",
            "loss: 0.082718 [ 3296/70000]\n",
            "loss: 0.079790 [ 3312/70000]\n",
            "loss: 0.075238 [ 3328/70000]\n",
            "loss: 0.083979 [ 3344/70000]\n",
            "loss: 0.095176 [ 3360/70000]\n",
            "loss: 0.086466 [ 3376/70000]\n",
            "loss: 0.093385 [ 3392/70000]\n",
            "loss: 0.084694 [ 3408/70000]\n",
            "loss: 0.083985 [ 3424/70000]\n",
            "loss: 0.084844 [ 3440/70000]\n",
            "loss: 0.093190 [ 3456/70000]\n",
            "loss: 0.077989 [ 3472/70000]\n",
            "loss: 0.095601 [ 3488/70000]\n",
            "loss: 0.100307 [ 3504/70000]\n",
            "loss: 0.088852 [ 3520/70000]\n",
            "loss: 0.082269 [ 3536/70000]\n",
            "loss: 0.085646 [ 3552/70000]\n",
            "loss: 0.094302 [ 3568/70000]\n",
            "loss: 0.082694 [ 3584/70000]\n",
            "loss: 0.083026 [ 3600/70000]\n",
            "loss: 0.091172 [ 3616/70000]\n",
            "loss: 0.078366 [ 3632/70000]\n",
            "loss: 0.087675 [ 3648/70000]\n",
            "loss: 0.089036 [ 3664/70000]\n",
            "loss: 0.084995 [ 3680/70000]\n",
            "loss: 0.092491 [ 3696/70000]\n",
            "loss: 0.090035 [ 3712/70000]\n",
            "loss: 0.092277 [ 3728/70000]\n",
            "loss: 0.080244 [ 3744/70000]\n",
            "loss: 0.081766 [ 3760/70000]\n",
            "loss: 0.089854 [ 3776/70000]\n",
            "loss: 0.082341 [ 3792/70000]\n",
            "loss: 0.091080 [ 3808/70000]\n",
            "loss: 0.095357 [ 3824/70000]\n",
            "loss: 0.085868 [ 3840/70000]\n",
            "loss: 0.085418 [ 3856/70000]\n",
            "loss: 0.083854 [ 3872/70000]\n",
            "loss: 0.079560 [ 3888/70000]\n",
            "loss: 0.085232 [ 3904/70000]\n",
            "loss: 0.079391 [ 3920/70000]\n",
            "loss: 0.087596 [ 3936/70000]\n",
            "loss: 0.084051 [ 3952/70000]\n",
            "loss: 0.091356 [ 3968/70000]\n",
            "loss: 0.084163 [ 3984/70000]\n",
            "loss: 0.089057 [ 4000/70000]\n",
            "loss: 0.083907 [ 4016/70000]\n",
            "loss: 0.085453 [ 4032/70000]\n",
            "loss: 0.085429 [ 4048/70000]\n",
            "loss: 0.089828 [ 4064/70000]\n",
            "loss: 0.083533 [ 4080/70000]\n",
            "loss: 0.082752 [ 4096/70000]\n",
            "loss: 0.089625 [ 4112/70000]\n",
            "loss: 0.083510 [ 4128/70000]\n",
            "loss: 0.091720 [ 4144/70000]\n",
            "loss: 0.091371 [ 4160/70000]\n",
            "loss: 0.091035 [ 4176/70000]\n",
            "loss: 0.087359 [ 4192/70000]\n",
            "loss: 0.081509 [ 4208/70000]\n",
            "loss: 0.086464 [ 4224/70000]\n",
            "loss: 0.088217 [ 4240/70000]\n",
            "loss: 0.089288 [ 4256/70000]\n",
            "loss: 0.081718 [ 4272/70000]\n",
            "loss: 0.088424 [ 4288/70000]\n",
            "loss: 0.086112 [ 4304/70000]\n",
            "loss: 0.079400 [ 4320/70000]\n",
            "loss: 0.082153 [ 4336/70000]\n",
            "loss: 0.092560 [ 4352/70000]\n",
            "loss: 0.082718 [ 4368/70000]\n",
            "loss: 0.086077 [ 4384/70000]\n",
            "loss: 0.083483 [ 4400/70000]\n",
            "loss: 0.090585 [ 4416/70000]\n",
            "loss: 0.078534 [ 4432/70000]\n",
            "loss: 0.089917 [ 4448/70000]\n",
            "loss: 0.088925 [ 4464/70000]\n",
            "loss: 0.082914 [ 4480/70000]\n",
            "loss: 0.094738 [ 4496/70000]\n",
            "loss: 0.085201 [ 4512/70000]\n",
            "loss: 0.085293 [ 4528/70000]\n",
            "loss: 0.089872 [ 4544/70000]\n",
            "loss: 0.086061 [ 4560/70000]\n",
            "loss: 0.089650 [ 4576/70000]\n",
            "loss: 0.105670 [ 4592/70000]\n",
            "loss: 0.083709 [ 4608/70000]\n",
            "loss: 0.085392 [ 4624/70000]\n",
            "loss: 0.087454 [ 4640/70000]\n",
            "loss: 0.090436 [ 4656/70000]\n",
            "loss: 0.085425 [ 4672/70000]\n",
            "loss: 0.094745 [ 4688/70000]\n",
            "loss: 0.102093 [ 4704/70000]\n",
            "loss: 0.084086 [ 4720/70000]\n",
            "loss: 0.095501 [ 4736/70000]\n",
            "loss: 0.090692 [ 4752/70000]\n",
            "loss: 0.087658 [ 4768/70000]\n",
            "loss: 0.089106 [ 4784/70000]\n",
            "loss: 0.083535 [ 4800/70000]\n",
            "loss: 0.090690 [ 4816/70000]\n",
            "loss: 0.097114 [ 4832/70000]\n",
            "loss: 0.094055 [ 4848/70000]\n",
            "loss: 0.091013 [ 4864/70000]\n",
            "loss: 0.095156 [ 4880/70000]\n",
            "loss: 0.079145 [ 4896/70000]\n",
            "loss: 0.096922 [ 4912/70000]\n",
            "loss: 0.079714 [ 4928/70000]\n",
            "loss: 0.101791 [ 4944/70000]\n",
            "loss: 0.092846 [ 4960/70000]\n",
            "loss: 0.092777 [ 4976/70000]\n",
            "loss: 0.094970 [ 4992/70000]\n",
            "loss: 0.079393 [ 5008/70000]\n",
            "loss: 0.093567 [ 5024/70000]\n",
            "loss: 0.088814 [ 5040/70000]\n",
            "loss: 0.086091 [ 5056/70000]\n",
            "loss: 0.084020 [ 5072/70000]\n",
            "loss: 0.094622 [ 5088/70000]\n",
            "loss: 0.095468 [ 5104/70000]\n",
            "loss: 0.074557 [ 5120/70000]\n",
            "loss: 0.093870 [ 5136/70000]\n",
            "loss: 0.082121 [ 5152/70000]\n",
            "loss: 0.101532 [ 5168/70000]\n",
            "loss: 0.077448 [ 5184/70000]\n",
            "loss: 0.091539 [ 5200/70000]\n",
            "loss: 0.087750 [ 5216/70000]\n",
            "loss: 0.076538 [ 5232/70000]\n",
            "loss: 0.081559 [ 5248/70000]\n",
            "loss: 0.084029 [ 5264/70000]\n",
            "loss: 0.087168 [ 5280/70000]\n",
            "loss: 0.099853 [ 5296/70000]\n",
            "loss: 0.082774 [ 5312/70000]\n",
            "loss: 0.085946 [ 5328/70000]\n",
            "loss: 0.097901 [ 5344/70000]\n",
            "loss: 0.083152 [ 5360/70000]\n",
            "loss: 0.087249 [ 5376/70000]\n",
            "loss: 0.083556 [ 5392/70000]\n",
            "loss: 0.088065 [ 5408/70000]\n",
            "loss: 0.088547 [ 5424/70000]\n",
            "loss: 0.091100 [ 5440/70000]\n",
            "loss: 0.083218 [ 5456/70000]\n",
            "loss: 0.088878 [ 5472/70000]\n",
            "loss: 0.082661 [ 5488/70000]\n",
            "loss: 0.084466 [ 5504/70000]\n",
            "loss: 0.087931 [ 5520/70000]\n",
            "loss: 0.097627 [ 5536/70000]\n",
            "loss: 0.082800 [ 5552/70000]\n",
            "loss: 0.085669 [ 5568/70000]\n",
            "loss: 0.082950 [ 5584/70000]\n",
            "loss: 0.092822 [ 5600/70000]\n",
            "loss: 0.079894 [ 5616/70000]\n",
            "loss: 0.091649 [ 5632/70000]\n",
            "loss: 0.081557 [ 5648/70000]\n",
            "loss: 0.091027 [ 5664/70000]\n",
            "loss: 0.094501 [ 5680/70000]\n",
            "loss: 0.094509 [ 5696/70000]\n",
            "loss: 0.091414 [ 5712/70000]\n",
            "loss: 0.084740 [ 5728/70000]\n",
            "loss: 0.082263 [ 5744/70000]\n",
            "loss: 0.087796 [ 5760/70000]\n",
            "loss: 0.089141 [ 5776/70000]\n",
            "loss: 0.088094 [ 5792/70000]\n",
            "loss: 0.088806 [ 5808/70000]\n",
            "loss: 0.099436 [ 5824/70000]\n",
            "loss: 0.085596 [ 5840/70000]\n",
            "loss: 0.087317 [ 5856/70000]\n",
            "loss: 0.083861 [ 5872/70000]\n",
            "loss: 0.088613 [ 5888/70000]\n",
            "loss: 0.084936 [ 5904/70000]\n",
            "loss: 0.101287 [ 5920/70000]\n",
            "loss: 0.089269 [ 5936/70000]\n",
            "loss: 0.084256 [ 5952/70000]\n",
            "loss: 0.093601 [ 5968/70000]\n",
            "loss: 0.083992 [ 5984/70000]\n",
            "loss: 0.081541 [ 6000/70000]\n",
            "loss: 0.085321 [ 6016/70000]\n",
            "loss: 0.089463 [ 6032/70000]\n",
            "loss: 0.080593 [ 6048/70000]\n",
            "loss: 0.093027 [ 6064/70000]\n",
            "loss: 0.085385 [ 6080/70000]\n",
            "loss: 0.084691 [ 6096/70000]\n",
            "loss: 0.097499 [ 6112/70000]\n",
            "loss: 0.079088 [ 6128/70000]\n",
            "loss: 0.081794 [ 6144/70000]\n",
            "loss: 0.085346 [ 6160/70000]\n",
            "loss: 0.098558 [ 6176/70000]\n",
            "loss: 0.090217 [ 6192/70000]\n",
            "loss: 0.081763 [ 6208/70000]\n",
            "loss: 0.091433 [ 6224/70000]\n",
            "loss: 0.085202 [ 6240/70000]\n",
            "loss: 0.088221 [ 6256/70000]\n",
            "loss: 0.088700 [ 6272/70000]\n",
            "loss: 0.079579 [ 6288/70000]\n",
            "loss: 0.089105 [ 6304/70000]\n",
            "loss: 0.096603 [ 6320/70000]\n",
            "loss: 0.095885 [ 6336/70000]\n",
            "loss: 0.090776 [ 6352/70000]\n",
            "loss: 0.088867 [ 6368/70000]\n",
            "loss: 0.089569 [ 6384/70000]\n",
            "loss: 0.080574 [ 6400/70000]\n",
            "loss: 0.078061 [ 6416/70000]\n",
            "loss: 0.098226 [ 6432/70000]\n",
            "loss: 0.073994 [ 6448/70000]\n",
            "loss: 0.086997 [ 6464/70000]\n",
            "loss: 0.086564 [ 6480/70000]\n",
            "loss: 0.095687 [ 6496/70000]\n",
            "loss: 0.085346 [ 6512/70000]\n",
            "loss: 0.084933 [ 6528/70000]\n",
            "loss: 0.091688 [ 6544/70000]\n",
            "loss: 0.083743 [ 6560/70000]\n",
            "loss: 0.088844 [ 6576/70000]\n",
            "loss: 0.087168 [ 6592/70000]\n",
            "loss: 0.084566 [ 6608/70000]\n",
            "loss: 0.081482 [ 6624/70000]\n",
            "loss: 0.080042 [ 6640/70000]\n",
            "loss: 0.084523 [ 6656/70000]\n",
            "loss: 0.106811 [ 6672/70000]\n",
            "loss: 0.084044 [ 6688/70000]\n",
            "loss: 0.098986 [ 6704/70000]\n",
            "loss: 0.090454 [ 6720/70000]\n",
            "loss: 0.086016 [ 6736/70000]\n",
            "loss: 0.087199 [ 6752/70000]\n",
            "loss: 0.087894 [ 6768/70000]\n",
            "loss: 0.088391 [ 6784/70000]\n",
            "loss: 0.087780 [ 6800/70000]\n",
            "loss: 0.090614 [ 6816/70000]\n",
            "loss: 0.095239 [ 6832/70000]\n",
            "loss: 0.075590 [ 6848/70000]\n",
            "loss: 0.093360 [ 6864/70000]\n",
            "loss: 0.089926 [ 6880/70000]\n",
            "loss: 0.090387 [ 6896/70000]\n",
            "loss: 0.085751 [ 6912/70000]\n",
            "loss: 0.089183 [ 6928/70000]\n",
            "loss: 0.095037 [ 6944/70000]\n",
            "loss: 0.086051 [ 6960/70000]\n",
            "loss: 0.078706 [ 6976/70000]\n",
            "loss: 0.076840 [ 6992/70000]\n",
            "loss: 0.074817 [ 7008/70000]\n",
            "loss: 0.100620 [ 7024/70000]\n",
            "loss: 0.080280 [ 7040/70000]\n",
            "loss: 0.082557 [ 7056/70000]\n",
            "loss: 0.083743 [ 7072/70000]\n",
            "loss: 0.078947 [ 7088/70000]\n",
            "loss: 0.089713 [ 7104/70000]\n",
            "loss: 0.091614 [ 7120/70000]\n",
            "loss: 0.093543 [ 7136/70000]\n",
            "loss: 0.076956 [ 7152/70000]\n",
            "loss: 0.085373 [ 7168/70000]\n",
            "loss: 0.085892 [ 7184/70000]\n",
            "loss: 0.081851 [ 7200/70000]\n",
            "loss: 0.080548 [ 7216/70000]\n",
            "loss: 0.097170 [ 7232/70000]\n",
            "loss: 0.091683 [ 7248/70000]\n",
            "loss: 0.080781 [ 7264/70000]\n",
            "loss: 0.083813 [ 7280/70000]\n",
            "loss: 0.089444 [ 7296/70000]\n",
            "loss: 0.077997 [ 7312/70000]\n",
            "loss: 0.088987 [ 7328/70000]\n",
            "loss: 0.079664 [ 7344/70000]\n",
            "loss: 0.092916 [ 7360/70000]\n",
            "loss: 0.083491 [ 7376/70000]\n",
            "loss: 0.088874 [ 7392/70000]\n",
            "loss: 0.092870 [ 7408/70000]\n",
            "loss: 0.094693 [ 7424/70000]\n",
            "loss: 0.097940 [ 7440/70000]\n",
            "loss: 0.079273 [ 7456/70000]\n",
            "loss: 0.083850 [ 7472/70000]\n",
            "loss: 0.080840 [ 7488/70000]\n",
            "loss: 0.084719 [ 7504/70000]\n",
            "loss: 0.092183 [ 7520/70000]\n",
            "loss: 0.084108 [ 7536/70000]\n",
            "loss: 0.082870 [ 7552/70000]\n",
            "loss: 0.092811 [ 7568/70000]\n",
            "loss: 0.091643 [ 7584/70000]\n",
            "loss: 0.085605 [ 7600/70000]\n",
            "loss: 0.089746 [ 7616/70000]\n",
            "loss: 0.091021 [ 7632/70000]\n",
            "loss: 0.085349 [ 7648/70000]\n",
            "loss: 0.086525 [ 7664/70000]\n",
            "loss: 0.095278 [ 7680/70000]\n",
            "loss: 0.093909 [ 7696/70000]\n",
            "loss: 0.087893 [ 7712/70000]\n",
            "loss: 0.079396 [ 7728/70000]\n",
            "loss: 0.085234 [ 7744/70000]\n",
            "loss: 0.091852 [ 7760/70000]\n",
            "loss: 0.089187 [ 7776/70000]\n",
            "loss: 0.079329 [ 7792/70000]\n",
            "loss: 0.086907 [ 7808/70000]\n",
            "loss: 0.079639 [ 7824/70000]\n",
            "loss: 0.086653 [ 7840/70000]\n",
            "loss: 0.089437 [ 7856/70000]\n",
            "loss: 0.090416 [ 7872/70000]\n",
            "loss: 0.082931 [ 7888/70000]\n",
            "loss: 0.090095 [ 7904/70000]\n",
            "loss: 0.083685 [ 7920/70000]\n",
            "loss: 0.082077 [ 7936/70000]\n",
            "loss: 0.081662 [ 7952/70000]\n",
            "loss: 0.091153 [ 7968/70000]\n",
            "loss: 0.094529 [ 7984/70000]\n",
            "loss: 0.087278 [ 8000/70000]\n",
            "loss: 0.091626 [ 8016/70000]\n",
            "loss: 0.083863 [ 8032/70000]\n",
            "loss: 0.095558 [ 8048/70000]\n",
            "loss: 0.085435 [ 8064/70000]\n",
            "loss: 0.085040 [ 8080/70000]\n",
            "loss: 0.093100 [ 8096/70000]\n",
            "loss: 0.088172 [ 8112/70000]\n",
            "loss: 0.091035 [ 8128/70000]\n",
            "loss: 0.089743 [ 8144/70000]\n",
            "loss: 0.087870 [ 8160/70000]\n",
            "loss: 0.091179 [ 8176/70000]\n",
            "loss: 0.084612 [ 8192/70000]\n",
            "loss: 0.091752 [ 8208/70000]\n",
            "loss: 0.081156 [ 8224/70000]\n",
            "loss: 0.086718 [ 8240/70000]\n",
            "loss: 0.085068 [ 8256/70000]\n",
            "loss: 0.081017 [ 8272/70000]\n",
            "loss: 0.076566 [ 8288/70000]\n",
            "loss: 0.079694 [ 8304/70000]\n",
            "loss: 0.096966 [ 8320/70000]\n",
            "loss: 0.082126 [ 8336/70000]\n",
            "loss: 0.082101 [ 8352/70000]\n",
            "loss: 0.080858 [ 8368/70000]\n",
            "loss: 0.085530 [ 8384/70000]\n",
            "loss: 0.101299 [ 8400/70000]\n",
            "loss: 0.102775 [ 8416/70000]\n",
            "loss: 0.079940 [ 8432/70000]\n",
            "loss: 0.093549 [ 8448/70000]\n",
            "loss: 0.099209 [ 8464/70000]\n",
            "loss: 0.089428 [ 8480/70000]\n",
            "loss: 0.081495 [ 8496/70000]\n",
            "loss: 0.097423 [ 8512/70000]\n",
            "loss: 0.093040 [ 8528/70000]\n",
            "loss: 0.083790 [ 8544/70000]\n",
            "loss: 0.077276 [ 8560/70000]\n",
            "loss: 0.096884 [ 8576/70000]\n",
            "loss: 0.089020 [ 8592/70000]\n",
            "loss: 0.083355 [ 8608/70000]\n",
            "loss: 0.091645 [ 8624/70000]\n",
            "loss: 0.085313 [ 8640/70000]\n",
            "loss: 0.090703 [ 8656/70000]\n",
            "loss: 0.079163 [ 8672/70000]\n",
            "loss: 0.086450 [ 8688/70000]\n",
            "loss: 0.092353 [ 8704/70000]\n",
            "loss: 0.079205 [ 8720/70000]\n",
            "loss: 0.085865 [ 8736/70000]\n",
            "loss: 0.089450 [ 8752/70000]\n",
            "loss: 0.079593 [ 8768/70000]\n",
            "loss: 0.084953 [ 8784/70000]\n",
            "loss: 0.081102 [ 8800/70000]\n",
            "loss: 0.082504 [ 8816/70000]\n",
            "loss: 0.091816 [ 8832/70000]\n",
            "loss: 0.089947 [ 8848/70000]\n",
            "loss: 0.086451 [ 8864/70000]\n",
            "loss: 0.079494 [ 8880/70000]\n",
            "loss: 0.079378 [ 8896/70000]\n",
            "loss: 0.080823 [ 8912/70000]\n",
            "loss: 0.092868 [ 8928/70000]\n",
            "loss: 0.086214 [ 8944/70000]\n",
            "loss: 0.094011 [ 8960/70000]\n",
            "loss: 0.096776 [ 8976/70000]\n",
            "loss: 0.077455 [ 8992/70000]\n",
            "loss: 0.099836 [ 9008/70000]\n",
            "loss: 0.075631 [ 9024/70000]\n",
            "loss: 0.094553 [ 9040/70000]\n",
            "loss: 0.081622 [ 9056/70000]\n",
            "loss: 0.082173 [ 9072/70000]\n",
            "loss: 0.084806 [ 9088/70000]\n",
            "loss: 0.080061 [ 9104/70000]\n",
            "loss: 0.086531 [ 9120/70000]\n",
            "loss: 0.086709 [ 9136/70000]\n",
            "loss: 0.080410 [ 9152/70000]\n",
            "loss: 0.082157 [ 9168/70000]\n",
            "loss: 0.083580 [ 9184/70000]\n",
            "loss: 0.090143 [ 9200/70000]\n",
            "loss: 0.091829 [ 9216/70000]\n",
            "loss: 0.087780 [ 9232/70000]\n",
            "loss: 0.088244 [ 9248/70000]\n",
            "loss: 0.085995 [ 9264/70000]\n",
            "loss: 0.093016 [ 9280/70000]\n",
            "loss: 0.097829 [ 9296/70000]\n",
            "loss: 0.086701 [ 9312/70000]\n",
            "loss: 0.087691 [ 9328/70000]\n",
            "loss: 0.086309 [ 9344/70000]\n",
            "loss: 0.094401 [ 9360/70000]\n",
            "loss: 0.086795 [ 9376/70000]\n",
            "loss: 0.088068 [ 9392/70000]\n",
            "loss: 0.082949 [ 9408/70000]\n",
            "loss: 0.085762 [ 9424/70000]\n",
            "loss: 0.090588 [ 9440/70000]\n",
            "loss: 0.084769 [ 9456/70000]\n",
            "loss: 0.085673 [ 9472/70000]\n",
            "loss: 0.080809 [ 9488/70000]\n",
            "loss: 0.092706 [ 9504/70000]\n",
            "loss: 0.093225 [ 9520/70000]\n",
            "loss: 0.091785 [ 9536/70000]\n",
            "loss: 0.088793 [ 9552/70000]\n",
            "loss: 0.079718 [ 9568/70000]\n",
            "loss: 0.081880 [ 9584/70000]\n",
            "loss: 0.086806 [ 9600/70000]\n",
            "loss: 0.083990 [ 9616/70000]\n",
            "loss: 0.079184 [ 9632/70000]\n",
            "loss: 0.083897 [ 9648/70000]\n",
            "loss: 0.096811 [ 9664/70000]\n",
            "loss: 0.090130 [ 9680/70000]\n",
            "loss: 0.092376 [ 9696/70000]\n",
            "loss: 0.088432 [ 9712/70000]\n",
            "loss: 0.097473 [ 9728/70000]\n",
            "loss: 0.094985 [ 9744/70000]\n",
            "loss: 0.084587 [ 9760/70000]\n",
            "loss: 0.085654 [ 9776/70000]\n",
            "loss: 0.089898 [ 9792/70000]\n",
            "loss: 0.083191 [ 9808/70000]\n",
            "loss: 0.076659 [ 9824/70000]\n",
            "loss: 0.088279 [ 9840/70000]\n",
            "loss: 0.089282 [ 9856/70000]\n",
            "loss: 0.095124 [ 9872/70000]\n",
            "loss: 0.101708 [ 9888/70000]\n",
            "loss: 0.092449 [ 9904/70000]\n",
            "loss: 0.088113 [ 9920/70000]\n",
            "loss: 0.084492 [ 9936/70000]\n",
            "loss: 0.082624 [ 9952/70000]\n",
            "loss: 0.079634 [ 9968/70000]\n",
            "loss: 0.088888 [ 9984/70000]\n",
            "loss: 0.093625 [10000/70000]\n",
            "loss: 0.083891 [10016/70000]\n",
            "loss: 0.101985 [10032/70000]\n",
            "loss: 0.081814 [10048/70000]\n",
            "loss: 0.085952 [10064/70000]\n",
            "loss: 0.081558 [10080/70000]\n",
            "loss: 0.088783 [10096/70000]\n",
            "loss: 0.083374 [10112/70000]\n",
            "loss: 0.074822 [10128/70000]\n",
            "loss: 0.090442 [10144/70000]\n",
            "loss: 0.084674 [10160/70000]\n",
            "loss: 0.088270 [10176/70000]\n",
            "loss: 0.090826 [10192/70000]\n",
            "loss: 0.095875 [10208/70000]\n",
            "loss: 0.103528 [10224/70000]\n",
            "loss: 0.096562 [10240/70000]\n",
            "loss: 0.092661 [10256/70000]\n",
            "loss: 0.079748 [10272/70000]\n",
            "loss: 0.082675 [10288/70000]\n",
            "loss: 0.082896 [10304/70000]\n",
            "loss: 0.079945 [10320/70000]\n",
            "loss: 0.087864 [10336/70000]\n",
            "loss: 0.089985 [10352/70000]\n",
            "loss: 0.086189 [10368/70000]\n",
            "loss: 0.085583 [10384/70000]\n",
            "loss: 0.090117 [10400/70000]\n",
            "loss: 0.078147 [10416/70000]\n",
            "loss: 0.074775 [10432/70000]\n",
            "loss: 0.084843 [10448/70000]\n",
            "loss: 0.091481 [10464/70000]\n",
            "loss: 0.093069 [10480/70000]\n",
            "loss: 0.082157 [10496/70000]\n",
            "loss: 0.083316 [10512/70000]\n",
            "loss: 0.082069 [10528/70000]\n",
            "loss: 0.096522 [10544/70000]\n",
            "loss: 0.084982 [10560/70000]\n",
            "loss: 0.088272 [10576/70000]\n",
            "loss: 0.086955 [10592/70000]\n",
            "loss: 0.085570 [10608/70000]\n",
            "loss: 0.093947 [10624/70000]\n",
            "loss: 0.075824 [10640/70000]\n",
            "loss: 0.092495 [10656/70000]\n",
            "loss: 0.086690 [10672/70000]\n",
            "loss: 0.087054 [10688/70000]\n",
            "loss: 0.087414 [10704/70000]\n",
            "loss: 0.096542 [10720/70000]\n",
            "loss: 0.098389 [10736/70000]\n",
            "loss: 0.088321 [10752/70000]\n",
            "loss: 0.097896 [10768/70000]\n",
            "loss: 0.086743 [10784/70000]\n",
            "loss: 0.088741 [10800/70000]\n",
            "loss: 0.078747 [10816/70000]\n",
            "loss: 0.092253 [10832/70000]\n",
            "loss: 0.088064 [10848/70000]\n",
            "loss: 0.090316 [10864/70000]\n",
            "loss: 0.090835 [10880/70000]\n",
            "loss: 0.084391 [10896/70000]\n",
            "loss: 0.081645 [10912/70000]\n",
            "loss: 0.094409 [10928/70000]\n",
            "loss: 0.091566 [10944/70000]\n",
            "loss: 0.089814 [10960/70000]\n",
            "loss: 0.086460 [10976/70000]\n",
            "loss: 0.085389 [10992/70000]\n",
            "loss: 0.085959 [11008/70000]\n",
            "loss: 0.088605 [11024/70000]\n",
            "loss: 0.096257 [11040/70000]\n",
            "loss: 0.085636 [11056/70000]\n",
            "loss: 0.079847 [11072/70000]\n",
            "loss: 0.088106 [11088/70000]\n",
            "loss: 0.087170 [11104/70000]\n",
            "loss: 0.098287 [11120/70000]\n",
            "loss: 0.075860 [11136/70000]\n",
            "loss: 0.081612 [11152/70000]\n",
            "loss: 0.090622 [11168/70000]\n",
            "loss: 0.091990 [11184/70000]\n",
            "loss: 0.084086 [11200/70000]\n",
            "loss: 0.084532 [11216/70000]\n",
            "loss: 0.078427 [11232/70000]\n",
            "loss: 0.096846 [11248/70000]\n",
            "loss: 0.094857 [11264/70000]\n",
            "loss: 0.083807 [11280/70000]\n",
            "loss: 0.088394 [11296/70000]\n",
            "loss: 0.087693 [11312/70000]\n",
            "loss: 0.088998 [11328/70000]\n",
            "loss: 0.082979 [11344/70000]\n",
            "loss: 0.094491 [11360/70000]\n",
            "loss: 0.089795 [11376/70000]\n",
            "loss: 0.078765 [11392/70000]\n",
            "loss: 0.084234 [11408/70000]\n",
            "loss: 0.084478 [11424/70000]\n",
            "loss: 0.088584 [11440/70000]\n",
            "loss: 0.084274 [11456/70000]\n",
            "loss: 0.089011 [11472/70000]\n",
            "loss: 0.096046 [11488/70000]\n",
            "loss: 0.085489 [11504/70000]\n",
            "loss: 0.090960 [11520/70000]\n",
            "loss: 0.094137 [11536/70000]\n",
            "loss: 0.081196 [11552/70000]\n",
            "loss: 0.082593 [11568/70000]\n",
            "loss: 0.085667 [11584/70000]\n",
            "loss: 0.080296 [11600/70000]\n",
            "loss: 0.085140 [11616/70000]\n",
            "loss: 0.086838 [11632/70000]\n",
            "loss: 0.080910 [11648/70000]\n",
            "loss: 0.075755 [11664/70000]\n",
            "loss: 0.072610 [11680/70000]\n",
            "loss: 0.092865 [11696/70000]\n",
            "loss: 0.080679 [11712/70000]\n",
            "loss: 0.083869 [11728/70000]\n",
            "loss: 0.087432 [11744/70000]\n",
            "loss: 0.084707 [11760/70000]\n",
            "loss: 0.084446 [11776/70000]\n",
            "loss: 0.095481 [11792/70000]\n",
            "loss: 0.083790 [11808/70000]\n",
            "loss: 0.080639 [11824/70000]\n",
            "loss: 0.086952 [11840/70000]\n",
            "loss: 0.085985 [11856/70000]\n",
            "loss: 0.088637 [11872/70000]\n",
            "loss: 0.079971 [11888/70000]\n",
            "loss: 0.081159 [11904/70000]\n",
            "loss: 0.087141 [11920/70000]\n",
            "loss: 0.098783 [11936/70000]\n",
            "loss: 0.079656 [11952/70000]\n",
            "loss: 0.080601 [11968/70000]\n",
            "loss: 0.081314 [11984/70000]\n",
            "loss: 0.085782 [12000/70000]\n",
            "loss: 0.094308 [12016/70000]\n",
            "loss: 0.080783 [12032/70000]\n",
            "loss: 0.093590 [12048/70000]\n",
            "loss: 0.100007 [12064/70000]\n",
            "loss: 0.094315 [12080/70000]\n",
            "loss: 0.086542 [12096/70000]\n",
            "loss: 0.087983 [12112/70000]\n",
            "loss: 0.092364 [12128/70000]\n",
            "loss: 0.081807 [12144/70000]\n",
            "loss: 0.087951 [12160/70000]\n",
            "loss: 0.080625 [12176/70000]\n",
            "loss: 0.080105 [12192/70000]\n",
            "loss: 0.084941 [12208/70000]\n",
            "loss: 0.097311 [12224/70000]\n",
            "loss: 0.079841 [12240/70000]\n",
            "loss: 0.078360 [12256/70000]\n",
            "loss: 0.080261 [12272/70000]\n",
            "loss: 0.089500 [12288/70000]\n",
            "loss: 0.086895 [12304/70000]\n",
            "loss: 0.088986 [12320/70000]\n",
            "loss: 0.098601 [12336/70000]\n",
            "loss: 0.085157 [12352/70000]\n",
            "loss: 0.087566 [12368/70000]\n",
            "loss: 0.083434 [12384/70000]\n",
            "loss: 0.096880 [12400/70000]\n",
            "loss: 0.107252 [12416/70000]\n",
            "loss: 0.090795 [12432/70000]\n",
            "loss: 0.088774 [12448/70000]\n",
            "loss: 0.089902 [12464/70000]\n",
            "loss: 0.086271 [12480/70000]\n",
            "loss: 0.090473 [12496/70000]\n",
            "loss: 0.085934 [12512/70000]\n",
            "loss: 0.094601 [12528/70000]\n",
            "loss: 0.084792 [12544/70000]\n",
            "loss: 0.097501 [12560/70000]\n",
            "loss: 0.102051 [12576/70000]\n",
            "loss: 0.093040 [12592/70000]\n",
            "loss: 0.089622 [12608/70000]\n",
            "loss: 0.089583 [12624/70000]\n",
            "loss: 0.076799 [12640/70000]\n",
            "loss: 0.086193 [12656/70000]\n",
            "loss: 0.089854 [12672/70000]\n",
            "loss: 0.099618 [12688/70000]\n",
            "loss: 0.087912 [12704/70000]\n",
            "loss: 0.081606 [12720/70000]\n",
            "loss: 0.079882 [12736/70000]\n",
            "loss: 0.088899 [12752/70000]\n",
            "loss: 0.091635 [12768/70000]\n",
            "loss: 0.074083 [12784/70000]\n",
            "loss: 0.102040 [12800/70000]\n",
            "loss: 0.083559 [12816/70000]\n",
            "loss: 0.086080 [12832/70000]\n",
            "loss: 0.091944 [12848/70000]\n",
            "loss: 0.088628 [12864/70000]\n",
            "loss: 0.084583 [12880/70000]\n",
            "loss: 0.085698 [12896/70000]\n",
            "loss: 0.086701 [12912/70000]\n",
            "loss: 0.090469 [12928/70000]\n",
            "loss: 0.083604 [12944/70000]\n",
            "loss: 0.087150 [12960/70000]\n",
            "loss: 0.088884 [12976/70000]\n",
            "loss: 0.090408 [12992/70000]\n",
            "loss: 0.084304 [13008/70000]\n",
            "loss: 0.091158 [13024/70000]\n",
            "loss: 0.083074 [13040/70000]\n",
            "loss: 0.094952 [13056/70000]\n",
            "loss: 0.084939 [13072/70000]\n",
            "loss: 0.091208 [13088/70000]\n",
            "loss: 0.091059 [13104/70000]\n",
            "loss: 0.084720 [13120/70000]\n",
            "loss: 0.088145 [13136/70000]\n",
            "loss: 0.084064 [13152/70000]\n",
            "loss: 0.092398 [13168/70000]\n",
            "loss: 0.094775 [13184/70000]\n",
            "loss: 0.095146 [13200/70000]\n",
            "loss: 0.085914 [13216/70000]\n",
            "loss: 0.081766 [13232/70000]\n",
            "loss: 0.098823 [13248/70000]\n",
            "loss: 0.082258 [13264/70000]\n",
            "loss: 0.095287 [13280/70000]\n",
            "loss: 0.096550 [13296/70000]\n",
            "loss: 0.094165 [13312/70000]\n",
            "loss: 0.082578 [13328/70000]\n",
            "loss: 0.085461 [13344/70000]\n",
            "loss: 0.083075 [13360/70000]\n",
            "loss: 0.095825 [13376/70000]\n",
            "loss: 0.087644 [13392/70000]\n",
            "loss: 0.081721 [13408/70000]\n",
            "loss: 0.081020 [13424/70000]\n",
            "loss: 0.091306 [13440/70000]\n",
            "loss: 0.085799 [13456/70000]\n",
            "loss: 0.091664 [13472/70000]\n",
            "loss: 0.084920 [13488/70000]\n",
            "loss: 0.078410 [13504/70000]\n",
            "loss: 0.085796 [13520/70000]\n",
            "loss: 0.091783 [13536/70000]\n",
            "loss: 0.087934 [13552/70000]\n",
            "loss: 0.082309 [13568/70000]\n",
            "loss: 0.090513 [13584/70000]\n",
            "loss: 0.078040 [13600/70000]\n",
            "loss: 0.087214 [13616/70000]\n",
            "loss: 0.095272 [13632/70000]\n",
            "loss: 0.079053 [13648/70000]\n",
            "loss: 0.089483 [13664/70000]\n",
            "loss: 0.091934 [13680/70000]\n",
            "loss: 0.079746 [13696/70000]\n",
            "loss: 0.085159 [13712/70000]\n",
            "loss: 0.093784 [13728/70000]\n",
            "loss: 0.098066 [13744/70000]\n",
            "loss: 0.094591 [13760/70000]\n",
            "loss: 0.083755 [13776/70000]\n",
            "loss: 0.086858 [13792/70000]\n",
            "loss: 0.090505 [13808/70000]\n",
            "loss: 0.077119 [13824/70000]\n",
            "loss: 0.096511 [13840/70000]\n",
            "loss: 0.091071 [13856/70000]\n",
            "loss: 0.096405 [13872/70000]\n",
            "loss: 0.093697 [13888/70000]\n",
            "loss: 0.074295 [13904/70000]\n",
            "loss: 0.081259 [13920/70000]\n",
            "loss: 0.085650 [13936/70000]\n",
            "loss: 0.091053 [13952/70000]\n",
            "loss: 0.087522 [13968/70000]\n",
            "loss: 0.085365 [13984/70000]\n",
            "loss: 0.080524 [14000/70000]\n",
            "loss: 0.083243 [14016/70000]\n",
            "loss: 0.076297 [14032/70000]\n",
            "loss: 0.087287 [14048/70000]\n",
            "loss: 0.089403 [14064/70000]\n",
            "loss: 0.093999 [14080/70000]\n",
            "loss: 0.088901 [14096/70000]\n",
            "loss: 0.089893 [14112/70000]\n",
            "loss: 0.089640 [14128/70000]\n",
            "loss: 0.087926 [14144/70000]\n",
            "loss: 0.085660 [14160/70000]\n",
            "loss: 0.087910 [14176/70000]\n",
            "loss: 0.077685 [14192/70000]\n",
            "loss: 0.081322 [14208/70000]\n",
            "loss: 0.090848 [14224/70000]\n",
            "loss: 0.082488 [14240/70000]\n",
            "loss: 0.086430 [14256/70000]\n",
            "loss: 0.090877 [14272/70000]\n",
            "loss: 0.093519 [14288/70000]\n",
            "loss: 0.099880 [14304/70000]\n",
            "loss: 0.094940 [14320/70000]\n",
            "loss: 0.092335 [14336/70000]\n",
            "loss: 0.097465 [14352/70000]\n",
            "loss: 0.083322 [14368/70000]\n",
            "loss: 0.092665 [14384/70000]\n",
            "loss: 0.084923 [14400/70000]\n",
            "loss: 0.075160 [14416/70000]\n",
            "loss: 0.089065 [14432/70000]\n",
            "loss: 0.097998 [14448/70000]\n",
            "loss: 0.078815 [14464/70000]\n",
            "loss: 0.100527 [14480/70000]\n",
            "loss: 0.086067 [14496/70000]\n",
            "loss: 0.086328 [14512/70000]\n",
            "loss: 0.088305 [14528/70000]\n",
            "loss: 0.090290 [14544/70000]\n",
            "loss: 0.092581 [14560/70000]\n",
            "loss: 0.080334 [14576/70000]\n",
            "loss: 0.098513 [14592/70000]\n",
            "loss: 0.090572 [14608/70000]\n",
            "loss: 0.088618 [14624/70000]\n",
            "loss: 0.086715 [14640/70000]\n",
            "loss: 0.087542 [14656/70000]\n",
            "loss: 0.091276 [14672/70000]\n",
            "loss: 0.081335 [14688/70000]\n",
            "loss: 0.079811 [14704/70000]\n",
            "loss: 0.096852 [14720/70000]\n",
            "loss: 0.100719 [14736/70000]\n",
            "loss: 0.086899 [14752/70000]\n",
            "loss: 0.087025 [14768/70000]\n",
            "loss: 0.089678 [14784/70000]\n",
            "loss: 0.087829 [14800/70000]\n",
            "loss: 0.079578 [14816/70000]\n",
            "loss: 0.078822 [14832/70000]\n",
            "loss: 0.084467 [14848/70000]\n",
            "loss: 0.094521 [14864/70000]\n",
            "loss: 0.090832 [14880/70000]\n",
            "loss: 0.092847 [14896/70000]\n",
            "loss: 0.081010 [14912/70000]\n",
            "loss: 0.086830 [14928/70000]\n",
            "loss: 0.100759 [14944/70000]\n",
            "loss: 0.078093 [14960/70000]\n",
            "loss: 0.087512 [14976/70000]\n",
            "loss: 0.091694 [14992/70000]\n",
            "loss: 0.090550 [15008/70000]\n",
            "loss: 0.087273 [15024/70000]\n",
            "loss: 0.094254 [15040/70000]\n",
            "loss: 0.087756 [15056/70000]\n",
            "loss: 0.093238 [15072/70000]\n",
            "loss: 0.090822 [15088/70000]\n",
            "loss: 0.091836 [15104/70000]\n",
            "loss: 0.081611 [15120/70000]\n",
            "loss: 0.083955 [15136/70000]\n",
            "loss: 0.084796 [15152/70000]\n",
            "loss: 0.080626 [15168/70000]\n",
            "loss: 0.094632 [15184/70000]\n",
            "loss: 0.085190 [15200/70000]\n",
            "loss: 0.085418 [15216/70000]\n",
            "loss: 0.081843 [15232/70000]\n",
            "loss: 0.096574 [15248/70000]\n",
            "loss: 0.091107 [15264/70000]\n",
            "loss: 0.092357 [15280/70000]\n",
            "loss: 0.078244 [15296/70000]\n",
            "loss: 0.092720 [15312/70000]\n",
            "loss: 0.096656 [15328/70000]\n",
            "loss: 0.094265 [15344/70000]\n",
            "loss: 0.095631 [15360/70000]\n",
            "loss: 0.085567 [15376/70000]\n",
            "loss: 0.083108 [15392/70000]\n",
            "loss: 0.081577 [15408/70000]\n",
            "loss: 0.093676 [15424/70000]\n",
            "loss: 0.079221 [15440/70000]\n",
            "loss: 0.081247 [15456/70000]\n",
            "loss: 0.080612 [15472/70000]\n",
            "loss: 0.090875 [15488/70000]\n",
            "loss: 0.103652 [15504/70000]\n",
            "loss: 0.087553 [15520/70000]\n",
            "loss: 0.085314 [15536/70000]\n",
            "loss: 0.083380 [15552/70000]\n",
            "loss: 0.105103 [15568/70000]\n",
            "loss: 0.089367 [15584/70000]\n",
            "loss: 0.091168 [15600/70000]\n",
            "loss: 0.092640 [15616/70000]\n",
            "loss: 0.096363 [15632/70000]\n",
            "loss: 0.100433 [15648/70000]\n",
            "loss: 0.082173 [15664/70000]\n",
            "loss: 0.087325 [15680/70000]\n",
            "loss: 0.091053 [15696/70000]\n",
            "loss: 0.090885 [15712/70000]\n",
            "loss: 0.100053 [15728/70000]\n",
            "loss: 0.073820 [15744/70000]\n",
            "loss: 0.091395 [15760/70000]\n",
            "loss: 0.088801 [15776/70000]\n",
            "loss: 0.089237 [15792/70000]\n",
            "loss: 0.077329 [15808/70000]\n",
            "loss: 0.088222 [15824/70000]\n",
            "loss: 0.087033 [15840/70000]\n",
            "loss: 0.087344 [15856/70000]\n",
            "loss: 0.092781 [15872/70000]\n",
            "loss: 0.089255 [15888/70000]\n",
            "loss: 0.096003 [15904/70000]\n",
            "loss: 0.093128 [15920/70000]\n",
            "loss: 0.088035 [15936/70000]\n",
            "loss: 0.085649 [15952/70000]\n",
            "loss: 0.078423 [15968/70000]\n",
            "loss: 0.087677 [15984/70000]\n",
            "loss: 0.082560 [16000/70000]\n",
            "loss: 0.078729 [16016/70000]\n",
            "loss: 0.096516 [16032/70000]\n",
            "loss: 0.094609 [16048/70000]\n",
            "loss: 0.094915 [16064/70000]\n",
            "loss: 0.091209 [16080/70000]\n",
            "loss: 0.076480 [16096/70000]\n",
            "loss: 0.090049 [16112/70000]\n",
            "loss: 0.084756 [16128/70000]\n",
            "loss: 0.092280 [16144/70000]\n",
            "loss: 0.099823 [16160/70000]\n",
            "loss: 0.081966 [16176/70000]\n",
            "loss: 0.090215 [16192/70000]\n",
            "loss: 0.093712 [16208/70000]\n",
            "loss: 0.081860 [16224/70000]\n",
            "loss: 0.093654 [16240/70000]\n",
            "loss: 0.083309 [16256/70000]\n",
            "loss: 0.082731 [16272/70000]\n",
            "loss: 0.094443 [16288/70000]\n",
            "loss: 0.097478 [16304/70000]\n",
            "loss: 0.081561 [16320/70000]\n",
            "loss: 0.090918 [16336/70000]\n",
            "loss: 0.084002 [16352/70000]\n",
            "loss: 0.082644 [16368/70000]\n",
            "loss: 0.086298 [16384/70000]\n",
            "loss: 0.084805 [16400/70000]\n",
            "loss: 0.085827 [16416/70000]\n",
            "loss: 0.083267 [16432/70000]\n",
            "loss: 0.075587 [16448/70000]\n",
            "loss: 0.079748 [16464/70000]\n",
            "loss: 0.097015 [16480/70000]\n",
            "loss: 0.094572 [16496/70000]\n",
            "loss: 0.089878 [16512/70000]\n",
            "loss: 0.079028 [16528/70000]\n",
            "loss: 0.096139 [16544/70000]\n",
            "loss: 0.094127 [16560/70000]\n",
            "loss: 0.082060 [16576/70000]\n",
            "loss: 0.084623 [16592/70000]\n",
            "loss: 0.084813 [16608/70000]\n",
            "loss: 0.094540 [16624/70000]\n",
            "loss: 0.092742 [16640/70000]\n",
            "loss: 0.102677 [16656/70000]\n",
            "loss: 0.095151 [16672/70000]\n",
            "loss: 0.089804 [16688/70000]\n",
            "loss: 0.090955 [16704/70000]\n",
            "loss: 0.082224 [16720/70000]\n",
            "loss: 0.087091 [16736/70000]\n",
            "loss: 0.081125 [16752/70000]\n",
            "loss: 0.082111 [16768/70000]\n",
            "loss: 0.076885 [16784/70000]\n",
            "loss: 0.090079 [16800/70000]\n",
            "loss: 0.094610 [16816/70000]\n",
            "loss: 0.074648 [16832/70000]\n",
            "loss: 0.090701 [16848/70000]\n",
            "loss: 0.088310 [16864/70000]\n",
            "loss: 0.081303 [16880/70000]\n",
            "loss: 0.087831 [16896/70000]\n",
            "loss: 0.078630 [16912/70000]\n",
            "loss: 0.081204 [16928/70000]\n",
            "loss: 0.085570 [16944/70000]\n",
            "loss: 0.089090 [16960/70000]\n",
            "loss: 0.095565 [16976/70000]\n",
            "loss: 0.091801 [16992/70000]\n",
            "loss: 0.087312 [17008/70000]\n",
            "loss: 0.075504 [17024/70000]\n",
            "loss: 0.074812 [17040/70000]\n",
            "loss: 0.088705 [17056/70000]\n",
            "loss: 0.079616 [17072/70000]\n",
            "loss: 0.097191 [17088/70000]\n",
            "loss: 0.088000 [17104/70000]\n",
            "loss: 0.085297 [17120/70000]\n",
            "loss: 0.080653 [17136/70000]\n",
            "loss: 0.081805 [17152/70000]\n",
            "loss: 0.091483 [17168/70000]\n",
            "loss: 0.082076 [17184/70000]\n",
            "loss: 0.086387 [17200/70000]\n",
            "loss: 0.075858 [17216/70000]\n",
            "loss: 0.078090 [17232/70000]\n",
            "loss: 0.086442 [17248/70000]\n",
            "loss: 0.096092 [17264/70000]\n",
            "loss: 0.082369 [17280/70000]\n",
            "loss: 0.081148 [17296/70000]\n",
            "loss: 0.089137 [17312/70000]\n",
            "loss: 0.078256 [17328/70000]\n",
            "loss: 0.088390 [17344/70000]\n",
            "loss: 0.090033 [17360/70000]\n",
            "loss: 0.082120 [17376/70000]\n",
            "loss: 0.082045 [17392/70000]\n",
            "loss: 0.085783 [17408/70000]\n",
            "loss: 0.089309 [17424/70000]\n",
            "loss: 0.088473 [17440/70000]\n",
            "loss: 0.095390 [17456/70000]\n",
            "loss: 0.084455 [17472/70000]\n",
            "loss: 0.080718 [17488/70000]\n",
            "loss: 0.081161 [17504/70000]\n",
            "loss: 0.082125 [17520/70000]\n",
            "loss: 0.091295 [17536/70000]\n",
            "loss: 0.098871 [17552/70000]\n",
            "loss: 0.074666 [17568/70000]\n",
            "loss: 0.081297 [17584/70000]\n",
            "loss: 0.091316 [17600/70000]\n",
            "loss: 0.082474 [17616/70000]\n",
            "loss: 0.092601 [17632/70000]\n",
            "loss: 0.095288 [17648/70000]\n",
            "loss: 0.091671 [17664/70000]\n",
            "loss: 0.082224 [17680/70000]\n",
            "loss: 0.092689 [17696/70000]\n",
            "loss: 0.080536 [17712/70000]\n",
            "loss: 0.077058 [17728/70000]\n",
            "loss: 0.095627 [17744/70000]\n",
            "loss: 0.090787 [17760/70000]\n",
            "loss: 0.078875 [17776/70000]\n",
            "loss: 0.087059 [17792/70000]\n",
            "loss: 0.090856 [17808/70000]\n",
            "loss: 0.086748 [17824/70000]\n",
            "loss: 0.090363 [17840/70000]\n",
            "loss: 0.093098 [17856/70000]\n",
            "loss: 0.094849 [17872/70000]\n",
            "loss: 0.091764 [17888/70000]\n",
            "loss: 0.091997 [17904/70000]\n",
            "loss: 0.077242 [17920/70000]\n",
            "loss: 0.085357 [17936/70000]\n",
            "loss: 0.092014 [17952/70000]\n",
            "loss: 0.076503 [17968/70000]\n",
            "loss: 0.085486 [17984/70000]\n",
            "loss: 0.085150 [18000/70000]\n",
            "loss: 0.089534 [18016/70000]\n",
            "loss: 0.086576 [18032/70000]\n",
            "loss: 0.089401 [18048/70000]\n",
            "loss: 0.088129 [18064/70000]\n",
            "loss: 0.081401 [18080/70000]\n",
            "loss: 0.095119 [18096/70000]\n",
            "loss: 0.085737 [18112/70000]\n",
            "loss: 0.085290 [18128/70000]\n",
            "loss: 0.072333 [18144/70000]\n",
            "loss: 0.095748 [18160/70000]\n",
            "loss: 0.078256 [18176/70000]\n",
            "loss: 0.083978 [18192/70000]\n",
            "loss: 0.097422 [18208/70000]\n",
            "loss: 0.082638 [18224/70000]\n",
            "loss: 0.088159 [18240/70000]\n",
            "loss: 0.087796 [18256/70000]\n",
            "loss: 0.089376 [18272/70000]\n",
            "loss: 0.079611 [18288/70000]\n",
            "loss: 0.080679 [18304/70000]\n",
            "loss: 0.095513 [18320/70000]\n",
            "loss: 0.081004 [18336/70000]\n",
            "loss: 0.070726 [18352/70000]\n",
            "loss: 0.082095 [18368/70000]\n",
            "loss: 0.077535 [18384/70000]\n",
            "loss: 0.087363 [18400/70000]\n",
            "loss: 0.082304 [18416/70000]\n",
            "loss: 0.092425 [18432/70000]\n",
            "loss: 0.098582 [18448/70000]\n",
            "loss: 0.088380 [18464/70000]\n",
            "loss: 0.094887 [18480/70000]\n",
            "loss: 0.075982 [18496/70000]\n",
            "loss: 0.091553 [18512/70000]\n",
            "loss: 0.083494 [18528/70000]\n",
            "loss: 0.081585 [18544/70000]\n",
            "loss: 0.084406 [18560/70000]\n",
            "loss: 0.082244 [18576/70000]\n",
            "loss: 0.078496 [18592/70000]\n",
            "loss: 0.079026 [18608/70000]\n",
            "loss: 0.096703 [18624/70000]\n",
            "loss: 0.077779 [18640/70000]\n",
            "loss: 0.087970 [18656/70000]\n",
            "loss: 0.086037 [18672/70000]\n",
            "loss: 0.088621 [18688/70000]\n",
            "loss: 0.083935 [18704/70000]\n",
            "loss: 0.085006 [18720/70000]\n",
            "loss: 0.087559 [18736/70000]\n",
            "loss: 0.090619 [18752/70000]\n",
            "loss: 0.087222 [18768/70000]\n",
            "loss: 0.073052 [18784/70000]\n",
            "loss: 0.087783 [18800/70000]\n",
            "loss: 0.096201 [18816/70000]\n",
            "loss: 0.094461 [18832/70000]\n",
            "loss: 0.081782 [18848/70000]\n",
            "loss: 0.091796 [18864/70000]\n",
            "loss: 0.080437 [18880/70000]\n",
            "loss: 0.092178 [18896/70000]\n",
            "loss: 0.081402 [18912/70000]\n",
            "loss: 0.086415 [18928/70000]\n",
            "loss: 0.099024 [18944/70000]\n",
            "loss: 0.084619 [18960/70000]\n",
            "loss: 0.080378 [18976/70000]\n",
            "loss: 0.084227 [18992/70000]\n",
            "loss: 0.087061 [19008/70000]\n",
            "loss: 0.076830 [19024/70000]\n",
            "loss: 0.082787 [19040/70000]\n",
            "loss: 0.077454 [19056/70000]\n",
            "loss: 0.081716 [19072/70000]\n",
            "loss: 0.098403 [19088/70000]\n",
            "loss: 0.092093 [19104/70000]\n",
            "loss: 0.089221 [19120/70000]\n",
            "loss: 0.090576 [19136/70000]\n",
            "loss: 0.084337 [19152/70000]\n",
            "loss: 0.089531 [19168/70000]\n",
            "loss: 0.080350 [19184/70000]\n",
            "loss: 0.090937 [19200/70000]\n",
            "loss: 0.094178 [19216/70000]\n",
            "loss: 0.095516 [19232/70000]\n",
            "loss: 0.091197 [19248/70000]\n",
            "loss: 0.091192 [19264/70000]\n",
            "loss: 0.080257 [19280/70000]\n",
            "loss: 0.069155 [19296/70000]\n",
            "loss: 0.091063 [19312/70000]\n",
            "loss: 0.085737 [19328/70000]\n",
            "loss: 0.095961 [19344/70000]\n",
            "loss: 0.081540 [19360/70000]\n",
            "loss: 0.097045 [19376/70000]\n",
            "loss: 0.084083 [19392/70000]\n",
            "loss: 0.098717 [19408/70000]\n",
            "loss: 0.083768 [19424/70000]\n",
            "loss: 0.083563 [19440/70000]\n",
            "loss: 0.098994 [19456/70000]\n",
            "loss: 0.091497 [19472/70000]\n",
            "loss: 0.075807 [19488/70000]\n",
            "loss: 0.096758 [19504/70000]\n",
            "loss: 0.088032 [19520/70000]\n",
            "loss: 0.085958 [19536/70000]\n",
            "loss: 0.075418 [19552/70000]\n",
            "loss: 0.102591 [19568/70000]\n",
            "loss: 0.082330 [19584/70000]\n",
            "loss: 0.093122 [19600/70000]\n",
            "loss: 0.084249 [19616/70000]\n",
            "loss: 0.090674 [19632/70000]\n",
            "loss: 0.092043 [19648/70000]\n",
            "loss: 0.088348 [19664/70000]\n",
            "loss: 0.079415 [19680/70000]\n",
            "loss: 0.086128 [19696/70000]\n",
            "loss: 0.086989 [19712/70000]\n",
            "loss: 0.095987 [19728/70000]\n",
            "loss: 0.091593 [19744/70000]\n",
            "loss: 0.081347 [19760/70000]\n",
            "loss: 0.092649 [19776/70000]\n",
            "loss: 0.083770 [19792/70000]\n",
            "loss: 0.095730 [19808/70000]\n",
            "loss: 0.086970 [19824/70000]\n",
            "loss: 0.078584 [19840/70000]\n",
            "loss: 0.084421 [19856/70000]\n",
            "loss: 0.094294 [19872/70000]\n",
            "loss: 0.091600 [19888/70000]\n",
            "loss: 0.077561 [19904/70000]\n",
            "loss: 0.086776 [19920/70000]\n",
            "loss: 0.093278 [19936/70000]\n",
            "loss: 0.080911 [19952/70000]\n",
            "loss: 0.086808 [19968/70000]\n",
            "loss: 0.090206 [19984/70000]\n",
            "loss: 0.087035 [20000/70000]\n",
            "loss: 0.082645 [20016/70000]\n",
            "loss: 0.077999 [20032/70000]\n",
            "loss: 0.089533 [20048/70000]\n",
            "loss: 0.094642 [20064/70000]\n",
            "loss: 0.083778 [20080/70000]\n",
            "loss: 0.087968 [20096/70000]\n",
            "loss: 0.081505 [20112/70000]\n",
            "loss: 0.091936 [20128/70000]\n",
            "loss: 0.088371 [20144/70000]\n",
            "loss: 0.084277 [20160/70000]\n",
            "loss: 0.090154 [20176/70000]\n",
            "loss: 0.090127 [20192/70000]\n",
            "loss: 0.091628 [20208/70000]\n",
            "loss: 0.089662 [20224/70000]\n",
            "loss: 0.082623 [20240/70000]\n",
            "loss: 0.091912 [20256/70000]\n",
            "loss: 0.092166 [20272/70000]\n",
            "loss: 0.091290 [20288/70000]\n",
            "loss: 0.091148 [20304/70000]\n",
            "loss: 0.091785 [20320/70000]\n",
            "loss: 0.082710 [20336/70000]\n",
            "loss: 0.083999 [20352/70000]\n",
            "loss: 0.081044 [20368/70000]\n",
            "loss: 0.082897 [20384/70000]\n",
            "loss: 0.081044 [20400/70000]\n",
            "loss: 0.085076 [20416/70000]\n",
            "loss: 0.085854 [20432/70000]\n",
            "loss: 0.085270 [20448/70000]\n",
            "loss: 0.098441 [20464/70000]\n",
            "loss: 0.097856 [20480/70000]\n",
            "loss: 0.091571 [20496/70000]\n",
            "loss: 0.083998 [20512/70000]\n",
            "loss: 0.077265 [20528/70000]\n",
            "loss: 0.086362 [20544/70000]\n",
            "loss: 0.081226 [20560/70000]\n",
            "loss: 0.092728 [20576/70000]\n",
            "loss: 0.102540 [20592/70000]\n",
            "loss: 0.093071 [20608/70000]\n",
            "loss: 0.085064 [20624/70000]\n",
            "loss: 0.093768 [20640/70000]\n",
            "loss: 0.090582 [20656/70000]\n",
            "loss: 0.089352 [20672/70000]\n",
            "loss: 0.078760 [20688/70000]\n",
            "loss: 0.086756 [20704/70000]\n",
            "loss: 0.083726 [20720/70000]\n",
            "loss: 0.086755 [20736/70000]\n",
            "loss: 0.075834 [20752/70000]\n",
            "loss: 0.077394 [20768/70000]\n",
            "loss: 0.075660 [20784/70000]\n",
            "loss: 0.088087 [20800/70000]\n",
            "loss: 0.085941 [20816/70000]\n",
            "loss: 0.087671 [20832/70000]\n",
            "loss: 0.083742 [20848/70000]\n",
            "loss: 0.089749 [20864/70000]\n",
            "loss: 0.089546 [20880/70000]\n",
            "loss: 0.092867 [20896/70000]\n",
            "loss: 0.081523 [20912/70000]\n",
            "loss: 0.088596 [20928/70000]\n",
            "loss: 0.091801 [20944/70000]\n",
            "loss: 0.082545 [20960/70000]\n",
            "loss: 0.083669 [20976/70000]\n",
            "loss: 0.086358 [20992/70000]\n",
            "loss: 0.089105 [21008/70000]\n",
            "loss: 0.094704 [21024/70000]\n",
            "loss: 0.089700 [21040/70000]\n",
            "loss: 0.078022 [21056/70000]\n",
            "loss: 0.079803 [21072/70000]\n",
            "loss: 0.091540 [21088/70000]\n",
            "loss: 0.089212 [21104/70000]\n",
            "loss: 0.084311 [21120/70000]\n",
            "loss: 0.085796 [21136/70000]\n",
            "loss: 0.079471 [21152/70000]\n",
            "loss: 0.080478 [21168/70000]\n",
            "loss: 0.095876 [21184/70000]\n",
            "loss: 0.081118 [21200/70000]\n",
            "loss: 0.094439 [21216/70000]\n",
            "loss: 0.088804 [21232/70000]\n",
            "loss: 0.090708 [21248/70000]\n",
            "loss: 0.083479 [21264/70000]\n",
            "loss: 0.097113 [21280/70000]\n",
            "loss: 0.085743 [21296/70000]\n",
            "loss: 0.080739 [21312/70000]\n",
            "loss: 0.087955 [21328/70000]\n",
            "loss: 0.088710 [21344/70000]\n",
            "loss: 0.105158 [21360/70000]\n",
            "loss: 0.089584 [21376/70000]\n",
            "loss: 0.082234 [21392/70000]\n",
            "loss: 0.095268 [21408/70000]\n",
            "loss: 0.083579 [21424/70000]\n",
            "loss: 0.091782 [21440/70000]\n",
            "loss: 0.096202 [21456/70000]\n",
            "loss: 0.089814 [21472/70000]\n",
            "loss: 0.081399 [21488/70000]\n",
            "loss: 0.078314 [21504/70000]\n",
            "loss: 0.085743 [21520/70000]\n",
            "loss: 0.089714 [21536/70000]\n",
            "loss: 0.078947 [21552/70000]\n",
            "loss: 0.094346 [21568/70000]\n",
            "loss: 0.086958 [21584/70000]\n",
            "loss: 0.089724 [21600/70000]\n",
            "loss: 0.102349 [21616/70000]\n",
            "loss: 0.094677 [21632/70000]\n",
            "loss: 0.086985 [21648/70000]\n",
            "loss: 0.080977 [21664/70000]\n",
            "loss: 0.076017 [21680/70000]\n",
            "loss: 0.080864 [21696/70000]\n",
            "loss: 0.080336 [21712/70000]\n",
            "loss: 0.086952 [21728/70000]\n",
            "loss: 0.088880 [21744/70000]\n",
            "loss: 0.081641 [21760/70000]\n",
            "loss: 0.089407 [21776/70000]\n",
            "loss: 0.073890 [21792/70000]\n",
            "loss: 0.086804 [21808/70000]\n",
            "loss: 0.081004 [21824/70000]\n",
            "loss: 0.101437 [21840/70000]\n",
            "loss: 0.082613 [21856/70000]\n",
            "loss: 0.082520 [21872/70000]\n",
            "loss: 0.077931 [21888/70000]\n",
            "loss: 0.077986 [21904/70000]\n",
            "loss: 0.080755 [21920/70000]\n",
            "loss: 0.090114 [21936/70000]\n",
            "loss: 0.085883 [21952/70000]\n",
            "loss: 0.086247 [21968/70000]\n",
            "loss: 0.093648 [21984/70000]\n",
            "loss: 0.096941 [22000/70000]\n",
            "loss: 0.095417 [22016/70000]\n",
            "loss: 0.079915 [22032/70000]\n",
            "loss: 0.092726 [22048/70000]\n",
            "loss: 0.095054 [22064/70000]\n",
            "loss: 0.098014 [22080/70000]\n",
            "loss: 0.088334 [22096/70000]\n",
            "loss: 0.094755 [22112/70000]\n",
            "loss: 0.089917 [22128/70000]\n",
            "loss: 0.084737 [22144/70000]\n",
            "loss: 0.088950 [22160/70000]\n",
            "loss: 0.081884 [22176/70000]\n",
            "loss: 0.092128 [22192/70000]\n",
            "loss: 0.080788 [22208/70000]\n",
            "loss: 0.076526 [22224/70000]\n",
            "loss: 0.086458 [22240/70000]\n",
            "loss: 0.085998 [22256/70000]\n",
            "loss: 0.093091 [22272/70000]\n",
            "loss: 0.098265 [22288/70000]\n",
            "loss: 0.091521 [22304/70000]\n",
            "loss: 0.091527 [22320/70000]\n",
            "loss: 0.083506 [22336/70000]\n",
            "loss: 0.086128 [22352/70000]\n",
            "loss: 0.080833 [22368/70000]\n",
            "loss: 0.093627 [22384/70000]\n",
            "loss: 0.087005 [22400/70000]\n",
            "loss: 0.082361 [22416/70000]\n",
            "loss: 0.086041 [22432/70000]\n",
            "loss: 0.083466 [22448/70000]\n",
            "loss: 0.091768 [22464/70000]\n",
            "loss: 0.101434 [22480/70000]\n",
            "loss: 0.094316 [22496/70000]\n",
            "loss: 0.086747 [22512/70000]\n",
            "loss: 0.094924 [22528/70000]\n",
            "loss: 0.078309 [22544/70000]\n",
            "loss: 0.096253 [22560/70000]\n",
            "loss: 0.086985 [22576/70000]\n",
            "loss: 0.085258 [22592/70000]\n",
            "loss: 0.082115 [22608/70000]\n",
            "loss: 0.091430 [22624/70000]\n",
            "loss: 0.090753 [22640/70000]\n",
            "loss: 0.091606 [22656/70000]\n",
            "loss: 0.088397 [22672/70000]\n",
            "loss: 0.091098 [22688/70000]\n",
            "loss: 0.084378 [22704/70000]\n",
            "loss: 0.083015 [22720/70000]\n",
            "loss: 0.098664 [22736/70000]\n",
            "loss: 0.077827 [22752/70000]\n",
            "loss: 0.088970 [22768/70000]\n",
            "loss: 0.084111 [22784/70000]\n",
            "loss: 0.101083 [22800/70000]\n",
            "loss: 0.082026 [22816/70000]\n",
            "loss: 0.090990 [22832/70000]\n",
            "loss: 0.085513 [22848/70000]\n",
            "loss: 0.095582 [22864/70000]\n",
            "loss: 0.091316 [22880/70000]\n",
            "loss: 0.092827 [22896/70000]\n",
            "loss: 0.076480 [22912/70000]\n",
            "loss: 0.080357 [22928/70000]\n",
            "loss: 0.083065 [22944/70000]\n",
            "loss: 0.082683 [22960/70000]\n",
            "loss: 0.093548 [22976/70000]\n",
            "loss: 0.086265 [22992/70000]\n",
            "loss: 0.080719 [23008/70000]\n",
            "loss: 0.093830 [23024/70000]\n",
            "loss: 0.090656 [23040/70000]\n",
            "loss: 0.083535 [23056/70000]\n",
            "loss: 0.080522 [23072/70000]\n",
            "loss: 0.093672 [23088/70000]\n",
            "loss: 0.089781 [23104/70000]\n",
            "loss: 0.084532 [23120/70000]\n",
            "loss: 0.084701 [23136/70000]\n",
            "loss: 0.090132 [23152/70000]\n",
            "loss: 0.081138 [23168/70000]\n",
            "loss: 0.087215 [23184/70000]\n",
            "loss: 0.081751 [23200/70000]\n",
            "loss: 0.081297 [23216/70000]\n",
            "loss: 0.081964 [23232/70000]\n",
            "loss: 0.087675 [23248/70000]\n",
            "loss: 0.085908 [23264/70000]\n",
            "loss: 0.082230 [23280/70000]\n",
            "loss: 0.077379 [23296/70000]\n",
            "loss: 0.085763 [23312/70000]\n",
            "loss: 0.090860 [23328/70000]\n",
            "loss: 0.091969 [23344/70000]\n",
            "loss: 0.093496 [23360/70000]\n",
            "loss: 0.084973 [23376/70000]\n",
            "loss: 0.103213 [23392/70000]\n",
            "loss: 0.095656 [23408/70000]\n",
            "loss: 0.084569 [23424/70000]\n",
            "loss: 0.099040 [23440/70000]\n",
            "loss: 0.088976 [23456/70000]\n",
            "loss: 0.082053 [23472/70000]\n",
            "loss: 0.085358 [23488/70000]\n",
            "loss: 0.084260 [23504/70000]\n",
            "loss: 0.086418 [23520/70000]\n",
            "loss: 0.086985 [23536/70000]\n",
            "loss: 0.074366 [23552/70000]\n",
            "loss: 0.086891 [23568/70000]\n",
            "loss: 0.083374 [23584/70000]\n",
            "loss: 0.083628 [23600/70000]\n",
            "loss: 0.087614 [23616/70000]\n",
            "loss: 0.085653 [23632/70000]\n",
            "loss: 0.095496 [23648/70000]\n",
            "loss: 0.086139 [23664/70000]\n",
            "loss: 0.083339 [23680/70000]\n",
            "loss: 0.084431 [23696/70000]\n",
            "loss: 0.086185 [23712/70000]\n",
            "loss: 0.083725 [23728/70000]\n",
            "loss: 0.082039 [23744/70000]\n",
            "loss: 0.092696 [23760/70000]\n",
            "loss: 0.079761 [23776/70000]\n",
            "loss: 0.081200 [23792/70000]\n",
            "loss: 0.079895 [23808/70000]\n",
            "loss: 0.087043 [23824/70000]\n",
            "loss: 0.091303 [23840/70000]\n",
            "loss: 0.091632 [23856/70000]\n",
            "loss: 0.089929 [23872/70000]\n",
            "loss: 0.084459 [23888/70000]\n",
            "loss: 0.079560 [23904/70000]\n",
            "loss: 0.097074 [23920/70000]\n",
            "loss: 0.089701 [23936/70000]\n",
            "loss: 0.084921 [23952/70000]\n",
            "loss: 0.092785 [23968/70000]\n",
            "loss: 0.088891 [23984/70000]\n",
            "loss: 0.079386 [24000/70000]\n",
            "loss: 0.103204 [24016/70000]\n",
            "loss: 0.084629 [24032/70000]\n",
            "loss: 0.084820 [24048/70000]\n",
            "loss: 0.087268 [24064/70000]\n",
            "loss: 0.097351 [24080/70000]\n",
            "loss: 0.083918 [24096/70000]\n",
            "loss: 0.069312 [24112/70000]\n",
            "loss: 0.082048 [24128/70000]\n",
            "loss: 0.078852 [24144/70000]\n",
            "loss: 0.093435 [24160/70000]\n",
            "loss: 0.078464 [24176/70000]\n",
            "loss: 0.091902 [24192/70000]\n",
            "loss: 0.078337 [24208/70000]\n",
            "loss: 0.092650 [24224/70000]\n",
            "loss: 0.086524 [24240/70000]\n",
            "loss: 0.093398 [24256/70000]\n",
            "loss: 0.090600 [24272/70000]\n",
            "loss: 0.079249 [24288/70000]\n",
            "loss: 0.078877 [24304/70000]\n",
            "loss: 0.092662 [24320/70000]\n",
            "loss: 0.074126 [24336/70000]\n",
            "loss: 0.098018 [24352/70000]\n",
            "loss: 0.078948 [24368/70000]\n",
            "loss: 0.090465 [24384/70000]\n",
            "loss: 0.083803 [24400/70000]\n",
            "loss: 0.082109 [24416/70000]\n",
            "loss: 0.085996 [24432/70000]\n",
            "loss: 0.097792 [24448/70000]\n",
            "loss: 0.086026 [24464/70000]\n",
            "loss: 0.092988 [24480/70000]\n",
            "loss: 0.087482 [24496/70000]\n",
            "loss: 0.078838 [24512/70000]\n",
            "loss: 0.086645 [24528/70000]\n",
            "loss: 0.082422 [24544/70000]\n",
            "loss: 0.093322 [24560/70000]\n",
            "loss: 0.093069 [24576/70000]\n",
            "loss: 0.091582 [24592/70000]\n",
            "loss: 0.087685 [24608/70000]\n",
            "loss: 0.089548 [24624/70000]\n",
            "loss: 0.088758 [24640/70000]\n",
            "loss: 0.094017 [24656/70000]\n",
            "loss: 0.095276 [24672/70000]\n",
            "loss: 0.075667 [24688/70000]\n",
            "loss: 0.078374 [24704/70000]\n",
            "loss: 0.080581 [24720/70000]\n",
            "loss: 0.082508 [24736/70000]\n",
            "loss: 0.081614 [24752/70000]\n",
            "loss: 0.085699 [24768/70000]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import time\n",
        "start = time.time()\n",
        "\n",
        "for t in range(tp,epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    lr=optimizer.param_groups[0][\"lr\"]\n",
        "    print(lr)\n",
        "    train_ls = strain(train_loader, model, loss_fn, optimizer, scheduler)\n",
        "    correct, test_loss = test(test_loader, model, loss_fn)\n",
        "    train_lst.extend(train_ls)\n",
        "    test_lst.append(test_loss)\n",
        "    acc_lst.append(correct)\n",
        "\n",
        "    checkpoint = { # https://discuss.pytorch.org/t/saving-model-and-optimiser-and-scheduler/52030\n",
        "    'epoch': t+1,\n",
        "    'model': model.state_dict(),\n",
        "    'optimizer': optimizer.state_dict(),\n",
        "    'lr_sched': scheduler.state_dict()}\n",
        "    torch.save(checkpoint, pth)\n",
        "    # torch.save(model.state_dict(), pth)\n",
        "\n",
        "# print(\"Done!\")\n",
        "\n",
        "# end = time.time()\n",
        "# print(\"time: \",end - start)\n",
        "\n",
        "# print(len(train_lst), len(test_lst))\n",
        "# import matplotlib.pyplot as plt\n",
        "# plt.plot(train_lst)\n",
        "# plt.plot(np.linspace(0,len(train_lst),len(test_lst)), test_lst)\n",
        "# plt.show()\n",
        "# plt.plot(acc_lst)\n",
        "# plt.show()\n",
        "# plt.close()\n",
        "\n",
        "\n",
        "# torch.save(model.state_dict(), \"model.pth\")\n",
        "\n",
        "# resnet 18, 60/61 38.4%, 528s\n",
        "# resnet 18, 58/61 39.8%, 523s\n",
        "# resnet 18 compile , 58/61 40.4%, 555s\n",
        "# resnet 18 compile augment , 58/61 36.4%, 1941 # augment on cpu, takes longer\n",
        "# resnet 18 augment lr3e-4:3e-3, 58/61 37.7%, 1863s\n",
        "# resnet 18 augment 10epoch lr1e-5:3e-4, 58/61 33.5%, 3387s\n",
        "# resnet 18 compile lr1e-5:3e-4, 58/61 35.0%, 493s\n",
        "# resnet 18 compile scratch lr1e-5:1e-3, 58/61 26.8%, 475s\n",
        "# resnet 18 compile lr1e-5:1e-3, 55/61 47.3%, 480s\n",
        "# resnet 18 compile lr1e-5:1e-3, 52/61 51.7% 503s\n",
        "# resnet 18 compile lr1e-5:1e-3, unfreeze 51.0%, 550s\n",
        "# resnet 18 compile lr1e-5:1e-4, unfreeze 52.7%, 518s\n",
        "# resnet 34 compile lr1e-5:1e-4, unfreeze bitsadamW batch16*4\n",
        "# resnet 152 compileoverhead lr3e-7:3e-6, bitsadamW batch16*4 ckpt 53.8%, 2066s\n",
        "# resnet 152 from53.8% augment+cutout lr3e-7:3e-6, 53.8%, 2088s\n",
        "# resnet 152 comile augment+cutout lr1e-5 /4 1epoch 48.3%, 454s\n",
        "# resnet 152 comile augment+cutout lr1e-5 1epoch 48.4%, 446s\n",
        "# resnet 152 comilemaxautotue augment+cutout lr1e-5 1epoch 47.7%, 448\n",
        "# resnet 152 clipclean comilemax augment+cutout lr1e-5 10epoch 45.1%, 1585 *2\n",
        "# resnet 152 clipclean comilemax augmax lr1e-6:3e-5 20epoch \n",
        "# resnet 152 clipclean compilemax lr3e-7:1e-5,  \n",
        "# resnet 152 70k augment compilemax lr3e-7:3e-6 5epoch, 65.4% 11585s*5/3=19300s = 5h20m\n",
        "\n",
        "\n",
        "# vit b16 lr1e-5 5epochs 41.3%, 466s # 4.4ram, 5.5vram\n",
        "# vit l16 lr1e-5 5epochs # 32.0%, 1242s 4.5ram, 8.0vram\n",
        "# vit l16 lr3e-7;1e-5 5epochs # 45.4%, 1315s 4.5ram, 8.0vram\n",
        "# vit l32 lr1e-5 5epochs # .ram, .vram\n",
        "# vit_large_patch16_384\n",
        "# vit_base_patch16_224 maxcompile nockpt lr3e-7;1e-5 5epochs # 45.2%, 2272s 5.3ram, 11.0vram\n",
        "# vit_base_patch16_224 maxcompile nockpt gradacc batch16 1e-5, 1e-2 explode\n",
        "# vit_base_patch16_224 maxcompile nockpt gradacc batch16 1e-6, 1e-5 low acc, test nan\n",
        "# vit_base_patch16_224 maxcompile nockpt gradacc batch16 3e-7, 3e-6 4/5 epoch50%\n",
        "# vit_base_patch16_224 maxcompile nockpt gradacc batch16 1e-6, 1e-5 0.5aug\n",
        "\n",
        "\n",
        "\n",
        "# inception\n",
        "# inception og 10kclean 1e-7, 3e-5 batch64 nockpt 5epochs 0.020348  51.4%, 0.024312 20m; 1st 40.5% max 52.1\n",
        "# inception hid 10kclean 1e-7, 3e-5 batch64 nockpt 1/5epochs 0.019635 53.6%, 0.024120 1st 32.8%, max 53.6%\n",
        "# inception og 10kclean 1e-7, 1e-4 batch64 nockpt 2/10epochs 0.020348  0.020923 39.9%, 0.026105\n",
        "# inception og 70kg 1e-7 nope\n",
        "# inception og 70kg 3e-6 3e-5 17%\n",
        "# inception og 70kg 1e-6, 1e-5\n",
        "# inception og 70kg 3e-6 3e-5 0.5aug\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "correct, test_loss = test(test_loader, model, loss_fn)\n",
        "print(correct, test_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PFsnIRVz7Xp5",
        "outputId": "76ddef55-6687-41a3-b780-683303fe1226"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 46.1%, Avg loss: 0.099935\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pytorch release gpu ram after training\n",
        "# https://discuss.pytorch.org/t/free-all-gpu-memory-used-in-between-runs/168202/2\n",
        "\n",
        "# torch.cuda.empty_cache()\n",
        "\n",
        "# correct, test_loss = test(test_loader, model, loss_fn)\n",
        "# train_lst.extend(train_ls)\n",
        "# test_lst.append(test_loss)\n",
        "# acc_lst.append(correct)\n",
        "\n"
      ],
      "metadata": {
        "id": "ayRePbr_rq9F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 793
        },
        "id": "REiP7-nvhc4s",
        "outputId": "c354217f-b805-4791-8daa-6a999b98d6f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "\n",
            "===================================BUG REPORT===================================\n",
            "Welcome to bitsandbytes. For bug reports, please run\n",
            "\n",
            "python -m bitsandbytes\n",
            "\n",
            " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
            "================================================================================\n",
            "bin /usr/local/lib/python3.10/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so\n",
            "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
            "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
            "CUDA SETUP: Highest compute capability among GPUs detected: 7.5\n",
            "CUDA SETUP: Detected CUDA version 118\n",
            "CUDA SETUP: Loading binary /usr/local/lib/python3.10/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/lib64-nvidia did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/sys/fs/cgroup/memory.events /var/colab/cgroup/jupyter-children/memory.events')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('8013'), PosixPath('http'), PosixPath('//172.28.0.1')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//colab.research.google.com/tun/m/cc48301118ce562b961b3c22d803539adc1e0c19/gpu-t4-s-3jfl8a11tr2ty --tunnel_background_save_delay=10s --tunnel_periodic_background_save_frequency=30m0s --enable_output_coalescing=true --output_coalescing_required=true'), PosixPath('--logtostderr --listen_host=172.28.0.12 --target_host=172.28.0.12 --tunnel_background_save_url=https')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/env/python')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//ipykernel.pylab.backend_inline')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so'), PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
            "Either way, this might cause trouble in the future:\n",
            "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
            "  warn(msg)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-e59b6cefdc2e>\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mtp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodelsd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimsd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschedsd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodelsd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimsd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschedsd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'optimizer' is not defined"
          ]
        }
      ],
      "source": [
        "# @title save\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# # pth='/content/drive/MyDrive/frame/vit.pth'\n",
        "# pth='/content/drive/MyDrive/frame/res55.pth'\n",
        "# pth='/content/drive/MyDrive/frame/resnet152.pth'\n",
        "# pth='/content/drive/MyDrive/frame/inception.pth'\n",
        "# pth='/content/inception2.pth'\n",
        "\n",
        "pth='/content/res15270kg.pth'\n",
        "\n",
        "\n",
        "tp, modelsd, optimsd, schedsd = torch.load(pth).values()\n",
        "model.load_state_dict(modelsd)\n",
        "optimizer.load_state_dict(optimsd)\n",
        "scheduler.load_state_dict(schedsd)\n",
        "\n",
        "\n",
        "# checkpoint = { # https://discuss.pytorch.org/t/saving-model-and-optimiser-and-scheduler/52030\n",
        "# 'epoch': t,\n",
        "# 'model': model.state_dict(),\n",
        "# 'optimizer': optimizer.state_dict(),\n",
        "# 'lr_sched': scheduler.state_dict()}\n",
        "# torch.save(checkpoint, pth)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# torch.save(model.state_dict(), pth)\n",
        "# model.load_state_dict(torch.load(pth))\n",
        "# # model.load_state_dict(torch.load(\"model.pth\"))\n",
        "\n",
        "# !gdown 1---4fdFbOUBTrS-VP5Va6pKowfgoU2UN -O inception2.pth\n",
        "# !gdown 1visTNvWmnuV7jAm2TBiAIIrNjbOAi1Fv -O resnet152.pth\n",
        "\n",
        "# # t, modelsd, optimsd, scheduler = torch.load('/content/drive/MyDrive/frame/resnet152.pth').values()\n",
        "# t, modelsd, optimsd, scheduler = torch.load('/content/resnet152.pth').values()\n",
        "# model.load_state_dict(modelsd)\n",
        "# # optimizer.load_state_dict(optimsd)\n",
        "\n",
        "# # matt152 # https://drive.google.com/file/d/1MQ0xLfHbio458uEVbn2VyMpD3bij2A4J/view?usp=sharing\n",
        "# !gdown 1MQ0xLfHbio458uEVbn2VyMpD3bij2A4J -O res152.pth\n",
        "# model.load_state_dict(torch.load(\"res152.pth\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A57FKq_YGadX",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title trash\n",
        "# dir='/content/google_street_view'\n",
        "\n",
        "# # data = datasets.ImageFolder(dir, transform=transform)\n",
        "# data = datasets.ImageFolder(dir, transform=None)\n",
        "# torch.manual_seed(0)\n",
        "# train_data, test_data = torch.utils.data.random_split(data, [.85,.15])\n",
        "\n",
        "# # train_data = DatasetWrap(train_data, TrainTransform()) # apply data augmentation to train dataset only\n",
        "# train_data = DatasetWrap(train_data, transform)\n",
        "# test_data = DatasetWrap(test_data, transform)\n",
        "\n",
        "\n",
        "# batch_size = 16 # 64\n",
        "# num_batches=int(np.ceil(len(train_data)/batch_size))\n",
        "\n",
        "# train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
        "# test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
        "# # data, train_data, test_data = None, None, None\n",
        "\n",
        "\n",
        "# dataiter = iter(train_loader)\n",
        "# images, labels = next(dataiter)\n",
        "# imshow(torchvision.utils.make_grid(images))\n",
        "# dataiter=None\n",
        "\n",
        "\n",
        "%matplotlib inline \n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# dataiter = iter(train_data)\n",
        "\n",
        "\n",
        "# img,label = next(iter(sample_ds))\n",
        "i=5\n",
        "# print(len(test_data))\n",
        "# img,label=test_data[i]\n",
        "img,label=sample_ds[i]\n",
        "print(img.shape)\n",
        "# print(type(img))\n",
        "model.eval()\n",
        "pred=model(img.unsqueeze(0).to(device))\n",
        "pred_probab = nn.Softmax(dim=1)(pred)\n",
        "y_pred = pred_probab.argmax(1)\n",
        "print(pred_probab)\n",
        "print(\"pred: \",y_pred.item())\n",
        "# print(img)\n",
        "# image=images[0]\n",
        "\n",
        "print(\"actual: \",label)\n",
        "plt.imshow(np.transpose(img, (1, 2, 0)))\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q_3Cttn1qHcc",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title trash\n",
        "\n",
        "model.eval()\n",
        "count=0\n",
        "i=138\n",
        "rong_lst=[]\n",
        "\n",
        "while count<20:\n",
        "    img,label=test_data[i]\n",
        "    pred=model(img.unsqueeze(0).to(device))\n",
        "    pred_probab = nn.Softmax(dim=1)(pred)\n",
        "    y_pred = pred_probab.argmax(1)\n",
        "    if y_pred.item() != label:\n",
        "        print(\"pred: \",y_pred.item(),\", actual: \",label)\n",
        "        # plt.imshow(np.transpose(img, (1, 2, 0)))\n",
        "        # plt.show()\n",
        "        imshow(img)\n",
        "        rong_lst.append(img)\n",
        "        count+=1\n",
        "    i+=1\n",
        "\n",
        "\n",
        "# 20/137 wrong\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OJs2bVL0rXnG"
      },
      "outputs": [],
      "source": [
        "print(i)\n",
        "# imshow(torchvision.utils.make_grid(images,nrow=4))\n",
        "# print(torch.stack(rong_lst).shape)\n",
        "# print(len(rong_lst))\n",
        "# print(rong_lst[0].shape)\n",
        "imshow(torchvision.utils.make_grid(torch.stack(rong_lst),nrow=4))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}