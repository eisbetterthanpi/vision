{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eisbetterthanpi/vision/blob/main/CLIP_cleaner.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## from clean"
      ],
      "metadata": {
        "id": "KkdGmMCTTVhN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "IBRVTY9lbGm8"
      },
      "outputs": [],
      "source": [
        "# @title setup / model\n",
        "# https://colab.research.google.com/github/openai/clip/blob/master/notebooks/Interacting_with_CLIP.ipynb\n",
        "! pip install ftfy regex tqdm\n",
        "! pip install git+https://github.com/openai/CLIP.git\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "# from pkg_resources import packaging\n",
        "# print(\"Torch version:\", torch.__version__)\n",
        "import clip\n",
        "# clip.available_models()\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model, preprocess = clip.load(\"ViT-B/32\") # preprocess: normalize intensity using dataset mean and sd then resize and center-crop to conform to the model expects. \n",
        "# model.cuda().eval()\n",
        "model.to(device).eval()\n",
        "input_resolution = model.visual.input_resolution # 224\n",
        "context_length = model.context_length # 77\n",
        "vocab_size = model.vocab_size # 49408\n",
        "# print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\") # 151,277,313\n",
        "# clip.tokenize(\"Hello World!\") # case-insensitive tokenizer, padded to become 77 tokens\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "2kQGpnynh2oQ"
      },
      "outputs": [],
      "source": [
        "# @title download 10k (old)\n",
        "# https://drive.google.com/file/d/1NkCNecLpFG3i7bo3Vl9RQSwzBpSRQ29q/view?usp=sharing\n",
        "!gdown 1NkCNecLpFG3i7bo3Vl9RQSwzBpSRQ29q -O gsv.zip\n",
        "!unzip /content/gsv.zip -d /content\n",
        "import shutil\n",
        "shutil.rmtree('/content/google_street_view/meta_data', ignore_errors=True) # delete the meta_data folder\n",
        "\n",
        "shutil.rmtree('/content/google_street_view/01/02 Drop (More than 90% Obscured)', ignore_errors=True)\n",
        "shutil.rmtree('/content/google_street_view/01/03 Drop (Multiple Types of Property)', ignore_errors=True)\n",
        "\n",
        "# move files # https://www.geeksforgeeks.org/how-to-move-all-files-from-one-directory-to-another-using-python/\n",
        "source = '/content/google_street_view/01/01 Keep'\n",
        "destination = '/content/google_street_view/01'\n",
        "import os\n",
        "allfiles = os.listdir(source)\n",
        "for f in allfiles:\n",
        "    src_path = os.path.join(source, f)\n",
        "    dst_path = os.path.join(destination, f)\n",
        "    shutil.move(src_path, dst_path)\n",
        "\n",
        "shutil.rmtree('/content/google_street_view/01/01 Keep', ignore_errors=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xOhdXG3Vf36C",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title download 70k\n",
        "# https://drive.google.com/file/d/1-7ZC29k4VxXQkpnOuLfj7Ag_SFTM4LV8/view?usp=share_link\n",
        "# !gdown 1-7ZC29k4VxXQkpnOuLfj7Ag_SFTM4LV8 -O gsv.zip\n",
        "# !unzip /content/gsv.zip -d /\n",
        "# !rm -R /content/gsv70k/.ipynb_checkpoints\n",
        "# !rm -R /content/gsv70k/01/.ipynb_checkpoints\n",
        "# !rm -R /content/gsv70k/02/.ipynb_checkpoints\n",
        "# !rm -R /content/gsv70k/03/.ipynb_checkpoints\n",
        "# !rm -R /content/gsv70k/04/.ipynb_checkpoints\n",
        "# !rm -R /content/gsv70k/05/.ipynb_checkpoints\n",
        "# !rm -R /content/gsv70k/06/.ipynb_checkpoints\n",
        "\n",
        "# 70k_clip\n",
        "# https://drive.google.com/file/d/1OqKQcwWpb6TFKAgcqeq0UhzWYRBvlyEj/view?usp=sharing\n",
        "!gdown 1OqKQcwWpb6TFKAgcqeq0UhzWYRBvlyEj -O gsv.zip\n",
        "!unzip /content/gsv.zip -d /\n",
        "!rm -R /content/gsv70k_clip/.ipynb_checkpoints\n",
        "!rm -R /content/gsv70k_clip/01/.ipynb_checkpoints\n",
        "!rm -R /content/gsv70k_clip/02/.ipynb_checkpoints\n",
        "!rm -R /content/gsv70k_clip/03/.ipynb_checkpoints\n",
        "!rm -R /content/gsv70k_clip/04/.ipynb_checkpoints\n",
        "!rm -R /content/gsv70k_clip/05/.ipynb_checkpoints\n",
        "!rm -R /content/gsv70k_clip/06/.ipynb_checkpoints\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title gmap_frombad\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!unzip /content/drive/MyDrive/ggmap_frombad3.zip -d /\n",
        "!unzip /content/drive/MyDrive/gmap_frombad3.zip -d /\n",
        "\n",
        "!rm -R /content/ggmap_frombad/.ipynb_checkpoints\n",
        "!rm -R /content/ggmap_frombad/01/.ipynb_checkpoints\n",
        "!rm -R /content/ggmap_frombad/02/.ipynb_checkpoints\n",
        "!rm -R /content/ggmap_frombad/03/.ipynb_checkpoints\n",
        "!rm -R /content/ggmap_frombad/04/.ipynb_checkpoints\n",
        "!rm -R /content/ggmap_frombad/05/.ipynb_checkpoints\n",
        "!rm -R /content/ggmap_frombad/06/.ipynb_checkpoints\n",
        "\n",
        "!rm -R /content/gmap_frombad/.ipynb_checkpoints\n",
        "!rm -R /content/gmap_frombad/01/.ipynb_checkpoints\n",
        "!rm -R /content/gmap_frombad/02/.ipynb_checkpoints\n",
        "!rm -R /content/gmap_frombad/03/.ipynb_checkpoints\n",
        "!rm -R /content/gmap_frombad/04/.ipynb_checkpoints\n",
        "!rm -R /content/gmap_frombad/05/.ipynb_checkpoints\n",
        "!rm -R /content/gmap_frombad/06/.ipynb_checkpoints\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "2nJnsQJloKai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CPO1zSyzzvXw",
        "outputId": "9cbafadb-afd5-4cd1-fc1a-4209ef286407",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f46c8238d90>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "# @title data\n",
        "from torchvision import datasets#, transforms\n",
        "\n",
        "# dir='/content/google_street_view'\n",
        "dir='/content/gsv70k'\n",
        "\n",
        "# # data = datasets.ImageFolder(dir, transform=transform)\n",
        "data = datasets.ImageFolder(dir, transform=None)\n",
        "torch.manual_seed(0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HBgCanxi8JKw",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title test texts\n",
        "images=[]\n",
        "og_img=[]\n",
        "\n",
        "c=30\n",
        "# for i in range(c,c+10):\n",
        "for i in range(c,c+15):\n",
        "    img,label=data[i]\n",
        "    images.append(preprocess(img))\n",
        "    og_img.append(img)\n",
        "\n",
        "# 21\n",
        "\n",
        "# blur 7 43 122\n",
        "# repeat 88-89\n",
        "# texts=['a clear image of a house','a blurred image','a plain background']\n",
        "# an image without a house ; \n",
        "# texts=['a clear image of a house','an image of a house ','a blurred image','a plain background']\n",
        "# texts=['a clear image of a house','an image of a wall','an image of trees','an image of a vehicle','a blurred image','a plain background']\n",
        "# texts=['an image of a house','a house facade','an image of a construction site','a plain background']\n",
        "texts=['a house','a construction site','an image of a wall','an image of trees','an image of a vehicle','a blurred image','a plain background','a plain shade of color']\n",
        "\n",
        "# housing facade, construction site, house front\n",
        "\n",
        "\n",
        "# normalize images, tokenize text input, forward pass model to get image text features\n",
        "image_input = torch.tensor(np.stack(images)).to(device)\n",
        "text_tokens = clip.tokenize([\"This is \" + desc for desc in texts]).to(device)\n",
        "# text_tokens = clip.tokenize(texts).to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    image_features = model.encode_image(image_input).float()\n",
        "    text_features = model.encode_text(text_tokens).float()\n",
        "\n",
        "# Calculating cosine similarity: normalize features and calculate dot product of each pair.\n",
        "image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "similarity = text_features.cpu().numpy() @ image_features.cpu().numpy().T\n",
        "# print(similarity)\n",
        "\n",
        "count = len(texts)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "# plt.figure(figsize=(20, 14))\n",
        "plt.figure(figsize=(30, 14))\n",
        "plt.imshow(similarity, vmin=0.1, vmax=0.3)\n",
        "# plt.colorbar()\n",
        "plt.yticks(range(count), texts, fontsize=18)\n",
        "plt.xticks([])\n",
        "for i, image in enumerate(og_img):\n",
        "    plt.imshow(image, extent=(i - 0.5, i + 0.5, -1.6, -0.6), origin=\"lower\")\n",
        "for x in range(similarity.shape[1]):\n",
        "    for y in range(similarity.shape[0]):\n",
        "        plt.text(x, y, f\"{similarity[y, x]:.2f}\", ha=\"center\", va=\"center\", size=12)\n",
        "\n",
        "for side in [\"left\", \"top\", \"right\", \"bottom\"]:\n",
        "    plt.gca().spines[side].set_visible(False)\n",
        "\n",
        "plt.xlim([-0.5, len(og_img) - 0.5])\n",
        "# plt.ylim([count + 0.5, -2])\n",
        "plt.ylim([count - 0.5, -1.5]) # bottom, top margin\n",
        "plt.show()\n",
        "# plt.title(\"Cosine similarity between text and image features\", size=20)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "hBTkUZDJ8Rs-"
      },
      "outputs": [],
      "source": [
        "# @title base get similarity\n",
        "\n",
        "images=[]\n",
        "og_img=[]\n",
        "\n",
        "c=110\n",
        "for i in range(c,c+10):\n",
        "    img,label=data[i]\n",
        "    images.append(preprocess(img))\n",
        "    og_img.append(img)\n",
        "\n",
        "# blur 7 43\n",
        "# repeat 88-89\n",
        "texts=['a clear image of a house','an image without a house','a blurred image','a plain background']\n",
        "\n",
        "# normalize images, tokenize text input, forward pass model to get image text features\n",
        "image_input = torch.tensor(np.stack(images)).to(device)\n",
        "text_tokens = clip.tokenize([\"This is \" + desc for desc in texts]).to(device)\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "    image_features = model.encode_image(image_input).float()\n",
        "    text_features = model.encode_text(text_tokens).float()\n",
        "\n",
        "# Calculating cosine similarity: normalize features and calculate dot product of each pair.\n",
        "image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "# similarity = text_features.cpu().numpy() @ image_features.cpu().numpy().T\n",
        "\n",
        "similarity = text_features @ image_features.T\n",
        "print(similarity)\n",
        "\n",
        "# blur 0.23\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "3j_xML2VFcfT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "outputId": "9bd4bef2-95f1-4026-8b4c-d330d740259f"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-49-779bf85a11cb>\u001b[0m in \u001b[0;36m<cell line: 25>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RGB\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mimg\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mNormalized\u001b[0m \u001b[0mTensor\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \"\"\"\n\u001b[0;32m--> 277\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mnormalize\u001b[0;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[1;32m    357\u001b[0m     \"\"\"\n\u001b[1;32m    358\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_scripting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_tracing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m         \u001b[0m_log_api_usage_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"img should be Tensor Image. Got {type(tensor)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/utils.py\u001b[0m in \u001b[0;36m_log_api_usage_once\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m    560\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFunctionType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m         \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 562\u001b[0;31m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_log_api_usage_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{module}.{name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# @title collect\n",
        "# use clip to get similarity scores for all images\n",
        "import os\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import transforms\n",
        "\n",
        "# texts=['a clear image of a house','an image without a house','a blurred image','a plain background']\n",
        "# texts=['a clear image of a house','an image of a wall','an image of trees','an image of a vehicle','a blurred image','a plain background']\n",
        "texts=['a house','a construction site','an image of a wall','an image of trees','an image of a vehicle','a blurred image','a plain background','a plain shade of color']\n",
        "\n",
        "text_tokens = clip.tokenize([\"This is \" + desc for desc in texts]).to(device)\n",
        "with torch.no_grad(): text_features = model.encode_text(text_tokens).float()\n",
        "text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "\n",
        "# img_dir = '/content/google_street_view/06'\n",
        "# img_dir = '/content/gsv70k/01'\n",
        "# img_dir = '/content/gmap_frombad/01'\n",
        "# img_dir = '/content/ggmap_frombad/01'\n",
        "\n",
        "sall=[]\n",
        "\n",
        "t=0\n",
        "for x in range(1,7):\n",
        "    # img_dir = '/content/gmap_frombad/0'+str(x)\n",
        "    # img_dir = '/content/ggmap_frombad/0'+str(x)\n",
        "    img_dir = '/content/gsv70k_clip/0'+str(x)\n",
        "    for filename in os.listdir(img_dir):\n",
        "        # print(filename)\n",
        "        name = os.path.splitext(filename)[0]\n",
        "        img_file=os.path.join(img_dir, filename)\n",
        "        image = Image.open(img_file).convert(\"RGB\")\n",
        "        img= preprocess(image).to(device).unsqueeze(0)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            image_features = model.encode_image(img).float()\n",
        "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "        similarity = text_features @ image_features.T\n",
        "        # print(similarity.T)\n",
        "\n",
        "        # plt.figure(figsize=(4, 1.25)) # plt.figure(figsize=(16, 5))\n",
        "        # plt.imshow(image)\n",
        "        # plt.show()\n",
        "\n",
        "        sall.append([img_file,similarity.squeeze()])\n",
        "\n",
        "        # t+=1\n",
        "        # if t >=5: break\n",
        "\n",
        "# print(sall)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "id": "P1ubEgNLM2Z1",
        "outputId": "95fb690a-5891-4d15-9d51-e344b12002fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sall 2918\n",
            "good 1502\n",
            "wall 545\n",
            "obscured 56\n",
            "blurred 321\n",
            "missing 0\n",
            "left 494\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# @title \n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.axis('off')\n",
        "\n",
        "\n",
        "def imshow(img):\n",
        "    # img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.figure(figsize = (16,5))\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "def file2img(img_files):\n",
        "    imgs=[]\n",
        "    for img_file in img_files:\n",
        "        image = Image.open(img_file).convert(\"RGB\")\n",
        "        image=transforms.ToTensor()(image)\n",
        "        imgs.append(image)\n",
        "    return imgs\n",
        "\n",
        "good, wall, obscured, blurred, missing, left = [],[],[],[],[],[]\n",
        "good_s, wall_s, obscured_s, blurred_s, missing_s, left_s = [],[],[],[],[],[]\n",
        "\n",
        "# texts=['a house','a construction site','an image of a wall','an image of trees','an image of a vehicle','a blurred image','a plain background','a plain shade of color']\n",
        "\n",
        "t=0\n",
        "for img_file, similarity in sall:\n",
        "    houseness = similarity[0]\n",
        "    constructionness = similarity[1]\n",
        "    wallness = similarity[2]\n",
        "    treeness = similarity[3]\n",
        "    vehicleness = similarity[4]\n",
        "    blurrness = similarity[5]\n",
        "    missingness = (similarity[6]+similarity[7])/2\n",
        "\n",
        "    # print(img_file, similarity)\n",
        "    if wallness>=0.22: # wall 0.22\n",
        "    # if wallness>=0.21 and wallness<0.22: # wall\n",
        "        wall.append(img_file)\n",
        "        wall_s.append(similarity)\n",
        "    # # elif houseness<0.21: # obscured? house\n",
        "    elif houseness<0.21: # obscured? house 0.21\n",
        "    # if houseness<0.22 and houseness>=0.21: # obscured? house 0.21\n",
        "        obscured.append(img_file)\n",
        "        obscured_s.append(similarity)\n",
        "    elif blurrness>=0.22: # blur\n",
        "        blurred.append(img_file)\n",
        "        blurred_s.append(similarity)\n",
        "    elif missingness>=0.22: # missing\n",
        "        missing.append(img_file)\n",
        "        missing_s.append(similarity)\n",
        "    # elif constructionness-houseness>=-0.01: #-0.01\n",
        "    # elif treeness-houseness>=-0.01:\n",
        "    # elif vehicleness-houseness>=-0.01:\n",
        "    # elif similarity[1:].sum()-7*similarity[0]>=-0.01: #0.75\n",
        "    # elif constructionness-houseness>=0.01 or treeness-houseness>=0.01 or vehicleness-houseness>=0.01 or similarity[1:].sum()-7*similarity[0]>=0.01:\n",
        "    elif houseness-constructionness<0.01 or houseness-treeness<0.01 or houseness-vehicleness<0.01 or 7*houseness-similarity[1:].sum()<0.18:\n",
        "        left.append(img_file)\n",
        "        left_s.append(7*houseness-similarity[1:].sum())\n",
        "    else:\n",
        "        good.append(img_file)\n",
        "        good_s.append(similarity)\n",
        "    # print(similarity[1:].sum()-similarity[0])\n",
        "    # t+=1\n",
        "    # if t >=5: break\n",
        "\n",
        "\n",
        "\n",
        "print(\"sall\",len(sall))\n",
        "print(\"good\",len(good))\n",
        "print(\"wall\",len(wall))\n",
        "print(\"obscured\",len(obscured))\n",
        "print(\"blurred\",len(blurred))\n",
        "print(\"missing\",len(missing))\n",
        "print(\"left\",len(left))\n",
        "\n",
        "c=0\n",
        "# imshow(torchvision.utils.make_grid(file2img(good[c:c+64])))\n",
        "# imshow(torchvision.utils.make_grid(file2img(wall),nrow=8))\n",
        "# imshow(torchvision.utils.make_grid(file2img(obscured),nrow=6))\n",
        "# imshow(torchvision.utils.make_grid(file2img(blurred),nrow=5))\n",
        "# imshow(torchvision.utils.make_grid(file2img(missing),nrow=5))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NR2txF0jkQ7-",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title display imgs\n",
        "c=0\n",
        "\n",
        "def imshow(img):\n",
        "    # img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    # plt.figure(figsize = (20,15))\n",
        "    plt.figure(figsize=(30, 14))\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "# display a grid of the images in each category, make sure images are collated correctly\n",
        "# careful not to display too many images, else ram will explode\n",
        "\n",
        "# imshow(torchvision.utils.make_grid(file2img(good[c:c+64])))\n",
        "\n",
        "imshow(torchvision.utils.make_grid(file2img(good[c:c+64])))\n",
        "# imshow(torchvision.utils.make_grid(file2img(wall[c:c+64]),nrow=8))\n",
        "# imshow(torchvision.utils.make_grid(file2img(obscured[c:c+64]),nrow=8))\n",
        "# imshow(torchvision.utils.make_grid(file2img(obscured[c:]),nrow=10))\n",
        "# imshow(torchvision.utils.make_grid(file2img(blurred[c:64]),nrow=6))\n",
        "# imshow(torchvision.utils.make_grid(file2img(missing[c:]),nrow=10))\n",
        "# imshow(torchvision.utils.make_grid(file2img(left[c:c+100]),nrow=10))\n",
        "# imshow(torchvision.utils.make_grid(file2img(left[c:c+64])))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(obscured[c:c+64])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xg_yG5AOMf3D",
        "outputId": "2a061ea2-bed7-4177-d971-6c38aec65cc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['/content/gsv70k/01/01. 20 BUTTERFLY AVE 2022-03.jpg', '/content/gsv70k/01/01. 9 JLN LABU AYER 2019-05.jpg', '/content/gsv70k/01/01. 7 LUCKY CRES 2022-09.jpg', '/content/gsv70k/01/01. 1A GUILLEMARD RD 2022-04.jpg', '/content/gsv70k/01/01. 102 JLN KURAS 2019-08.jpg', '/content/gsv70k/01/01. 38 PEACH GDN 2022-10.jpg', '/content/gsv70k/01/01. 30 JLN ASAS 2022-11.jpg', '/content/gsv70k/01/01. 17 ELITE TER 2022-09.jpg', '/content/gsv70k/01/01. 8 HAIG AVE 2022-05.jpg', '/content/gsv70k/01/01. 22 LIMAU GDN 2022-09.jpg', '/content/gsv70k/01/01. 92A GUILLEMARD RD 2022-09.jpg', '/content/gsv70k/01/01. 94 BRANKSOME RD 2022-03.jpg', '/content/gsv70k/01/01. 4 FIDELIO ST 2023-01.jpg', '/content/gsv70k/01/01. 132 MARSHALL RD 2022-10.jpg', '/content/gsv70k/01/01. 54 LICHI AVE 2018-03.jpg', '/content/gsv70k/01/01. 17 NAROOMA RD 2022-09.jpg', '/content/gsv70k/01/01. 22A BRIGHTON AVE 2019-04.jpg', '/content/gsv70k/01/01. 24 ROBERTS LN 2022-10.jpg', '/content/gsv70k/01/01. 442 MACPHERSON RD 2022-03.jpg', '/content/gsv70k/01/01. 23 JLN MENGKUDU 2011-03.jpg', '/content/gsv70k/01/01. 61 JLN SETIA 2018-01.jpg', '/content/gsv70k/01/01. 18 LI PO AVE 2019-08.jpg', '/content/gsv70k/01/01. 48 WEST COAST LN 2022-05.jpg', '/content/gsv70k/01/01. 9A JLN HJ SALAM 2022-09.jpg', '/content/gsv70k/01/01. 6 COLCHESTER GR 2022-03.jpg', '/content/gsv70k/01/01. 441 PUNGGOL RD 2022-08.jpg', '/content/gsv70k/01/01. 7 HAPPY AVE CENTRAL 2022-03.jpg', '/content/gsv70k/01/01. 255 MACPHERSON RD 2020-10.jpg', '/content/gsv70k/01/01. 161 HILLCREST RD 2015-04.jpg', '/content/gsv70k/01/01. 393 GUILLEMARD RD 2020-02.jpg', '/content/gsv70k/01/01. 38 FABER TER 2022-02.jpg', '/content/gsv70k/01/01. 697 UP CHANGI RD EAST 2022-10.jpg', '/content/gsv70k/01/01. 8 JLN NOVENA SELATAN 2022-05.jpg', '/content/gsv70k/01/01. 2 MAYFLOWER PL 2021-06.jpg', '/content/gsv70k/01/01. 379 GUILLEMARD RD 2022-09.jpg', '/content/gsv70k/01/01. 60 GREENLEAF DR 2022-11.jpg', '/content/gsv70k/01/01. 2 NAMLY DR 2019-06.jpg', '/content/gsv70k/01/01. 15 JLN BINGKA 2008-12.jpg', '/content/gsv70k/01/01. 545 PASIR PANJANG RD 2021-11.jpg', '/content/gsv70k/01/01. 30 BIN TONG PK 2022-03.jpg', '/content/gsv70k/01/01. 41 SOMMERVILLE RD 2013-01.jpg', '/content/gsv70k/01/01. 92 GUILLEMARD RD 2022-09.jpg', '/content/gsv70k/01/01. 6 MERINO CRES 2022-02.jpg', '/content/gsv70k/01/01. 45 ONAN RD 2022-10.jpg', '/content/gsv70k/01/01. 399A GUILLEMARD RD 2018-04.jpg', '/content/gsv70k/01/01. 8 HOLLAND GR TER 2022-09.jpg', '/content/gsv70k/01/01. 5 HOW SUN RD 2022-03.jpg', '/content/gsv70k/01/01. 4D DYSON RD 2023-04.jpg', '/content/gsv70k/01/01. 257A BT TIMAH RD 2023-04.jpg', '/content/gsv70k/01/01. 87 JLN GELENGGANG 2022-08.jpg', '/content/gsv70k/01/01. 4 LAKME ST 2022-09.jpg', '/content/gsv70k/01/01. 8 NORFOLK RD 2023-01.jpg', '/content/gsv70k/01/01. 10 LINDEN DR 2023-03.jpg', '/content/gsv70k/01/01. 671 EAST COAST RD 2023-01.jpg', '/content/gsv70k/01/01. 103 JLN PARI BURONG 2022-09.jpg', '/content/gsv70k/01/01. 22 JLN KETUMBIT 2022-09.jpg', '/content/gsv70k/01/01. 33 JLN LEPAS 2022-09.jpg', '/content/gsv70k/01/01. 82 JLN PARI BURONG 2022-09.jpg', '/content/gsv70k/01/01. 13A EDEN GR 2021-11.jpg', '/content/gsv70k/01/01. 3 CHIAP GUAN AVE 2022-04.jpg', '/content/gsv70k/01/01. 75 JLN ASAS 2022-11.jpg', '/content/gsv70k/01/01. 2 JLN SHAER 2022-05.jpg', '/content/gsv70k/01/01. 60 CARDIFF GR 2022-03.jpg', '/content/gsv70k/01/01. 77 JLN ASAS 2020-09.jpg']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i,x in enumerate(left_s):\n",
        "    print(i, x.item())"
      ],
      "metadata": {
        "id": "5WhiFg75l4Ax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ni8N7RVX4-j7",
        "outputId": "5aecd438-d1b1-46f1-81c7-293980df6a71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 14717\n",
            "2 8540\n",
            "3 19817\n",
            "4 14812\n",
            "5 9559\n",
            "6 493\n",
            "tt 67938\n"
          ]
        }
      ],
      "source": [
        "# @title count imgs\n",
        "# count num of imgs in each class\n",
        "tt=0\n",
        "for x in range(1,7):\n",
        "    allfiles = os.listdir('/content/gsv70kg/0'+str(x))\n",
        "    # allfiles = os.listdir('/content/gmap_clean/0'+str(x))\n",
        "    # allfiles = os.listdir('/content/ggmap/0'+str(x))\n",
        "    # allfiles = os.listdir('/content/ggmap_frombad/0'+str(x))\n",
        "    print(x, len(allfiles))\n",
        "    tt+=len(allfiles)\n",
        "print(\"tt\",tt)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title delete bad files\n",
        "# delete files collated in wall, obscured, blurred, missing, left\n",
        "import os\n",
        "c=0\n",
        "for lst in [wall, obscured, blurred, missing, left]:\n",
        "    for x in lst:\n",
        "        # print(x)\n",
        "        try: os.remove(x)\n",
        "        except OSError: pass\n",
        "        # c+=1\n",
        "        # if c>5: break\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "SpelubpdydL-"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title make new folders\n",
        "import os\n",
        "for x in range(1,7):\n",
        "    fol = '/content/ggmap/0'+str(x)\n",
        "    if not os.path.exists(fol):\n",
        "        os.makedirs(fol)"
      ],
      "metadata": {
        "id": "DL7t6SGS_Vta",
        "cellView": "form"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "y4EJBjkWq3yn"
      },
      "outputs": [],
      "source": [
        "# @title move files\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# allfiles = os.listdir(source)\n",
        "# for f in allfiles:\n",
        "#     src_path = os.path.join(source, f)\n",
        "#     dst_path = os.path.join(destination, f)\n",
        "#     shutil.copyfile(src_path, dst_path)\n",
        "\n",
        "\n",
        "c=0\n",
        "source = '/content/gmap_clean/'\n",
        "destination = '/content/gsv70k_clip/'\n",
        "for x in range(1,7):\n",
        "    allfiles = os.listdir(source+'0'+str(x))\n",
        "    # # for src_path in good:\n",
        "    for f in allfiles:\n",
        "        # f=src_path.split('/')[-1]\n",
        "\n",
        "        src_path = source+f[:2]+'/' +f\n",
        "        dst_path = destination+f[:2]+'/' +f\n",
        "        # print(src_path, dst_path)\n",
        "        shutil.copyfile(src_path, dst_path)\n",
        "        # c+=1\n",
        "        # if c>5: break\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title resize imgs\n",
        "# gmap: 1024,648 ; api: 640, 400\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "c=0\n",
        "for x in range(1,7):\n",
        "    fol = '/content/gmap/0'+str(x)\n",
        "    gfol = '/content/ggmap/0'+str(x)\n",
        "    files=os.listdir(fol)\n",
        "    for file in files:\n",
        "        # print(file)\n",
        "        im = Image.open(fol+'/'+file)\n",
        "        width, height = im.size\n",
        "        # print(width, height)\n",
        "        im = im.resize((640, 400))\n",
        "        im = im.convert('RGB')\n",
        "        im.save(gfol+'/'+file)\n",
        "        # print(fol+'/'+file)\n",
        "        # print(gfol+'/'+file)\n",
        "        # c+=1\n",
        "        # if c>5: break\n"
      ],
      "metadata": {
        "id": "ZALozrR7AKvI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1s0ZndLj9BWk",
        "outputId": "92f17ffc-1ae8-4edb-f685-4839c5d1d3c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SZ-H2B03842e"
      },
      "outputs": [],
      "source": [
        "!ls -a /content/gsv70kg\n",
        "# !ls\n",
        "\n",
        "!rm -R /content/gsv70kg/.ipynb_checkpoints\n",
        "!rm -R /content/gsv70kg/01/.ipynb_checkpoints\n",
        "!rm -R /content/gsv70kg/02/.ipynb_checkpoints\n",
        "!rm -R /content/gsv70kg/03/.ipynb_checkpoints\n",
        "!rm -R /content/gsv70kg/04/.ipynb_checkpoints\n",
        "!rm -R /content/gsv70kg/05/.ipynb_checkpoints\n",
        "!rm -R /content/gsv70kg/06/.ipynb_checkpoints\n",
        "\n",
        "!zip -r /content/drive/MyDrive/gsv70kg.zip /content/gsv70kg\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ys0gy78NAETm"
      },
      "outputs": [],
      "source": [
        "# !unzip /content/gsv.zip -d /content\n",
        "!unzip /content/drive/MyDrive/gmap_clean.zip -d /\n",
        "# !zip -r /content/drive/MyDrive/gmap_clean.zip /content/gmap_clean\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "shutil.rmtree('/content/sampl/', ignore_errors=True)\n"
      ],
      "metadata": {
        "id": "Va179pCmXgCo"
      },
      "execution_count": 45,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}