{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eisbetterthanpi/vision/blob/main/CLIP_cleaner.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## from clean"
      ],
      "metadata": {
        "id": "KkdGmMCTTVhN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "IBRVTY9lbGm8"
      },
      "outputs": [],
      "source": [
        "# @title setup / model\n",
        "# https://colab.research.google.com/github/openai/clip/blob/master/notebooks/Interacting_with_CLIP.ipynb\n",
        "! pip install ftfy regex tqdm\n",
        "! pip install git+https://github.com/openai/CLIP.git\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "# from pkg_resources import packaging\n",
        "# print(\"Torch version:\", torch.__version__)\n",
        "import clip\n",
        "# clip.available_models()\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model, preprocess = clip.load(\"ViT-B/32\") # preprocess: normalize intensity using dataset mean and sd then resize and center-crop to conform to the model expects. \n",
        "# model.cuda().eval()\n",
        "model.to(device).eval()\n",
        "input_resolution = model.visual.input_resolution # 224\n",
        "context_length = model.context_length # 77\n",
        "vocab_size = model.vocab_size # 49408\n",
        "# print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\") # 151,277,313\n",
        "# clip.tokenize(\"Hello World!\") # case-insensitive tokenizer, padded to become 77 tokens\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "2kQGpnynh2oQ"
      },
      "outputs": [],
      "source": [
        "# @title download 10k (old)\n",
        "# https://drive.google.com/file/d/1NkCNecLpFG3i7bo3Vl9RQSwzBpSRQ29q/view?usp=sharing\n",
        "!gdown 1NkCNecLpFG3i7bo3Vl9RQSwzBpSRQ29q -O gsv.zip\n",
        "!unzip /content/gsv.zip -d /content\n",
        "import shutil\n",
        "shutil.rmtree('/content/google_street_view/meta_data', ignore_errors=True) # delete the meta_data folder\n",
        "\n",
        "shutil.rmtree('/content/google_street_view/01/02 Drop (More than 90% Obscured)', ignore_errors=True)\n",
        "shutil.rmtree('/content/google_street_view/01/03 Drop (Multiple Types of Property)', ignore_errors=True)\n",
        "\n",
        "# move files # https://www.geeksforgeeks.org/how-to-move-all-files-from-one-directory-to-another-using-python/\n",
        "source = '/content/google_street_view/01/01 Keep'\n",
        "destination = '/content/google_street_view/01'\n",
        "import os\n",
        "allfiles = os.listdir(source)\n",
        "for f in allfiles:\n",
        "    src_path = os.path.join(source, f)\n",
        "    dst_path = os.path.join(destination, f)\n",
        "    shutil.move(src_path, dst_path)\n",
        "\n",
        "shutil.rmtree('/content/google_street_view/01/01 Keep', ignore_errors=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xOhdXG3Vf36C",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title download 70k\n",
        "# https://drive.google.com/file/d/1-7ZC29k4VxXQkpnOuLfj7Ag_SFTM4LV8/view?usp=share_link\n",
        "!gdown 1-7ZC29k4VxXQkpnOuLfj7Ag_SFTM4LV8 -O gsv.zip\n",
        "!unzip /content/gsv.zip -d /\n",
        "\n",
        "!rm -R /content/gsv70k/.ipynb_checkpoints\n",
        "!rm -R /content/gsv70k/01/.ipynb_checkpoints\n",
        "!rm -R /content/gsv70k/02/.ipynb_checkpoints\n",
        "!rm -R /content/gsv70k/03/.ipynb_checkpoints\n",
        "!rm -R /content/gsv70k/04/.ipynb_checkpoints\n",
        "!rm -R /content/gsv70k/05/.ipynb_checkpoints\n",
        "!rm -R /content/gsv70k/06/.ipynb_checkpoints\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CPO1zSyzzvXw",
        "outputId": "9cbafadb-afd5-4cd1-fc1a-4209ef286407",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f46c8238d90>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "# @title data\n",
        "from torchvision import datasets#, transforms\n",
        "\n",
        "# dir='/content/google_street_view'\n",
        "dir='/content/gsv70k'\n",
        "\n",
        "# # data = datasets.ImageFolder(dir, transform=transform)\n",
        "data = datasets.ImageFolder(dir, transform=None)\n",
        "torch.manual_seed(0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HBgCanxi8JKw"
      },
      "outputs": [],
      "source": [
        "# @title test texts\n",
        "images=[]\n",
        "og_img=[]\n",
        "\n",
        "c=30\n",
        "# for i in range(c,c+10):\n",
        "for i in range(c,c+15):\n",
        "    img,label=data[i]\n",
        "    images.append(preprocess(img))\n",
        "    og_img.append(img)\n",
        "\n",
        "# 21\n",
        "\n",
        "# blur 7 43 122\n",
        "# repeat 88-89\n",
        "# texts=['a clear image of a house','a blurred image','a plain background']\n",
        "# an image without a house ; \n",
        "# texts=['a clear image of a house','an image of a house ','a blurred image','a plain background']\n",
        "# texts=['a clear image of a house','an image of a wall','an image of trees','an image of a vehicle','a blurred image','a plain background']\n",
        "# texts=['an image of a house','a house facade','an image of a construction site','a plain background']\n",
        "texts=['a house','a construction site','an image of a wall','an image of trees','an image of a vehicle','a blurred image','a plain background','a plain shade of color']\n",
        "\n",
        "# housing facade, construction site, house front\n",
        "\n",
        "\n",
        "# normalize images, tokenize text input, forward pass model to get image text features\n",
        "image_input = torch.tensor(np.stack(images)).to(device)\n",
        "text_tokens = clip.tokenize([\"This is \" + desc for desc in texts]).to(device)\n",
        "# text_tokens = clip.tokenize(texts).to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    image_features = model.encode_image(image_input).float()\n",
        "    text_features = model.encode_text(text_tokens).float()\n",
        "\n",
        "# Calculating cosine similarity: normalize features and calculate dot product of each pair.\n",
        "image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "similarity = text_features.cpu().numpy() @ image_features.cpu().numpy().T\n",
        "# print(similarity)\n",
        "\n",
        "count = len(texts)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "# plt.figure(figsize=(20, 14))\n",
        "plt.figure(figsize=(30, 14))\n",
        "plt.imshow(similarity, vmin=0.1, vmax=0.3)\n",
        "# plt.colorbar()\n",
        "plt.yticks(range(count), texts, fontsize=18)\n",
        "plt.xticks([])\n",
        "for i, image in enumerate(og_img):\n",
        "    plt.imshow(image, extent=(i - 0.5, i + 0.5, -1.6, -0.6), origin=\"lower\")\n",
        "for x in range(similarity.shape[1]):\n",
        "    for y in range(similarity.shape[0]):\n",
        "        plt.text(x, y, f\"{similarity[y, x]:.2f}\", ha=\"center\", va=\"center\", size=12)\n",
        "\n",
        "for side in [\"left\", \"top\", \"right\", \"bottom\"]:\n",
        "    plt.gca().spines[side].set_visible(False)\n",
        "\n",
        "plt.xlim([-0.5, len(og_img) - 0.5])\n",
        "# plt.ylim([count + 0.5, -2])\n",
        "plt.ylim([count - 0.5, -1.5]) # bottom, top margin\n",
        "plt.show()\n",
        "# plt.title(\"Cosine similarity between text and image features\", size=20)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "hBTkUZDJ8Rs-"
      },
      "outputs": [],
      "source": [
        "# @title base get similarity\n",
        "\n",
        "images=[]\n",
        "og_img=[]\n",
        "\n",
        "c=110\n",
        "for i in range(c,c+10):\n",
        "    img,label=data[i]\n",
        "    images.append(preprocess(img))\n",
        "    og_img.append(img)\n",
        "\n",
        "# blur 7 43\n",
        "# repeat 88-89\n",
        "texts=['a clear image of a house','an image without a house','a blurred image','a plain background']\n",
        "\n",
        "# normalize images, tokenize text input, forward pass model to get image text features\n",
        "image_input = torch.tensor(np.stack(images)).to(device)\n",
        "text_tokens = clip.tokenize([\"This is \" + desc for desc in texts]).to(device)\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "    image_features = model.encode_image(image_input).float()\n",
        "    text_features = model.encode_text(text_tokens).float()\n",
        "\n",
        "# Calculating cosine similarity: normalize features and calculate dot product of each pair.\n",
        "image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "# similarity = text_features.cpu().numpy() @ image_features.cpu().numpy().T\n",
        "\n",
        "similarity = text_features @ image_features.T\n",
        "print(similarity)\n",
        "\n",
        "# blur 0.23\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3j_xML2VFcfT"
      },
      "outputs": [],
      "source": [
        "# @title collect\n",
        "import os\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import transforms\n",
        "\n",
        "# texts=['a clear image of a house','an image without a house','a blurred image','a plain background']\n",
        "# texts=['a clear image of a house','an image of a wall','an image of trees','an image of a vehicle','a blurred image','a plain background']\n",
        "texts=['a house','a construction site','an image of a wall','an image of trees','an image of a vehicle','a blurred image','a plain background','a plain shade of color']\n",
        "\n",
        "text_tokens = clip.tokenize([\"This is \" + desc for desc in texts]).to(device)\n",
        "with torch.no_grad(): text_features = model.encode_text(text_tokens).float()\n",
        "text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "\n",
        "# img_dir = '/content/google_street_view/06'\n",
        "img_dir = '/content/gsv70k/01'\n",
        "\n",
        "sall=[]\n",
        "\n",
        "t=0\n",
        "for filename in os.listdir(img_dir):\n",
        "    # print(filename)\n",
        "    name = os.path.splitext(filename)[0]\n",
        "    img_file=os.path.join(img_dir, filename)\n",
        "    image = Image.open(img_file).convert(\"RGB\")\n",
        "   \n",
        "    img= preprocess(image).to(device).unsqueeze(0)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        image_features = model.encode_image(img).float()\n",
        "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "    similarity = text_features @ image_features.T\n",
        "    # print(similarity.T)\n",
        "\n",
        "    # plt.figure(figsize=(4, 1.25)) # plt.figure(figsize=(16, 5))\n",
        "    # plt.imshow(image)\n",
        "    # plt.show()\n",
        "\n",
        "    sall.append([img_file,similarity.squeeze()])\n",
        "\n",
        "    # t+=1\n",
        "    # if t >=5: break\n",
        "\n",
        "# print(sall)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "id": "P1ubEgNLM2Z1",
        "outputId": "6e2b52f7-793a-4db1-85e1-2795d2f72186"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sall 15234\n",
            "good 14556\n",
            "wall 0\n",
            "obscured 345\n",
            "blurred 298\n",
            "missing 35\n",
            "left 0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# @title \n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "plt.axis('off')\n",
        "\n",
        "\n",
        "def imshow(img):\n",
        "    # img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.figure(figsize = (16,5))\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "def file2img(img_files):\n",
        "    imgs=[]\n",
        "    for img_file in img_files:\n",
        "        image = Image.open(img_file).convert(\"RGB\")\n",
        "        image=transforms.ToTensor()(image)\n",
        "        imgs.append(image)\n",
        "    return imgs\n",
        "\n",
        "good, wall, obscured, blurred, missing, left = [],[],[],[],[],[]\n",
        "good_s, wall_s, obscured_s, blurred_s, missing_s, left_s = [],[],[],[],[],[]\n",
        "\n",
        "# texts=['a house','a construction site','an image of a wall','an image of trees','an image of a vehicle','a blurred image','a plain background','a plain shade of color']\n",
        "\n",
        "t=0\n",
        "for img_file, similarity in sall:\n",
        "    houseness = similarity[0]\n",
        "    constructionness = similarity[1]\n",
        "    wallness = similarity[2]\n",
        "    treeness = similarity[3]\n",
        "    vehicleness = similarity[4]\n",
        "    blurrness = similarity[5]\n",
        "    missingness = (similarity[6]+similarity[7])/2\n",
        "\n",
        "    # print(img_file, similarity)\n",
        "    # if wallness>=0.23: # wall\n",
        "    # if wallness>=0.21 and wallness<0.22: # wall\n",
        "    #     wall.append(img_file)\n",
        "    #     wall_s.append(similarity)\n",
        "    # elif houseness<0.21: # obscured? house\n",
        "    if houseness<0.21: # obscured? house\n",
        "        obscured.append(img_file)\n",
        "        obscured_s.append(similarity)\n",
        "    elif blurrness>=0.23: # blur\n",
        "        blurred.append(img_file)\n",
        "        blurred_s.append(similarity)\n",
        "    elif missingness>=0.23: # missing\n",
        "        missing.append(img_file)\n",
        "        missing_s.append(similarity)\n",
        "    # elif similarity[1:].sum()-similarity[0]>=0.75: #0.75\n",
        "    #     left.append(img_file)\n",
        "    #     left_s.append(similarity)\n",
        "    else:\n",
        "        good.append(img_file)\n",
        "        good_s.append(similarity)\n",
        "    # print(similarity[1:].sum()-similarity[0])\n",
        "    # t+=1\n",
        "    # if t >=5: break\n",
        "\n",
        "\n",
        "\n",
        "print(\"sall\",len(sall))\n",
        "print(\"good\",len(good))\n",
        "print(\"wall\",len(wall))\n",
        "print(\"obscured\",len(obscured))\n",
        "print(\"blurred\",len(blurred))\n",
        "print(\"missing\",len(missing))\n",
        "print(\"left\",len(left))\n",
        "\n",
        "c=0\n",
        "# imshow(torchvision.utils.make_grid(file2img(good[c:c+64])))\n",
        "# imshow(torchvision.utils.make_grid(file2img(wall),nrow=8))\n",
        "# imshow(torchvision.utils.make_grid(file2img(obscured),nrow=6))\n",
        "# imshow(torchvision.utils.make_grid(file2img(blurred),nrow=5))\n",
        "# imshow(torchvision.utils.make_grid(file2img(missing),nrow=5))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NR2txF0jkQ7-"
      },
      "outputs": [],
      "source": [
        "c=0\n",
        "\n",
        "def imshow(img):\n",
        "    # img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    # plt.figure(figsize = (20,15))\n",
        "    plt.figure(figsize=(30, 14))\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "# imshow(torchvision.utils.make_grid(file2img(good[c:c+64])))\n",
        "\n",
        "# imshow(torchvision.utils.make_grid(file2img(good[c:c+64])))\n",
        "# imshow(torchvision.utils.make_grid(file2img(wall[c:c+64]),nrow=8))\n",
        "imshow(torchvision.utils.make_grid(file2img(obscured[c:c+64]),nrow=8))\n",
        "# imshow(torchvision.utils.make_grid(file2img(blurred),nrow=6))\n",
        "# imshow(torchvision.utils.make_grid(file2img(left),nrow=7))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(obscured[c:c+64])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xg_yG5AOMf3D",
        "outputId": "2a061ea2-bed7-4177-d971-6c38aec65cc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['/content/gsv70k/01/01. 20 BUTTERFLY AVE 2022-03.jpg', '/content/gsv70k/01/01. 9 JLN LABU AYER 2019-05.jpg', '/content/gsv70k/01/01. 7 LUCKY CRES 2022-09.jpg', '/content/gsv70k/01/01. 1A GUILLEMARD RD 2022-04.jpg', '/content/gsv70k/01/01. 102 JLN KURAS 2019-08.jpg', '/content/gsv70k/01/01. 38 PEACH GDN 2022-10.jpg', '/content/gsv70k/01/01. 30 JLN ASAS 2022-11.jpg', '/content/gsv70k/01/01. 17 ELITE TER 2022-09.jpg', '/content/gsv70k/01/01. 8 HAIG AVE 2022-05.jpg', '/content/gsv70k/01/01. 22 LIMAU GDN 2022-09.jpg', '/content/gsv70k/01/01. 92A GUILLEMARD RD 2022-09.jpg', '/content/gsv70k/01/01. 94 BRANKSOME RD 2022-03.jpg', '/content/gsv70k/01/01. 4 FIDELIO ST 2023-01.jpg', '/content/gsv70k/01/01. 132 MARSHALL RD 2022-10.jpg', '/content/gsv70k/01/01. 54 LICHI AVE 2018-03.jpg', '/content/gsv70k/01/01. 17 NAROOMA RD 2022-09.jpg', '/content/gsv70k/01/01. 22A BRIGHTON AVE 2019-04.jpg', '/content/gsv70k/01/01. 24 ROBERTS LN 2022-10.jpg', '/content/gsv70k/01/01. 442 MACPHERSON RD 2022-03.jpg', '/content/gsv70k/01/01. 23 JLN MENGKUDU 2011-03.jpg', '/content/gsv70k/01/01. 61 JLN SETIA 2018-01.jpg', '/content/gsv70k/01/01. 18 LI PO AVE 2019-08.jpg', '/content/gsv70k/01/01. 48 WEST COAST LN 2022-05.jpg', '/content/gsv70k/01/01. 9A JLN HJ SALAM 2022-09.jpg', '/content/gsv70k/01/01. 6 COLCHESTER GR 2022-03.jpg', '/content/gsv70k/01/01. 441 PUNGGOL RD 2022-08.jpg', '/content/gsv70k/01/01. 7 HAPPY AVE CENTRAL 2022-03.jpg', '/content/gsv70k/01/01. 255 MACPHERSON RD 2020-10.jpg', '/content/gsv70k/01/01. 161 HILLCREST RD 2015-04.jpg', '/content/gsv70k/01/01. 393 GUILLEMARD RD 2020-02.jpg', '/content/gsv70k/01/01. 38 FABER TER 2022-02.jpg', '/content/gsv70k/01/01. 697 UP CHANGI RD EAST 2022-10.jpg', '/content/gsv70k/01/01. 8 JLN NOVENA SELATAN 2022-05.jpg', '/content/gsv70k/01/01. 2 MAYFLOWER PL 2021-06.jpg', '/content/gsv70k/01/01. 379 GUILLEMARD RD 2022-09.jpg', '/content/gsv70k/01/01. 60 GREENLEAF DR 2022-11.jpg', '/content/gsv70k/01/01. 2 NAMLY DR 2019-06.jpg', '/content/gsv70k/01/01. 15 JLN BINGKA 2008-12.jpg', '/content/gsv70k/01/01. 545 PASIR PANJANG RD 2021-11.jpg', '/content/gsv70k/01/01. 30 BIN TONG PK 2022-03.jpg', '/content/gsv70k/01/01. 41 SOMMERVILLE RD 2013-01.jpg', '/content/gsv70k/01/01. 92 GUILLEMARD RD 2022-09.jpg', '/content/gsv70k/01/01. 6 MERINO CRES 2022-02.jpg', '/content/gsv70k/01/01. 45 ONAN RD 2022-10.jpg', '/content/gsv70k/01/01. 399A GUILLEMARD RD 2018-04.jpg', '/content/gsv70k/01/01. 8 HOLLAND GR TER 2022-09.jpg', '/content/gsv70k/01/01. 5 HOW SUN RD 2022-03.jpg', '/content/gsv70k/01/01. 4D DYSON RD 2023-04.jpg', '/content/gsv70k/01/01. 257A BT TIMAH RD 2023-04.jpg', '/content/gsv70k/01/01. 87 JLN GELENGGANG 2022-08.jpg', '/content/gsv70k/01/01. 4 LAKME ST 2022-09.jpg', '/content/gsv70k/01/01. 8 NORFOLK RD 2023-01.jpg', '/content/gsv70k/01/01. 10 LINDEN DR 2023-03.jpg', '/content/gsv70k/01/01. 671 EAST COAST RD 2023-01.jpg', '/content/gsv70k/01/01. 103 JLN PARI BURONG 2022-09.jpg', '/content/gsv70k/01/01. 22 JLN KETUMBIT 2022-09.jpg', '/content/gsv70k/01/01. 33 JLN LEPAS 2022-09.jpg', '/content/gsv70k/01/01. 82 JLN PARI BURONG 2022-09.jpg', '/content/gsv70k/01/01. 13A EDEN GR 2021-11.jpg', '/content/gsv70k/01/01. 3 CHIAP GUAN AVE 2022-04.jpg', '/content/gsv70k/01/01. 75 JLN ASAS 2022-11.jpg', '/content/gsv70k/01/01. 2 JLN SHAER 2022-05.jpg', '/content/gsv70k/01/01. 60 CARDIFF GR 2022-03.jpg', '/content/gsv70k/01/01. 77 JLN ASAS 2020-09.jpg']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(left_s)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5WhiFg75l4Ax",
        "outputId": "00e54b0c-e7b3-4c03-b342-5b80c84d6457"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[tensor([0.2588, 0.2153, 0.2204, 0.1922, 0.2036, 0.1981], device='cuda:0'), tensor([0.2622, 0.2066, 0.2095, 0.1831, 0.2175, 0.2039], device='cuda:0'), tensor([0.2368, 0.2295, 0.1959, 0.1861, 0.1971, 0.1988], device='cuda:0'), tensor([0.2632, 0.2237, 0.1719, 0.2090, 0.2146, 0.1990], device='cuda:0'), tensor([0.2475, 0.2091, 0.2055, 0.1842, 0.2139, 0.1887], device='cuda:0'), tensor([0.2735, 0.2111, 0.2127, 0.2035, 0.2118, 0.1952], device='cuda:0'), tensor([0.2473, 0.2092, 0.1994, 0.1879, 0.2120, 0.2006], device='cuda:0'), tensor([0.2725, 0.2243, 0.2244, 0.2021, 0.2206, 0.2066], device='cuda:0'), tensor([0.2566, 0.2152, 0.1986, 0.1996, 0.2087, 0.1870], device='cuda:0'), tensor([0.2710, 0.2086, 0.2057, 0.2141, 0.2058, 0.1955], device='cuda:0'), tensor([0.2617, 0.2294, 0.2168, 0.2003, 0.2072, 0.1885], device='cuda:0'), tensor([0.2557, 0.2176, 0.2092, 0.1921, 0.1966, 0.1934], device='cuda:0'), tensor([0.2471, 0.2273, 0.2330, 0.1875, 0.1929, 0.1968], device='cuda:0'), tensor([0.2473, 0.2198, 0.2385, 0.1931, 0.2003, 0.1906], device='cuda:0'), tensor([0.2580, 0.2071, 0.1972, 0.1982, 0.2122, 0.1999], device='cuda:0'), tensor([0.2676, 0.2248, 0.2204, 0.1869, 0.2067, 0.2092], device='cuda:0'), tensor([0.2349, 0.2234, 0.2391, 0.1860, 0.1953, 0.1978], device='cuda:0'), tensor([0.2551, 0.2242, 0.2048, 0.1956, 0.2034, 0.1951], device='cuda:0'), tensor([0.2557, 0.2186, 0.2186, 0.1909, 0.2067, 0.2017], device='cuda:0'), tensor([0.2306, 0.1901, 0.1916, 0.1954, 0.2077, 0.1997], device='cuda:0'), tensor([0.2443, 0.2014, 0.2331, 0.1852, 0.2051, 0.1945], device='cuda:0'), tensor([0.2631, 0.2111, 0.2421, 0.1851, 0.2043, 0.1974], device='cuda:0'), tensor([0.2561, 0.2188, 0.2063, 0.1888, 0.2131, 0.1904], device='cuda:0'), tensor([0.2528, 0.2166, 0.2291, 0.1916, 0.2043, 0.1896], device='cuda:0'), tensor([0.2699, 0.2236, 0.2152, 0.2010, 0.2077, 0.1935], device='cuda:0'), tensor([0.2525, 0.2156, 0.1980, 0.1937, 0.2116, 0.2034], device='cuda:0'), tensor([0.2543, 0.2280, 0.2050, 0.2087, 0.2035, 0.1933], device='cuda:0'), tensor([0.2486, 0.2268, 0.2588, 0.2220, 0.2224, 0.2104], device='cuda:0'), tensor([0.2327, 0.2147, 0.2292, 0.1871, 0.2020, 0.2046], device='cuda:0'), tensor([0.2504, 0.2094, 0.2183, 0.1867, 0.2060, 0.1901], device='cuda:0'), tensor([0.2775, 0.2092, 0.1988, 0.2210, 0.2123, 0.1935], device='cuda:0'), tensor([0.2488, 0.1946, 0.2185, 0.1970, 0.2131, 0.2018], device='cuda:0'), tensor([0.2441, 0.2075, 0.2285, 0.1874, 0.2060, 0.1933], device='cuda:0'), tensor([0.2538, 0.2281, 0.2022, 0.1986, 0.2018, 0.1793], device='cuda:0'), tensor([0.2570, 0.2282, 0.1919, 0.2188, 0.2169, 0.2080], device='cuda:0'), tensor([0.2488, 0.2175, 0.2012, 0.1887, 0.2048, 0.2086], device='cuda:0'), tensor([0.2474, 0.2136, 0.2103, 0.1879, 0.1987, 0.1952], device='cuda:0'), tensor([0.2543, 0.2211, 0.2109, 0.1887, 0.1947, 0.1965], device='cuda:0'), tensor([0.2440, 0.2104, 0.2157, 0.1878, 0.1973, 0.1844], device='cuda:0'), tensor([0.2444, 0.2111, 0.2403, 0.2047, 0.2217, 0.2027], device='cuda:0'), tensor([0.2625, 0.2248, 0.2308, 0.2090, 0.2146, 0.1991], device='cuda:0'), tensor([0.2401, 0.2022, 0.2288, 0.2028, 0.1983, 0.1913], device='cuda:0'), tensor([0.2337, 0.1937, 0.1737, 0.2301, 0.2038, 0.1848], device='cuda:0'), tensor([0.2426, 0.2251, 0.1793, 0.1976, 0.2035, 0.1968], device='cuda:0'), tensor([0.2666, 0.2122, 0.2199, 0.1993, 0.2061, 0.2021], device='cuda:0'), tensor([0.2673, 0.2296, 0.2313, 0.2135, 0.2162, 0.2118], device='cuda:0'), tensor([0.2423, 0.2256, 0.2104, 0.1935, 0.1984, 0.1968], device='cuda:0'), tensor([0.2587, 0.2147, 0.2255, 0.1850, 0.2031, 0.1871], device='cuda:0'), tensor([0.2540, 0.2088, 0.2153, 0.1824, 0.2024, 0.2004], device='cuda:0'), tensor([0.2750, 0.2216, 0.1941, 0.2001, 0.2184, 0.1922], device='cuda:0'), tensor([0.2468, 0.1987, 0.1977, 0.1964, 0.2128, 0.2029], device='cuda:0'), tensor([0.2466, 0.1946, 0.2267, 0.1773, 0.2141, 0.1940], device='cuda:0'), tensor([0.2429, 0.2084, 0.2096, 0.1791, 0.2061, 0.2024], device='cuda:0'), tensor([0.2385, 0.2192, 0.2220, 0.1788, 0.1975, 0.2031], device='cuda:0'), tensor([0.2584, 0.2204, 0.2070, 0.1882, 0.2062, 0.1988], device='cuda:0'), tensor([0.2519, 0.2201, 0.2336, 0.2006, 0.2162, 0.2194], device='cuda:0'), tensor([0.2420, 0.2160, 0.2259, 0.1909, 0.2085, 0.1896], device='cuda:0'), tensor([0.2311, 0.2015, 0.1808, 0.2370, 0.1893, 0.1842], device='cuda:0'), tensor([0.2446, 0.2214, 0.2079, 0.2419, 0.1955, 0.1940], device='cuda:0'), tensor([0.2519, 0.2264, 0.2078, 0.1841, 0.2026, 0.1978], device='cuda:0'), tensor([0.2709, 0.2256, 0.2063, 0.1960, 0.2093, 0.1984], device='cuda:0'), tensor([0.2338, 0.2075, 0.2053, 0.1784, 0.2068, 0.1936], device='cuda:0'), tensor([0.2358, 0.2042, 0.2254, 0.1837, 0.1946, 0.1849], device='cuda:0'), tensor([0.2594, 0.2285, 0.2017, 0.1952, 0.2178, 0.1936], device='cuda:0'), tensor([0.2472, 0.2004, 0.2350, 0.2026, 0.2256, 0.1845], device='cuda:0'), tensor([0.2558, 0.2074, 0.2208, 0.1967, 0.2190, 0.1976], device='cuda:0'), tensor([0.2591, 0.2288, 0.2472, 0.1888, 0.1980, 0.2085], device='cuda:0'), tensor([0.2602, 0.2263, 0.2089, 0.1877, 0.2053, 0.2122], device='cuda:0'), tensor([0.2671, 0.2184, 0.2134, 0.2023, 0.2152, 0.2081], device='cuda:0'), tensor([0.2414, 0.2195, 0.2371, 0.2070, 0.2058, 0.2034], device='cuda:0')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PqMLY8uJ09MV"
      },
      "outputs": [],
      "source": [
        "# for i, sim in enumerate(good_s):\n",
        "# for i, sim in enumerate(left_s):\n",
        "for i, sim in enumerate(obscured_s):\n",
        "    print(i,sim,(sim.sum()-2*sim[0]).item())\n",
        "\n",
        "# print(good)\n",
        "# 03: 1 10\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ni8N7RVX4-j7",
        "outputId": "d97e69f5-ed39-4cc8-e019-bcc48999b70e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "381\n"
          ]
        }
      ],
      "source": [
        "# shutil.rmtree('/content/gsv/02/', ignore_errors=True)\n",
        "\n",
        "allfiles = os.listdir('/content/gsv/06/')\n",
        "print(len(allfiles))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y4EJBjkWq3yn"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# allfiles = os.listdir(source)\n",
        "# for f in allfiles:\n",
        "#     src_path = os.path.join(source, f)\n",
        "#     dst_path = os.path.join(destination, f)\n",
        "#     shutil.copyfile(src_path, dst_path)\n",
        "\n",
        "source = '/content/google_street_view/06/'\n",
        "destination = '/content/gsv/06'\n",
        "c=0\n",
        "for src_path in good:\n",
        "    f=src_path.split('/')[-1]\n",
        "    # print('/'.split(name))\n",
        "    # print(destination, f)\n",
        "    dst_path = os.path.join(destination, f)\n",
        "    shutil.copyfile(src_path, dst_path)\n",
        "    # print(src_path, dst_path)\n",
        "    # c+=1\n",
        "    # if c>5: break\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1s0ZndLj9BWk",
        "outputId": "54beed70-7c44-4b58-ef6c-4c62e9f1c433"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZ-H2B03842e",
        "outputId": "b749e148-9610-4547-b665-cdd32d344b16"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ".  ..  01  02  03  04  05  06\n",
            "drive  google_street_view  gsv\tgsv_clip.zip  gsv.zip  sample_data\n"
          ]
        }
      ],
      "source": [
        "!ls -a /content/gsv\n",
        "# !rm -R /content/gsv/.ipynb_checkpoints\n",
        "# %cd /content\n",
        "!ls\n",
        "# !zip -r /content/drive/MyDrive/gsv/gsv_clip.zip /content/gsv\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ys0gy78NAETm"
      },
      "outputs": [],
      "source": [
        "!unzip /content/gsv.zip -d /content\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}