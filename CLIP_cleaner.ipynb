{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eisbetterthanpi/vision/blob/main/CLIP_cleaner.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0BpdJkdBssk9",
        "cellView": "form"
      },
      "source": [
        "# @title setup\n",
        "# https://colab.research.google.com/github/openai/clip/blob/master/notebooks/Interacting_with_CLIP.ipynb\n",
        "! pip install ftfy regex tqdm\n",
        "! pip install git+https://github.com/openai/CLIP.git\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBRVTY9lbGm8",
        "cellView": "form"
      },
      "source": [
        "# @title model\n",
        "import numpy as np\n",
        "import torch\n",
        "# from pkg_resources import packaging\n",
        "# print(\"Torch version:\", torch.__version__)\n",
        "import clip\n",
        "# clip.available_models()\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model, preprocess = clip.load(\"ViT-B/32\") # preprocess: normalize intensity using dataset mean and sd then resize and center-crop to conform to the model expects. \n",
        "# model.cuda().eval()\n",
        "model.to(device).eval()\n",
        "input_resolution = model.visual.input_resolution # 224\n",
        "context_length = model.context_length # 77\n",
        "vocab_size = model.vocab_size # 49408\n",
        "# print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\") # 151,277,313\n",
        "# clip.tokenize(\"Hello World!\") # case-insensitive tokenizer, padded to become 77 tokens\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "2kQGpnynh2oQ"
      },
      "outputs": [],
      "source": [
        "# @title download\n",
        "# https://drive.google.com/file/d/1NkCNecLpFG3i7bo3Vl9RQSwzBpSRQ29q/view?usp=sharing\n",
        "!gdown 1NkCNecLpFG3i7bo3Vl9RQSwzBpSRQ29q -O gsv.zip\n",
        "!unzip /content/gsv.zip -d /content\n",
        "import shutil\n",
        "shutil.rmtree('/content/google_street_view/meta_data', ignore_errors=True) # delete the meta_data folder\n",
        "\n",
        "shutil.rmtree('/content/google_street_view/01/02 Drop (More than 90% Obscured)', ignore_errors=True)\n",
        "shutil.rmtree('/content/google_street_view/01/03 Drop (Multiple Types of Property)', ignore_errors=True)\n",
        "\n",
        "# move files # https://www.geeksforgeeks.org/how-to-move-all-files-from-one-directory-to-another-using-python/\n",
        "source = '/content/google_street_view/01/01 Keep'\n",
        "destination = '/content/google_street_view/01'\n",
        "import os\n",
        "allfiles = os.listdir(source)\n",
        "for f in allfiles:\n",
        "    src_path = os.path.join(source, f)\n",
        "    dst_path = os.path.join(destination, f)\n",
        "    shutil.move(src_path, dst_path)\n",
        "\n",
        "shutil.rmtree('/content/google_street_view/01/01 Keep', ignore_errors=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title data\n",
        "from torchvision import datasets#, transforms\n",
        "\n",
        "dir='/content/google_street_view'\n",
        "\n",
        "# data = datasets.ImageFolder(dir, transform=transform)\n",
        "data = datasets.ImageFolder(dir, transform=None)\n",
        "torch.manual_seed(0)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CPO1zSyzzvXw",
        "outputId": "046d4d9f-663b-4e8b-b948-86c6cca0e337",
        "cellView": "form"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fd8dc89dc10>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBgCanxi8JKw",
        "cellView": "form"
      },
      "source": [
        "# @title test texts\n",
        "images=[]\n",
        "og_img=[]\n",
        "\n",
        "c=0\n",
        "for i in range(c,c+10):\n",
        "    img,label=data[i]\n",
        "    images.append(preprocess(img))\n",
        "    og_img.append(img)\n",
        "\n",
        "# blur 7 43 122\n",
        "# repeat 88-89\n",
        "# texts=['a clear image of a house','a blurred image','a plain background']\n",
        "# an image without a house ; \n",
        "# texts=['a clear image of a house','an image of a house ','a blurred image','a plain background']\n",
        "texts=['a clear image of a house','an image of a wall','an image of trees','an image of a vehicle','an image of a ','a blurred image','a plain background']\n",
        "\n",
        "\n",
        "# normalize images, tokenize text input, forward pass model to get image text features\n",
        "image_input = torch.tensor(np.stack(images)).to(device)\n",
        "text_tokens = clip.tokenize([\"This is \" + desc for desc in texts]).to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    image_features = model.encode_image(image_input).float()\n",
        "    text_features = model.encode_text(text_tokens).float()\n",
        "\n",
        "# Calculating cosine similarity: normalize features and calculate dot product of each pair.\n",
        "image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "similarity = text_features.cpu().numpy() @ image_features.cpu().numpy().T\n",
        "# print(similarity)\n",
        "\n",
        "count = len(texts)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(20, 14))\n",
        "plt.imshow(similarity, vmin=0.1, vmax=0.3)\n",
        "# plt.colorbar()\n",
        "plt.yticks(range(count), texts, fontsize=18)\n",
        "plt.xticks([])\n",
        "for i, image in enumerate(og_img):\n",
        "    plt.imshow(image, extent=(i - 0.5, i + 0.5, -1.6, -0.6), origin=\"lower\")\n",
        "for x in range(similarity.shape[1]):\n",
        "    for y in range(similarity.shape[0]):\n",
        "        plt.text(x, y, f\"{similarity[y, x]:.2f}\", ha=\"center\", va=\"center\", size=12)\n",
        "\n",
        "for side in [\"left\", \"top\", \"right\", \"bottom\"]:\n",
        "    plt.gca().spines[side].set_visible(False)\n",
        "\n",
        "plt.xlim([-0.5, len(og_img) - 0.5])\n",
        "# plt.ylim([count + 0.5, -2])\n",
        "plt.ylim([count - 0.5, -1.5]) # bottom, top margin\n",
        "plt.show()\n",
        "# plt.title(\"Cosine similarity between text and image features\", size=20)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title base get similarity\n",
        "\n",
        "images=[]\n",
        "og_img=[]\n",
        "\n",
        "c=110\n",
        "for i in range(c,c+10):\n",
        "    img,label=data[i]\n",
        "    images.append(preprocess(img))\n",
        "    og_img.append(img)\n",
        "\n",
        "# blur 7 43\n",
        "# repeat 88-89\n",
        "texts=['a clear image of a house','an image without a house','a blurred image','a plain background']\n",
        "\n",
        "# normalize images, tokenize text input, forward pass model to get image text features\n",
        "image_input = torch.tensor(np.stack(images)).to(device)\n",
        "text_tokens = clip.tokenize([\"This is \" + desc for desc in texts]).to(device)\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "    image_features = model.encode_image(image_input).float()\n",
        "    text_features = model.encode_text(text_tokens).float()\n",
        "\n",
        "# Calculating cosine similarity: normalize features and calculate dot product of each pair.\n",
        "image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "# similarity = text_features.cpu().numpy() @ image_features.cpu().numpy().T\n",
        "\n",
        "similarity = text_features @ image_features.T\n",
        "print(similarity)\n",
        "\n",
        "# blur 0.23\n",
        "\n"
      ],
      "metadata": {
        "id": "hBTkUZDJ8Rs-",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title collect\n",
        "import os\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import transforms\n",
        "\n",
        "# texts=['a clear image of a house','an image without a house','a blurred image','a plain background']\n",
        "texts=['a clear image of a house','an image of a wall','an image of trees','an image of a vehicle','a blurred image','a plain background']\n",
        "text_tokens = clip.tokenize([\"This is \" + desc for desc in texts]).to(device)\n",
        "with torch.no_grad(): text_features = model.encode_text(text_tokens).float()\n",
        "text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "good = []\n",
        "test = []\n",
        "obscured = []\n",
        "blurred = []\n",
        "missing = []\n",
        "\n",
        "img_dir = '/content/google_street_view/01'\n",
        "\n",
        "t=0\n",
        "# for filename in [filename for filename in os.listdir(img_dir) if filename.endswith(\".png\") or filename.endswith(\".jpg\")]:\n",
        "for filename in os.listdir(img_dir):\n",
        "    print(filename)\n",
        "    name = os.path.splitext(filename)[0]\n",
        "    img_file=os.path.join(img_dir, filename)\n",
        "    image = Image.open(img_file).convert(\"RGB\")\n",
        "   \n",
        "    img= preprocess(image).to(device).unsqueeze(0)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        image_features = model.encode_image(img).float()\n",
        "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "    similarity = text_features @ image_features.T\n",
        "    # print(similarity.T)\n",
        "\n",
        "    # plt.figure(figsize=(4, 1.25)) # plt.figure(figsize=(16, 5))\n",
        "    # plt.imshow(image)\n",
        "    # plt.show()\n",
        "\n",
        "    # image=transforms.ToTensor()(image)\n",
        "    if similarity[1]>=0.23: # wall\n",
        "        test.append(img_file)\n",
        "    elif similarity[0]<0.23: # obscured?\n",
        "        obscured.append(img_file)\n",
        "    elif similarity[-2]>=0.23: # blur\n",
        "        blurred.append(img_file)\n",
        "    elif similarity[-1]>=0.23: # missing\n",
        "        missing.append(img_file)\n",
        "    else:\n",
        "        good.append(img_file)\n",
        "    # texts.append(descriptions[name])\n",
        "\n",
        "    t+=1\n",
        "    # if t >=50: break\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3j_xML2VFcfT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "from torchvision import transforms\n",
        "plt.axis('off')\n",
        "\n",
        "\n",
        "def imshow(img):\n",
        "    # img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    # plt.figure(figsize = (16,5))\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "def file2img(img_files):\n",
        "    imgs=[]\n",
        "    for img_file in img_files:\n",
        "        image = Image.open(img_file).convert(\"RGB\")\n",
        "        image=transforms.ToTensor()(image)\n",
        "        imgs.append(image)\n",
        "    return imgs\n",
        "\n",
        "print(\"good\",len(good))\n",
        "print(\"test\",len(test))\n",
        "print(\"obscured\",len(obscured))\n",
        "print(\"blurred\",len(blurred))\n",
        "print(\"missing\",len(missing))\n",
        "\n",
        "c=0\n",
        "# imshow(torchvision.utils.make_grid(good[c:c+64]))\n",
        "# imshow(torchvision.utils.make_grid(file2img(test),nrow=8))\n",
        "imshow(torchvision.utils.make_grid(file2img(obscured),nrow=6))\n",
        "# imshow(torchvision.utils.make_grid(file2img(blurred),nrow=5))\n",
        "# imshow(torchvision.utils.make_grid(file2img(missing),nrow=5))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "P1ubEgNLM2Z1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c=64\n",
        "\n",
        "# imshow(torchvision.utils.make_grid(file2img(test),nrow=8))\n",
        "imshow(torchvision.utils.make_grid(file2img(good[c:c+64])))\n",
        "# imshow(torchvision.utils.make_grid(file2img(obscured),nrow=6))\n"
      ],
      "metadata": {
        "id": "NR2txF0jkQ7-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}