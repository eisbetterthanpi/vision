{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eisbetterthanpi/vision/blob/main/Meta_Pseudo_Labels.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v9XrV9ZACgjm"
      },
      "outputs": [],
      "source": [
        "# Meta Pseudo Labels mar 2021 https://arxiv.org/pdf/2003.10580v4.pdf\n",
        "# https://github.com/kekmodel/MPL-pytorch\n",
        "'''\n",
        "Meta Pseudo Labels by google research, implimentation by kekmodel\n",
        "modified to save a significant amount of memory\n",
        "\n",
        "other methods like ordinal regression , ensembling, test time augmentation con only give a slight increase in accuracy\n",
        "this is the most promissing method , with the added benefit of producing a more robust classifier\n",
        "\n",
        "will require a huge unlabelled dataset of houses and a very clean dataset of labelled houses\n",
        "\n",
        "cant seem to make it train properly\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "z9AEoUWZekls"
      },
      "outputs": [],
      "source": [
        "# @title download\n",
        "# # # google images unlabeled\n",
        "# !gdown 1ncx2DJ-GXqrQd6nL5UEmj6GLT4w-9qYs -O house.zip\n",
        "# !unzip /content/house.zip -d /\n",
        "# !rm -R /content/house/.ipynb_checkpoints\n",
        "\n",
        "# # # 70k+gmap\n",
        "# !gdown 1-CZp7TbhJLeRQpbKQCyT8ofGg89Yt137 -O gsv.zip\n",
        "# !unzip /content/gsv.zip -d /\n",
        "# !rm -R /content/gsv70kg/.ipynb_checkpoints\n",
        "# !rm -R /content/gsv70kg/01/.ipynb_checkpoints\n",
        "# !rm -R /content/gsv70kg/02/.ipynb_checkpoints\n",
        "# !rm -R /content/gsv70kg/03/.ipynb_checkpoints\n",
        "# !rm -R /content/gsv70kg/04/.ipynb_checkpoints\n",
        "# !rm -R /content/gsv70kg/05/.ipynb_checkpoints\n",
        "# !rm -R /content/gsv70kg/06/.ipynb_checkpoints\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "1MAfJG1xTW3b"
      },
      "outputs": [],
      "source": [
        "# @title data\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "from torchvision import datasets, transforms\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
        "\n",
        "labeled_dir='/content/gsv70kg'\n",
        "\n",
        "labeled_data = datasets.ImageFolder(labeled_dir, transform=transform)\n",
        "torch.manual_seed(0)\n",
        "\n",
        "# import random\n",
        "# random.seed(0)\n",
        "np.random.seed(0)\n",
        "torch.manual_seed(0)\n",
        "torch.cuda.manual_seed_all(0)\n",
        "\n",
        "\n",
        "train_data, test_data = torch.utils.data.random_split(labeled_data, [.9,.1])\n",
        "# train_data, _ = torch.utils.data.random_split(train_data, [.01,.99])\n",
        "# test_data, _ = torch.utils.data.random_split(test_data, [.01,.99])\n",
        "finetune_dataset = train_data\n",
        "\n",
        "unlabel_dir='/content/house'\n",
        "# unlabel_data = datasets.ImageFolder(unlabel_dir, transform=transform)\n",
        "\n",
        "import os\n",
        "from PIL import Image\n",
        "class Datasetme(torch.utils.data.Dataset):\n",
        "    def __init__(self, dir, transform=None):\n",
        "        self.dir = dir\n",
        "        self.data = os.listdir(dir)\n",
        "        self.transform = transform\n",
        "    def __getitem__(self, index):\n",
        "        img_file = self.data[index]\n",
        "        img_file = os.path.join(self.dir, img_file)\n",
        "        image = Image.open(img_file).convert(\"RGB\")\n",
        "        if self.transform: image = self.transform(image)\n",
        "        return image\n",
        "    def __len__(self): return len(self.data)\n",
        "\n",
        "unlabel_data = Datasetme(unlabel_dir, transform=transform)\n",
        "# unlabel_data, _ = torch.utils.data.random_split(unlabel_data, [.01,.99])\n",
        "\n",
        "\n",
        "batch_size = 64 # 16 is max for res152; default 64/ mainargs128\n",
        "grad_acc = 1\n",
        "\n",
        "# res152 batch16 gradacc4\n",
        "\n",
        "num_batches=int(np.ceil(len(train_data)/batch_size))\n",
        "\n",
        "# train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, num_workers=4, pin_memory=True)\n",
        "# test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, num_workers=4, pin_memory=True)\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, pin_memory=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, pin_memory=True)\n",
        "\n",
        "train_sampler = RandomSampler\n",
        "# labeled_loader = DataLoader(labeled_data, sampler=train_sampler(labeled_data), batch_size=batch_size, num_workers=4, drop_last=True)\n",
        "# unlabeled_loader = DataLoader(unlabel_data, sampler=train_sampler(unlabel_data), batch_size=batch_size * 7, num_workers=4, drop_last=True) # mu=7 ,coefficient of unlabeled batch size\n",
        "# test_loader = DataLoader(test_data, sampler=SequentialSampler(test_data), batch_size=batch_size, num_workers=4)\n",
        "\n",
        "labeled_loader = DataLoader(labeled_data, sampler=train_sampler(labeled_data), batch_size=batch_size*grad_acc, drop_last=True)\n",
        "mu=7 # coefficient of unlabeled batch size\n",
        "unlabeled_loader = DataLoader(unlabel_data, sampler=train_sampler(unlabel_data), batch_size=batch_size*grad_acc * mu, drop_last=True)\n",
        "test_loader = DataLoader(test_data, sampler=SequentialSampler(test_data), batch_size=batch_size*grad_acc)\n",
        "\n",
        "del labeled_data, train_data, test_data, unlabel_data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "6iYyVffJcYB5"
      },
      "outputs": [],
      "source": [
        "# @title torch augment\n",
        "# https://github.com/facebookresearch/vicreg/blob/main/augmentations.py\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "class TrainTransform(object):\n",
        "    def __init__(self):\n",
        "        # self.transform = transforms.RandomApply([transforms.Compose([\n",
        "        self.transform = transforms.Compose([\n",
        "                transforms.RandomPerspective(distortion_scale=0.3, p=0.5), # me\n",
        "                transforms.RandomResizedCrop((400,640), scale=(0.7, 1.0), ratio=(0.8, 1.25), interpolation=transforms.InterpolationMode.BICUBIC),\n",
        "                # transforms.RandomResizedCrop((32,32), scale=(0.7, 1.0), ratio=(0.8, 1.25), interpolation=transforms.InterpolationMode.BICUBIC),\n",
        "                transforms.RandomHorizontalFlip(p=0.5), # 0.5\n",
        "                transforms.Lambda(lambda x : torch.clamp(x, 0., 1.)), # clamp else ColorJitter will return nan https://discuss.pytorch.org/t/input-is-nan-after-transformation/125455/6\n",
        "                transforms.RandomApply([transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1)], p=0.8,), # brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1)], p=0.8\n",
        "                transforms.RandomGrayscale(p=0.2), # 0.2\n",
        "                # # transforms.RandomChoice(transforms.ColorJitter , transforms.RandomGrayscale(p=1.)\n",
        "                transforms.RandomApply([transforms.GaussianBlur(kernel_size=5, sigma=(0.1, 2.0)),], p=1.0),\n",
        "                # # transforms.RandomSolarize(threshold=130, p=0.5)\n",
        "                transforms.RandomErasing(p=1., scale=(0.1, 0.11), ratio=(1,1), value=(0.485, 0.456, 0.406)),\n",
        "                # transforms.ToTensor(), # ToTensored at dataset level, no need to ToTensor again\n",
        "                # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), # normalised at dataset level. default 0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225\n",
        "                ])\n",
        "            # ], p=1.)\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        dims = len(sample.shape)\n",
        "        if dims==3: x1 = self.transform(sample) # same transforms per minibatch\n",
        "        elif dims==4: x1 = transforms.Lambda(lambda x: torch.stack([self.transform(x_) for x_ in x]))(sample) # diff transforms per img in minibatch\n",
        "        # x1 = self.transform(sample)\n",
        "        return x1\n",
        "\n",
        "trs=TrainTransform()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "q_bJYwwSDNNy"
      },
      "outputs": [],
      "source": [
        "# @title utils\n",
        "# https://github.com/kekmodel/MPL-pytorch/blob/main/utils.py\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "def create_loss_fn():\n",
        "    label_smoothing = 0 # default 0 / mainargs 0.15\n",
        "    # if label_smoothing > 0:\n",
        "    #     criterion = SmoothCrossEntropyV2(alpha=label_smoothing)\n",
        "    # else:\n",
        "    criterion = nn.CrossEntropyLoss(label_smoothing=label_smoothing)\n",
        "    return criterion.to(device)\n",
        "\n",
        "from collections import OrderedDict\n",
        "def module_load_state_dict(model, state_dict):\n",
        "    try:\n",
        "        new_state_dict = OrderedDict()\n",
        "        for k, v in state_dict.items():\n",
        "            name = k[7:]  # remove `module.`\n",
        "            new_state_dict[name] = v\n",
        "        model.load_state_dict(new_state_dict)\n",
        "    except:\n",
        "        new_state_dict = OrderedDict()\n",
        "        for k, v in state_dict.items():\n",
        "            name = f'module.{k}'  # add `module.`\n",
        "            new_state_dict[name] = v\n",
        "        model.load_state_dict(new_state_dict)\n",
        "\n",
        "def model_load_state_dict(model, state_dict):\n",
        "    try: model.load_state_dict(state_dict)\n",
        "    except: module_load_state_dict(model, state_dict)\n",
        "\n",
        "\n",
        "def accuracy(output, target, topk=(1,)):\n",
        "    output = output.to(torch.device('cpu'))\n",
        "    target = target.to(torch.device('cpu'))\n",
        "    maxk = max(topk)\n",
        "    batch_size = target.shape[0]\n",
        "    _, idx = output.sort(dim=1, descending=True)\n",
        "    pred = idx.narrow(1, 0, maxk).t()\n",
        "    correct = pred.eq(target.reshape(1, -1).expand_as(pred))\n",
        "    res = []\n",
        "    for k in topk:\n",
        "        correct_k = correct[:k].reshape(-1).float().sum(dim=0, keepdim=True)\n",
        "        res.append(correct_k.mul_(100.0 / batch_size))\n",
        "    return res\n",
        "\n",
        "\n",
        "class SmoothCrossEntropy(nn.Module):\n",
        "    def __init__(self, alpha=0.1):\n",
        "        super(SmoothCrossEntropy, self).__init__()\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def forward(self, logits, labels):\n",
        "        if self.alpha == 0:\n",
        "            loss = F.cross_entropy(logits, labels)\n",
        "        else:\n",
        "            num_classes = logits.shape[-1]\n",
        "            alpha_div_k = self.alpha / num_classes\n",
        "            target_probs = F.one_hot(labels, num_classes=num_classes).float() * (1. - self.alpha) + alpha_div_k\n",
        "            loss = (-(target_probs * torch.log_softmax(logits, dim=-1)).sum(dim=-1)).mean()\n",
        "        return loss\n",
        "\n",
        "class SmoothCrossEntropyV2(nn.Module):\n",
        "    \"\"\"NLL loss with label smoothing.\"\"\"\n",
        "    def __init__(self, label_smoothing=0.1):\n",
        "        super().__init__()\n",
        "        assert label_smoothing < 1.0\n",
        "        self.smoothing = label_smoothing\n",
        "        self.confidence = 1. - label_smoothing\n",
        "\n",
        "    def forward(self, x, target):\n",
        "        if self.smoothing == 0:\n",
        "            loss = F.cross_entropy(x, target)\n",
        "        else:\n",
        "            logprobs = F.log_softmax(x, dim=-1)\n",
        "            nll_loss = -logprobs.gather(dim=-1, index=target.unsqueeze(1))\n",
        "            nll_loss = nll_loss.squeeze(1)\n",
        "            smooth_loss = -logprobs.mean(dim=-1)\n",
        "            loss = (self.confidence * nll_loss + self.smoothing * smooth_loss).mean()\n",
        "        return loss\n",
        "\n",
        "\n",
        "# from main\n",
        "import math\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "def get_cosine_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps, num_wait_steps=0, num_cycles=0.5, last_epoch=-1):\n",
        "    def lr_lambda(current_step):\n",
        "        if current_step < num_wait_steps:\n",
        "            return 0.0\n",
        "        if current_step < num_warmup_steps + num_wait_steps:\n",
        "            return float(current_step) / float(max(1, num_warmup_steps + num_wait_steps))\n",
        "        progress = float(current_step - num_warmup_steps - num_wait_steps) / float(max(1, num_training_steps - num_warmup_steps - num_wait_steps))\n",
        "        return max(0.0, 0.5 * (1.0 + math.cos(math.pi * float(num_cycles) * 2.0 * progress)))\n",
        "    return LambdaLR(optimizer, lr_lambda, last_epoch)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title plot lr scheduler\n",
        "# import matplotlib\n",
        "# import matplotlib.pyplot as plt\n",
        "# # matplotlib.rcParams['figure.dpi'] = 300\n",
        "# # plt.axis('off')\n",
        "\n",
        "# from torch import optim\n",
        "# t_optimizer = optim.SGD(teacher_parameters, lr=1.5e-4, momentum=0.9, nesterov=True)\n",
        "# s_optimizer = optim.SGD(student_parameters, lr=3e-4, momentum=0.9, nesterov=True)\n",
        "\n",
        "# total_steps=1000 # 300000\n",
        "# warmup_steps = 100 # default 0 / mainargs 5000\n",
        "# # t_scheduler = get_cosine_schedule_with_warmup(t_optimizer, warmup_steps, total_steps)\n",
        "# t_scheduler = get_cosine_schedule_with_warmup(t_optimizer, warmup_steps, total_steps,10)\n",
        "# student_wait_steps = 50 # default 0 / mainargs 3000\n",
        "# s_scheduler = get_cosine_schedule_with_warmup(s_optimizer, warmup_steps, total_steps, student_wait_steps)\n",
        "\n",
        "\n",
        "# tlr_lst=[]\n",
        "# slr_lst=[]\n",
        "# for t in range(total_steps):\n",
        "#     tlr=t_optimizer.param_groups[0][\"lr\"]\n",
        "#     tlr_lst.append(tlr)\n",
        "#     slr=s_optimizer.param_groups[0][\"lr\"]\n",
        "#     slr_lst.append(slr)\n",
        "#     t_scheduler.step()\n",
        "#     s_scheduler.step()\n",
        "# plt.plot(tlr_lst)\n",
        "# plt.plot(slr_lst)\n",
        "\n",
        "# plt.show()\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "fCYDnCuD3UWo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "4J3GeedWa6QO"
      },
      "outputs": [],
      "source": [
        "# @title ModelEMA\n",
        "# exponential moving average, smoothen model parameters\n",
        "# https://github.com/kekmodel/MPL-pytorch/blob/main/models.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from copy import deepcopy\n",
        "\n",
        "class ModelEMA(nn.Module):\n",
        "    def __init__(self, model, decay=0.9999, device=None):\n",
        "        super().__init__()\n",
        "        self.module = deepcopy(model)\n",
        "        self.module.eval()\n",
        "        self.decay = decay\n",
        "        self.device = device\n",
        "        if self.device is not None:\n",
        "            self.module.to(device=device)\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.module(input)\n",
        "\n",
        "    def _update(self, model, update_fn):\n",
        "        with torch.no_grad():\n",
        "            for ema_v, model_v in zip(self.module.parameters(), model.parameters()):\n",
        "                if self.device is not None:\n",
        "                    model_v = model_v.to(device=self.device)\n",
        "                ema_v.copy_(update_fn(ema_v, model_v))\n",
        "            for ema_v, model_v in zip(self.module.buffers(), model.buffers()):\n",
        "                if self.device is not None:\n",
        "                    model_v = model_v.to(device=self.device)\n",
        "                ema_v.copy_(model_v)\n",
        "\n",
        "    def update_parameters(self, model):\n",
        "        self._update(model, update_fn=lambda e, m: self.decay * e + (1. - self.decay) * m)\n",
        "\n",
        "    def state_dict(self):\n",
        "        return self.module.state_dict()\n",
        "\n",
        "    def load_state_dict(self, state_dict):\n",
        "        self.module.load_state_dict(state_dict)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2n6C1jpb-WOk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0887a7aa-94f0-4084-fc0a-6b1a07aa2342"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "uh\n"
          ]
        }
      ],
      "source": [
        "# @title big teacher\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "# !gdown 1ysJfdsvwMiWbCdkvFHwNqAUnJTtm6KbT -O res152adamw71.pth # ty\n",
        "# !gdown 1VaPxGoaLjmt7K9VHi0FWbJ5efEZTLhwd -O res18teacher.pth # A\n",
        "# !pip install bitsandbytes\n",
        "\n",
        "model = models.resnet18(weights='DEFAULT') # 18 34 50 101 152\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Sequential( # og (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
        "    # nn.Linear(num_ftrs, num_classes, bias=False),\n",
        "    # nn.Softmax(dim=1),\n",
        "    )\n",
        "\n",
        "# model.load_state_dict(torch.load('/content/bigTeacher.pth'))\n",
        "# _, modelsd, _,_ = torch.load('/content/bigTeacher.pth').values()\n",
        "# _, modelsd, _,_ = torch.load('/content/res152adamw71.pth').values()\n",
        "_, modelsd, _,_ = torch.load('/content/res18teacher.pth').values()\n",
        "model.load_state_dict(modelsd, strict=False)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "# model = torch.compile(model.to(device)) # compiling teacher leads to significantly higher vram usage\n",
        "\n",
        "\n",
        "model.eval()\n",
        "print('uh')\n",
        "\n",
        "\n",
        "# torch.save(model.state_dict(), '/content/model.pth')\n",
        "# modelsd = torch.load('/content/model.pth')\n",
        "# model.load_state_dict(modelsd, strict=False)\n",
        "# # model = torch.compile(model.to(device))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PVFBeYZgUya1"
      },
      "outputs": [],
      "source": [
        "# @title model\n",
        "\n",
        "num_classes = 6\n",
        "# if dataset == \"cifar10\": depth, widen_factor = 28, 2\n",
        "# elif dataset == 'cifar100': depth, widen_factor = 28, 8\n",
        "# teacher_model = WideResNet(num_classes=num_classes, depth=depth, widen_factor=widen_factor, dropout=0, dense_dropout=0.2)\n",
        "# student_model = WideResNet(num_classes=num_classes, depth=depth, widen_factor=widen_factor, dropout=0, dense_dropout=0.2)\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "def get_resnet():\n",
        "    model = models.resnet18(weights='DEFAULT') # 18 34 50 101 152\n",
        "    num_ftrs = model.fc.in_features\n",
        "    model.fc = nn.Sequential( # og (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
        "        # nn.Linear(num_ftrs, num_classes),\n",
        "        nn.Linear(num_ftrs, num_classes, bias=False),\n",
        "        nn.Softmax(dim=1),\n",
        "        )\n",
        "    # model = model.to(device)\n",
        "    model = torch.compile(model.to(device))\n",
        "    return model\n",
        "\n",
        "class Small(nn.Module):\n",
        "    def __init__(self, embed_dim, output_dim):\n",
        "        super(Small, self).__init__()\n",
        "        hidden_size=512\n",
        "        self.lin = nn.Sequential(\n",
        "            nn.Linear(embed_dim, hidden_size), nn.ReLU(),\n",
        "            nn.Linear(hidden_size, hidden_size), nn.ReLU(),\n",
        "            nn.Linear(hidden_size, hidden_size), nn.ReLU(),\n",
        "            nn.Linear(hidden_size, hidden_size), nn.ReLU(),\n",
        "            # nn.Linear(hidden_size, output_dim),\n",
        "            nn.Linear(hidden_size, output_dim, bias=False),\n",
        "            nn.Softmax(dim=1), # teacher need output logits!, not softmax?\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        logits = self.lin(x)\n",
        "        return logits\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# @title ensemble\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Ensemble(nn.Module):\n",
        "    def __init__(self, embed_dim, output_dim):\n",
        "        super().__init__()\n",
        "        self.output_dim = output_dim # 6\n",
        "        self.embed_dim = embed_dim\n",
        "        h_dim = 512\n",
        "        self.fwd = nn.Sequential(\n",
        "            nn.Linear(self.embed_dim, h_dim), nn.ReLU(),\n",
        "            # nn.Linear(h_dim, h_dim), nn.ReLU(),\n",
        "            # Block(h_dim, h_dim, 0.5),\n",
        "            Block(h_dim, h_dim),\n",
        "            Block(h_dim, h_dim),\n",
        "            Block(h_dim, h_dim),\n",
        "            nn.Linear(h_dim, self.output_dim),\n",
        "            # nn.Linear(h_dim, self.output_dim, bias=False),\n",
        "            # nn.Softmax(dim=1),\n",
        "            )\n",
        "    def forward(self, x):\n",
        "        out = self.fwd(x)\n",
        "        return out\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim, drop=None):\n",
        "        super().__init__()\n",
        "        if drop: self.fwd = nn.Sequential(nn.BatchNorm1d(in_dim), nn.Dropout(drop), nn.Linear(in_dim, out_dim), nn.ReLU(),)\n",
        "        else: self.fwd = nn.Sequential(nn.Linear(in_dim, out_dim), nn.ReLU(),)\n",
        "    def forward(self, x):\n",
        "        return x + self.fwd(x)\n",
        "\n",
        "# teacher_model = Ensemble(2048, 6).to(device)\n",
        "# teacher_model = torch.compile(Ensemble(2048, 6).to(device))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# teacher_model = Small(num_ftrs,6).to(device)\n",
        "teacher_model = torch.compile(Small(num_ftrs,6).to(device))\n",
        "# teacher_model = get_resnet()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "student_model = get_resnet()\n",
        "# _, modelsd, _,_ = torch.load('/content/res18teacher.pth').values()\n",
        "# student_model.load_state_dict(modelsd, strict=False)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "student_model = student_model.to(device)\n",
        "# student_model = torch.compile(student_model.to(device)) #\n",
        "\n",
        "\n",
        "avg_student_model = None\n",
        "ema = 0.995 # default 0 / mainargs 0.995\n",
        "\n",
        "if ema > 0: avg_student_model = ModelEMA(student_model, ema)\n",
        "\n",
        "no_decay = ['bn']\n",
        "weight_decay = 5e-4 # default 0 / mainargs 5e-4\n",
        "teacher_parameters = [{'params': [p for n, p in teacher_model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': weight_decay},\n",
        "    {'params': [p for n, p in teacher_model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}]\n",
        "student_parameters = [{'params': [p for n, p in student_model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': weight_decay},\n",
        "    {'params': [p for n, p in student_model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}]\n",
        "\n",
        "# teacher_model.zero_grad()\n",
        "# student_model.zero_grad()\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ABqGijQ6H27_"
      },
      "outputs": [],
      "source": [
        "# @title try\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# from torchvision import models\n",
        "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# # # 1.9, 1.7, 1.2\n",
        "# im = torch.rand(32,3,400,680,device=device) #\n",
        "# # print(32*3*400*680*32/8)\n",
        "# # print(im.element_size() * im.nelement())\n",
        "\n",
        "# def count_parameters(model): return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "# # print(count_parameters(model))\n",
        "\n",
        "\n",
        "# model = models.resnet152(weights='DEFAULT') # 18 34 50 101 152\n",
        "# model.fc = nn.Sequential()\n",
        "# model = model.to(device)\n",
        "# # model = torch.compile(model.to(device))\n",
        "# # 32, 50:3.3-5=1.7\n",
        "# # print(count_parameters(model))\n",
        "\n",
        "# # amp res152 student cant train on batch 32, compile or not\n",
        "\n",
        "# torch.backends.cuda.matmul.allow_tf32 = True\n",
        "# torch.backends.cudnn.allow_tf32 = True\n",
        "# model.eval()\n",
        "# student_model.eval()\n",
        "# with torch.cuda.amp.autocast():\n",
        "#     with torch.no_grad(): #\n",
        "#         # out = model(im) # 1:2.7, 4:6.3, 8:12.8\n",
        "#         out = student_model(im) #\n",
        "# # 16*15: 11.6\n",
        "# 32:1.3-8.7=6.4\n",
        "\n",
        "# student_model.train()\n",
        "# out = student_model(im) #\n",
        "\n",
        "# model.train()\n",
        "# with torch.cuda.amp.autocast():\n",
        "#     out = model(im) #\n",
        "\n",
        "\n",
        "# # 152:3.2\n",
        "# # im = torch.rand(4,3,400,680,device=device) #\n",
        "# # out = model(im) # 5.9\n",
        "\n",
        "# im = torch.rand(16,2048,device=device) #\n",
        "# out = teacher_model(im) # 5.9\n",
        "\n",
        "# torch.cuda.empty_cache()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "2UGs03MKAHxs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "outputId": "5f50a2cc-f283-4e08-e89c-a933fc311449"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.15.8)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.6)\n",
            "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.32)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.29.2)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.10/dist-packages (from wandb) (0.1.2)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.10)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2023.7.22)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbobdole\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.15.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230810_044726-ihm5al6f</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/bobdole/mpl/runs/ihm5al6f' target=\"_blank\">misunderstood-dust-22</a></strong> to <a href='https://wandb.ai/bobdole/mpl' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/bobdole/mpl' target=\"_blank\">https://wandb.ai/bobdole/mpl</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/bobdole/mpl/runs/ihm5al6f' target=\"_blank\">https://wandb.ai/bobdole/mpl/runs/ihm5al6f</a>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# @title wandb\n",
        "# https://docs.wandb.ai/quickstart\n",
        "!pip install wandb\n",
        "import wandb\n",
        "wandb.login() # 487a2109e55dce4e13fc70681781de9f50f27be7\n",
        "run = wandb.init(\n",
        "    project=\"mpl\",\n",
        "    config={\n",
        "        \"model\": \"scratch 18\",\n",
        "        # \"optim\": \"adamw\",\n",
        "        \"optim\": \"sgd\",\n",
        "        # \"lr\": lr,\n",
        "        # \"epochs\": epochs,\n",
        "    })\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "om2hOCE1BgrE"
      },
      "outputs": [],
      "source": [
        "# @title mpl grad acc\n",
        "# https://github.com/kekmodel/MPL-pytorch/blob/main/main.py\n",
        "import torch\n",
        "from torch.cuda import amp\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "\n",
        "t_scaler = amp.GradScaler()\n",
        "s_scaler = amp.GradScaler()\n",
        "def train(labeled_loader, unlabeled_loader, teacher_model, student_model,\n",
        "        avg_student_model, criterion, t_optimizer, s_optimizer, t_scheduler=None, s_scheduler=None):\n",
        "    labeled_iter = iter(labeled_loader)\n",
        "    unlabeled_iter = iter(unlabeled_loader)\n",
        "    size = len(unlabeled_loader)\n",
        "\n",
        "    # for author's code formula\n",
        "    # moving_dot_product = torch.empty(1).to(device)\n",
        "    # limit = 3.0**(0.5)  # 3 = 6 / (f_in + f_out)\n",
        "    # nn.init.uniform_(moving_dot_product, -limit, limit)\n",
        "\n",
        "    teacher_model.train()\n",
        "    student_model.train()\n",
        "    for step in range(len(unlabeled_loader)):\n",
        "\n",
        "        try: cimages_l, ctargets = next(labeled_iter)\n",
        "        except:\n",
        "            labeled_iter = iter(labeled_loader)\n",
        "            cimages_l, ctargets = next(labeled_iter)\n",
        "        try: cimages_uw = next(unlabeled_iter) # images_uw, _ = next(unlabeled_iter)\n",
        "        except:\n",
        "            unlabeled_iter = iter(unlabeled_loader)\n",
        "            cimages_uw = next(unlabeled_iter) # me\n",
        "        cimages_l, ctargets = cimages_l.to(device), ctargets.to(device)\n",
        "        cimages_uw = cimages_uw.to(device)\n",
        "        cimages_us = trs(cimages_uw)\n",
        "\n",
        "        t_loss_l = 0\n",
        "        t_loss_wu = 0\n",
        "        s_loss = 0\n",
        "        s_loss_l_old = 0\n",
        "        ct_logits_us = torch.empty(0, device=device)\n",
        "        for images_l, targets, images_uw, images_us in zip(cimages_l.chunk(grad_acc), ctargets.chunk(grad_acc), cimages_uw.chunk(grad_acc), cimages_us.chunk(grad_acc)): # for grad acc 1/2\n",
        "        # for images_l, targets, images_uw, images_us, s_loss_l_old in zip(cimages_l.chunk(grad_acc), ctargets.chunk(grad_acc), cimages_uw.chunk(grad_acc), cimages_us.chunk(grad_acc), cs_loss_l_old.chunk(grad_acc)): # for grad acc 1/2\n",
        "            with amp.autocast():\n",
        "                batch_size = images_l.shape[0]\n",
        "                with torch.no_grad(): m_logits_l = model(images_l) # reduced pml # big teacher\n",
        "                t_logits_l = teacher_model(m_logits_l)\n",
        "                t_l_l = criterion(t_logits_l, targets)\n",
        "            # t_scaler.scale(t_loss_l).backward(retain_graph=True) # me backward first\n",
        "            t_scaler.scale(t_l_l).backward() # me backward first\n",
        "            t_loss_l += t_l_l\n",
        "            # del t_loss_l\n",
        "\n",
        "            with amp.autocast():\n",
        "                with torch.no_grad(): s_logits_l = student_model(images_l)\n",
        "                s_l_l_old = F.cross_entropy(s_logits_l, targets)\n",
        "                s_loss_l_old += s_l_l_old\n",
        "\n",
        "            for i_us, i_uw in zip(images_us.chunk(mu), images_uw.chunk(mu)):\n",
        "                with amp.autocast():\n",
        "                    with torch.no_grad():\n",
        "                        m_i_us = model(i_us) # reduced pml # big teacher\n",
        "                        m_i_uw = model(i_uw) # reduced pml # big teacher\n",
        "                        t_l_uw = teacher_model(m_i_uw) # t_logits_uw no need grad\n",
        "                    t_l_us = teacher_model(m_i_us)\n",
        "                    ct_logits_us = torch.cat((ct_logits_us, t_l_us))\n",
        "\n",
        "                    temperature = 0.7 # default 1 / mainargs 0.7\n",
        "                    # soft_pseudo_label = torch.softmax(t_logits_uw.detach() / temperature, dim=-1)\n",
        "                    soft_pseudo_label = torch.softmax(t_l_uw / temperature, dim=-1)\n",
        "                    max_probs, hard_pseudo_label = torch.max(soft_pseudo_label, dim=-1) # all no grads\n",
        "\n",
        "                    threshold = 0.6 # default 0.95 / mainargs 0.6\n",
        "                    mask = max_probs.ge(threshold).float()\n",
        "                    # print((soft_pseudo_label * torch.log_softmax(t_l_us, dim=-1)).sum(dim=-1) , mask)\n",
        "                    t_loss_u = torch.mean(-(soft_pseudo_label * torch.log_softmax(t_l_us, dim=-1)).sum(dim=-1) * mask)\n",
        "                    lambda_u = 8 # default 1 / mainargs 8 coefficient of unlabeled loss\n",
        "                    uda_steps = 10 # default 1 / mainargs 5000 warmup steps of lambda-u\n",
        "                    weight_u = lambda_u * min(1., (step + 1) / uda_steps) # >0\n",
        "                    # t_loss_uda = t_loss_l + weight_u * t_loss_u\n",
        "\n",
        "                    # i_us.retain_grad()\n",
        "                    i_us.requires_grad=True\n",
        "                    s_l_us = student_model(i_us)\n",
        "                    s_l = criterion(s_l_us, hard_pseudo_label)\n",
        "                s_scaler.scale(s_l).backward()\n",
        "                s_loss += s_l\n",
        "                t_scaler.scale(weight_u * t_loss_u).backward(retain_graph=True)\n",
        "                # t_scaler.scale(weight_u * t_loss_u).backward()\n",
        "                t_loss_wu += weight_u * t_loss_u\n",
        "                # del t_loss_u, soft_pseudo_label# t_loss_l\n",
        "                # del s_logits_l, s_logits_us, hard_pseudo_label#, s_loss\n",
        "\n",
        "        t_loss_uda = t_loss_l + t_loss_wu\n",
        "        print('t_loss_l',t_loss_l.item())\n",
        "        # print('t_loss_wu',t_loss_wu.item())\n",
        "        print('t_loss_wu',weight_u, t_loss_u.item())\n",
        "\n",
        "        # if grad_clip > 0:\n",
        "        s_scaler.unscale_(s_optimizer)\n",
        "        nn.utils.clip_grad_norm_(student_model.parameters(), 1e9)\n",
        "        s_scaler.step(s_optimizer)\n",
        "        s_scaler.update()\n",
        "        if s_scheduler: s_scheduler.step()\n",
        "        student_model.zero_grad()\n",
        "        if ema > 0: avg_student_model.update_parameters(student_model)\n",
        "\n",
        "\n",
        "        s_loss_l_new = 0\n",
        "        # for images_l, targets, images_uw, images_us in zip(cimages_l.chunk(grad_acc), ctargets.chunk(grad_acc), cimages_uw.chunk(grad_acc), cimages_us.chunk(grad_acc)): # for grad acc 2/2\n",
        "        # for images_l, targets, images_uw, images_us, t_logits_us in zip(cimages_l.chunk(grad_acc), ctargets.chunk(grad_acc), cimages_uw.chunk(grad_acc), cimages_us.chunk(grad_acc), ct_logits_us.chunk(grad_acc)): # for grad acc 2/2\n",
        "        for images_l, targets in zip(cimages_l.chunk(grad_acc), ctargets.chunk(grad_acc)): # for grad acc 2/2\n",
        "            with amp.autocast():\n",
        "                with torch.no_grad(): s_logits_l = student_model(images_l)\n",
        "                s_l_l_new = F.cross_entropy(s_logits_l, targets)\n",
        "                s_loss_l_new += s_l_l_new\n",
        "        dot_product = s_loss_l_old - s_loss_l_new\n",
        "        # # moving_dot_product = moving_dot_product * 0.99 + dot_product * 0.01\n",
        "        # # dot_product = dot_product - moving_dot_product\n",
        "\n",
        "        t_loss_mpl = 0\n",
        "        # for images_l, targets, images_uw, images_us, t_logits_us in zip(cimages_l.chunk(grad_acc), ctargets.chunk(grad_acc), cimages_uw.chunk(grad_acc), cimages_us.chunk(grad_acc), ct_logits_us.chunk(grad_acc)): # for grad acc 2/2\n",
        "        for t_logits_us in ct_logits_us.chunk(grad_acc): # for grad acc 2/2\n",
        "            for t_l_us in t_logits_us.chunk(mu):\n",
        "                with amp.autocast():\n",
        "                    _, hard_pseudo_label = torch.max(t_l_us.detach(), dim=-1)\n",
        "                    t_l_mpl = dot_product * F.cross_entropy(t_l_us, hard_pseudo_label) # dot_product no grad\n",
        "                t_scaler.scale(t_l_mpl).backward(retain_graph=True)\n",
        "                t_loss_mpl += t_l_mpl\n",
        "\n",
        "        t_loss = t_loss_uda + t_loss_mpl\n",
        "        print('t_loss_mpl',t_loss_mpl.item())\n",
        "        # print(\"t_loss, s_loss\", t_loss.item(), s_loss.item())\n",
        "        if step%10==0: print(step,\"/\",size,\" \", \"t_loss: \", t_loss.item(), \"s_loss: \", s_loss.item())\n",
        "        try: wandb.log({\"t_loss\": t_loss.item(), \"s_loss\": s_loss.item()})\n",
        "        except: pass\n",
        "        # del s_logits_l, dot_product, s_loss_l_old, s_loss_l_new, t_logits_us, hard_pseudo_label, #t_loss, t_loss_uda, t_loss_mpl\n",
        "        # del s_loss\n",
        "\n",
        "\n",
        "        t_scaler.unscale_(t_optimizer) # if grad_clip > 0:\n",
        "        nn.utils.clip_grad_norm_(teacher_model.parameters(), 1e9)\n",
        "        t_scaler.step(t_optimizer)\n",
        "        t_scaler.update()\n",
        "        if t_scheduler: t_scheduler.step()\n",
        "        teacher_model.zero_grad()\n",
        "\n",
        "    return\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "W9URMRghTLc_"
      },
      "outputs": [],
      "source": [
        "# @title strain eval\n",
        "\n",
        "def evaluate(test_loader, model, criterion, verbose=True):\n",
        "    size = len(test_loader.dataset)\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for step, (images, targets) in enumerate(test_loader):\n",
        "            batch_size = images.shape[0]\n",
        "            images = images.to(device)\n",
        "            targets = targets.to(device)\n",
        "            with amp.autocast():\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, targets)\n",
        "            test_loss += loss.item()\n",
        "        #     acc1, acc5 = accuracy(outputs, targets, (1, 5))\n",
        "        #     losses, top1, top5 = loss.item(), acc1[0], acc5[0]\n",
        "        # # return losses, top1, top5\n",
        "            correct += (outputs.argmax(1) == targets).type(torch.float).sum().item()\n",
        "    test_loss /= size\n",
        "    correct /= size\n",
        "    try: wandb.log({\"test loss\": test_loss})\n",
        "    except: pass\n",
        "    if verbose: print(f\"Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f}\")\n",
        "    return correct, test_loss\n",
        "\n",
        "\n",
        "# @title train test function\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "# https://github.com/prigoyal/pytorch_memonger/blob/master/models/optimized/resnet_new.py\n",
        "from torch.utils.checkpoint import checkpoint, checkpoint_sequential\n",
        "\n",
        "trs=TrainTransform() # for image augmentation during train time\n",
        "# train function with automatic mixed precision\n",
        "def strain(dataloader, model, loss_fn, optimizer, scheduler=None, verbose=True):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    loss_list = []\n",
        "    for batch, (x, y) in enumerate(dataloader):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        with torch.cuda.amp.autocast(): # automatic mixed percision\n",
        "            x = trs(x) # image augmentation during train time to use gpu\n",
        "            pred = model(x) # default\n",
        "            loss = loss_fn(pred, y)\n",
        "        scaler.scale(loss).backward()\n",
        "        if ((batch + 1) % 4 == 0) or (batch + 1 == len(dataloader)): # gradient accumulation\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            optimizer.zero_grad()\n",
        "            if scheduler is not None:\n",
        "                scheduler.step()\n",
        "                # print(\"### lr: \", optimizer.param_groups[0][\"lr\"])\n",
        "\n",
        "        # print(model.state_dict()['_orig_mod.bn1.running_mean'][0])\n",
        "        train_loss = loss.item()/len(y)\n",
        "        loss_list.append(train_loss)\n",
        "        try: wandb.log({\"train loss\": train_loss})\n",
        "        except: pass\n",
        "        # if batch % (size//(10* len(y))) == 0:\n",
        "        current = batch * len(x)\n",
        "        if verbose: print(f\"loss: {train_loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "    return loss_list\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w_2qJyKBwL-V",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title save\n",
        "\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "# checkpoint = {\n",
        "# 'epoch': t+1,\n",
        "# 'teacher_model': teacher_model.state_dict(),\n",
        "# 'student_model': student_model.state_dict(),\n",
        "# 'avg_student_model': avg_student_model.state_dict(),\n",
        "# 't_optimizer': t_optimizer.state_dict(),\n",
        "# 's_optimizer': s_optimizer.state_dict(),\n",
        "# 't_scheduler': t_scheduler.state_dict(),\n",
        "# 's_scheduler': s_scheduler.state_dict(),}\n",
        "# # torch.save(checkpoint, pth)\n",
        "# torch.save(checkpoint, 'ckpt.pth')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aw0nwnUAh2Of"
      },
      "outputs": [],
      "source": [
        "# @title wwwwwwwwwww\n",
        "\n",
        "criterion = create_loss_fn()\n",
        "\n",
        "from torch import optim\n",
        "# lr default 0.01/ mainargs 0.05\n",
        "# og:t0.05s0.05 , psl:t1e-3s3e-4, rpsl:t\n",
        "# t_optimizer = optim.SGD(teacher_parameters, lr=0.05, momentum=0.9, nesterov=True)\n",
        "# s_optimizer = optim.SGD(student_parameters, lr=0.05, momentum=0.9, nesterov=True)\n",
        "# t_optimizer = optim.SGD(teacher_parameters, lr=3e-4, momentum=0.9, nesterov=True)\n",
        "t_optimizer = optim.SGD(teacher_parameters, lr=1e-4, momentum=0.9, nesterov=True)\n",
        "s_optimizer = optim.SGD(student_parameters, lr=3e-4, momentum=0.9, nesterov=True)\n",
        "# s_optimizer = optim.SGD(student_parameters, lr=1e-3, momentum=0.9, nesterov=True)\n",
        "# optimizer = bnb.optim.AdamW(model.parameters(), lr=1e-5, betas=(0.9, 0.999), optim_bits=8)\n",
        "# optimizer = Lamb(model.parameters(), lr=1e-5, betas=(0.9, 0.999), eps=1e-08, weight_decay=3e-6)\n",
        "\n",
        "# res18 batch64 sgd from scratch 3e-5 - 3e-4 - 1e-3?\n",
        "\n",
        "# 3e-5 27.9->25.0\n",
        "# 3e-3 27.9->26.5\n",
        "# 0.05 29.4 -> 22.1\n",
        "\n",
        "epochs = 5\n",
        "num_batches=len(train_loader)\n",
        "total_steps=int(np.ceil(num_batches/grad_acc)*epochs +1) # +1 to excluse uptick at the end of onecycle\n",
        "# total_steps=100 # 300000\n",
        "warmup_steps = 50 # default 0 / mainargs 5000\n",
        "# t_scheduler = get_cosine_schedule_with_warmup(t_optimizer, warmup_steps, total_steps)\n",
        "t_scheduler = get_cosine_schedule_with_warmup(t_optimizer, warmup_steps, total_steps,10)\n",
        "student_wait_steps = 30 # default 0 / mainargs 3000\n",
        "s_scheduler = get_cosine_schedule_with_warmup(s_optimizer, warmup_steps, total_steps, student_wait_steps)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vfpCXUfEzHZV",
        "outputId": "f1d4c059-ab0a-4ee0-b352-7c0b3cff3210"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "t_lr,s_lr 0.0 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "t_loss_l 1.7913426160812378\n",
            "t_loss_wu 0.8 0.0\n",
            "t_loss_mpl 0.0\n",
            "0 / 157   t_loss:  1.7913426160812378 s_loss:  12.253329277038574\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "t_loss_l 1.791520118713379\n",
            "t_loss_wu 1.6 0.0\n",
            "t_loss_mpl 0.0\n",
            "t_loss_l 1.7914379835128784\n",
            "t_loss_wu 2.4 0.0\n",
            "t_loss_mpl 0.0\n",
            "t_loss_l 1.7915395498275757\n",
            "t_loss_wu 3.2 0.0\n",
            "t_loss_mpl 0.0\n",
            "t_loss_l 1.791473627090454\n",
            "t_loss_wu 4.0 0.0\n",
            "t_loss_mpl 0.0\n",
            "t_loss_l 1.7915624380111694\n",
            "t_loss_wu 4.8 0.0\n",
            "t_loss_mpl 0.0\n",
            "t_loss_l 1.7914503812789917\n",
            "t_loss_wu 5.6 0.0\n",
            "t_loss_mpl -0.010902708396315575\n",
            "t_loss_l 1.7915102243423462\n",
            "t_loss_wu 6.4 0.0\n",
            "t_loss_mpl -0.0071145701222121716\n",
            "t_loss_l 1.7916902303695679\n",
            "t_loss_wu 7.2 0.0\n",
            "t_loss_mpl -0.03939010575413704\n",
            "t_loss_l 1.7915313243865967\n",
            "t_loss_wu 8.0 0.0\n",
            "t_loss_mpl -0.05762220546603203\n",
            "t_loss_l 1.791648507118225\n",
            "t_loss_wu 8.0 0.0\n",
            "t_loss_mpl -0.05073339864611626\n",
            "10 / 157   t_loss:  1.740915060043335 s_loss:  11.991691589355469\n",
            "t_loss_l 1.7914429903030396\n",
            "t_loss_wu 8.0 0.0\n",
            "t_loss_mpl -0.09036998450756073\n",
            "t_loss_l 1.7916089296340942\n",
            "t_loss_wu 8.0 0.0\n",
            "t_loss_mpl -0.07143785059452057\n",
            "t_loss_l 1.7916063070297241\n",
            "t_loss_wu 8.0 0.0\n",
            "t_loss_mpl -0.10903612524271011\n",
            "t_loss_l 1.7915129661560059\n",
            "t_loss_wu 8.0 0.0\n",
            "t_loss_mpl -0.1430089771747589\n",
            "t_loss_l 1.7915140390396118\n",
            "t_loss_wu 8.0 0.0\n",
            "t_loss_mpl -0.13050511479377747\n",
            "t_loss_l 1.7914392948150635\n",
            "t_loss_wu 8.0 0.0\n",
            "t_loss_mpl -0.1743396371603012\n",
            "t_loss_l 1.7915995121002197\n",
            "t_loss_wu 8.0 0.0\n",
            "t_loss_mpl -0.1745551973581314\n",
            "t_loss_l 1.7914273738861084\n",
            "t_loss_wu 8.0 0.0\n",
            "t_loss_mpl -0.12624330818653107\n",
            "t_loss_l 1.7914758920669556\n",
            "t_loss_wu 8.0 0.0\n",
            "t_loss_mpl -0.17902317643165588\n",
            "t_loss_l 1.7914562225341797\n",
            "t_loss_wu 8.0 0.0\n",
            "t_loss_mpl -0.12484367936849594\n",
            "20 / 157   t_loss:  1.6666125059127808 s_loss:  10.229545593261719\n",
            "t_loss_l 1.7915101051330566\n",
            "t_loss_wu 8.0 0.0\n",
            "t_loss_mpl -0.09287627786397934\n",
            "t_loss_l 1.7914977073669434\n",
            "t_loss_wu 8.0 0.0\n",
            "t_loss_mpl -0.07836388051509857\n",
            "t_loss_l 1.7916407585144043\n",
            "t_loss_wu 8.0 0.0\n",
            "t_loss_mpl -0.07879205048084259\n",
            "t_loss_l 1.7914601564407349\n",
            "t_loss_wu 8.0 0.0\n",
            "t_loss_mpl -0.05437754839658737\n",
            "t_loss_l 1.7913386821746826\n",
            "t_loss_wu 8.0 0.0\n",
            "t_loss_mpl -0.03920731693506241\n",
            "t_loss_l 1.7915534973144531\n",
            "t_loss_wu 8.0 0.0\n",
            "t_loss_mpl -0.033561818301677704\n",
            "t_loss_l 1.7915151119232178\n",
            "t_loss_wu 8.0 0.0\n",
            "t_loss_mpl -0.013818630017340183\n",
            "t_loss_l 1.7914153337478638\n",
            "t_loss_wu 8.0 0.0\n",
            "t_loss_mpl -0.022982977330684662\n",
            "t_loss_l 1.7914478778839111\n",
            "t_loss_wu 8.0 0.0\n",
            "t_loss_mpl -0.012411536648869514\n",
            "t_loss_l 1.7914845943450928\n",
            "t_loss_wu 8.0 0.0\n",
            "t_loss_mpl -0.012611458078026772\n",
            "30 / 157   t_loss:  1.778873085975647 s_loss:  10.168047904968262\n",
            "t_loss_l 1.791632890701294\n",
            "t_loss_wu 8.0 0.0\n",
            "t_loss_mpl -0.010901407338678837\n",
            "t_loss_l 1.7915009260177612\n",
            "t_loss_wu 8.0 0.0\n",
            "t_loss_mpl -0.0052834441885352135\n",
            "t_loss_l 1.7915549278259277\n",
            "t_loss_wu 8.0 0.0\n",
            "t_loss_mpl -0.0032025601249188185\n",
            "t_loss_l 1.7914716005325317\n",
            "t_loss_wu 8.0 0.0\n",
            "t_loss_mpl -0.0053176903165876865\n",
            "t_loss_l 1.7916077375411987\n",
            "t_loss_wu 8.0 0.0\n",
            "t_loss_mpl -0.0034684843849390745\n",
            "t_loss_l 1.7915009260177612\n",
            "t_loss_wu 8.0 0.0\n",
            "t_loss_mpl -0.0032743189949542284\n",
            "t_loss_l 1.7913731336593628\n",
            "t_loss_wu 8.0 0.0\n",
            "t_loss_mpl -0.0016879442846402526\n",
            "t_loss_l 1.7915221452713013\n",
            "t_loss_wu 8.0 0.0\n",
            "t_loss_mpl -0.002248066011816263\n",
            "t_loss_l 1.7914046049118042\n",
            "t_loss_wu 8.0 0.0\n",
            "t_loss_mpl -0.0018462520092725754\n",
            "t_loss_l 1.7913531064987183\n",
            "t_loss_wu 8.0 0.0\n",
            "t_loss_mpl -0.0009186482056975365\n",
            "40 / 157   t_loss:  1.79043447971344 s_loss:  9.829707145690918\n",
            "t_loss_l 1.7915382385253906\n",
            "t_loss_wu 8.0 0.0\n",
            "t_loss_mpl -0.0010784650221467018\n",
            "t_loss_l 1.7915290594100952\n",
            "t_loss_wu 8.0 0.0\n",
            "t_loss_mpl -0.0013174866326153278\n",
            "t_loss_l 1.7915232181549072\n",
            "t_loss_wu 8.0 0.0\n",
            "t_loss_mpl -0.000743878073990345\n",
            "t_loss_l 1.79134202003479\n",
            "t_loss_wu 8.0 0.0\n",
            "t_loss_mpl -0.0005765985115431249\n",
            "t_loss_l 1.7915840148925781\n",
            "t_loss_wu 8.0 0.0\n",
            "t_loss_mpl -0.0007125253905542195\n",
            "t_loss_l 1.7915804386138916\n",
            "t_loss_wu 8.0 0.0\n",
            "t_loss_mpl -0.0004600799293257296\n",
            "t_loss_l 1.7914750576019287\n",
            "t_loss_wu 8.0 0.0\n",
            "t_loss_mpl -0.0006049524527043104\n",
            "t_loss_l 1.7914738655090332\n",
            "t_loss_wu 8.0 0.0\n",
            "t_loss_mpl -0.000392854621168226\n",
            "t_loss_l 1.7914267778396606\n",
            "t_loss_wu 8.0 0.0\n",
            "t_loss_mpl -0.0003017362905666232\n",
            "t_loss_l 1.7914445400238037\n",
            "t_loss_wu 8.0 0.0\n",
            "t_loss_mpl -0.00032414429006166756\n",
            "50 / 157   t_loss:  1.7911204099655151 s_loss:  10.043588638305664\n",
            "t_loss_l 1.7914520502090454\n",
            "t_loss_wu 8.0 0.0\n",
            "t_loss_mpl -0.0003286278515588492\n",
            "t_loss_l 1.7914743423461914\n",
            "t_loss_wu 8.0 0.0\n",
            "t_loss_mpl -0.00024945937911979854\n",
            "t_loss_l 1.7913438081741333\n",
            "t_loss_wu 8.0 0.0\n",
            "t_loss_mpl -0.00020464410772547126\n",
            "t_loss_l 1.7913336753845215\n",
            "t_loss_wu 8.0 0.0\n",
            "t_loss_mpl -0.00013443760690279305\n",
            "t_loss_l 1.7915114164352417\n",
            "t_loss_wu 8.0 0.0\n",
            "t_loss_mpl -0.00015833770157769322\n",
            "t_loss_l 1.791500449180603\n",
            "t_loss_wu 8.0 0.0\n",
            "t_loss_mpl -0.00019418535521253943\n",
            "t_loss_l 1.7914378643035889\n",
            "t_loss_wu 8.0 0.0\n",
            "t_loss_mpl -0.00018821567937266082\n",
            "t_loss_l 1.7913436889648438\n",
            "t_loss_wu 8.0 0.0\n",
            "t_loss_mpl -7.618183735758066e-05\n",
            "t_loss_l 1.7914958000183105\n",
            "t_loss_wu 8.0 0.0\n",
            "t_loss_mpl -0.00014190567890182137\n",
            "t_loss_l 1.7915465831756592\n",
            "t_loss_wu 8.0 0.0\n",
            "t_loss_mpl -9.112029511015862e-05\n",
            "60 / 157   t_loss:  1.7914555072784424 s_loss:  9.917743682861328\n",
            "t_loss_l 1.791490077972412\n",
            "t_loss_wu 8.0 0.0\n",
            "t_loss_mpl -0.000131452179630287\n",
            "t_loss_l 1.791522741317749\n",
            "t_loss_wu 8.0 0.0\n",
            "t_loss_mpl -0.00012099453306291252\n",
            "t_loss_l 1.791345238685608\n",
            "t_loss_wu 8.0 0.0\n",
            "t_loss_mpl -0.00010008270328398794\n",
            "t_loss_l 1.7915067672729492\n",
            "t_loss_wu 8.0 0.0\n",
            "t_loss_mpl -8.364969107788056e-05\n",
            "t_loss_l 1.7913599014282227\n",
            "t_loss_wu 8.0 0.0\n",
            "t_loss_mpl -6.572590791620314e-05\n",
            "t_loss_l 1.7915204763412476\n",
            "t_loss_wu 8.0 0.0\n",
            "t_loss_mpl -4.331973832449876e-05\n",
            "t_loss_l 1.7913962602615356\n",
            "t_loss_wu 8.0 0.0\n",
            "t_loss_mpl -5.52689925825689e-05\n",
            "t_loss_l 1.7915674448013306\n",
            "t_loss_wu 8.0 0.0\n",
            "t_loss_mpl -4.33186833106447e-05\n",
            "t_loss_l 1.7912620306015015\n",
            "t_loss_wu 8.0 0.0\n",
            "t_loss_mpl -5.676276487065479e-05\n",
            "t_loss_l 1.7915294170379639\n",
            "t_loss_wu 8.0 0.0\n",
            "t_loss_mpl -5.228096415521577e-05\n",
            "70 / 157   t_loss:  1.791477084159851 s_loss:  10.36961555480957\n",
            "t_loss_l 1.7914187908172607\n",
            "t_loss_wu 8.0 0.0\n",
            "t_loss_mpl -5.3775547712575644e-05\n",
            "t_loss_l 1.79133141040802\n",
            "t_loss_wu 8.0 0.0\n",
            "t_loss_mpl -2.5393890609848313e-05\n",
            "t_loss_l 1.7916384935379028\n",
            "t_loss_wu 8.0 0.0\n",
            "t_loss_mpl -3.136918894597329e-05\n",
            "t_loss_l 1.791383147239685\n",
            "t_loss_wu 8.0 0.0\n",
            "t_loss_mpl -4.33190944022499e-05\n",
            "t_loss_l 1.7913089990615845\n",
            "t_loss_wu 8.0 0.0\n",
            "t_loss_mpl -3.7344587326515466e-05\n",
            "t_loss_l 1.7914416790008545\n",
            "t_loss_wu 8.0 0.0\n",
            "t_loss_mpl -2.3900360247353092e-05\n",
            "t_loss_l 1.7914927005767822\n",
            "t_loss_wu 8.0 0.0\n",
            "t_loss_mpl -2.2406135030905716e-05\n",
            "t_loss_l 1.791466474533081\n",
            "t_loss_wu 8.0 0.0\n",
            "t_loss_mpl -2.688788481464144e-05\n",
            "t_loss_l 1.791491985321045\n",
            "t_loss_wu 8.0 0.0\n",
            "t_loss_mpl -3.7344012525863945e-05\n",
            "t_loss_l 1.7913997173309326\n",
            "t_loss_wu 8.0 0.0\n",
            "t_loss_mpl -2.8381467927829362e-05\n",
            "80 / 157   t_loss:  1.7913713455200195 s_loss:  10.463176727294922\n",
            "t_loss_l 1.791472315788269\n",
            "t_loss_wu 8.0 0.0\n",
            "t_loss_mpl -2.539364868425764e-05\n",
            "t_loss_l 1.7913765907287598\n",
            "t_loss_wu 8.0 0.0\n",
            "t_loss_mpl -1.7925100110005587e-05\n",
            "t_loss_l 1.7915964126586914\n",
            "t_loss_wu 8.0 0.0\n",
            "t_loss_mpl -2.0912764739478007e-05\n",
            "t_loss_l 1.7914685010910034\n",
            "t_loss_wu 8.0 0.0\n",
            "t_loss_mpl -1.7925385691341944e-05\n",
            "t_loss_l 1.7913713455200195\n",
            "t_loss_wu 8.0 0.0\n",
            "t_loss_mpl -2.6888268621405587e-05\n",
            "t_loss_l 1.7914502620697021\n",
            "t_loss_wu 8.0 0.0\n",
            "t_loss_mpl -2.8381777156027965e-05\n",
            "t_loss_l 1.7913446426391602\n",
            "t_loss_wu 8.0 0.0\n",
            "t_loss_mpl -2.2406560674426146e-05\n",
            "t_loss_l 1.7915363311767578\n",
            "t_loss_wu 8.0 0.0\n",
            "t_loss_mpl -1.643145333218854e-05\n",
            "t_loss_l 1.7913439273834229\n",
            "t_loss_wu 8.0 0.0\n",
            "t_loss_mpl -1.7924938219948672e-05\n",
            "t_loss_l 1.7913509607315063\n",
            "t_loss_wu 8.0 0.0\n",
            "t_loss_mpl -1.3443976058624685e-05\n",
            "90 / 157   t_loss:  1.791337490081787 s_loss:  10.291875839233398\n",
            "t_loss_l 1.791450023651123\n",
            "t_loss_wu 8.0 0.0\n",
            "t_loss_mpl -1.3444043361232616e-05\n",
            "t_loss_l 1.791456699371338\n",
            "t_loss_wu 8.0 0.0\n",
            "t_loss_mpl -2.0912866602884606e-05\n",
            "t_loss_l 1.7913638353347778\n",
            "t_loss_wu 8.0 0.0\n",
            "t_loss_mpl -1.4937630112399347e-05\n",
            "t_loss_l 1.7913678884506226\n",
            "t_loss_wu 8.0 0.0\n",
            "t_loss_mpl -4.481350970308995e-06\n",
            "t_loss_l 1.7915892601013184\n",
            "t_loss_wu 8.0 0.0\n",
            "t_loss_mpl -1.643161340325605e-05\n",
            "t_loss_l 1.7913682460784912\n",
            "t_loss_wu 8.0 0.0\n",
            "t_loss_mpl -1.9418854208197445e-05\n",
            "t_loss_l 1.7912647724151611\n",
            "t_loss_wu 8.0 0.0\n",
            "t_loss_mpl -7.468898729712237e-06\n",
            "t_loss_l 1.7912888526916504\n",
            "t_loss_wu 8.0 0.0\n",
            "t_loss_mpl -1.941909249580931e-05\n",
            "t_loss_l 1.791458249092102\n",
            "t_loss_wu 8.0 0.0\n",
            "t_loss_mpl -1.1949999134230893e-05\n",
            "t_loss_l 1.7912161350250244\n",
            "t_loss_wu 8.0 0.0\n",
            "t_loss_mpl -8.962667379819322e-06\n",
            "100 / 157   t_loss:  1.791207194328308 s_loss:  10.275727272033691\n",
            "t_loss_l 1.7914135456085205\n",
            "t_loss_wu 8.0 0.0\n",
            "t_loss_mpl -1.0456245945533738e-05\n",
            "t_loss_l 1.7913316488265991\n",
            "t_loss_wu 8.0 0.0\n",
            "t_loss_mpl -5.975018211756833e-06\n",
            "t_loss_l 1.791526436805725\n",
            "t_loss_wu 8.0 0.0\n",
            "t_loss_mpl -1.1950132829952054e-05\n",
            "t_loss_l 1.791479468345642\n",
            "t_loss_wu 8.0 0.0\n",
            "t_loss_mpl -8.962531865108758e-06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Process ForkProcess-2:\n",
            "Process ForkProcess-1:\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
            "    call_item = call_queue.get(block=True)\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
            "    call_item = call_queue.get(block=True)\n",
            "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 103, in get\n",
            "    res = self._recv_bytes()\n",
            "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 216, in recv_bytes\n",
            "    buf = self._recv_bytes(maxlength)\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 414, in _recv_bytes\n",
            "    buf = self._recv(4)\n",
            "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 379, in _recv\n",
            "    chunk = read(handle, remaining)\n",
            "KeyboardInterrupt\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-6770c6a277ab>\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0ms_lr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ms_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"lr\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m't_lr,s_lr'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt_lr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ms_lr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     train(labeled_loader, unlabeled_loader, teacher_model, student_model,\n\u001b[0m\u001b[1;32m     21\u001b[0m         avg_student_model, criterion, t_optimizer, s_optimizer, t_scheduler, s_scheduler)\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-b10da706acb3>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(labeled_loader, unlabeled_loader, teacher_model, student_model, avg_student_model, criterion, t_optimizer, s_optimizer, t_scheduler, s_scheduler)\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mcimages_l\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcimages_l\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mcimages_uw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcimages_uw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mcimages_us\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcimages_uw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mt_loss_l\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-b1f007d8f950>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, sample)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mdims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdims\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# same transforms per minibatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0;32melif\u001b[0m \u001b[0mdims\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLambda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# diff transforms per img in minibatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0;31m# x1 = self.transform(sample)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlambd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    487\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-b1f007d8f950>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mdims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdims\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# same transforms per minibatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0;32melif\u001b[0m \u001b[0mdims\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLambda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# diff transforms per img in minibatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0;31m# x1 = self.transform(sample)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-b1f007d8f950>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mdims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdims\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# same transforms per minibatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0;32melif\u001b[0m \u001b[0mdims\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLambda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# diff transforms per img in minibatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0;31m# x1 = self.transform(sample)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    808\u001b[0m                 \u001b[0mfill\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfill\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 810\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    811\u001b[0m             \u001b[0mstartpoints\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendpoints\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistortion_scale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    812\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperspective\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstartpoints\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendpoints\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\n",
        "# scheduler = torch.optim.lr_scheduler.PolynomialLR(optimizer, total_iters=int(np.ceil(num_batches/4)*3), power=1.0)\n",
        "# scheduler = PolynomialLR(optimizer, total_iters=4, power=1.0)\n",
        "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=10**(-1/2))\n",
        "\n",
        "\n",
        "# t_optimizer.param_groups[0][\"lr\"]\n",
        "# s_optimizer.param_groups[0][\"lr\"]\n",
        "\n",
        "# import time\n",
        "# start = time.time()\n",
        "\n",
        "pth='/content/mpl.pth' # ty\n",
        "# pth='/content/drive/MyDrive/frame/mpl18.pth' # M\n",
        "\n",
        "for t in range(0,epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    t_lr=t_optimizer.param_groups[0][\"lr\"]\n",
        "    s_lr=s_optimizer.param_groups[0][\"lr\"]\n",
        "    print('t_lr,s_lr',t_lr,s_lr)\n",
        "    # train(labeled_loader, unlabeled_loader, teacher_model, student_model,\n",
        "    #     avg_student_model, criterion, t_optimizer, s_optimizer, t_scheduler, s_scheduler)\n",
        "    train(labeled_loader, unlabeled_loader, teacher_model, student_model,\n",
        "        avg_student_model, criterion, t_optimizer, s_optimizer)\n",
        "\n",
        "    # evaluate(test_loader, student_model, criterion)\n",
        "    evaluate(test_loader, avg_student_model, criterion)\n",
        "\n",
        "    checkpoint = {\n",
        "    'epoch': t+1,\n",
        "    'teacher_model': teacher_model.state_dict(),\n",
        "    'student_model': student_model.state_dict(),\n",
        "    'avg_student_model': avg_student_model.state_dict(),\n",
        "    't_optimizer': t_optimizer.state_dict(),\n",
        "    's_optimizer': s_optimizer.state_dict(),\n",
        "    't_scheduler': t_scheduler.state_dict(),\n",
        "    's_scheduler': s_scheduler.state_dict(),}\n",
        "    torch.save(checkpoint, pth)\n",
        "\n",
        "\n",
        "# res34, batch4 8.8\n",
        "# res18, batch16 11.6 nocompilemodel\n",
        "\n",
        "# 16m28s\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4GuEvf0MzBTD"
      },
      "outputs": [],
      "source": [
        "\n",
        "# finetune\n",
        "# model = student_model\n",
        "model = avg_student_model\n",
        "model.drop = nn.Identity()\n",
        "# labeled_loader = DataLoader(finetune_dataset, batch_size=128, num_workers=4, pin_memory=True) # batch_size=512\n",
        "labeled_loader = DataLoader(finetune_dataset, batch_size=128, pin_memory=True) # batch_size=512\n",
        "optimizer = optim.SGD(model.parameters(), lr=3e-5, momentum=0.9, weight_decay=0, nesterov=True)\n",
        "# scaler = amp.GradScaler()\n",
        "for epoch in range(1): #625\n",
        "    # train_ls = strain(labeled_loader, model, loss_fn, optimizer, scheduler)\n",
        "    train_ls = strain(labeled_loader, model, criterion, optimizer)\n",
        "    evaluate(test_loader, student_model, criterion)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JlQClGYfs5L1"
      },
      "outputs": [],
      "source": [
        "correct, test_loss = evaluate(test_loader, student_model, criterion)\n",
        "print(correct, test_loss)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}