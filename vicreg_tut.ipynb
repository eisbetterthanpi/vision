{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eisbetterthanpi/vision/blob/main/vicreg_tut.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "TZ4wM1O48nGS"
      },
      "outputs": [],
      "source": [
        "# https://arxiv.org/pdf/2105.04906.pdf\n",
        "# https://github.com/facebookresearch/vicreg\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title augmentations\n",
        "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
        "# All rights reserved.\n",
        "\n",
        "# This source code is licensed under the license found in the\n",
        "# LICENSE file in the root directory of this source tree.\n",
        "# https://github.com/facebookresearch/vicreg/blob/main/augmentations.py\n",
        "\n",
        "from PIL import ImageOps, ImageFilter\n",
        "import numpy as np\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.transforms import InterpolationMode\n",
        "\n",
        "class GaussianBlur(object):\n",
        "    def __init__(self, p):\n",
        "        self.p = p\n",
        "\n",
        "    def __call__(self, img):\n",
        "        if np.random.rand() < self.p:\n",
        "            sigma = np.random.rand() * 1.9 + 0.1\n",
        "            # return img.filter(ImageFilter.GaussianBlur(sigma))\n",
        "            return transforms.GaussianBlur(kernel_size=5, sigma=sigma)(img)\n",
        "        else:\n",
        "            return img\n",
        "\n",
        "class Solarization(object):\n",
        "    def __init__(self, p):\n",
        "        self.p = p\n",
        "    def __call__(self, img):\n",
        "        if np.random.rand() < self.p:\n",
        "            return ImageOps.solarize(img)\n",
        "        else:\n",
        "            return img\n",
        "\n",
        "\n",
        "class TrainTransform(object):\n",
        "    def __init__(self):\n",
        "        self.transform = transforms.Compose([\n",
        "                # transforms.RandomResizedCrop(32, interpolation=InterpolationMode.BICUBIC),#224\n",
        "                transforms.RandomHorizontalFlip(p=0.5),\n",
        "                transforms.RandomApply([transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1)], p=0.8,),\n",
        "                transforms.RandomGrayscale(p=0.2),\n",
        "                GaussianBlur(p=1.0),\n",
        "                Solarization(p=0.0),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "            ])\n",
        "        self.transform_prime = transforms.Compose([\n",
        "                # transforms.RandomResizedCrop(32, interpolation=InterpolationMode.BICUBIC),#224\n",
        "                transforms.RandomHorizontalFlip(p=0.5),\n",
        "                transforms.RandomApply([transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1)], p=0.8,),\n",
        "                transforms.RandomGrayscale(p=0.2),\n",
        "                GaussianBlur(p=0.1),\n",
        "                Solarization(p=0.2),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "            ])\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        # print(\"sample.shape\",sample.shape)\n",
        "        # sample=torch.squeeze(sample)\n",
        "        # sample=transforms.ToPILImage()(sample)\n",
        "        # sample = torch.vmap(transforms.ToPILImage(),sample)\n",
        "        x1 = self.transform(sample)\n",
        "        x2 = self.transform_prime(sample)\n",
        "        # x2 = transforms.ToTensor()(sample)\n",
        "        return x1, x2\n"
      ],
      "metadata": {
        "id": "hEUffQ24mkRY",
        "cellView": "form"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rG3xjMTtLmYN",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title data\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor #, Lambda, Compose\n",
        "import matplotlib.pyplot as plt\n",
        "# https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html\n",
        "\n",
        "# training_data = datasets.FashionMNIST(root=\"data\", train=True, download=True,transform=ToTensor(),)\n",
        "# test_data = datasets.FashionMNIST(root=\"data\", train=False, download=True,transform=ToTensor(),)\n",
        "\n",
        "# need rgb imgs?\n",
        "# training_data = datasets.CIFAR10(root=\"data\", train=True, download=True,transform=transforms.Compose([TrainTransform(), ToTensor()]),)\n",
        "training_data = datasets.CIFAR10(root=\"data\", train=True, download=True,transform=TrainTransform(),)\n",
        "# test_data = datasets.CIFAR10(root=\"data\", train=False, download=True, transform=transforms.Compose([transforms.RandomResizedCrop(32, interpolation=InterpolationMode.BICUBIC), ToTensor()]),)\n",
        "test_data = datasets.CIFAR10(root=\"data\", train=False, download=True,transform=ToTensor(),)\n",
        "\n",
        "\n",
        "batch_size = 64 #64\n",
        "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
        "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
        "ctraining_data = datasets.CIFAR10(root=\"data\", train=True, download=True,transform=ToTensor(),)\n",
        "ctrain_dataloader = DataLoader(ctraining_data, batch_size=batch_size)\n",
        "\n",
        "\n",
        "dataiter = iter(test_dataloader)\n",
        "x, labels = dataiter.next() # images, labels\n",
        "# print(labels)\n",
        "# print(y.shape)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "def imshow(img):\n",
        "    # img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "import torchvision\n",
        "# imshow(torchvision.utils.make_grid(x))\n",
        "# imshow(torchvision.utils.make_grid(y))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title vicreg\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def off_diagonal(x):\n",
        "    # print(\"off_diagonal\",x.shape)\n",
        "    n, m = x.shape\n",
        "    assert n == m\n",
        "    return x.flatten()[:-1].view(n - 1, n + 1)[:, 1:].flatten()\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")\n",
        "# https://github.com/facebookresearch/vicreg/blob/main/resnet.py\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        # dim_out=10\n",
        "        dim_class=10\n",
        "        dim_exp=128\n",
        "        self.conv = nn.Sequential( # nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0),\n",
        "            # nn.Conv2d(3, 8, 3, 1, 1), nn.BatchNorm2d(8), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "            # nn.Conv2d(8, 16, 5, 1, 2), nn.BatchNorm2d(16), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "            # nn.Conv2d(16, 16, 7, 1, 3), nn.BatchNorm2d(16), nn.ReLU(), #nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(3, 64, 3, 1, 1), nn.BatchNorm2d(64), nn.ReLU(), #nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(64, 128, 5, 1, 2), nn.BatchNorm2d(128), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(128, 256, 7, 1, 3), nn.BatchNorm2d(256), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "        )\n",
        "        self.lin = nn.Sequential(\n",
        "            nn.Linear(256 * 8 * 8, 256), nn.ReLU(),\n",
        "            nn.Linear(256, 128), nn.ReLU(),\n",
        "            nn.Linear(128, 64),\n",
        "        )\n",
        "\n",
        "        f=[80,100,128]\n",
        "        self.exp = nn.Sequential(\n",
        "            nn.Linear(64, f[0]), nn.BatchNorm1d(f[0]), nn.ReLU(),\n",
        "            nn.Linear(f[0], f[1]), nn.BatchNorm1d(f[1]), nn.ReLU(),\n",
        "            nn.Linear(f[1], f[-1])\n",
        "            )\n",
        "        self.classifier = nn.Linear(64, dim_class)\n",
        "\n",
        "    def vicreg(self, x, y): # https://github.com/facebookresearch/vicreg/blob/main/main_vicreg.py\n",
        "        # invariance loss\n",
        "        repr_loss = F.mse_loss(x, y)\n",
        "        \n",
        "        # x = torch.cat(FullGatherLayer.apply(x), dim=0)\n",
        "        # y = torch.cat(FullGatherLayer.apply(y), dim=0)\n",
        "        x = x - x.mean(dim=0)\n",
        "        y = y - y.mean(dim=0)\n",
        "\n",
        "        # variance loss\n",
        "        std_x = torch.sqrt(x.var(dim=0) + 0.0001)\n",
        "        std_y = torch.sqrt(y.var(dim=0) + 0.0001)\n",
        "        # print(x.var(dim=0),y.var(dim=0))\n",
        "        # print(std_x , std_y)\n",
        "        std_loss = torch.mean(F.relu(1 - std_x)) / 2 + torch.mean(F.relu(1 - std_y)) / 2\n",
        "        # std_loss=0.02\n",
        "        # print(torch.mean(F.relu(1 - std_x)) , torch.mean(F.relu(1 - std_y)))\n",
        "\n",
        "        # # covariance loss\n",
        "        # cov_x = (x.T @ x) / (self.args.batch_size - 1)\n",
        "        # cov_y = (y.T @ y) / (self.args.batch_size - 1)\n",
        "        # cov_loss = off_diagonal(cov_x).pow_(2).sum().div(self.num_features)\\\n",
        "        #  + off_diagonal(cov_y).pow_(2).sum().div(self.num_features)\n",
        "        # loss = (self.args.sim_coeff * repr_loss + self.args.std_coeff * std_loss + self.args.cov_coeff * cov_loss)\n",
        "\n",
        "        batch_size=x.size(dim=0)\n",
        "        num_features=32\n",
        "        sim_coeff=25.0 # λ / µ?\n",
        "        std_coeff=25.0\n",
        "        cov_coeff=1.0 # ν?\n",
        "\n",
        "        # print(\"x.dim()\",x.dim())\n",
        "        if x.dim() == 1:\n",
        "            x = x.view(-1, 1)\n",
        "        if y.dim() == 1:\n",
        "            y = y.view(-1, 1)\n",
        "        x=x.T\n",
        "        y=y.T\n",
        "        # print(\"x\",x.shape)\n",
        "        cov_x = (x.T @ x) / (batch_size - 1)\n",
        "        cov_y = (y.T @ y) / (batch_size - 1)\n",
        "        # print(\"cov_x\",cov_x.shape)\n",
        "        cov_loss = off_diagonal(cov_x).pow_(2).sum().div(num_features)\\\n",
        "         + off_diagonal(cov_y).pow_(2).sum().div(num_features)\n",
        "        # print(\"in vicreg\",repr_loss , std_loss , cov_loss)\n",
        "        loss = (sim_coeff * repr_loss + std_coeff * std_loss + cov_coeff * cov_loss)\n",
        "        return loss\n",
        "        \n",
        "    def loss(self, sx,sy):\n",
        "        sx = self.forward(sx)\n",
        "        sy = self.forward(sy)\n",
        "        vx = self.exp(sx)\n",
        "        vy = self.exp(sy)\n",
        "        loss = self.vicreg(vx,vy)\n",
        "        return loss\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        # print(\"forward x\",x.shape)\n",
        "        x = nn.Flatten()(x)\n",
        "        x = self.lin(x)\n",
        "        return x\n",
        "    def classify(self, x):\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "# softmax = nn.Softmax(dim=1)\n",
        "# pred_probab = softmax(logits)\n",
        "model = NeuralNetwork().to(device) # create an instance and move it to device (cache?)\n",
        "# print(model)\n",
        "\n",
        "# LARGE BATCH TRAINING OF CONVOLUTIONAL NETWORKS\n",
        "# https://arxiv.org/pdf/1708.03888.pdf\n",
        "\n",
        "# Barlow Twins: Self-Supervised Learning via Redundancy Reduction\n",
        "# https://arxiv.org/pdf/2103.03230.pdf\n",
        "# https://github.com/facebookresearch/barlowtwins/blob/main/main.py\n",
        "\n",
        "# https://arxiv.org/search/?query=vicreg&searchtype=all\n",
        "\n",
        "loss_list=[]\n"
      ],
      "metadata": {
        "id": "RGYE1gWOMeuU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "361fc629-78f4-4a15-efe6-88ab2cf723d9",
        "cellView": "form"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "X = torch.rand(64, 3, 32, 32, device=device)\n",
        "logits = model(X)\n",
        "print(logits.shape)\n",
        "print(logits[0])\n",
        "# print(logits[0].argmax(1))\n",
        "pred_probab = nn.Softmax(dim=1)(logits)\n",
        "y_pred = pred_probab.argmax(1)\n",
        "print(f\"Predicted class: {y_pred}\")"
      ],
      "metadata": {
        "id": "e6f8dWWjhNA6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b191108-47cc-4715-8ddb-3c7d63387a7a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "forward x torch.Size([64, 256, 8, 8])\n",
            "torch.Size([64, 64])\n",
            "tensor([ 0.0572, -0.0649, -0.1722, -0.2338,  0.1145,  0.1187,  0.0763,  0.0216,\n",
            "        -0.0654,  0.1771, -0.0271, -0.2069, -0.1035,  0.2459,  0.2247, -0.0541,\n",
            "        -0.0312,  0.0158, -0.3151, -0.0317, -0.0440, -0.1179, -0.0012, -0.1164,\n",
            "        -0.0386, -0.0592, -0.1001,  0.0885,  0.1681, -0.0692, -0.3456,  0.0081,\n",
            "         0.1034,  0.1631, -0.1176,  0.0624,  0.0389, -0.0354, -0.0676,  0.1597,\n",
            "         0.0184,  0.2048, -0.1722,  0.0567,  0.0818,  0.0456, -0.2405, -0.2981,\n",
            "        -0.1709,  0.0708, -0.0306,  0.0887, -0.1182,  0.1387, -0.1528, -0.0787,\n",
            "         0.1444, -0.0729, -0.0276, -0.0507, -0.2167, -0.0387, -0.1427,  0.0666],\n",
            "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
            "Predicted class: tensor([13, 49, 14, 41, 14, 41, 53, 14, 49,  0, 14, 49, 41, 41, 14, 14, 14, 14,\n",
            "        41, 49, 49, 49, 41, 14, 33, 41, 41, 14, 13, 41, 41, 14, 28, 14, 41, 14,\n",
            "        13, 33, 33, 41, 33, 14, 33,  0,  0, 14, 49, 33, 14, 41, 41, 41, 14, 14,\n",
            "        27, 41, 14, 41,  0, 41, 14, 14, 41, 41], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title train test function\n",
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    # for batch, (x, y) in enumerate(dataloader):\n",
        "    for batch, ((x,y), labels) in enumerate(dataloader):\n",
        "        sx, sy = x.to(device), y.to(device)\n",
        "        # print(\"sx sy\",sx.shape,sy.shape)\n",
        "        # pred = model(sx)\n",
        "        # loss = loss_fn(pred, sy)\n",
        "        loss = model.loss(sx,sy)\n",
        "        optimizer.zero_grad() # reset gradients of model parameters, to prevent double-counting\n",
        "        loss.backward() # Backpropagate gradients\n",
        "        optimizer.step() # adjust the parameters by the gradients\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), batch * len(x)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "def train_classifier(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, (x, y) in enumerate(dataloader):\n",
        "    # for batch, ((x,y), labels) in enumerate(dataloader):\n",
        "        sx, sy = x.to(device), y.to(device)\n",
        "        sx = model(sx)\n",
        "        pred = model.classify(sx)\n",
        "        loss = loss_fn(pred, sy)\n",
        "        optimizer.zero_grad() # reset gradients of model parameters, to prevent double-counting\n",
        "        loss.backward() # Backpropagate gradients\n",
        "        optimizer.step() # adjust the parameters by the gradients\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), batch * len(x)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "def test(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            x, y = X.to(device), y.to(device)\n",
        "            sx = model(x)\n",
        "            pred = model.classify(sx)\n",
        "            loss = loss_fn(pred, y)\n",
        "            # predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n"
      ],
      "metadata": {
        "id": "fsealXK3OPQa",
        "cellView": "form"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  0.1020,  0.0527,  0.0185,  0.0295, -0.0470, -0.0641,  0.0206, -0.1019\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
        "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.7) # 0.75(20)-0.9(100)\n",
        "\n",
        "# optimizer = optim.SGD(param_groups, 0, momentum=0.9, weight_decay=args.weight_decay)\n",
        "# scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, args.epochs)\n",
        "\n",
        "coptimizer = torch.optim.SGD(model.classifier.parameters(), lr=1e-3)\n",
        "lr_list=[]\n",
        "\n",
        "epochs = 10 #5 40\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    lr=optimizer.param_groups[0][\"lr\"]\n",
        "    lr_list.append(lr)\n",
        "    # print(lr)\n",
        "    train(train_dataloader, model, loss_fn, optimizer)\n",
        "    train_classifier(ctrain_dataloader, model, loss_fn, coptimizer)\n",
        "    test(test_dataloader, model, loss_fn)\n",
        "    scheduler.step()\n",
        "print(\"Done!\")\n",
        "torch.save(model.state_dict(), \"model.pth\")\n",
        "\n",
        "# model = NeuralNetwork().to(device)\n",
        "# model.load_state_dict(torch.load(\"model.pth\"))\n"
      ],
      "metadata": {
        "id": "kDBEk-l-Oxjn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de17b4df-deaa-47de-a12a-4abe18f48e08"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 13.043016  [    0/50000]\n",
            "loss: 12.302605  [ 6400/50000]\n",
            "loss: 15.480703  [12800/50000]\n",
            "loss: 14.747604  [19200/50000]\n",
            "loss: 15.016106  [25600/50000]\n",
            "loss: 14.490833  [32000/50000]\n",
            "loss: 13.559613  [38400/50000]\n",
            "loss: 13.266211  [44800/50000]\n",
            "loss: 2.229071  [    0/50000]\n",
            "loss: 2.217223  [ 6400/50000]\n",
            "loss: 2.188015  [12800/50000]\n",
            "loss: 2.260777  [19200/50000]\n",
            "loss: 2.246653  [25600/50000]\n",
            "loss: 2.241529  [32000/50000]\n",
            "loss: 2.228412  [38400/50000]\n",
            "loss: 2.202658  [44800/50000]\n",
            "Test Error: \n",
            " Accuracy: 21.7%, Avg loss: 2.229793 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 15.018920  [    0/50000]\n",
            "loss: 15.993645  [ 6400/50000]\n",
            "loss: 14.504047  [12800/50000]\n",
            "loss: 13.416109  [19200/50000]\n",
            "loss: 14.144498  [25600/50000]\n",
            "loss: 15.445381  [32000/50000]\n",
            "loss: 16.451962  [38400/50000]\n",
            "loss: 12.380365  [44800/50000]\n",
            "loss: 2.224599  [    0/50000]\n",
            "loss: 2.222846  [ 6400/50000]\n",
            "loss: 2.187388  [12800/50000]\n",
            "loss: 2.262511  [19200/50000]\n",
            "loss: 2.230584  [25600/50000]\n",
            "loss: 2.233196  [32000/50000]\n",
            "loss: 2.219213  [38400/50000]\n",
            "loss: 2.197441  [44800/50000]\n",
            "Test Error: \n",
            " Accuracy: 21.5%, Avg loss: 2.225257 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 13.804581  [    0/50000]\n",
            "loss: 14.580417  [ 6400/50000]\n",
            "loss: 13.785217  [12800/50000]\n",
            "loss: 12.941315  [19200/50000]\n",
            "loss: 14.738903  [25600/50000]\n",
            "loss: 14.114182  [32000/50000]\n",
            "loss: 13.714507  [38400/50000]\n",
            "loss: 11.539676  [44800/50000]\n",
            "loss: 2.238175  [    0/50000]\n",
            "loss: 2.210559  [ 6400/50000]\n",
            "loss: 2.162517  [12800/50000]\n",
            "loss: 2.240217  [19200/50000]\n",
            "loss: 2.226769  [25600/50000]\n",
            "loss: 2.218073  [32000/50000]\n",
            "loss: 2.205549  [38400/50000]\n",
            "loss: 2.193398  [44800/50000]\n",
            "Test Error: \n",
            " Accuracy: 23.7%, Avg loss: 2.214183 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 14.888677  [    0/50000]\n",
            "loss: 13.580329  [ 6400/50000]\n",
            "loss: 14.049342  [12800/50000]\n",
            "loss: 13.030638  [19200/50000]\n",
            "loss: 14.798996  [25600/50000]\n",
            "loss: 13.775658  [32000/50000]\n",
            "loss: 13.729812  [38400/50000]\n",
            "loss: 13.841660  [44800/50000]\n",
            "loss: 2.246049  [    0/50000]\n",
            "loss: 2.220869  [ 6400/50000]\n",
            "loss: 2.178367  [12800/50000]\n",
            "loss: 2.243510  [19200/50000]\n",
            "loss: 2.236002  [25600/50000]\n",
            "loss: 2.227017  [32000/50000]\n",
            "loss: 2.207036  [38400/50000]\n",
            "loss: 2.206376  [44800/50000]\n",
            "Test Error: \n",
            " Accuracy: 23.9%, Avg loss: 2.220673 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 13.442152  [    0/50000]\n",
            "loss: 13.235713  [ 6400/50000]\n",
            "loss: 13.644487  [12800/50000]\n",
            "loss: 16.266863  [19200/50000]\n",
            "loss: 13.723909  [25600/50000]\n",
            "loss: 13.319675  [32000/50000]\n",
            "loss: 12.740163  [38400/50000]\n",
            "loss: 12.299337  [44800/50000]\n",
            "loss: 2.243531  [    0/50000]\n",
            "loss: 2.216240  [ 6400/50000]\n",
            "loss: 2.175696  [12800/50000]\n",
            "loss: 2.242464  [19200/50000]\n",
            "loss: 2.231075  [25600/50000]\n",
            "loss: 2.224873  [32000/50000]\n",
            "loss: 2.213305  [38400/50000]\n",
            "loss: 2.197722  [44800/50000]\n",
            "Test Error: \n",
            " Accuracy: 24.0%, Avg loss: 2.219379 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 13.863812  [    0/50000]\n",
            "loss: 11.643306  [ 6400/50000]\n",
            "loss: 13.632338  [12800/50000]\n",
            "loss: 13.192205  [19200/50000]\n",
            "loss: 14.412369  [25600/50000]\n",
            "loss: 13.166018  [32000/50000]\n",
            "loss: 16.878792  [38400/50000]\n",
            "loss: 11.560781  [44800/50000]\n",
            "loss: 2.241394  [    0/50000]\n",
            "loss: 2.217469  [ 6400/50000]\n",
            "loss: 2.170982  [12800/50000]\n",
            "loss: 2.242885  [19200/50000]\n",
            "loss: 2.230399  [25600/50000]\n",
            "loss: 2.226150  [32000/50000]\n",
            "loss: 2.209721  [38400/50000]\n",
            "loss: 2.202199  [44800/50000]\n",
            "Test Error: \n",
            " Accuracy: 25.0%, Avg loss: 2.218254 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 12.078138  [    0/50000]\n",
            "loss: 12.630894  [ 6400/50000]\n",
            "loss: 15.638628  [12800/50000]\n",
            "loss: 13.503989  [19200/50000]\n",
            "loss: 13.846626  [25600/50000]\n",
            "loss: 12.947969  [32000/50000]\n",
            "loss: 12.757861  [38400/50000]\n",
            "loss: 11.548713  [44800/50000]\n",
            "loss: 2.242488  [    0/50000]\n",
            "loss: 2.216307  [ 6400/50000]\n",
            "loss: 2.173928  [12800/50000]\n",
            "loss: 2.240937  [19200/50000]\n",
            "loss: 2.233757  [25600/50000]\n",
            "loss: 2.226979  [32000/50000]\n",
            "loss: 2.206949  [38400/50000]\n",
            "loss: 2.199691  [44800/50000]\n",
            "Test Error: \n",
            " Accuracy: 24.2%, Avg loss: 2.218799 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 13.157888  [    0/50000]\n",
            "loss: 12.201242  [ 6400/50000]\n",
            "loss: 13.970703  [12800/50000]\n",
            "loss: 12.281435  [19200/50000]\n",
            "loss: 14.290707  [25600/50000]\n",
            "loss: 12.394731  [32000/50000]\n",
            "loss: 13.360939  [38400/50000]\n",
            "loss: 12.275893  [44800/50000]\n",
            "loss: 2.241531  [    0/50000]\n",
            "loss: 2.214892  [ 6400/50000]\n",
            "loss: 2.170426  [12800/50000]\n",
            "loss: 2.240134  [19200/50000]\n",
            "loss: 2.231423  [25600/50000]\n",
            "loss: 2.223619  [32000/50000]\n",
            "loss: 2.205685  [38400/50000]\n",
            "loss: 2.196557  [44800/50000]\n",
            "Test Error: \n",
            " Accuracy: 24.4%, Avg loss: 2.217489 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 13.128487  [    0/50000]\n",
            "loss: 11.112196  [ 6400/50000]\n",
            "loss: 13.280273  [12800/50000]\n",
            "loss: 13.589889  [19200/50000]\n",
            "loss: 15.801453  [25600/50000]\n",
            "loss: 13.675314  [32000/50000]\n",
            "loss: 12.972927  [38400/50000]\n",
            "loss: 14.974581  [44800/50000]\n",
            "loss: 2.242099  [    0/50000]\n",
            "loss: 2.216443  [ 6400/50000]\n",
            "loss: 2.173560  [12800/50000]\n",
            "loss: 2.242972  [19200/50000]\n",
            "loss: 2.232169  [25600/50000]\n",
            "loss: 2.225493  [32000/50000]\n",
            "loss: 2.206910  [38400/50000]\n",
            "loss: 2.196711  [44800/50000]\n",
            "Test Error: \n",
            " Accuracy: 24.4%, Avg loss: 2.218222 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 13.646474  [    0/50000]\n",
            "loss: 13.497561  [ 6400/50000]\n",
            "loss: 12.532915  [12800/50000]\n",
            "loss: 15.203319  [19200/50000]\n",
            "loss: 14.953829  [25600/50000]\n",
            "loss: 12.728375  [32000/50000]\n",
            "loss: 15.400953  [38400/50000]\n",
            "loss: 12.647416  [44800/50000]\n",
            "loss: 2.241115  [    0/50000]\n",
            "loss: 2.213361  [ 6400/50000]\n",
            "loss: 2.172311  [12800/50000]\n",
            "loss: 2.239991  [19200/50000]\n",
            "loss: 2.229300  [25600/50000]\n",
            "loss: 2.222746  [32000/50000]\n",
            "loss: 2.203060  [38400/50000]\n",
            "loss: 2.193208  [44800/50000]\n",
            "Test Error: \n",
            " Accuracy: 24.7%, Avg loss: 2.215686 \n",
            "\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(loss_list)\n",
        "# plt.yscale('log')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(lr_list)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "P2wbvqI8v2Fw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title save\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "PATH=\"/content/gdrive/MyDrive/torch_save/\" # for saving to google drive\n",
        "name='vicreg_tut.pth'\n",
        "# PATH=\"/content/\" # for saving on colab only\n",
        "# name='model.pth'\n",
        "\n",
        "torch.save(model.state_dict(), PATH+name)\n",
        "\n",
        "model = NeuralNetwork().to(device)\n",
        "model.load_state_dict(torch.load(PATH+name))\n"
      ],
      "metadata": {
        "id": "wH6LkL0QnPTg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e3a0b68-4b3e-4712-cb5c-81c55e643f23"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def test():\n",
        "    model.eval()\n",
        "    n_correct = 0\n",
        "    n_samples = 0\n",
        "    with torch.no_grad():\n",
        "        n_class_correct = [0 for i in range(10)]\n",
        "        n_class_samples = [0 for i in range(10)]\n",
        "        for images, labels in test_dataloader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            # outputs = model(images)\n",
        "            sx = model(images)\n",
        "            outputs = model.classify(sx)\n",
        "            # max returns (value ,index)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            n_samples += labels.size(0)\n",
        "            n_correct += (predicted == labels).sum().item()\n",
        "            for i in range(batch_size):\n",
        "                print(len(labels))\n",
        "                label = labels[i]\n",
        "                pred = predicted[i]\n",
        "                if (label == pred):\n",
        "                    n_class_correct[label] += 1\n",
        "                n_class_samples[label] += 1\n",
        "        acc = 100.0 * n_correct / n_samples\n",
        "        print(f'Accuracy of the network: {acc} %')\n",
        "        for i in range(10):\n",
        "            acc = 100.0 * n_class_correct[i] / n_class_samples[i]\n",
        "            print(f'Accuracy of {classes[i]}: {acc} %')\n",
        "\n",
        "test()"
      ],
      "metadata": {
        "id": "7S6mWmf_xom6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classes = [\"T-shirt/top\",\"Trouser\",\"Pullover\",\"Dress\",\"Coat\",\"Sandal\",\"Shirt\",\"Sneaker\",\"Bag\",\"Ankle boot\",]\n",
        "model.eval()\n",
        "import random\n",
        "n=random.randint(0,1000)\n",
        "print(n)\n",
        "x, y = test_data[n]\n",
        "# print(x)\n",
        "with torch.no_grad():\n",
        "    # pred = model(x.to(device))\n",
        "    # print(pred)\n",
        "    # predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
        "\n",
        "    # x, y = x.to(device), y.to(device)\n",
        "    x = x.to(device)\n",
        "    sx = model(x)\n",
        "    pred = model.classify(sx)\n",
        "    pred = torch.argmax(pred, dim=1).item()\n",
        "    print(pred)\n",
        "    print(y)\n",
        "    # predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
        "    # print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')\n",
        "\n"
      ],
      "metadata": {
        "id": "PoDyJMwUO4gX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}