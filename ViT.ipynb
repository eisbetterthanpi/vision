{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eisbetterthanpi/vision/blob/main/ViT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LxACli7GdyGq",
        "outputId": "784f514a-de12-4a48-c897-a17d5662dc11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:14<00:00, 12.2MB/s]\n"
          ]
        }
      ],
      "source": [
        "# @title data\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "# transform = transforms.Compose([transforms.ToTensor(),])\n",
        "\n",
        "# CIFAR10: 60000 32x32 color images in 10 classes, with 6000 images per class\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='data', train=True, download=True, transform=transform)\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='data', train=False, download=True, transform=transform)\n",
        "batch_size = 128 # 4\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "def imshow(img):\n",
        "    # img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "# dataiter = iter(train_loader) # get some random training images\n",
        "# images, labels = next(dataiter)\n",
        "# print(images.shape) # [batch, 3, 32, 32]\n",
        "# imshow(torchvision.utils.make_grid(images))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ViT me simple\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, n_heads=None, d_head=8, cond_dim=None, dropout=0.): # .1\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.d_head = d_head\n",
        "        self.n_heads = d_model // d_head\n",
        "        # self.d_head = d_model // n_heads\n",
        "        self.cond_dim = cond_dim\n",
        "        self.q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.kv = nn.Linear(cond_dim or d_model, 2*d_model, bias=False)\n",
        "        # self.k = nn.Sequential(nn.Dropout(dropout), nn.Linear(cond_dim, d_model, bias=False))\n",
        "        self.lin = nn.Linear(d_model, d_model)\n",
        "        # self.lin = zero_module(nn.Linear(d_model, d_model))\n",
        "        # self.lin = nn.Sequential(nn.Dropout(dropout), zero_module(nn.Linear(d_model, d_model)))\n",
        "        self.drop = nn.Dropout(dropout) # indp before q,k,v; after linout\n",
        "        self.scale = self.d_head ** -.5\n",
        "\n",
        "    def forward(self, x, cond=None, mask=None): # [batch, T, d_model]=[batch, h*w, c], [batch, num_tok, cond_dim], [batch,T]\n",
        "        batch = x.shape[0]\n",
        "        if self.cond_dim==None: cond=x # is self attn\n",
        "        Q = self.q(x).view(batch, -1, self.n_heads, self.d_head).transpose(1, 2) # [batch, T, d_model] -> [batch, n_heads, T, d_head]\n",
        "        # K = self.k(x).view(batch, -1, self.n_heads, self.d_head).transpose(1, 2)\n",
        "        K, V = self.kv(cond).view(batch, -1, self.n_heads, 2*self.d_head).transpose(1, 2).chunk(2, dim=-1) # [batch, n_heads, T/num_tok, d_head]\n",
        "\n",
        "        # # linear attention # Softmax(Q) @ (Softmax(K).T @ V)\n",
        "        if mask != None:\n",
        "            mask = mask[:, None, :, None] # [batch,T] -> [batch,1,T,1]\n",
        "            K, V = K.masked_fill(mask, -torch.finfo(x.dtype).max), V.masked_fill(mask, -torch.finfo(x.dtype).max)\n",
        "        Q, K = Q.softmax(dim=-1)*self.scale, K.softmax(dim=-2)\n",
        "        context = K.transpose(-2,-1) @ V # [batch, n_heads, d_head, d_head]\n",
        "        out = Q @ context # [batch, n_heads, T/num_tok, d_head]\n",
        "\n",
        "        # # (quadratic) attention # Softmax(Q @ K.T) @ V\n",
        "        # attn = Q @ K.transpose(-2,-1) * self.scale # [batch, n_heads, T] # [batch, n_heads, T, T/num_tok]\n",
        "        # if mask != None: attn = attn.masked_fill(mask[:, None, :, None], -torch.finfo(attn.dtype).max) # [batch,T]->[batch,1,T,1]\n",
        "        # attention = torch.softmax(attn, dim=-1)\n",
        "        # out = self.drop(attention) @ V # [batch, n_heads, T, d_head]\n",
        "\n",
        "        out = out.transpose(1, 2).flatten(2)\n",
        "        return self.drop(self.lin(out)) # [batch, T, d_model]\n",
        "\n",
        "# if self, dont pass cond_dim in init, dont pass cond in fwd\n",
        "# Softmax(Q @ K.T) @ V ~ Softmax(Q) @ Softmax(K).T @ V\n",
        "\n",
        "# https://github.com/lucidrains/x-transformers/blob/main/x_transformers/x_transformers.py#L1855\n",
        "\n",
        "class AttentionBlock(nn.Module):\n",
        "    def __init__(self, d_model, d_head, cond_dim=None, ff_mult=1, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.norm1 = nn.RMSNorm(d_model) # LayerNorm RMSNorm\n",
        "        self.norm2 = nn.RMSNorm(d_model)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.cond_dim = cond_dim\n",
        "        self.self = MultiHeadAttention(d_model, d_head=d_head, dropout=0)\n",
        "        if self.cond_dim!=None: self.cross = MultiHeadAttention(d_model, d_head=d_head, cond_dim=cond_dim, dropout=0)\n",
        "        act = nn.GELU() # ReLU GELU\n",
        "        ff_dim=d_model*ff_mult\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.RMSNorm(d_model), nn.Dropout(dropout), nn.Linear(d_model, ff_dim), act, # ReLU GELU\n",
        "            nn.RMSNorm(d_model), nn.Dropout(dropout), nn.Linear(ff_dim, d_model)\n",
        "            # nn.RMSNorm(d_model), act, nn.Dropout(dropout), nn.Linear(d_model, ff_dim),\n",
        "            # nn.RMSNorm(ff_dim), act, nn.Dropout(dropout), nn.Linear(ff_dim, d_model)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, cond=None, mask=None): # [b,c,h,w], [batch, num_tok, cond_dim], [batch,T]\n",
        "        # bchw = x.shape\n",
        "        # x = x.flatten(2).transpose(1,2) # [b,h*w,c]\n",
        "\n",
        "        if self.cond_dim==None: cond=None # is self attn\n",
        "        x = x + self.drop(self.self(self.norm1(x)))\n",
        "        if self.cond_dim!=None: x = x + self.cross(self.norm2(x), cond, mask)\n",
        "        x = x + self.ff(x)\n",
        "        # x = x + self.drop(self.ff(x))\n",
        "\n",
        "        # return x.transpose(1,2).reshape(*bchw)\n",
        "        return x\n",
        "\n",
        "\n",
        "class SimpleViT(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim, dim, depth, heads, mlp_dim, channels=3, dim_head=8):\n",
        "        super().__init__()\n",
        "        self.to_patch_embedding = nn.Sequential( # in, out, kernel, stride, pad\n",
        "            nn.Conv2d(in_dim, dim, kernel_size=7, stride=2, padding=7//2, bias=False), nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "            # nn.Conv2d(in_dim, dim, kernel_size=7, stride=2, padding=7//2, bias=False), nn.MaxPool2d(2,2)\n",
        "            )\n",
        "        # self.pos_embedding = LearnedRoPE2D(dim) # LearnedRoPE2D, RoPE2D\n",
        "        self.positional_emb = nn.Parameter(torch.zeros(1, 8*8, dim)) # positional_embedding == 'learnable'\n",
        "        self.transformer = AttentionBlock(d_model=dim, d_head=dim_head)\n",
        "        # self.transformer_blocks = nn.ModuleList([AttentionBlock(d_model=dim, d_head=dim_head) for i in range(depth)])\n",
        "\n",
        "        self.attention_pool = nn.Linear(dim, 1)\n",
        "        self.out = nn.Linear(dim, out_dim, bias=False)\n",
        "\n",
        "    def forward(self, img):\n",
        "        device = img.device\n",
        "        x = self.to_patch_embedding(img)\n",
        "        # x = self.pos_embedding(x)\n",
        "        x = x.flatten(-2).transpose(-2,-1) # b c h w -> b (h w) c\n",
        "        x = x + self.positional_emb\n",
        "        x = self.transformer(x)\n",
        "        # for blk in self.predictor_blocks: x = blk(x)\n",
        "\n",
        "        # x = x.flatten(-2).mean(dim=-1) # mean pool\n",
        "        attn_weights = self.attention_pool(x).squeeze(-1) # [batch, (h,w)] # seq_pool\n",
        "        x = (attn_weights.softmax(dim = 1).unsqueeze(1) @ x).squeeze(1) # [batch, 1, (h,w)] @ [batch, (h,w), dim]\n",
        "        # cls_token = repeat(self.class_emb, '1 1 d -> b 1 d', b = b)\n",
        "        # x = torch.cat((cls_token, x), dim=1)\n",
        "        # x = x[:, 0] # first token\n",
        "        return self.out(x)\n",
        "\n",
        "\n",
        "# pos_emb rope < learn < learned\n",
        "# conv > pixel?\n",
        "# droppath not required\n",
        "\n",
        "# dim = 64\n",
        "# dim_head = 8\n",
        "# heads = dim // dim_head\n",
        "# num_classes = 10\n",
        "# # model = SimpleViT(image_size=32, patch_size=4, num_classes=num_classes, dim=dim, depth=1, heads=heads, mlp_dim=dim*4, channels = 3, dim_head = dim_head)\n",
        "# model = SimpleViT(in_dim=3, out_dim=num_classes, dim=dim, depth=1, heads=heads, mlp_dim=dim*4, channels = 3, dim_head = dim_head).to(device)\n",
        "# print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 59850\n",
        "# optim = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "\n",
        "# # print(images.shape) # [batch, 3, 32, 32]\n",
        "# x = torch.rand(64, 3, 32, 32, device=device)\n",
        "# # x = torch.rand(64, 3, 28,28, device=device)\n",
        "# logits = model(x)\n",
        "# print(logits.shape)\n",
        "# pred_probab = nn.Softmax(dim=1)(logits)\n",
        "# y_pred = pred_probab.argmax(1)\n",
        "# # print(y_pred)\n"
      ],
      "metadata": {
        "id": "GHzr3NVs2EcG",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JYMQDoL578HQ"
      },
      "outputs": [],
      "source": [
        "# @title efficientvit nn/ops.py down\n",
        "# https://github.com/mit-han-lab/efficientvit/blob/master/efficientvit/models/nn/ops.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SameCh(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super().__init__()\n",
        "        # print('samech', in_ch, out_ch, in_ch//out_ch, out_ch//in_ch)\n",
        "        # if out_ch//in_ch > 1: self.func = lambda x: x.repeat_interleave(out_ch//in_ch, dim=1) # [b,i,h,w] -> [b,o,h,w]\n",
        "        # elif in_ch//out_ch > 1:\n",
        "        #     self.func = lambda x: torch.unflatten(x, 1, (out_ch, in_ch//out_ch)).mean(dim=2) # [b,i,h,w] -> [b,o,i/o,h,w] -> [b,o,h,w]\n",
        "        # elif in_ch==out_ch: self.func = lambda x: x\n",
        "        if in_ch>=out_ch: # ave\n",
        "            # self.func = lambda x: F.interpolate(x.flatten(2).transpose(1,2), size=out_ch, mode='nearest-exact').transpose(1,2).unflatten(-1,(x.shape[-2:])) # pytorch.org/docs/stable/generated/torch.nn.functional.interpolate.html\n",
        "            self.func = lambda x: F.adaptive_avg_pool1d(x.flatten(2).transpose(1,2), out_ch).transpose(1,2).unflatten(-1,(x.shape[-2:])) # https://pytorch.org/docs/stable/nn.html#pooling-layers\n",
        "            # self.func = lambda x: F.adaptive_max_pool1d(x.flatten(2).transpose(1,2), out_ch).transpose(1,2).unflatten(-1,(x.shape[-2:])) # https://pytorch.org/docs/stable/nn.html#pooling-layers\n",
        "        elif in_ch<out_ch: # nearest\n",
        "            self.func = lambda x: F.interpolate(x.flatten(2).transpose(1,2), size=out_ch, mode='nearest-exact').transpose(1,2).unflatten(-1,(x.shape[-2:])) # pytorch.org/docs/stable/generated/torch.nn.functional.interpolate.html\n",
        "            # self.func = lambda x: F.adaptive_avg_pool1d(x.flatten(2).transpose(1,2), out_ch).transpose(1,2).unflatten(-1,(x.shape[-2:])) # https://pytorch.org/docs/stable/nn.html#pooling-layers\n",
        "            # self.func = lambda x: F.adaptive_max_pool1d(x.flatten(2).transpose(1,2), out_ch).transpose(1,2).unflatten(-1,(x.shape[-2:])) # https://pytorch.org/docs/stable/nn.html#pooling-layers\n",
        "        # else:\n",
        "        #     print('SameCh err', in_ch, out_ch)\n",
        "    def forward(self, x): return self.func(x) # [b,c,h,w] -> [b,o,h,w]\n",
        "\n",
        "class SameCh(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super().__init__()\n",
        "        if in_ch >= out_ch: self.func = lambda x: F.adaptive_avg_pool1d(x.flatten(2).transpose(1,2), out_ch).transpose(1,2).unflatten(-1,(x.shape[-2:])) # https://pytorch.org/docs/stable/nn.html#pooling-layers\n",
        "        elif in_ch < out_ch: self.func = lambda x: F.interpolate(x.flatten(2).transpose(1,2), size=out_ch, mode='nearest-exact').transpose(1,2).unflatten(-1,(x.shape[-2:])) # pytorch.org/docs/stable/generated/torch.nn.functional.interpolate.html\n",
        "    def forward(self, x): return self.func(x) # [b,c,h,w] -> [b,o,h,w]\n",
        "\n",
        "class PixelShortcut(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, r=1):\n",
        "        super().__init__()\n",
        "        self.r = r\n",
        "        r = max(r, int(1/r))\n",
        "        if self.r>1: self.net = nn.Sequential(SameCh(in_ch, out_ch*r**2), nn.PixelShuffle(r)) #\n",
        "        elif self.r<1: self.net = nn.Sequential(nn.PixelUnshuffle(r), SameCh(in_ch*r**2, out_ch)) #\n",
        "        else: self.net = SameCh(in_ch, out_ch)\n",
        "    def forward(self, x): return self.net(x) # [b,c,h,w] -> [b,o,r*h,r*w]\n",
        "\n",
        "def init_conv(conv, out_r=1, in_r=1):\n",
        "    o, i, h, w = conv.weight.shape\n",
        "    conv_weight = torch.empty(o//out_r**2, i//in_r**2, h, w)\n",
        "    nn.init.kaiming_uniform_(conv_weight)\n",
        "    conv.weight.data.copy_(conv_weight.repeat_interleave(out_r**2, dim=0).repeat_interleave(in_r**2, dim=1))\n",
        "    if conv.bias is not None: nn.init.zeros_(conv.bias)\n",
        "    return conv\n",
        "\n",
        "class PixelShuffleConv(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch=None, kernel=3, r=1):\n",
        "        super().__init__()\n",
        "        self.r = r\n",
        "        r = max(r, int(1/r))\n",
        "        out_ch = out_ch or in_ch\n",
        "        # if self.r>1: self.net = nn.Sequential(nn.Conv2d(in_ch, out_ch*r**2, kernel, 1, kernel//2), nn.PixelShuffle(r)) # PixelShuffle: [b,c*r^2,h,w] -> [b,c,h*r,w*r] # upscale by upscale factor r # https://arxiv.org/pdf/1609.05158v2\n",
        "        if self.r>1: self.net = nn.Sequential(SeparableConv2d(in_ch, out_ch), nn.PixelShuffle(r)) # PixelShuffle: [b,c*r^2,h,w] -> [b,c,h*r,w*r] # upscale by upscale factor r # https://arxiv.org/pdf/1609.05158v2\n",
        "        # elif self.r<1: self.net = nn.Sequential(nn.PixelUnshuffle(r), nn.Conv2d(in_ch*r**2, out_ch, kernel, 1, kernel//2)) # PixelUnshuffle: [b,c,h*r,w*r] -> [b,c*r^2,h,w]\n",
        "        elif self.r<1: self.net = nn.Sequential(nn.PixelUnshuffle(r), SeparableConv2d(in_ch, out_ch)) # PixelUnshuffle: [b,c,h*r,w*r] -> [b,c*r^2,h,w]\n",
        "        # elif self.r<1: self.net = nn.Sequential(nn.PixelUnshuffle(r), nn.Conv2d(in_ch*r**2, out_ch, kernel, 1, kernel//2, bias=False)) # PixelUnshuffle: [b,c,h*r,w*r] -> [b,c*r^2,h,w]\n",
        "        # elif self.r<1: self.net = nn.Sequential(nn.Conv2d(in_ch, out_ch//r**2, kernel, 1, kernel//2, bias=False), nn.PixelUnshuffle(r)) # PixelUnshuffle: [b,c,h*r,w*r] -> [b,c*r^2,h,w]\n",
        "\n",
        "        # if self.r>1: self.net = nn.Sequential(init_conv(nn.Conv2d(in_ch, out_ch*r**2, kernel, 1, kernel//2), out_r=r), nn.PixelShuffle(r)) # PixelShuffle: [b,c*r^2,h,w] -> [b,c,h*r,w*r] # upscale by upscale factor r # https://arxiv.org/pdf/1609.05158v2\n",
        "        # elif self.r<1: self.net = nn.Sequential(nn.PixelUnshuffle(r), init_conv(nn.Conv2d(in_ch*r**2, out_ch, kernel, 1, kernel//2), in_r=r)) # PixelUnshuffle: [b,c,h*r,w*r] -> [b,c*r^2,h,w]\n",
        "        # else: self.net = nn.Conv2d(in_ch, out_ch, kernel, 1, kernel//2)\n",
        "        else: self.net = SeparableConv2d(in_ch, out_ch)\n",
        "        # self.net.apply(self.init_conv_)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "class UpDownBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, r=1, kernel=3):\n",
        "        super().__init__()\n",
        "        self.block = PixelShuffleConv(in_ch, out_ch, kernel=kernel, r=r)\n",
        "        self.shortcut_block = PixelShortcut(in_ch, out_ch, r=r)\n",
        "    def forward(self, x):\n",
        "        # print('UpDownBlock', x.shape, self.block(x).shape, self.shortcut_block(x).shape)\n",
        "        return self.block(x) + self.shortcut_block(x)\n",
        "        # return self.shortcut_block(x)\n",
        "\n",
        "# model = UpDownBlock(3, 24, r=1/2)#.to(device)\n",
        "# model = UpDownBlock(width_list[0], out_ch, r=2)\n",
        "\n",
        "# x = torch.rand(64, 3, 32, 32, device=device)\n",
        "x = torch.rand(2, 3, 32, 32)\n",
        "# out = model(x)\n",
        "\n",
        "# print(out.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## out"
      ],
      "metadata": {
        "id": "TsAFDMEK2RyZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title pool tok mixer\n",
        "# MetaFormer is Actually What You Need for Vision\n",
        "# https://openaccess.thecvf.com/content/CVPR2022/papers/Yu_MetaFormer_Is_Actually_What_You_Need_for_Vision_CVPR_2022_paper.pdf\n",
        "\n",
        "import torch.nn as nn\n",
        "class Pooling(nn.Module):\n",
        "    def __init__(self, pool_size=3):\n",
        "        super().__init__()\n",
        "        self.pool = nn.AvgPool2d(pool_size, stride=1, padding=pool_size//2, count_include_pad=False)\n",
        "    def forward(self, x): # [b,c,h,w]\n",
        "        return self.pool(x) - x # Subtraction of the input itself is added since the block already has a residual connection.\n",
        "\n",
        "# model = Pooling()\n",
        "# x = torch.randn(2, 3, 7,9)\n",
        "# out = model(x)\n",
        "# print(out.shape)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "_RSJTKpk3rEw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title SeparableConv2d\n",
        "# Xception: Deep Learning with Depthwise Separable Convolutions https://arxiv.org/pdf/1610.02357\n",
        "class SeparableConv2d(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch=None):\n",
        "        super().__init__()\n",
        "        out_ch = out_ch or in_ch\n",
        "        self.conv = nn.Conv2d(in_ch, in_ch, 3, 1, 3//2, groups=in_ch, bias=False)\n",
        "        self.pointwise = nn.Conv2d(in_ch, out_ch, 1, bias=False)\n",
        "\n",
        "    def forward(self,x):\n",
        "        x = self.conv(x)\n",
        "        x = self.pointwise(x)\n",
        "        return x\n",
        "\n",
        "class SeparableConv1d(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch=None):\n",
        "        super().__init__()\n",
        "        out_ch = out_ch or in_ch\n",
        "        self.conv = nn.Convd(in_ch, in_ch, 3, 1, 3//2, groups=in_ch, bias=False)\n",
        "        self.pointwise = nn.Convd(in_ch, out_ch, 1, bias=False)\n",
        "\n",
        "    def forward(self,x):\n",
        "        x = self.conv(x)\n",
        "        x = self.pointwise(x)\n",
        "        return x\n",
        "\n",
        "# #  'all Convolution and SeparableConvolution layers are followed by batch normalization'?\n",
        "# in_ch, out_ch = 3, 3\n",
        "# model = SeparableConv2d(in_ch, out_ch)\n",
        "# x = torch.randn(2, in_ch, 7,9)\n",
        "# out = model(x)\n",
        "# print(out.shape)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "WGM1ASSN4d82"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title UniversalInvertedBottleneckBlock\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class UIB(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch=None, mult=4):\n",
        "        super().__init__()\n",
        "        act = nn.SiLU()\n",
        "        out_ch = out_ch or in_ch\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, in_ch, 3, 1, 3//2, groups=in_ch, bias=False), nn.BatchNorm2d(in_ch),\n",
        "            nn.Conv2d(in_ch, mult*in_ch, 1, bias=False), nn.BatchNorm2d(mult*in_ch), act,\n",
        "            nn.Conv2d(mult*in_ch, mult*in_ch, 3, 1, 3//2, groups=mult*in_ch, bias=False), nn.BatchNorm2d(mult*in_ch), act,\n",
        "            nn.Conv2d(mult*in_ch, out_ch, 1, bias=False), nn.BatchNorm2d(out_ch),\n",
        "        )\n",
        "        self.gamma = nn.Parameter(1e-5 * torch.ones(out_ch, 1, 1))\n",
        "\n",
        "    def forward(self,x):\n",
        "        x = self.conv(x) * self.gamma\n",
        "        return x\n",
        "\n",
        "in_ch, out_ch = 3, 3\n",
        "model = UIB(in_ch, out_ch)\n",
        "x = torch.randn(2, in_ch, 7,9)\n",
        "out = model(x)\n",
        "print(out.shape)\n",
        "# print(sum(p.numel() for p in model.conv[0].parameters() if p.requires_grad)) # 59850\n",
        "print([p.numel() for p in model.conv.parameters() if p.requires_grad]) # 59850\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ttu8Rw1oJdT9",
        "outputId": "11277bbe-659d-4c99-8f91-b90e6337e438"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 3, 7, 9])\n",
            "[27, 3, 3, 36, 12, 12, 108, 12, 12, 36, 3, 3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# fused?\n",
        "class UIB(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch=None, mult=4):\n",
        "        super().__init__()\n",
        "        act = nn.SiLU()\n",
        "        out_ch = out_ch or in_ch\n",
        "        self.conv = nn.Sequential( # og\n",
        "            nn.Conv2d(in_ch, in_ch, 3, 1, 3//2, groups=in_ch, bias=False), nn.BatchNorm2d(in_ch),\n",
        "            nn.Conv2d(in_ch, mult*in_ch, 1, bias=False), nn.BatchNorm2d(mult*in_ch), act,\n",
        "            nn.Conv2d(mult*in_ch, mult*in_ch, 3, 1, 3//2, groups=mult*in_ch, bias=False), nn.BatchNorm2d(mult*in_ch), act,\n",
        "            nn.Conv2d(mult*in_ch, out_ch, 1, bias=False), nn.BatchNorm2d(out_ch),\n",
        "        )\n",
        "        self.conv = nn.Sequential( # og\n",
        "            nn.Conv2d(in_ch, in_ch, 3, 1, 3//2, groups=in_ch, bias=False), nn.Conv2d(in_ch, mult*in_ch, 1, bias=False),\n",
        "            nn.BatchNorm2d(mult*in_ch), act,\n",
        "            nn.Conv2d(mult*in_ch, mult*in_ch, 3, 1, 3//2, groups=mult*in_ch, bias=False), nn.Conv2d(mult*in_ch, out_ch, 1, bias=False), #nn.BatchNorm2d(mult*in_ch), act,\n",
        "            nn.BatchNorm2d(out_ch),\n",
        "        )\n",
        "        # self.conv = nn.Sequential( # me\n",
        "        #     nn.Conv2d(in_ch, in_ch, 3, 1, 3//2, groups=in_ch, bias=False), nn.BatchNorm2d(in_ch),\n",
        "        #     GatedAdaLN(in_ch), nn.BatchNorm2d(in_ch), act,\n",
        "        #     nn.Conv2d(in_ch, in_ch, 3, 1, 3//2, groups=in_ch, bias=False), nn.BatchNorm2d(in_ch), act,\n",
        "        #     nn.Conv2d(in_ch, out_ch, 1, bias=False), nn.BatchNorm2d(out_ch),\n",
        "        # )\n",
        "        # self.conv = nn.Sequential( # fused\n",
        "        #     nn.Conv2d(in_ch, mult*in_ch, 3, 1, 3//2, groups=in_ch, bias=False), nn.BatchNorm2d(mult*in_ch),\n",
        "        #     # nn.Conv2d(in_ch, mult*in_ch, 1, bias=False), nn.BatchNorm2d(mult*in_ch), act,\n",
        "        #     nn.Conv2d(mult*in_ch, mult*in_ch, 3, 1, 3//2, groups=mult*in_ch, bias=False), nn.BatchNorm2d(mult*in_ch), act,\n",
        "        #     nn.Conv2d(mult*in_ch, out_ch, 1, bias=False), nn.BatchNorm2d(out_ch),\n",
        "        # )\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.BatchNorm2d(in_ch), act, nn.Conv2d(in_ch, in_ch, 3, 1, 3//2, groups=in_ch, bias=False),\n",
        "            nn.BatchNorm2d(in_ch), act, nn.Conv2d(in_ch, mult*in_ch, 1, bias=False),\n",
        "            nn.BatchNorm2d(mult*in_ch), act, nn.Conv2d(mult*in_ch, mult*in_ch, 3, 1, 3//2, groups=mult*in_ch, bias=False),\n",
        "            nn.BatchNorm2d(mult*in_ch), act, zero_module(nn.Conv2d(mult*in_ch, out_ch, 1, bias=False)),\n",
        "        )\n",
        "\n",
        "        # self.conv = nn.Sequential(\n",
        "        #     nn.BatchNorm2d(in_ch), act, nn.Conv2d(in_ch, mult*in_ch, 3, 1, 3//2, groups=in_ch, bias=False),\n",
        "        #     # nn.Conv2d(in_ch, mult*in_ch, 1, bias=False), nn.BatchNorm2d(mult*in_ch), act,\n",
        "        #     nn.Conv2d(mult*in_ch, mult*in_ch, 3, 1, 3//2, groups=mult*in_ch, bias=False), nn.BatchNorm2d(mult*in_ch), act,\n",
        "        #     nn.Conv2d(mult*in_ch, out_ch, 1, bias=False), nn.BatchNorm2d(out_ch),\n",
        "        # )\n",
        "        self.gamma = nn.Parameter(1e-5 * torch.ones(out_ch, 1, 1))\n",
        "\n",
        "    def forward(self,x):\n",
        "        x = self.conv(x) * self.gamma\n",
        "        return x"
      ],
      "metadata": {
        "id": "JTlW993MICIU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# @title Gated AdaLN\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def zero_module(module):\n",
        "    for p in module.parameters():\n",
        "        p.detach().zero_()\n",
        "    return module\n",
        "\n",
        "# class LayerNorm2d(nn.LayerNorm):\n",
        "class LayerNorm2d(nn.RMSNorm):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.permute(0, 2, 3, 1)\n",
        "        x = super().forward(x)\n",
        "        x = x.permute(0, 3, 1, 2)\n",
        "        return x\n",
        "\n",
        "class GatedAdaLN(nn.Module):\n",
        "    def __init__(self, d_model, cond_dim=None):\n",
        "        super().__init__()\n",
        "        self.cond_dim = cond_dim\n",
        "        self.adaLN = nn.Sequential(\n",
        "            nn.SiLU(), zero_module(nn.Conv2d(d_model, 2 * d_model, 1))\n",
        "            # nn.SiLU(), zero_module(nn.Conv2d(d_model, 3 * d_model, 1))\n",
        "        )\n",
        "        # self.mlp = nn.Sequential(\n",
        "        #     # nn.SiLU(), zero_module(nn.Conv2d(d_model, d_model, 1))\n",
        "        # )\n",
        "        # self.norm = LayerNorm2d(d_model, elementwise_affine=False, eps=1e-6)\n",
        "        self.norm = LayerNorm2d(d_model)\n",
        "        # self.norm = nn.GroupNorm(1, d_model)\n",
        "\n",
        "    def forward(self, x, cond=None): # [b,t,d] / [b,c,h,w]\n",
        "        # if self.cond_dim==None: cond=x # is self attn\n",
        "        # scale, shift = self.adaLN(x).chunk(2, dim=-1) # for btd\n",
        "        scale, shift = self.adaLN(x).chunk(2, dim=1) # for bchw\n",
        "        x = x + (1 + scale) * self.norm(x) + shift\n",
        "\n",
        "        # scale, shift, gate = self.adaLN(x).chunk(3, dim=-1)\n",
        "        # scale, shift, gate = self.adaLN(cond)[...,None,None].chunk(3, dim=-1)\n",
        "        # scale, shift, gate = self.adaLN(x).chunk(3, dim=1)\n",
        "\n",
        "        # gate = torch.sigmoid(gate)\n",
        "        # x = gate * ((1 + scale) * self.norm(x) + shift) + (1 - gate) * x\n",
        "\n",
        "        # x = x + gate * self.mlp((1 + scale) * self.norm(x) + shift)\n",
        "        # x = gate * self.mlp((1 + scale) * self.norm(x) + shift)\n",
        "\n",
        "        return x\n",
        "\n",
        "dim = 64\n",
        "model = GatedAdaLN(dim)\n",
        "\n",
        "# x = torch.randn(2, dim)\n",
        "x = torch.randn(2, dim, 7,9)\n",
        "out = model(x)\n",
        "print(out.shape)  # Should be (32, 64)\n",
        "\n"
      ],
      "metadata": {
        "id": "Xw9jurjLH220",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5ede836-21b7-4d01-fcda-57fe3ee8a868",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 64, 7, 9])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ResBlock\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch=None, emb_dim=None, drop=0.):\n",
        "        super().__init__()\n",
        "        if out_ch==None: out_ch=in_ch\n",
        "        act = nn.SiLU() #\n",
        "        # self.block1 = nn.Sequential(nn.BatchNorm2d(in_ch), act, nn.Conv2d(in_ch, out_ch, 3, padding=1))\n",
        "        # # self.block2 = Seq(nn.BatchNorm2d(out_ch), scale_shift(out_ch, emb_dim) if emb_dim != None else nn.Identity(), act, nn.Conv2d(out_ch, out_ch, 3, padding=1))\n",
        "        # self.block2 = nn.Sequential(nn.BatchNorm2d(out_ch), act, zero_module(nn.Conv2d(out_ch, out_ch, 3, padding=1)))\n",
        "        # # self.res_conv = nn.Conv2d(in_ch, out_ch, 1) if in_ch != out_ch else nn.Identity()\n",
        "        # self.res_conv = zero_module(nn.Conv2d(in_ch, out_ch, 1)) if in_ch != out_ch else nn.Identity()\n",
        "\n",
        "        self.block = nn.Sequential( # best?\n",
        "            nn.Conv2d(in_ch, out_ch, 3, padding=1), nn.BatchNorm2d(out_ch), act,\n",
        "            zero_module(nn.Conv2d(out_ch, out_ch, 3, padding=1)), nn.BatchNorm2d(out_ch), act,\n",
        "            )\n",
        "\n",
        "        self.block = nn.Sequential( # ConvMixer https://arxiv.org/pdf/2201.09792\n",
        "            nn.Conv2d(in_ch, in_ch, 3, 1, 3//2, groups=in_ch, bias=False), act, nn.BatchNorm2d(in_ch),\n",
        "            zero_module(nn.Conv2d(in_ch, in_ch, 3, 1, 3//2, groups=in_ch, bias=False)), act, nn.BatchNorm2d(in_ch),\n",
        "            )\n",
        "\n",
        "    def forward(self, x, emb=None): # [b,c,h,w], [batch, emb_dim]\n",
        "        # h = self.block1(x)\n",
        "        # # h = self.block2(h, emb)\n",
        "        # h = self.block2(h)\n",
        "        # return h + self.res_conv(x)\n",
        "        return self.block(x)\n"
      ],
      "metadata": {
        "id": "j3-vvMS1-gVn"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "model = nn.Conv2d(6, 4, 3, 1, 3//2, groups=2, bias=False)\n",
        "print(model.weight.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3toaMKUCTEk",
        "outputId": "08976c81-41fd-497b-fa67-f5dd31605a88"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 3, 3, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ViT me more\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def zero_module(module):\n",
        "    for p in module.parameters():\n",
        "        p.detach().zero_()\n",
        "    return module\n",
        "\n",
        "# class LayerNorm2d(nn.LayerNorm):\n",
        "class LayerNorm2d(nn.RMSNorm):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.permute(0, 2, 3, 1)\n",
        "        x = super().forward(x)\n",
        "        x = x.permute(0, 3, 1, 2)\n",
        "        return x\n",
        "\n",
        "class AttentionBlock(nn.Module):\n",
        "    def __init__(self, d_model, d_head, cond_dim=None, ff_mult=4, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.norm1 = nn.RMSNorm(d_model) # LayerNorm RMSNorm\n",
        "        # self.norm1 = LayerNorm2d(d_model) # LayerNorm RMSNorm\n",
        "        # self.norm2 = nn.RMSNorm(d_model)\n",
        "        self.norm2 = LayerNorm2d(d_model)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.cond_dim = cond_dim\n",
        "        self.self = MultiHeadAttention(d_model, d_head=d_head, dropout=0) # 16448\n",
        "        # self.self = Pooling()\n",
        "\n",
        "        act = nn.GELU() # ReLU GELU\n",
        "        # ff_dim=d_model*ff_mult\n",
        "        # self.ff = nn.Sequential(\n",
        "        #     nn.RMSNorm(d_model), nn.Dropout(dropout), nn.Linear(d_model, ff_dim), act, # ReLU GELU\n",
        "        #     nn.RMSNorm(ff_dim), nn.Dropout(dropout), nn.Linear(ff_dim, d_model)\n",
        "        #     # nn.RMSNorm(d_model), act, nn.Dropout(dropout), nn.Linear(d_model, ff_dim),\n",
        "        #     # nn.RMSNorm(ff_dim), act, nn.Dropout(dropout), nn.Linear(ff_dim, d_model)\n",
        "        # )\n",
        "        # self.ff = nn.Sequential(\n",
        "        #     nn.BatchNorm2d(d_model), act, SeparableConv2d(d_model, d_model),\n",
        "        #     nn.BatchNorm2d(d_model), act, SeparableConv2d(d_model, d_model),\n",
        "        #     nn.BatchNorm2d(d_model), act, SeparableConv2d(d_model, d_model),\n",
        "        #     )\n",
        "\n",
        "        self.ff = ResBlock(d_model) # 74112\n",
        "        # self.ff = UIB(d_model, mult=4) # uib m4 36992, m2 18944\n",
        "\n",
        "        # self.ff = nn.Sequential(\n",
        "        #     nn.RMSNorm(d_model), act, SeparableConv1d(d_model, d_model),\n",
        "        #     nn.RMSNorm(d_model), act, SeparableConv1d(d_model, d_model),\n",
        "        #     nn.RMSNorm(d_model), act, SeparableConv1d(d_model, d_model),\n",
        "        #     )\n",
        "\n",
        "\n",
        "    def forward(self, x, cond=None, mask=None): # [b,c,h,w], [batch, num_tok, cond_dim], [batch,T]\n",
        "        bchw = x.shape\n",
        "        x = x.flatten(2).transpose(1,2) # [b,h*w,c]\n",
        "        # print('attnblk fwd',x.shape)\n",
        "\n",
        "        # if self.cond_dim==None: cond=None # is self attn\n",
        "        x = x + self.drop(self.self(self.norm1(x)))\n",
        "        x = x.transpose(1,2).reshape(*bchw)\n",
        "        # x = x.flatten(2).transpose(1,2) # [b,h*w,c]\n",
        "        x = x + self.ff(x)\n",
        "        # x = self.ff(x)\n",
        "        # x = x + self.drop(self.norm2(self.ff(x)))\n",
        "        # x = x.flatten(2).transpose(1,2) # [b,h*w,c]\n",
        "\n",
        "        # return x.transpose(1,2).reshape(*bchw)\n",
        "        return x\n",
        "\n",
        "\n",
        "class SimpleViT(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim, dim, depth, heads, mlp_dim, channels=3, dim_head=8):\n",
        "        super().__init__()\n",
        "        self.to_patch_embedding = nn.Sequential( # in, out, kernel, stride, pad\n",
        "            nn.Conv2d(in_dim, dim, kernel_size=7, stride=2, padding=7//2, bias=False), nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "            # nn.Conv2d(in_dim, dim, kernel_size=7, stride=2, padding=7//2, bias=False), nn.MaxPool2d(2,2)\n",
        "            # UpDownBlock(in_dim, dim, r=1/2, kernel=3), UpDownBlock(dim, dim, r=1/2, kernel=3)\n",
        "            )\n",
        "        # self.pos_embedding = LearnedRoPE2D(dim) # LearnedRoPE2D, RoPE2D\n",
        "        # self.positional_emb = nn.Parameter(torch.zeros(1, 8*8, dim)) # positional_embedding == 'learnable'\n",
        "        self.positional_emb = nn.Parameter(torch.zeros(1, dim, 8,8)) # positional_embedding == 'learnable'\n",
        "        # self.transformer = AttentionBlock(d_model=dim, d_head=dim_head)\n",
        "        self.transformer_blocks = nn.ModuleList([AttentionBlock(d_model=dim, d_head=dim_head) for i in range(depth)])\n",
        "\n",
        "        self.attention_pool = nn.Linear(dim, 1)\n",
        "        self.out = nn.Linear(dim, out_dim, bias=False)\n",
        "\n",
        "    def forward(self, img):\n",
        "        # device = img.device\n",
        "        x = self.to_patch_embedding(img)\n",
        "        # x = self.pos_embedding(x)\n",
        "        bchw = x.shape\n",
        "        # x = x.flatten(-2).transpose(-2,-1) # b c h w -> b (h w) c\n",
        "        x = x + self.positional_emb\n",
        "        # x = self.transformer(x)\n",
        "        for blk in self.transformer_blocks: x = blk(x)\n",
        "\n",
        "        x = x.flatten(2).transpose(1,2) # [b,h*w,c]\n",
        "        attn_weights = self.attention_pool(x).squeeze(-1) # [batch, (h,w)] # seq_pool\n",
        "        x = (attn_weights.softmax(dim = 1).unsqueeze(1) @ x).squeeze(1) # [batch, 1, (h,w)] @ [batch, (h,w), dim]\n",
        "        return self.out(x)\n",
        "\n",
        "\n",
        "# pos_emb rope < learn < learned\n",
        "# conv > pixel?\n",
        "# droppath not required\n",
        "\n",
        "# norm,act,conv < conv,norm,act\n",
        "# 2*s1 < uib < resblock\n",
        "# gatedadaln 3 < 2 = 1 < ffmult4 = 2*gatedadaln\n",
        "\n",
        "dim = 64\n",
        "dim_head = 8\n",
        "heads = dim // dim_head\n",
        "num_classes = 10\n",
        "# model = SimpleViT(image_size=32, patch_size=4, num_classes=num_classes, dim=dim, depth=1, heads=heads, mlp_dim=dim*4, channels = 3, dim_head = dim_head)\n",
        "model = SimpleViT(in_dim=3, out_dim=num_classes, dim=dim, depth=1, heads=heads, mlp_dim=dim*4, channels = 3, dim_head = dim_head).to(device)\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 59850\n",
        "optim = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "\n",
        "# print(images.shape) # [batch, 3, 32, 32]\n",
        "x = torch.rand(64, 3, 32, 32, device=device)\n",
        "# x = torch.rand(64, 3, 28,28, device=device)\n",
        "logits = model(x)\n",
        "print(logits.shape)\n",
        "# print(logits[0])\n",
        "# print(logits[0].argmax(1))\n",
        "pred_probab = nn.Softmax(dim=1)(logits)\n",
        "y_pred = pred_probab.argmax(1)\n",
        "# print(f\"Predicted class: {y_pred}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86f49b8f-4679-4696-8328-68576af25b66",
        "id": "M-zdjdJixtOu"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "179009\n",
            "torch.Size([64, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(sum(p.numel() for p in model.transformer_blocks[0].self.parameters() if p.requires_grad)) # 59850\n",
        "print(sum(p.numel() for p in model.transformer_blocks[0].ff.parameters() if p.requires_grad)) # 59850\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1hq7DQhft6l",
        "outputId": "b9c448ef-6efc-4b5b-f87c-bde0bcc40479"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16448\n",
            "18944\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fsealXK3OPQa",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4dba0ff1-6707-4dd7-9443-4ada6c233b05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.333965  [    0/50000]\n",
            "loss: 1.716493  [ 4992/50000]\n",
            "loss: 1.613613  [ 9984/50000]\n",
            "loss: 1.376912  [14976/50000]\n",
            "loss: 1.391007  [19968/50000]\n",
            "loss: 1.429989  [24960/50000]\n",
            "loss: 1.337859  [29952/50000]\n",
            "loss: 1.412405  [34944/50000]\n",
            "loss: 1.350241  [39936/50000]\n",
            "loss: 1.237491  [44928/50000]\n",
            "Test Error: \n",
            " Accuracy: 54.6%, Avg loss: 1.222994 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 1.109748  [    0/50000]\n",
            "loss: 1.203068  [ 4992/50000]\n",
            "loss: 1.119532  [ 9984/50000]\n",
            "loss: 1.144718  [14976/50000]\n",
            "loss: 1.071926  [19968/50000]\n",
            "loss: 1.004030  [24960/50000]\n",
            "loss: 1.049392  [29952/50000]\n",
            "loss: 0.980945  [34944/50000]\n",
            "loss: 1.112558  [39936/50000]\n",
            "loss: 1.216085  [44928/50000]\n",
            "Test Error: \n",
            " Accuracy: 60.9%, Avg loss: 1.101023 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 0.974638  [    0/50000]\n",
            "loss: 0.831229  [ 4992/50000]\n",
            "loss: 0.957338  [ 9984/50000]\n",
            "loss: 0.845014  [14976/50000]\n",
            "loss: 0.894913  [19968/50000]\n",
            "loss: 0.838329  [24960/50000]\n",
            "loss: 0.797803  [29952/50000]\n",
            "loss: 0.869129  [34944/50000]\n",
            "loss: 0.865101  [39936/50000]\n",
            "loss: 0.895379  [44928/50000]\n",
            "Test Error: \n",
            " Accuracy: 65.4%, Avg loss: 0.975273 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 0.913194  [    0/50000]\n",
            "loss: 0.762865  [ 4992/50000]\n",
            "loss: 0.751774  [ 9984/50000]\n",
            "loss: 0.914006  [14976/50000]\n",
            "loss: 0.885346  [19968/50000]\n",
            "loss: 0.784001  [24960/50000]\n",
            "loss: 0.732347  [29952/50000]\n",
            "loss: 0.982735  [34944/50000]\n",
            "loss: 0.737831  [39936/50000]\n",
            "loss: 0.734103  [44928/50000]\n",
            "Test Error: \n",
            " Accuracy: 69.7%, Avg loss: 0.872432 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 0.876567  [    0/50000]\n",
            "loss: 0.915984  [ 4992/50000]\n",
            "loss: 0.790765  [ 9984/50000]\n",
            "loss: 0.892352  [14976/50000]\n",
            "loss: 0.714149  [19968/50000]\n",
            "loss: 0.746273  [24960/50000]\n",
            "loss: 0.875367  [29952/50000]\n",
            "loss: 0.732027  [34944/50000]\n",
            "loss: 0.752690  [39936/50000]\n",
            "loss: 0.713896  [44928/50000]\n",
            "Test Error: \n",
            " Accuracy: 69.4%, Avg loss: 0.881778 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 0.644114  [    0/50000]\n",
            "loss: 0.598938  [ 4992/50000]\n",
            "loss: 0.640183  [ 9984/50000]\n",
            "loss: 0.794750  [14976/50000]\n",
            "loss: 0.723420  [19968/50000]\n",
            "loss: 0.720593  [24960/50000]\n",
            "loss: 0.620131  [29952/50000]\n",
            "loss: 0.728671  [34944/50000]\n",
            "loss: 0.737392  [39936/50000]\n",
            "loss: 0.784379  [44928/50000]\n",
            "Test Error: \n",
            " Accuracy: 72.2%, Avg loss: 0.788241 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 0.602810  [    0/50000]\n",
            "loss: 0.590400  [ 4992/50000]\n",
            "loss: 0.684903  [ 9984/50000]\n",
            "loss: 0.605135  [14976/50000]\n",
            "loss: 0.616592  [19968/50000]\n",
            "loss: 0.716271  [24960/50000]\n",
            "loss: 0.636044  [29952/50000]\n",
            "loss: 0.649410  [34944/50000]\n",
            "loss: 0.842766  [39936/50000]\n",
            "loss: 0.607777  [44928/50000]\n",
            "Test Error: \n",
            " Accuracy: 72.0%, Avg loss: 0.816135 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 0.830912  [    0/50000]\n",
            "loss: 0.679191  [ 4992/50000]\n",
            "loss: 0.521693  [ 9984/50000]\n",
            "loss: 0.648238  [14976/50000]\n",
            "loss: 0.520754  [19968/50000]\n",
            "loss: 0.694859  [24960/50000]\n",
            "loss: 0.738227  [29952/50000]\n",
            "loss: 0.780348  [34944/50000]\n",
            "loss: 0.681171  [39936/50000]\n",
            "loss: 0.518868  [44928/50000]\n",
            "Test Error: \n",
            " Accuracy: 72.8%, Avg loss: 0.786704 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 0.391403  [    0/50000]\n",
            "loss: 0.626987  [ 4992/50000]\n",
            "loss: 0.566957  [ 9984/50000]\n",
            "loss: 0.417695  [14976/50000]\n",
            "loss: 0.511692  [19968/50000]\n",
            "loss: 0.717119  [24960/50000]\n",
            "loss: 0.566480  [29952/50000]\n",
            "loss: 0.621743  [34944/50000]\n",
            "loss: 0.595788  [39936/50000]\n",
            "loss: 0.557367  [44928/50000]\n",
            "Test Error: \n",
            " Accuracy: 73.0%, Avg loss: 0.787918 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 0.519862  [    0/50000]\n",
            "loss: 0.572779  [ 4992/50000]\n",
            "loss: 0.507308  [ 9984/50000]\n",
            "loss: 0.532515  [14976/50000]\n",
            "loss: 0.544929  [19968/50000]\n",
            "loss: 0.517346  [24960/50000]\n",
            "loss: 0.624653  [29952/50000]\n",
            "loss: 0.614757  [34944/50000]\n",
            "loss: 0.481014  [39936/50000]\n",
            "loss: 0.525504  [44928/50000]\n",
            "Test Error: \n",
            " Accuracy: 73.3%, Avg loss: 0.828842 \n",
            "\n",
            "Done!\n",
            "time:  177.05642580986023\n"
          ]
        }
      ],
      "source": [
        "# @title train test function\n",
        "\n",
        "def train(dataloader, model, loss_fn, optim):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, (x, y) in enumerate(dataloader):\n",
        "        sx, sy = x.to(device), y.to(device)\n",
        "        pred = model(sx)\n",
        "        loss = loss_fn(pred, sy)\n",
        "        optim.zero_grad() # reset gradients of model parameters, to prevent double-counting\n",
        "        loss.backward() # Backpropagate gradients\n",
        "        optim.step() # adjust the parameters by the gradients\n",
        "        if (batch) % (size//(10* len(x))) == 0:\n",
        "            loss, current = loss.item(), batch * len(x)\n",
        "            # loss_list.append(loss)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "        try: wandb.log({\"loss\": loss.item()})\n",
        "        except: pass\n",
        "\n",
        "def test(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    for X, y in dataloader:\n",
        "        x, y = X.to(device), y.to(device)\n",
        "        with torch.no_grad():\n",
        "            pred = model(x)\n",
        "        # loss = loss_fn(pred, y)\n",
        "        test_loss += loss_fn(pred, y).item()\n",
        "        correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "    return correct\n",
        "\n",
        "import time\n",
        "start = time.time()\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "for t in range(10):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(train_loader, model, loss_fn, optim)\n",
        "    test(test_loader, model, loss_fn)\n",
        "print(\"Done!\")\n",
        "\n",
        "end = time.time()\n",
        "print(\"time: \",end - start)\n",
        "\n",
        "torch.save(model.state_dict(), \"model.pth\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "2Nd-sGe6Ku4S",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "outputId": "8cc6b45e-6028-4d69-9cdf-c3009bbbc79f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>█▆▇▇▅▄▅▅▅▄▄▃▃▄▃▄▃▂▃▂▃▃▃▂▃▃▂▂▂▂▂▂▂▂▃▁▂▁▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>0.6628</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">earnest-feather-40</strong> at: <a href='https://wandb.ai/bobdole/vit/runs/0z3gduox' target=\"_blank\">https://wandb.ai/bobdole/vit/runs/0z3gduox</a><br> View project at: <a href='https://wandb.ai/bobdole/vit' target=\"_blank\">https://wandb.ai/bobdole/vit</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250321_160956-0z3gduox/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250321_161355-lc9xwznz</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/bobdole/vit/runs/lc9xwznz' target=\"_blank\">bumbling-dream-41</a></strong> to <a href='https://wandb.ai/bobdole/vit' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/bobdole/vit' target=\"_blank\">https://wandb.ai/bobdole/vit</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/bobdole/vit/runs/lc9xwznz' target=\"_blank\">https://wandb.ai/bobdole/vit/runs/lc9xwznz</a>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# @title wandb\n",
        "!pip install -q wandb\n",
        "import wandb # https://docs.wandb.ai/quickstart\n",
        "wandb.login(key='487a2109e55dce4e13fc70681781de9f50f27be7')\n",
        "try: run.finish()\n",
        "except NameError: pass\n",
        "run = wandb.init(project=\"vit\", config={\"model\": \"res18\",})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gBVIQsv4E2XB"
      },
      "outputs": [],
      "source": [
        "# @title save/load\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "folder='/content/drive/MyDrive/jepa/'\n",
        "\n",
        "modelsd, optimsd = torch.load(folder+'vit.pkl', map_location=device).values()\n",
        "model.load_state_dict(modelsd, strict=False)\n",
        "optim.load_state_dict(optimsd)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aC1TLfccE3W5"
      },
      "outputs": [],
      "source": [
        "checkpoint = {'model': model.state_dict(), 'optimizer': optim.state_dict()}\n",
        "# torch.save(checkpoint, folder+'vit.pkl')\n",
        "torch.save(checkpoint, 'cct.pkl')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## drawer"
      ],
      "metadata": {
        "id": "VeMRpak44QHm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "B8Pm-Fw6jn4A"
      },
      "outputs": [],
      "source": [
        "# @title mha me\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def zero_module(module):\n",
        "    \"\"\"Zero out the parameters of a module and return it.\"\"\"\n",
        "    for p in module.parameters():\n",
        "        p.detach().zero_()\n",
        "    return module\n",
        "\n",
        "# https://github.com/osmr/imgclsmob/blob/master/pytorch/pytorchcv/models/fractalnet_cifar.py\n",
        "    def calc_drop_mask(batch_size,\n",
        "                       glob_num_columns,\n",
        "                       curr_num_columns,\n",
        "                       max_num_columns,\n",
        "                       loc_drop_prob):\n",
        "        \"\"\"\n",
        "        Calculate drop path mask.\n",
        "\n",
        "        Parameters:\n",
        "        ----------\n",
        "        batch_size : int\n",
        "            Size of batch.\n",
        "        glob_num_columns : int\n",
        "            Number of columns in global drop path mask.\n",
        "        curr_num_columns : int\n",
        "            Number of active columns in the current level of block.\n",
        "        max_num_columns : int\n",
        "            Number of columns for all network.\n",
        "        loc_drop_prob : float\n",
        "            Local drop path probability.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        Tensor\n",
        "            Resulted mask.\n",
        "        \"\"\"\n",
        "        glob_batch_size = glob_num_columns.shape[0]\n",
        "        glob_drop_mask = np.zeros((curr_num_columns, glob_batch_size), dtype=np.float32)\n",
        "        glob_drop_num_columns = glob_num_columns - (max_num_columns - curr_num_columns)\n",
        "        glob_drop_indices = np.where(glob_drop_num_columns >= 0)[0]\n",
        "        glob_drop_mask[glob_drop_num_columns[glob_drop_indices], glob_drop_indices] = 1.0\n",
        "\n",
        "        loc_batch_size = batch_size - glob_batch_size\n",
        "        loc_drop_mask = np.random.binomial(\n",
        "            n=1,\n",
        "            p=(1.0 - loc_drop_prob),\n",
        "            size=(curr_num_columns, loc_batch_size)).astype(np.float32)\n",
        "        alive_count = loc_drop_mask.sum(axis=0)\n",
        "        dead_indices = np.where(alive_count == 0.0)[0]\n",
        "        loc_drop_mask[np.random.randint(0, curr_num_columns, size=dead_indices.shape), dead_indices] = 1.0\n",
        "\n",
        "        drop_mask = np.concatenate((glob_drop_mask, loc_drop_mask), axis=1)\n",
        "        return torch.from_numpy(drop_mask)\n",
        "\n",
        "# https://github.com/FrancescoSaverioZuppichini/DropPath\n",
        "\n",
        "\n",
        "class DropPath(nn.Module):\n",
        "    def __init__(self, drop_prob=None):\n",
        "        super().__init__()\n",
        "        self.drop_prob = float(drop_prob)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch, drop_prob, device, dtype = x.shape[0], self.drop_prob, x.device, x.dtype\n",
        "        if drop_prob <= 0. or not self.training: return x\n",
        "        keep_prob = 1 - self.drop_prob\n",
        "        shape = (batch, *((1,) * (x.ndim - 1)))\n",
        "        keep_mask = torch.zeros(shape, device = device).float().uniform_(0, 1) < keep_prob\n",
        "        output = x.div(keep_prob) * keep_mask.float()\n",
        "        return output\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# class DropPath(nn.Module):\n",
        "#     \"\"\"Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\"\"\"\n",
        "#     def __init__(self, drop_prob=0.1):\n",
        "#         super().__init__()\n",
        "#         self.drop_prob = drop_prob\n",
        "\n",
        "#     # def forward(self, x):\n",
        "#     # def drop_path(x, drop_prob: float = 0., training: bool = False):\n",
        "#     def forward(x, drop_prob: float = 0., training: bool = False):\n",
        "#         if drop_prob == 0. or not training: return x\n",
        "#         keep_prob = 1 - drop_prob\n",
        "#         shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n",
        "#         random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
        "#         random_tensor.floor_()  # binarize\n",
        "#         output = x.div(keep_prob) * random_tensor\n",
        "#         # return output\n",
        "#         return drop_path(x, self.drop_prob, self.training)\n",
        "\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, n_heads=None, d_head=8, cond_dim=None, dropout=0.): # .1\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.d_head = d_head\n",
        "        self.n_heads = d_model // d_head\n",
        "        # self.d_head = d_model // n_heads\n",
        "        self.cond_dim = cond_dim\n",
        "        self.q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.kv = nn.Linear(cond_dim or d_model, 2*d_model, bias=False)\n",
        "        # self.k = nn.Sequential(nn.Dropout(dropout), nn.Linear(cond_dim, d_model, bias=False))\n",
        "        self.lin = nn.Linear(d_model, d_model)\n",
        "        # self.lin = zero_module(nn.Linear(d_model, d_model))\n",
        "        # self.lin = nn.Sequential(nn.Dropout(dropout), zero_module(nn.Linear(d_model, d_model)))\n",
        "        self.drop = nn.Dropout(dropout) # indp before q,k,v; after linout\n",
        "        self.scale = self.d_head ** -.5\n",
        "\n",
        "    def forward(self, x, cond=None, mask=None): # [batch, T, d_model]=[batch, h*w, c], [batch, num_tok, cond_dim], [batch,T]\n",
        "        batch = x.shape[0]\n",
        "        if self.cond_dim==None: cond=x # is self attn\n",
        "        Q = self.q(x).view(batch, -1, self.n_heads, self.d_head).transpose(1, 2) # [batch, T, d_model] -> [batch, n_heads, T, d_head]\n",
        "        # K = self.k(x).view(batch, -1, self.n_heads, self.d_head).transpose(1, 2)\n",
        "        K, V = self.kv(cond).view(batch, -1, self.n_heads, 2*self.d_head).transpose(1, 2).chunk(2, dim=-1) # [batch, n_heads, T/num_tok, d_head]\n",
        "\n",
        "        # # linear attention # Softmax(Q) @ (Softmax(K).T @ V)\n",
        "        if mask != None:\n",
        "            mask = mask[:, None, :, None] # [batch,T] -> [batch,1,T,1]\n",
        "            K, V = K.masked_fill(mask, -torch.finfo(x.dtype).max), V.masked_fill(mask, -torch.finfo(x.dtype).max)\n",
        "        Q, K = Q.softmax(dim=-1)*self.scale, K.softmax(dim=-2)\n",
        "        context = K.transpose(-2,-1) @ V # [batch, n_heads, d_head, d_head]\n",
        "        out = Q @ context # [batch, n_heads, T/num_tok, d_head]\n",
        "\n",
        "        # # (quadratic) attention # Softmax(Q @ K.T) @ V\n",
        "        # attn = Q @ K.transpose(-2,-1) * self.scale # [batch, n_heads, T] # [batch, n_heads, T, T/num_tok]\n",
        "        # if mask != None: attn = attn.masked_fill(mask[:, None, :, None], -torch.finfo(attn.dtype).max) # [batch,T]->[batch,1,T,1]\n",
        "        # attention = torch.softmax(attn, dim=-1)\n",
        "        # out = self.drop(attention) @ V # [batch, n_heads, T, d_head]\n",
        "\n",
        "        out = out.transpose(1, 2).flatten(2)\n",
        "        return self.drop(self.lin(out)) # [batch, T, d_model]\n",
        "\n",
        "# if self, dont pass cond_dim in init, dont pass cond in fwd\n",
        "# Softmax(Q @ K.T) @ V ~ Softmax(Q) @ Softmax(K).T @ V\n",
        "\n",
        "# https://github.com/lucidrains/x-transformers/blob/main/x_transformers/x_transformers.py#L1855\n",
        "\n",
        "class AttentionBlock(nn.Module):\n",
        "    def __init__(self, d_model, d_head, cond_dim=None, ff_dim=None, dropout=0.):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.norm1 = nn.RMSNorm(d_model) # LayerNorm RMSNorm\n",
        "        self.norm2 = nn.RMSNorm(d_model)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        # drop_path_rate = 0.#0.1\n",
        "        # self.drop_path = DropPath(drop_path_rate)\n",
        "        self.cond_dim = cond_dim\n",
        "        self.self = MultiHeadAttention(d_model, d_head=d_head, dropout=0)\n",
        "        if self.cond_dim!=None: self.cross = MultiHeadAttention(d_model, d_head=d_head, cond_dim=cond_dim, dropout=0)\n",
        "        act = nn.GELU() # ReLU GELU\n",
        "        if ff_dim==None: ff_dim=d_model#*4\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.RMSNorm(d_model), nn.Linear(d_model, ff_dim), act, # ReLU GELU\n",
        "            nn.RMSNorm(d_model), nn.Dropout(dropout), nn.Linear(ff_dim, d_model)\n",
        "            # nn.RMSNorm(d_model), act, nn.Linear(d_model, ff_dim),\n",
        "            # nn.RMSNorm(ff_dim), act, nn.Linear(ff_dim, d_model)\n",
        "\n",
        "            # nn.RMSNorm(d_model), nn.Linear(d_model, ff_dim), nn.ReLU(), nn.Dropout(dropout), # ReLU GELU\n",
        "            # # nn.Linear(d_model, ff_dim), nn.ReLU(), nn.Dropout(dropout), # ReLU GELU\n",
        "            # nn.Linear(ff_dim, d_model), nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x, cond=None, mask=None): # [b,c,h,w], [batch, num_tok, cond_dim], [batch,T]\n",
        "        # bchw = x.shape\n",
        "        # x = x.flatten(2).transpose(1,2) # [b,h*w,c]\n",
        "\n",
        "        # if self.cond_dim==None: cond=None # is self attn\n",
        "        # x = x + self.drop(self.self(self.norm1(x)))\n",
        "        # if self.cond_dim!=None: x = x + self.cross(self.norm2(x), cond, mask)\n",
        "        # x = x + self.ff(x)\n",
        "\n",
        "        # x = x + self.drop_path(self.self(self.norm1(x)))\n",
        "        x = x + self.drop(self.self(self.norm1(x)))\n",
        "        # x = self.norm2(x)\n",
        "\n",
        "        # x = x + self.drop_path(self.ff(x))\n",
        "        x = x + self.drop(self.ff(x))\n",
        "\n",
        "        # return x.transpose(1,2).reshape(*bchw)\n",
        "        return x\n",
        "\n",
        "\n",
        "        # src = src + self.drop_path(self.self_attn(self.pre_norm(src)))\n",
        "        # src = self.norm1(src)\n",
        "        # src2 = self.linear2(self.dropout1(self.activation(self.linear1(src))))\n",
        "        # src = src + self.drop_path(self.dropout2(src2))\n",
        "\n",
        "            # embedding_dim=embedding_dim,\n",
        "            # seq_pool=True, # attn pool\n",
        "            # dropout_rate=0.,\n",
        "            # attention_dropout=0.1,\n",
        "            # stochastic_depth=0.1, # drop path\n",
        "\n",
        "\n",
        "# d_model=8\n",
        "# d_head=4\n",
        "# batch=4\n",
        "# h,w=5,6\n",
        "# x=torch.rand(batch,d_model,h,w)\n",
        "# cond_dim=10\n",
        "# cross = AttentionBlock(d_model=d_model, d_head=d_head,cond_dim=cond_dim)\n",
        "# num_tok=1\n",
        "# cond=torch.rand(batch,num_tok,cond_dim)\n",
        "# mask=torch.rand(batch,h*w)>0.5\n",
        "# out = cross(x, cond)\n",
        "# print(out.shape)\n",
        "# print(out)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "j9wUA7Vk0Y03"
      },
      "outputs": [],
      "source": [
        "# @title RotEmb\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "class RoPE(nn.Module): # Rotary Positional Embeddings\n",
        "    def __init__(self, dim, seq_len=512, base=10000):\n",
        "        super().__init__()\n",
        "        self.base = base\n",
        "        theta = 1.0 / (base ** (torch.arange(0, dim, step=2) / dim))\n",
        "        pos = torch.arange(seq_len).unsqueeze(1)\n",
        "        angles = (pos * theta).unsqueeze(-1) # [seq_len, 1] * [dim // 2] -> [seq_len, dim // 2, 1]\n",
        "        self.rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1).flatten(-2).unsqueeze(0) # [seq_len, dim // 2, 2] -> [1, seq_len, dim]\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch, seq_len, dim = x.shape\n",
        "        if self.rot_emb.shape[0] < seq_len: self.__init__(dim, seq_len, self.base)\n",
        "        # rot_emb = self.rot_emb[:seq_len].unsqueeze(0).expand(batch, -1, -1, -1) # [batch, seq_len, dim//2, 2]\n",
        "        # x = x.reshape(batch, seq_len, dim // 2, 2)\n",
        "        # rot_x = x * rot_emb\n",
        "        # return rot_x.flatten(-2)\n",
        "\n",
        "        return x * self.rot_emb[:seq_len] # [1, seq_len, dim]\n",
        "\n",
        "\n",
        "class RotEmb(nn.Module): # Rotary Positional Embeddings\n",
        "    def __init__(self, dim, top=torch.pi, base=10000):\n",
        "        super().__init__()\n",
        "        self.theta = top / (base ** (torch.arange(0, dim, step=2, device=device) / dim))\n",
        "        # self.theta = top / (base ** torch.linspace(0, 1, dim//2, device=device))\n",
        "\n",
        "    def forward(self, pos): # [batch] in [0,1]\n",
        "        angles = (pos.unsqueeze(-1) * self.theta).unsqueeze(-1) # [seq_len, 1] * [dim // 2] -> [seq_len, dim // 2, 1]\n",
        "        rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1) # [seq_len, dim // 2, 2]\n",
        "        return rot_emb.flatten(-2) # [seq_len, dim]\n",
        "\n",
        "# print(torch.arange(0, dim, step=2, device=device) / dim)\n",
        "# print(torch.linspace(0, 1, dim//2, device=device))\n",
        "\n",
        "class LearnedSinusoidalPosEmb(nn.Module):\n",
        "    \"\"\" https://github.com/crowsonkb/v-diffusion-jax/blob/master/diffusion/models/danbooru_128.py#L8 \"\"\"\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        half_dim = dim // 2\n",
        "        self.weights = nn.Parameter(torch.randn(1, half_dim))\n",
        "\n",
        "    def forward(self, x): # [b] in N\n",
        "        x = x.unsqueeze(-1) # [batch, 1]\n",
        "        freqs = x * self.weights * 2*math.pi # [b, 1] * [1, half_dim] = [b, half_dim]\n",
        "        fouriered = torch.cat((freqs.sin(), freqs.cos()), dim=-1) # [b, dim]\n",
        "        fouriered = torch.cat((x, fouriered), dim = -1) # [b, 1+dim]\n",
        "        return fouriered\n",
        "\n",
        "# rotemb = RotEmb(10)\n",
        "# seq_len=10\n",
        "# pos = torch.linspace(0,1,seq_len).to(device)#.unsqueeze(-1)\n",
        "# rot_emb = rotemb(pos)\n",
        "# print(rot_emb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5TZcQ7gVN60",
        "outputId": "39a21e64-958f-48b2-e5af-d731dfa648ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 16, 3, 5])\n"
          ]
        }
      ],
      "source": [
        "# @title learnt RoPE\n",
        "\n",
        "# b,c,h,w\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class LearnedRotEmb(nn.Module): # pos in R\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.weights = nn.Parameter(torch.randn(1, dim//2))\n",
        "\n",
        "    def forward(self, pos): # [batch] in [0,1]\n",
        "        angles = (self.weights * pos.unsqueeze(-1) * 2*math.pi).unsqueeze(-1) # [batch, 1] * [1, dim//2] -> [batch, dim//2, 1]\n",
        "        rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1) # [batch, dim // 2, 2]\n",
        "        return rot_emb.flatten(-2) # [batch, dim]\n",
        "\n",
        "class LearnedRotEmb2D(nn.Module): # pos in R\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.weight_xy = nn.Parameter(torch.randn(2, dim//2))\n",
        "\n",
        "    def forward(self, pos): # [batch,2] in [0,1]\n",
        "        angles = (pos @ self.weight_xy * 2*math.pi).unsqueeze(-1) # [batch, 2] @ [2, dim//2] -> [batch, dim//2, 1]\n",
        "        rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1) # [batch, dim//2, 2]\n",
        "        return rot_emb.flatten(-2) # [batch, dim]\n",
        "\n",
        "\n",
        "class LearnedRoPE(nn.Module): # learnt RoPE ; each tok is 1 pos\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.weights = nn.Parameter(torch.randn(1, dim//2))\n",
        "\n",
        "    def forward(self, x): #\n",
        "        batch, seq_len, dim = x.shape\n",
        "        # if rot_emb.shape[0] < seq_len: self.__init__(dim, seq_len)\n",
        "        pos = torch.arange(seq_len).unsqueeze(1)\n",
        "        angles = (self.weights * pos * 2*math.pi).unsqueeze(-1) # [seq_len, 1] * [dim // 2] -> [seq_len, dim // 2, 1]\n",
        "        rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1) # [seq_len, dim // 2, 2]\n",
        "        return x * rot_emb.flatten(-2).unsqueeze(0)\n",
        "\n",
        "class LearnedRoPE2D(nn.Module): # learnt RoPE ; each tok is 1 pos\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.weight_x, self.weight_y = nn.Parameter(torch.randn(1, dim//2)), nn.Parameter(torch.randn(1, dim//2))\n",
        "\n",
        "    def forward(self, img): #\n",
        "        batch, dim, h, w = img.shape\n",
        "        y, x = torch.meshgrid(torch.arange(h), torch.arange(w), indexing=\"ij\") # [h,w], y:row_num, x:col_num\n",
        "        angles = self.weight_x * x.reshape(-1,1) + self.weight_y * y.reshape(-1,1) # [1, dim//2] * [h*w, 1] = [h*w, dim//2]\n",
        "        angles = (angles * 2*torch.pi)[None,...,None] # [1,h*w, dim//2,1]\n",
        "        rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1) # [1,h*w, dim//2,2]\n",
        "        return img * rot_emb.flatten(-2).transpose(-2,-1).reshape(1,dim,h,w)\n",
        "\n",
        "\n",
        "\n",
        "class LearnedRoPE2D(nn.Module): # learnt RoPE ; each tok is 1 pos\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.weight_x, self.weight_y = nn.Parameter(torch.randn(1, dim//2)), nn.Parameter(torch.randn(1, dim//2))\n",
        "\n",
        "    def forward(self, img): #\n",
        "        batch, dim, h, w = img.shape\n",
        "        y, x = torch.meshgrid(torch.arange(h, device=device), torch.arange(w, device=device), indexing=\"ij\") # [h,w], y:row_num, x:col_num\n",
        "        angles = self.weight_x * x.reshape(-1,1) + self.weight_y * y.reshape(-1,1) # [1, dim//2] * [h*w, 1] = [h*w, dim//2]\n",
        "        angles = (angles * 2*torch.pi)[None,...,None] # [1,h*w, dim//2,1]\n",
        "        rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1) # [1,h*w, dim//2,2]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        return img * rot_emb.flatten(-2).transpose(-2,-1).reshape(1,dim,h,w).to(device)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# fixed, learnt, mixed learnt\n",
        "# disc tok pos vs pos in R\n",
        "# caartesian x y\n",
        "# polar r theta\n",
        "\n",
        "\n",
        "class RoPE2D(nn.Module): # Rotary Positional Embeddings\n",
        "    def __init__(self, dim, h=224, w=224, base=10000):\n",
        "        super().__init__()\n",
        "        # theta = 1. / (base ** (torch.arange(0, dim, step=4) / dim))\n",
        "        theta = 1. / (base**torch.linspace(0,1,dim//4)).unsqueeze(0)\n",
        "        # pos = torch.arange(seq_len).unsqueeze(1)\n",
        "        # angles = (pos * theta).unsqueeze(-1) # [seq_len, 1] * [dim // 2] -> [seq_len, dim // 2, 1]\n",
        "        # self.rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1) # [seq_len, dim // 2, 2]\n",
        "        self.dim, self.h, self.w = dim, h, w\n",
        "        y, x = torch.meshgrid(torch.arange(h), torch.arange(w), indexing=\"ij\") # print(y,x) # [h,w], y:row_num, x:col_num\n",
        "        # y, x = y.reshape(-1,1), x.reshape(-1,1) # [h*w,1]\n",
        "        # angles = (pos * theta).unsqueeze(-1) # [seq_len, 1] * [dim // 2] -> [seq_len, dim // 2, 1]\n",
        "        y, x = (y.reshape(-1,1) * theta).unsqueeze(-1), (x.reshape(-1,1) * theta).unsqueeze(-1) # [h*w,1]*[1,dim//4] = [h*w, dim//4, 1]\n",
        "        # self.rot_emb = torch.cat([x.sin(), x.cos(), y.sin(), y.cos()], dim=-1).flatten(-2) # [h*w, dim//4 ,4] -> [h*w, dim]\n",
        "        self.rot_emb = torch.cat([x.sin(), x.cos(), y.sin(), y.cos()], dim=-1).reshape(dim, h, w).to(device) # [h*w, dim//4 ,4] -> [h, w, dim]\n",
        "\n",
        "    def forward(self, img): #\n",
        "        batch, dim, h, w = img.shape\n",
        "        if self.h < h or self.w < w: self.__init__(self.dim, h, w)\n",
        "        rot_emb = self.rot_emb[:, :h, :w].unsqueeze(0) # [1, h, w, dim]\n",
        "        return img * rot_emb\n",
        "\n",
        "# def RoPE2D(dim, h=224, w=224, temp = 10000):\n",
        "#     y, x = torch.meshgrid(torch.arange(h), torch.arange(w), indexing=\"ij\") # print(y,x) # [h,w], y:row_num, x:col_num\n",
        "#     omega = 1. / (temp**torch.linspace(0,1,dim//4))\n",
        "#     # print(omega)\n",
        "#     y, x = y.reshape(-1,1) * omega.unsqueeze(0), x.reshape(-1,1) * omega.unsqueeze(0) # [h*w,1]*[1,dim//4] = [h*w,dim//4]\n",
        "#     # y, x = y.reshape(-1,1) * omega.unsqueeze(0), x.reshape(-1,1) * omega[None,...,None] # [h*w,1]*[1,dim//4] = [h*w,dim//4]\n",
        "#     # print(y.shape,x.shape) # [h,w], y:row num, x:col num\n",
        "#     pe = torch.cat((x.sin(), x.cos(), y.sin(), y.cos()), dim=1) # [h*w,dim]\n",
        "#     return pe\n",
        "\n",
        "\n",
        "def posemb_sincos_2d(h, w, dim, temp = 10000):\n",
        "    y, x = torch.meshgrid(torch.arange(h), torch.arange(w), indexing=\"ij\") # print(y,x) # [h,w], y:row_num, x:col_num\n",
        "    omega = 1. / (temp**torch.linspace(0,1,dim//4))\n",
        "    # print(omega)\n",
        "    y, x = y.reshape(-1,1) * omega.unsqueeze(0), x.reshape(-1,1) * omega.unsqueeze(0) # [h*w,1]*[1,dim//4] = [h*w,dim//4]\n",
        "    # print(y.shape,x.shape) # [h,w], y:row num, x:col num\n",
        "    pe = torch.cat((x.sin(), x.cos(), y.sin(), y.cos()), dim=1) # [h*w,dim]\n",
        "    return pe\n",
        "\n",
        "\n",
        "\n",
        "dim=16\n",
        "# seq_len=512\n",
        "# rope = RoPE(dim, seq_len, base=10000)\n",
        "# rope = LearnedRoPE2D(dim)\n",
        "rope = RoPE2D(dim)\n",
        "# x = torch.rand(4,64,dim)\n",
        "h,w = 3,5\n",
        "x = torch.rand((4,dim,h,w), device=device)\n",
        "out = rope(x)\n",
        "\n",
        "# [batch] -> []\n",
        "\n",
        "print(out.shape)\n",
        "\n",
        "# rot_emb = rope.rot_emb\n",
        "# print(rot_emb.shape)\n",
        "# print(rot_emb[:7])\n",
        "# # rot_emb = rot_emb.reshape(seq_len, dim // 2, 2)\n",
        "# # print(rot_emb)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "xTuZNWMXqNXP"
      },
      "outputs": [],
      "source": [
        "# @title test pos emb, polar\n",
        "import torch\n",
        "\n",
        "h,w = 3,5\n",
        "batch, dim = 2,8\n",
        "img = torch.rand(batch, dim, h, w)\n",
        "\n",
        "y, x = torch.meshgrid(torch.arange(h), torch.arange(w), indexing=\"ij\") # print(y,x) # [h,w], y:row_num, x:col_num\n",
        "# y, x = torch.meshgrid(torch.arange(h), torch.arange(w)) # print(y,x) # [h,w], y:row_num, x:col_num\n",
        "# print(y,x)\n",
        "\n",
        "# self.weights = nn.Parameter(torch.randn(1, dim//2))\n",
        "weight_x, weight_y = torch.arange(dim//2).unsqueeze(0), torch.arange(dim//2).unsqueeze(0)/10\n",
        "# weight_x, weight_y = torch.randn(1, dim//2), torch.randn(1, dim//2)\n",
        "# print(weight_x, weight_y)\n",
        "# out = weight_x * x + weight_y * y # [1, dim//2] * [h,w] = [h,w, dim//2]\n",
        "out = weight_x * x.reshape(-1,1) + weight_y * y.reshape(-1,1) # [1, dim//2] * [h*w] = [h*w, dim//2]\n",
        "# print(out)\n",
        "\n",
        "\n",
        "# y, x = torch.meshgrid(torch.arange(h), torch.arange(w), indexing=\"ij\") # print(y,x) # [h,w], y:row_num, x:col_num\n",
        "# (self.weight_x * x + self.weight_y * y) # [b,dim,h,w] * [1,h,w,]\n",
        "\n",
        "# angles = (self.weights * pos * 2*math.pi).unsqueeze(-1) # [seq_len, 1] * [dim // 2] -> [seq_len, dim // 2, 1]\n",
        "# angles = (weight_x * x + weight_y * y * 2*math.pi)[None,...,None] #.unsqueeze(-1) # [1,h,w,1] ; # [seq_len, 1] * [dim // 2] -> [seq_len, dim // 2, 1]\n",
        "angles = (out * 2*torch.pi)[None,...,None] #.unsqueeze(-1) # [1,h*w, dim//2,1] ; # [seq_len, 1] * [dim // 2] -> [seq_len, dim // 2, 1]\n",
        "rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1) # [1,h*w, dim//2,2]\n",
        "# return x * rot_emb.flatten(-2).unsqueeze(0)\n",
        "# return x * rot_emb.flatten(-2).transpose(-2,-1).reshape(1,dim,h,w)\n",
        "out = img * rot_emb.flatten(-2).transpose(-2,-1).reshape(1,dim,h,w)\n",
        "# print(out)\n",
        "\n",
        "\n",
        "pe = posemb_sincos_2d(h,w,dim)\n",
        "print(pe)\n",
        "\n",
        "\n",
        "# x = torch.randn(2,2, dtype=torch.cfloat)\n",
        "# x = torch.randn(3, 2)\n",
        "# y = torch.view_as_complex(x)\n",
        "# torch.view_as_real(y)\n",
        "# torch.polar(abs, angle)\n",
        "# torch.real(x)\n",
        "# torch.complex(x)\n",
        "# x.real\n",
        "# x.imag\n",
        "# x1=torch.tensor([3j, 4+4j])\n",
        "# x1.abs()\n",
        "# x1.angle()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title GlobalContext AttentionPool\n",
        "        # x = x.flatten(-2).mean(dim=-1) # mean pool\n",
        "        attn_weights = self.attention_pool(x).squeeze(-1) # [batch, (h,w)] # seq_pool\n",
        "        x = (attn_weights.softmax(dim = 1).unsqueeze(1) @ x).squeeze(1) # [batch, 1, (h,w)] @ [batch, (h,w), dim]\n",
        "        # cls_token = repeat(self.class_emb, '1 1 d -> b 1 d', b = b)\n",
        "        # x = torch.cat((cls_token, x), dim=1)\n",
        "        # x = x[:, 0] # first token\n",
        "\n",
        "# https://github.com/lucidrains/imagen-pytorch/blob/main/imagen_pytorch/imagen_pytorch.py#L969\n",
        "class GlobalContext(nn.Module):\n",
        "    \"\"\" basically a superior form of squeeze-excitation that is attention-esque \"\"\"\n",
        "    def __init__(self, *, dim_in, dim_out):\n",
        "        super().__init__()\n",
        "        self.to_k = nn.Conv2d(dim_in, 1, 1)\n",
        "        hidden_dim = max(3, dim_out // 2)\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(dim_in, hidden_dim, 1), nn.SiLU(),\n",
        "            nn.Conv2d(hidden_dim, dim_out, 1), nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        context = self.to_k(x)\n",
        "        x, context = map(lambda t: rearrange(t, 'b n ... -> b n (...)'), (x, context))\n",
        "        out = einsum('b i n, b c n -> b c i', context.softmax(dim = -1), x)\n",
        "        out = rearrange(out, '... -> ... 1')\n",
        "        return self.net(out)\n",
        "\n",
        "\n",
        "# https://github.com/lucidrains/enformer-pytorch/blob/main/enformer_pytorch/modeling_enformer.py#L159\n",
        "class AttentionPool(nn.Module):\n",
        "    def __init__(self, dim, pool_size = 2):\n",
        "        super().__init__()\n",
        "        self.pool_size = pool_size\n",
        "        self.pool_fn = Rearrange('b d (n p) -> b d n p', p = pool_size)\n",
        "        self.to_attn_logits = nn.Conv2d(dim, dim, 1, bias = False)\n",
        "        nn.init.dirac_(self.to_attn_logits.weight)\n",
        "        with torch.no_grad():\n",
        "            self.to_attn_logits.weight.mul_(2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, _, n = x.shape\n",
        "        remainder = n % self.pool_size\n",
        "        needs_padding = remainder > 0\n",
        "        if needs_padding:\n",
        "            x = F.pad(x, (0, remainder), value = 0)\n",
        "            mask = torch.zeros((b, 1, n), dtype = torch.bool, device = x.device)\n",
        "            mask = F.pad(mask, (0, remainder), value = True)\n",
        "        x = self.pool_fn(x)\n",
        "        logits = self.to_attn_logits(x)\n",
        "        if needs_padding:\n",
        "            mask_value = -torch.finfo(logits.dtype).max\n",
        "            logits = logits.masked_fill(self.pool_fn(mask), mask_value)\n",
        "        attn = logits.softmax(dim = -1)\n",
        "        return (x * attn).sum(dim = -1)\n"
      ],
      "metadata": {
        "id": "taw6x-Nyxai3",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QkeT7XoU5FiG",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title PixelShuffleConv\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def zero_module(module):\n",
        "    \"\"\"Zero out the parameters of a module and return it.\"\"\"\n",
        "    for p in module.parameters():\n",
        "        p.detach().zero_()\n",
        "    return module\n",
        "\n",
        "\n",
        "net = nn.Sequential(nn.Conv2d(3, 2*2**2, 3, 1, padding=3//2), nn.Conv2d(8, 1, 3, 1, padding=3//2))\n",
        "net1 = net\n",
        "\n",
        "\n",
        "\n",
        "# net = zero_module(net)\n",
        "# for n,p in net.named_parameters():\n",
        "#     print(n,p)\n",
        "\n",
        "# x = torch.rand(2,3,4,4)\n",
        "# out = net(x)\n",
        "# print(out.shape)\n",
        "\n",
        "# # loss = out.mean(\n",
        "# optim = torch.optim.SGD(net.parameters(), lr=1e-1)\n",
        "\n",
        "# y = torch.rand(2,1,4,4)\n",
        "# # loss = (out+y).sum() + (out+y)[0][0][0][0]\n",
        "# loss = F.mse_loss(y,out)\n",
        "\n",
        "# optim.zero_grad() # reset gradients of model parameters, to prevent double-counting\n",
        "# loss.backward() # Backpropagate gradients\n",
        "# optim.step()\n",
        "\n",
        "# for n,p in net.named_parameters():\n",
        "#     # print(n,p)\n",
        "#     print(n)\n",
        "\n",
        "net1 = init_conv_(net, up=2)\n",
        "\n",
        "for n,p in net1.named_parameters():\n",
        "    print(n,p)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def init_conv(conv, out_r=1, in_r=1):\n",
        "    o, i, h, w = conv.weight.shape\n",
        "    conv_weight = torch.empty(o//out_r**2, i//in_r**2, h, w)\n",
        "    nn.init.kaiming_uniform_(conv_weight)\n",
        "    conv.weight.data.copy_(conv_weight.repeat_interleave(out_r**2, dim=0).repeat_interleave(in_r**2, dim=1))\n",
        "    if conv.bias is not None: nn.init.zeros_(conv.bias)\n",
        "    return conv\n",
        "\n",
        "# Inline model initialization\n",
        "net = nn.Sequential(\n",
        "    # nn.PixelUnshuffle(2), nn.Conv2d(3 * 2**2, 4, 3, 1, padding=3//2),\n",
        "    # nn.Conv2d(4, 3 * 2**2, 3, 1, padding=3//2), nn.PixelShuffle(2)\n",
        "    # nn.PixelUnshuffle(2), init_conv(nn.Conv2d(3 * 2**2, 4, 3, 1, padding=3//2), down=2),\n",
        "    init_conv(nn.Conv2d(3, 3 * 2**2, 3, 1, padding=3//2), out_r=2), nn.PixelShuffle(2), # good\n",
        "    # nn.PixelShuffle(2), init_conv(nn.Conv2d(3* 2**2, 3, 3, 1, padding=3//2), in_r=2),\n",
        "\n",
        "    # nn.PixelUnshuffle(2), init_conv(nn.Conv2d(3 * 2**2, 2* 2**2, 3, 1, padding=3//2), up=2, down=2), nn.PixelUnshuffle(2),\n",
        "    # nn.PixelShuffle(2), init_conv(nn.Conv2d(2* 2**2, 3 * 2**2, 3, 1, padding=3//2), up=2, down=2), nn.PixelShuffle(2),\n",
        "\n",
        "    nn.PixelUnshuffle(2), init_conv(nn.Conv2d(3 * 2**2, 3, 3, 1, padding=3//2), in_r=2), # good\n",
        "    # init_conv(nn.Conv2d(3, 3, 3, 1, padding=3//2))\n",
        ")\n",
        "# self.net = nn.Sequential(nn.Conv2d(in_ch, out_ch * r**2, kernel_size, 1, padding=kernel_size//2), nn.PixelShuffle(r)) # PixelShuffle: [b,c*r^2,h,w] -> [b,c,h*r,w*r] # upscale by upscale factor r # https://arxiv.org/pdf/1609.05158v2\n",
        "# self.net = nn.Sequential(nn.PixelUnshuffle(r), nn.Conv2d(in_ch * r**2, out_ch, kernel_size, 1, padding=kernel_size//2)) # PixelUnshuffle: [b,c,h*r,w*r] -> [b,c*r^2,h,w]\n",
        "\n",
        "\n",
        "# for n, p in net.named_parameters():\n",
        "#     print(n, p)\n",
        "\n",
        "\n",
        "dataiter = iter(train_loader) # get some random training images\n",
        "images, labels = next(dataiter)\n",
        "out = net(images[0])\n",
        "# out = net(images[0].repeat_interleave(4, dim=0))\n",
        "\n",
        "print(out.shape)\n",
        "imshow(out.detach().cpu())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 486
        },
        "id": "QHhXxS4K2Xye",
        "outputId": "49bbc237-4cba-4dca-b23b-9a6b1bfb2105"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-13.823969..10.89641].\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 32, 32])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# x = torch.rand(1,2,4,4)\n",
        "# print(x)\n",
        "# x = nn.PixelUnshuffle(2)(x)\n",
        "# print(x) # [1,8,2,2]\n",
        "# print(x.shape) # [1,2,4,4]\n",
        "x = torch.rand(1,2,2,2).repeat(1,4,1,1)\n",
        "print(x)\n",
        "x = torch.rand(1,2,2,2).repeat_interleave(4,dim=1)\n",
        "print(x)\n",
        "\n",
        "# x = torch.rand(1,4,2,2)\n",
        "# print(x)\n",
        "# x = nn.PixelShuffle(2)(x)\n",
        "# print(x)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tW4kWJ71Aon0",
        "outputId": "2e8d3273-0a8b-4f0b-97e0-636383f0bbda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[[0.7029, 0.8369],\n",
            "          [0.8177, 0.0125]],\n",
            "\n",
            "         [[0.4732, 0.9710],\n",
            "          [0.8584, 0.2046]],\n",
            "\n",
            "         [[0.7029, 0.8369],\n",
            "          [0.8177, 0.0125]],\n",
            "\n",
            "         [[0.4732, 0.9710],\n",
            "          [0.8584, 0.2046]],\n",
            "\n",
            "         [[0.7029, 0.8369],\n",
            "          [0.8177, 0.0125]],\n",
            "\n",
            "         [[0.4732, 0.9710],\n",
            "          [0.8584, 0.2046]],\n",
            "\n",
            "         [[0.7029, 0.8369],\n",
            "          [0.8177, 0.0125]],\n",
            "\n",
            "         [[0.4732, 0.9710],\n",
            "          [0.8584, 0.2046]]]])\n",
            "tensor([[[[0.4048, 0.3870],\n",
            "          [0.2477, 0.6194]],\n",
            "\n",
            "         [[0.4048, 0.3870],\n",
            "          [0.2477, 0.6194]],\n",
            "\n",
            "         [[0.4048, 0.3870],\n",
            "          [0.2477, 0.6194]],\n",
            "\n",
            "         [[0.4048, 0.3870],\n",
            "          [0.2477, 0.6194]],\n",
            "\n",
            "         [[0.5305, 0.1096],\n",
            "          [0.8902, 0.9634]],\n",
            "\n",
            "         [[0.5305, 0.1096],\n",
            "          [0.8902, 0.9634]],\n",
            "\n",
            "         [[0.5305, 0.1096],\n",
            "          [0.8902, 0.9634]],\n",
            "\n",
            "         [[0.5305, 0.1096],\n",
            "          [0.8902, 0.9634]]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "id": "GxePWzy1aTWt",
        "outputId": "bd531abf-8e47-4cc6-86a8-b9381f3eea29",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'AttentionBlock' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-8eb3b9189f95>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0mnum_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;31m# model = SimpleViT(image_size=32, patch_size=4, num_classes=num_classes, dim=dim, depth=1, heads=heads, mlp_dim=dim*4, channels = 3, dim_head = dim_head)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSimpleViT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlp_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim_head\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdim_head\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 59850\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0moptim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdamW\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-33-8eb3b9189f95>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, in_dim, out_dim, dim, depth, heads, mlp_dim, channels, dim_head)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m# self.dropout = nn.Dropout(p=0.1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAttentionBlock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_head\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdim_head\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0;31m# self.norm = nn.LayerNorm(dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'AttentionBlock' is not defined"
          ]
        }
      ],
      "source": [
        "# @title ViT me\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class SimpleViT(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim, dim, depth, heads, mlp_dim, channels=3, dim_head=8):\n",
        "        super().__init__()\n",
        "\n",
        "        self.to_patch_embedding = nn.Sequential( # in, out, kernel, stride, pad\n",
        "            # nn.Conv2d(in_dim, dim, kernel_size=7, stride=2, padding=7//2, bias=False), nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "            # PixelShuffleConv(3, dim, 1, r=1/4),\n",
        "            nn.PixelUnshuffle(2), nn.Conv2d(in_dim * 2**2, dim//2**2, 7, 1, padding=7//2), nn.PixelUnshuffle(2),\n",
        "            )\n",
        "\n",
        "\n",
        "        # dim = h/2^2 * w/2^2\n",
        "        # self.pos_embedding = LearnedRoPE2D(dim)\n",
        "        # self.pos_embedding = RoPE2D(dim)\n",
        "        self.positional_emb = nn.Parameter(torch.zeros(1, 8*8, dim), requires_grad=True) # positional_embedding == 'learnable'\n",
        "        # nn.init.trunc_normal_(self.positional_emb, std=0.2)\n",
        "\n",
        "        # self.dropout = nn.Dropout(p=0.1)\n",
        "        self.transformer = AttentionBlock(d_model=dim, d_head=dim_head)\n",
        "        # self.norm = nn.LayerNorm(dim)\n",
        "\n",
        "        self.attention_pool = nn.Linear(dim, 1)\n",
        "        self.out = nn.Linear(dim, out_dim, bias=False)\n",
        "\n",
        "    def forward(self, img):\n",
        "        device = img.device\n",
        "        x = self.to_patch_embedding(img)\n",
        "        # x = self.pos_embedding(x)\n",
        "        x = x.flatten(-2).transpose(-2,-1) # b c h w -> b (h w) c\n",
        "        x = x + self.positional_emb\n",
        "        # x = self.dropout(x)\n",
        "\n",
        "        x = self.transformer(x)\n",
        "        # x = self.norm(x)\n",
        "\n",
        "        # x = x.flatten(-2).mean(dim=-1) # meal pool\n",
        "        attn_weights = self.attention_pool(x).squeeze(-1) # [batch, (h,w)] # seq_pool\n",
        "        x = (attn_weights.softmax(dim = 1).unsqueeze(1) @ x).squeeze(1) # [batch, 1, (h,w)] @ [batch, (h,w), dim]\n",
        "\n",
        "        # cls_token = repeat(self.class_emb, '1 1 d -> b 1 d', b = b)\n",
        "        # x = torch.cat((cls_token, x), dim=1)\n",
        "        # x = x[:, 0] # first token\n",
        "        return self.out(x)\n",
        "\n",
        "\n",
        "# pos_emb rope < learn < learned\n",
        "# conv > pixel?\n",
        "# droppath not required\n",
        "\n",
        "dim = 64\n",
        "dim_head = 8\n",
        "heads = dim // dim_head\n",
        "num_classes = 10\n",
        "# model = SimpleViT(image_size=32, patch_size=4, num_classes=num_classes, dim=dim, depth=1, heads=heads, mlp_dim=dim*4, channels = 3, dim_head = dim_head)\n",
        "model = SimpleViT(in_dim=3, out_dim=num_classes, dim=dim, depth=1, heads=heads, mlp_dim=dim*4, channels = 3, dim_head = dim_head).to(device)\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 59850\n",
        "optim = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "\n",
        "# print(images.shape) # [batch, 3, 32, 32]\n",
        "x = torch.rand(64, 3, 32, 32, device=device)\n",
        "# x = torch.rand(64, 3, 28,28, device=device)\n",
        "logits = model(x)\n",
        "print(logits.shape)\n",
        "# print(logits[0])\n",
        "# print(logits[0].argmax(1))\n",
        "pred_probab = nn.Softmax(dim=1)(logits)\n",
        "y_pred = pred_probab.argmax(1)\n",
        "# print(f\"Predicted class: {y_pred}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "H13D4xpGzHI3"
      },
      "outputs": [],
      "source": [
        "# @title lucidrains simple_vit.py\n",
        "# https://github.com/lucidrains/vit-pytorch/blob/main/vit_pytorch/simple_vit.py\n",
        "import torch\n",
        "from torch import nn\n",
        "from einops import rearrange\n",
        "from einops.layers.torch import Rearrange\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def posemb_sincos_2d(h, w, dim, temperature: int = 10000, dtype = torch.float32):\n",
        "    y, x = torch.meshgrid(torch.arange(h), torch.arange(w), indexing=\"ij\")\n",
        "    assert (dim % 4) == 0, \"feature dimension must be multiple of 4 for sincos emb\"\n",
        "    omega = torch.arange(dim//4) / (dim//4 - 1)\n",
        "    omega = 1.0 / (temperature ** omega)\n",
        "    y = y.flatten()[:, None] * omega[None, :]\n",
        "    x = x.flatten()[:, None] * omega[None, :]\n",
        "    pe = torch.cat((x.sin(), x.cos(), y.sin(), y.cos()), dim=1)\n",
        "    return pe.type(dtype)\n",
        "# pos_embedding = posemb_sincos_2d(h = image_height // patch_height, w = image_width // patch_width, dim = dim)\n",
        "# x = x+pos_embedding\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.LayerNorm(dim), nn.Linear(dim, hidden_dim), nn.GELU(),\n",
        "            nn.Linear(hidden_dim, dim),\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, heads = 8, dim_head = 64):\n",
        "        super().__init__()\n",
        "        inner_dim = dim_head *  heads\n",
        "        self.heads = heads\n",
        "        self.scale = dim_head ** -0.5\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.attend = nn.Softmax(dim = -1)\n",
        "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
        "        self.to_out = nn.Linear(inner_dim, dim, bias = False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.norm(x)\n",
        "        qkv = self.to_qkv(x).chunk(3, dim = -1)\n",
        "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), qkv)\n",
        "        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
        "        attn = self.attend(dots)\n",
        "        out = torch.matmul(attn, v)\n",
        "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
        "        return self.to_out(out)\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, dim, depth, heads, dim_head, mlp_dim):\n",
        "        super().__init__()\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.layers = nn.ModuleList([])\n",
        "        for _ in range(depth):\n",
        "            self.layers.append(nn.ModuleList([\n",
        "                Attention(dim, heads = heads, dim_head = dim_head),\n",
        "                FeedForward(dim, mlp_dim)\n",
        "            ]))\n",
        "    def forward(self, x):\n",
        "        for attn, ff in self.layers:\n",
        "            x = attn(x) + x\n",
        "            x = ff(x) + x\n",
        "        return self.norm(x)\n",
        "\n",
        "\n",
        "def pair(t): return t if isinstance(t, tuple) else (t, t)\n",
        "class SimpleViT(nn.Module):\n",
        "    def __init__(self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, channels = 3, dim_head = 64):\n",
        "        super().__init__()\n",
        "        image_height, image_width = pair(image_size)\n",
        "        patch_height, patch_width = pair(patch_size)\n",
        "        assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size.'\n",
        "        patch_dim = channels * patch_height * patch_width\n",
        "        self.to_patch_embedding = nn.Sequential(\n",
        "            Rearrange(\"b c (h p1) (w p2) -> b (h w) (p1 p2 c)\", p1 = patch_height, p2 = patch_width),\n",
        "            nn.LayerNorm(patch_dim), nn.Linear(patch_dim, dim),\n",
        "            nn.LayerNorm(dim),\n",
        "        )\n",
        "        self.pos_embedding = posemb_sincos_2d(h = image_height // patch_height, w = image_width // patch_width, dim = dim)\n",
        "        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim)\n",
        "        self.to_latent = nn.Identity()\n",
        "        self.linear_head = nn.Linear(dim, num_classes)\n",
        "\n",
        "    def forward(self, img):\n",
        "        device = img.device\n",
        "        x = self.to_patch_embedding(img)\n",
        "        x += self.pos_embedding.to(device, dtype=x.dtype)\n",
        "        x = self.transformer(x)\n",
        "        x = x.mean(dim = 1)\n",
        "        x = self.to_latent(x)\n",
        "        return self.linear_head(x)\n",
        "\n",
        "\n",
        "# dim = 64\n",
        "# dim_head = 8\n",
        "# heads = dim // dim_head\n",
        "# num_classes = 10\n",
        "# # model = SimpleViT(image_size=32, patch_size=4, num_classes=num_classes, dim=dim, depth=1, heads=heads, mlp_dim=dim*4, channels = 3, dim_head = dim_head)\n",
        "# model = SimpleViT(num_classes=num_classes, dim=dim, depth=1, heads=heads, mlp_dim=dim*4, channels = 3, dim_head = dim_head)\n",
        "# optim = torch.optim.AdamW(model.parameters(), lr=1e-3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vtKPXZWBAEjn",
        "outputId": "ced7d1f1-5bd9-4210-af16-e061e3ec3bc3",
        "cellView": "form"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "39371\n",
            "torch.Size([64, 10])\n"
          ]
        }
      ],
      "source": [
        "# @title lucid CCT\n",
        "# !pip install -q vit-pytorch\n",
        "\n",
        "import torch\n",
        "from vit_pytorch.cct import CCT\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "model = CCT(\n",
        "    img_size = (32,32),\n",
        "    embedding_dim = 64,\n",
        "    n_conv_layers = 1,\n",
        "    kernel_size = 7,\n",
        "    stride = 2,\n",
        "    padding = 3,\n",
        "    pooling_kernel_size = 3,\n",
        "    pooling_stride = 2,\n",
        "    # pooling_padding = 1,\n",
        "    num_layers = 1,\n",
        "    num_heads = 8,\n",
        "    mlp_ratio = 1.,\n",
        "    num_classes = 10,\n",
        "    positional_embedding = 'learnable', # ['sine', 'learnable', 'none']\n",
        "    dim_head = 8,\n",
        "    # pool='mean', # ['mean', 'cls', 'max']\n",
        ")\n",
        "\n",
        "\n",
        "# def __init__(self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, pool = 'cls', channels = 3, dim_head = 64, dropout = 0., emb_dropout = 0.):\n",
        "\n",
        "optim = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 236235\n",
        "\n",
        "\n",
        "x = torch.rand(64, 3, 32, 32, device=device)\n",
        "# x = torch.rand(64, 1, 28,28, device=device)\n",
        "logits = model(x)\n",
        "print(logits.shape)\n",
        "pred_probab = nn.Softmax(dim=1)(logits)\n",
        "y_pred = pred_probab.argmax(1)\n",
        "\n",
        "# loss: 1.057389  [49920/50000]\n",
        "# Test Error:\n",
        "#  Accuracy: 58.5%, Avg loss: 1.146490\n",
        "# 8: loss: 0.801141  [49920/50000]\n",
        "# Test Error:\n",
        "#  Accuracy: 69.9%, Avg loss: 0.885053\n",
        "\n",
        "# print(model)\n",
        "\n",
        "# # cct 39371 learn\n",
        "# loss: 1.426435  [49920/50000]\n",
        "# Test Error:\n",
        "#  Accuracy: 50.0%, Avg loss: 1.355331\n",
        "# 5: loss: 0.954245  [49920/50000]\n",
        "# Test Error:\n",
        "#  Accuracy: 64.1%, Avg loss: 1.000242"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "DAOLtmMKZMQR"
      },
      "outputs": [],
      "source": [
        "# @title sincos_pos_embed\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/models/vision_transformer.py\n",
        "import numpy as np\n",
        "\n",
        "def get_2d_sincos_pos_embed_from_grid(embed_dim, grid):\n",
        "    assert embed_dim % 2 == 0\n",
        "    # use half of dimensions to encode grid_h\n",
        "    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[0])  # (H*W, D/2)\n",
        "    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[1])  # (H*W, D/2)\n",
        "    emb = np.concatenate([emb_h, emb_w], axis=1)  # (H*W, D)\n",
        "    return emb\n",
        "\n",
        "def get_2d_sincos_pos_embed(embed_dim, grid_size, cls_token=False): # grid_size: int of the grid height and width\n",
        "    grid_h, grid_w = np.arange(grid_size, dtype=float), np.arange(grid_size, dtype=float)\n",
        "    grid = np.meshgrid(grid_w, grid_h)  # here w goes first\n",
        "    grid = np.stack(grid, axis=0)\n",
        "    grid = grid.reshape([2, 1, grid_size, grid_size])\n",
        "    pos_embed = get_2d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
        "    if cls_token:\n",
        "        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0) # prepend zeros for cls token later?\n",
        "    return pos_embed # [grid_size*grid_size, embed_dim] or [1+grid_size*grid_size, embed_dim] (w/ or w/o cls_token)\n",
        "\n",
        "\n",
        "def get_1d_sincos_pos_embed_from_grid(embed_dim, pos):\n",
        "    \"\"\"\n",
        "    embed_dim: output dimension for each position\n",
        "    pos: a list of positions to be encoded: size (M,)\n",
        "    out: (M, D)\n",
        "    \"\"\"\n",
        "    assert embed_dim % 2 == 0\n",
        "    omega = np.arange(embed_dim // 2, dtype=float)\n",
        "    omega /= embed_dim / 2.\n",
        "    omega = 1. / 10000**omega   # (D/2,)\n",
        "    pos = pos.reshape(-1)   # (M,)\n",
        "    out = np.einsum('m,d->md', pos, omega)   # (M, D/2), outer product\n",
        "    emb_sin = np.sin(out)  # (M, D/2)\n",
        "    emb_cos = np.cos(out)  # (M, D/2)\n",
        "    emb = np.concatenate([emb_sin, emb_cos], axis=1)  # (M, D)\n",
        "    return emb\n",
        "\n",
        "import math\n",
        "def interpolate_pos_encoding(x, pos_embed):\n",
        "    npatch = x.shape[1] - 1\n",
        "    N = pos_embed.shape[1] - 1\n",
        "    if npatch == N: return pos_embed\n",
        "    class_emb, pos_embed = pos_embed[:, 0], pos_embed[:, 1:]\n",
        "    dim = x.shape[-1]\n",
        "    pos_embed = nn.functional.interpolate(pos_embed.reshape(1, int(math.sqrt(N)), int(math.sqrt(N)), dim).permute(0, 3, 1, 2), scale_factor=math.sqrt(npatch / N), mode='bicubic',)\n",
        "    pos_embed = pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n",
        "    return torch.cat((class_emb.unsqueeze(0), pos_embed), dim=1)\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "class PatchEmbed(nn.Module):\n",
        "    \"\"\" Image to Patch Embedding\"\"\"\n",
        "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n",
        "        super().__init__()\n",
        "        self.num_patches = (img_size // patch_size) * (img_size // patch_size)\n",
        "        self.img_size, self.patch_size = img_size, patch_size\n",
        "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "    def forward(self, x): # [b,c,h,w]\n",
        "        # B, C, H, W = x.shape\n",
        "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
        "        return x\n",
        "\n",
        "num_patches = 16\n",
        "embed_dim = 8\n",
        "# pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim), requires_grad=False)\n",
        "pos_embed = torch.zeros(1, num_patches, embed_dim)\n",
        "# patch_embed = PatchEmbed(img_size=img_size[0], patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n",
        "patch_embed = PatchEmbed()\n",
        "# print(pos_embed)\n",
        "# patch_embed.num_patches = (224/16)^2 = 14^2 = 196\n",
        "# pos_embed = get_2d_sincos_pos_embed(pos_embed.shape[-1], int(patch_embed.num_patches**.5), cls_token=False)\n",
        "pos_embed = get_2d_sincos_pos_embed(embed_dim, 14)\n",
        "print(pos_embed)\n",
        "print(pos_embed.shape) # 14^2, embed_dim\n",
        "\n",
        "# x = torch.rand(4,224*224,embed_dim)\n",
        "# pos_embed.data.copy_(torch.from_numpy(pos_embed).float().unsqueeze(0))\n",
        "\n",
        "# pos_embed = interpolate_pos_encoding(x, pos_embed)\n",
        "# print(pos_embed)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.imshow(pos_embed)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "3CA-Eu6eGzVQ"
      },
      "outputs": [],
      "source": [
        "# @title posemb_sincos_2d\n",
        "def posemb_sincos_2d(h, w, dim, temperature: int = 10000, dtype = torch.float32):\n",
        "    print(h,w)\n",
        "    y, x = torch.meshgrid(torch.arange(h), torch.arange(w), indexing=\"ij\")\n",
        "    # print(y,x) # [h,w], y:row_num, x:col_num\n",
        "    # assert (dim % 4) == 0, \"feature dimension must be multiple of 4 for sincos emb\"\n",
        "    omega = 1. / (temperature**torch.linspace(0,1,dim//4))\n",
        "    # print(omega)\n",
        "    y = y.reshape(-1,1) * omega.unsqueeze(0) # [h*w,1]*[1,dim//4] = [h*w,dim//4]\n",
        "    x = x.reshape(-1,1) * omega.unsqueeze(0)\n",
        "    print(y.shape,x.shape) # [h,w], y:row num, x:col num\n",
        "    pe = torch.cat((x.sin(), x.cos(), y.sin(), y.cos()), dim=1) # [h*w,dim]\n",
        "    return pe.type(dtype)\n",
        "\n",
        "# image_height, image_width = 64, 64\n",
        "# patch_height, patch_width = 8, 8\n",
        "h=3\n",
        "w=5\n",
        "dim=18\n",
        "# pos_embedding = posemb_sincos_2d(h = image_height // patch_height, w = image_width // patch_width, dim = dim)\n",
        "pos_embedding = posemb_sincos_2d(h=h, w=w, dim=dim)\n",
        "# x = x+pos_embedding\n",
        "# print(pos_embedding)\n",
        "# print(pos_embedding.shape) # [h*w, dim]\n",
        "# for i,x in enumerate(pos_embedding):\n",
        "#     if i%h==0: print('### ', i, ' ###')\n",
        "#     print(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "XwpnHW4wn9S1"
      },
      "outputs": [],
      "source": [
        "# @title data\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "train_data = torchvision.datasets.MNIST(root=\"data\", train=True, download=True,transform=transforms.ToTensor(),) # do not normalise! want img in [0,1)\n",
        "batch_size = 512 # 64 512\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "test_data = torchvision.datasets.MNIST(root=\"data\", train=False, download=True, transform=transforms.ToTensor(),) #opt no download\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# dataiter = iter(train_data)\n",
        "# x,y = next(dataiter)\n",
        "# print(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xgGz_N2a_OZ6"
      },
      "outputs": [],
      "source": [
        "# @title lucid CCT\n",
        "# https://github.com/lucidrains/vit-pytorch/blob/main/vit_pytorch/cct.py\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "2p0-65KlpqyB"
      },
      "outputs": [],
      "source": [
        "# @title lucidrains cct.py\n",
        "# https://github.com/lucidrains/vit-pytorch/blob/main/vit_pytorch/cct.py\n",
        "\n",
        "\n",
        "\n",
        "def sinusoidal_embedding(n_channels, dim):\n",
        "    pe = torch.FloatTensor([[p / (10000 ** (2 * (i // 2) / dim)) for i in range(dim)]\n",
        "                            for p in range(n_channels)])\n",
        "    pe[:, 0::2] = torch.sin(pe[:, 0::2])\n",
        "    pe[:, 1::2] = torch.cos(pe[:, 1::2])\n",
        "    return rearrange(pe, '... -> 1 ...')\n",
        "\n",
        "# modules\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, num_heads=8, attention_dropout=0.1, projection_dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.heads = num_heads\n",
        "        head_dim = dim // self.heads\n",
        "        self.scale = head_dim ** -0.5\n",
        "        self.qkv = nn.Linear(dim, dim * 3, bias=False)\n",
        "        self.attn_drop = nn.Dropout(attention_dropout)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(projection_dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "        qkv = self.qkv(x).chunk(3, dim = -1)\n",
        "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), qkv)\n",
        "        q = q * self.scale\n",
        "        attn = einsum('b h i d, b h j d -> b h i j', q, k)\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "        x = einsum('b h i j, b h j d -> b h i d', attn, v)\n",
        "        x = rearrange(x, 'b h n d -> b n (h d)')\n",
        "        return self.proj_drop(self.proj(x))\n",
        "\n",
        "\n",
        "class TransformerEncoderLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Inspired by torch.nn.TransformerEncoderLayer and\n",
        "    rwightman's timm package.\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1,attention_dropout=0.1, drop_path_rate=0.1):\n",
        "        super().__init__()\n",
        "        self.pre_norm = nn.LayerNorm(d_model)\n",
        "        self.self_attn = Attention(dim=d_model, num_heads=nhead, attention_dropout=attention_dropout, projection_dropout=dropout)\n",
        "        self.linear1  = nn.Linear(d_model, dim_feedforward)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.norm1    = nn.LayerNorm(d_model)\n",
        "        self.linear2  = nn.Linear(dim_feedforward, d_model)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.drop_path = DropPath(drop_path_rate)\n",
        "        self.activation = F.gelu\n",
        "\n",
        "    def forward(self, src, *args, **kwargs):\n",
        "        src = src + self.drop_path(self.self_attn(self.pre_norm(src)))\n",
        "        src = self.norm1(src)\n",
        "        src2 = self.linear2(self.dropout1(self.activation(self.linear1(src))))\n",
        "        src = src + self.drop_path(self.dropout2(src2))\n",
        "        return src\n",
        "\n",
        "class DropPath(nn.Module):\n",
        "    def __init__(self, drop_prob=None):\n",
        "        super().__init__()\n",
        "        self.drop_prob = float(drop_prob)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch, drop_prob, device, dtype = x.shape[0], self.drop_prob, x.device, x.dtype\n",
        "\n",
        "        if drop_prob <= 0. or not self.training:\n",
        "            return x\n",
        "\n",
        "        keep_prob = 1 - self.drop_prob\n",
        "        shape = (batch, *((1,) * (x.ndim - 1)))\n",
        "\n",
        "        keep_mask = torch.zeros(shape, device = device).float().uniform_(0, 1) < keep_prob\n",
        "        output = x.div(keep_prob) * keep_mask.float()\n",
        "        return output\n",
        "\n",
        "\n",
        "class Tokenizer(nn.Module):\n",
        "    def __init__(self, kernel_size, stride, padding, pooling_kernel_size=3, pooling_stride=2, pooling_padding=1, n_conv_layers=1,\n",
        "                 n_input_channels=3, n_output_channels=64, in_planes=64,\n",
        "                 activation=None, max_pool=True, conv_bias=False):\n",
        "        super().__init__()\n",
        "        n_filter_list = [n_input_channels] + [in_planes for _ in range(n_conv_layers - 1)] + [n_output_channels]\n",
        "        n_filter_list_pairs = zip(n_filter_list[:-1], n_filter_list[1:])\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            *[nn.Sequential(\n",
        "                nn.Conv2d(chan_in, chan_out, kernel_size=(kernel_size, kernel_size), stride=(stride, stride), padding=(padding, padding), bias=conv_bias),\n",
        "                nn.Identity() if not exists(activation) else activation(),\n",
        "                nn.MaxPool2d(kernel_size=pooling_kernel_size, stride=pooling_stride, padding=pooling_padding) if max_pool else nn.Identity())\n",
        "                for chan_in, chan_out in n_filter_list_pairs\n",
        "            ])\n",
        "        self.apply(self.init_weight)\n",
        "\n",
        "    def sequence_length(self, n_channels=3, height=224, width=224):\n",
        "        return self.forward(torch.zeros((1, n_channels, height, width))).shape[1]\n",
        "\n",
        "    def forward(self, x):\n",
        "        return rearrange(self.conv_layers(x), 'b c h w -> b (h w) c')\n",
        "\n",
        "    @staticmethod\n",
        "    def init_weight(m):\n",
        "        if isinstance(m, nn.Conv2d):\n",
        "            nn.init.kaiming_normal_(m.weight)\n",
        "\n",
        "class Tokenizer(nn.Module):\n",
        "    def __init__(self, n_conv_layers=1, n_input_channels=3, n_output_channels=64, in_planes=64,):\n",
        "        super().__init__()\n",
        "        n_filter_list = [n_input_channels] + [in_planes for _ in range(n_conv_layers - 1)] + [n_output_channels]\n",
        "        # print(n_filter_list)\n",
        "        n_filter_list_pairs = zip(n_filter_list[:-1], n_filter_list[1:])\n",
        "        # print([x for x in n_filter_list_pairs])\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            *[nn.Sequential(\n",
        "                nn.Conv2d(chan_in, chan_out, kernel_size=7, stride=2, padding=7//2, bias=False),\n",
        "                # nn.Identity() if not exists(activation) else activation(), # None\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "                ) for chan_in, chan_out in n_filter_list_pairs\n",
        "            ])\n",
        "\n",
        "    # def sequence_length(self, n_channels=3, height=224, width=224):\n",
        "    #     return self.forward(torch.zeros((1, n_channels, height, width))).shape[1]\n",
        "\n",
        "    def forward(self, x):\n",
        "        # return rearrange(self.conv_layers(x), 'b c h w -> b (h w) c')\n",
        "        return self.conv_layers(x)\n",
        "\n",
        "class TransformerClassifier(nn.Module):\n",
        "    def __init__(self,\n",
        "                 seq_pool=True,\n",
        "                 embedding_dim=768,\n",
        "                 num_layers=12,\n",
        "                 num_heads=12,\n",
        "                 mlp_ratio=4.0,\n",
        "                 num_classes=1000,\n",
        "                 dropout_rate=0.1,\n",
        "                 attention_dropout=0.1,\n",
        "                 stochastic_depth_rate=0.1,\n",
        "                 positional_embedding='sine',\n",
        "                 sequence_length=None,\n",
        "                 *args, **kwargs):\n",
        "        super().__init__()\n",
        "        assert positional_embedding in {'sine', 'learnable', 'none'}\n",
        "\n",
        "        dim_feedforward = int(embedding_dim * mlp_ratio)\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.sequence_length = sequence_length\n",
        "        self.seq_pool = seq_pool\n",
        "\n",
        "        assert exists(sequence_length) or positional_embedding == 'none', \\\n",
        "            f\"Positional embedding is set to {positional_embedding} and\" \\\n",
        "            f\" the sequence length was not specified.\"\n",
        "        if not seq_pool:\n",
        "            sequence_length += 1\n",
        "            self.class_emb = nn.Parameter(torch.zeros(1, 1, self.embedding_dim), requires_grad=True)\n",
        "        else:\n",
        "            self.attention_pool = nn.Linear(self.embedding_dim, 1)\n",
        "        if positional_embedding == 'none':\n",
        "            self.positional_emb = None\n",
        "        elif positional_embedding == 'learnable':\n",
        "            self.positional_emb = nn.Parameter(torch.zeros(1, sequence_length, embedding_dim),\n",
        "                                               requires_grad=True)\n",
        "            nn.init.trunc_normal_(self.positional_emb, std=0.2)\n",
        "        else:\n",
        "            self.positional_emb = nn.Parameter(sinusoidal_embedding(sequence_length, embedding_dim),\n",
        "                                               requires_grad=False)\n",
        "        self.dropout = nn.Dropout(p=dropout_rate)\n",
        "        dpr = [x.item() for x in torch.linspace(0, stochastic_depth_rate, num_layers)]\n",
        "        self.blocks = nn.ModuleList([\n",
        "            TransformerEncoderLayer(d_model=embedding_dim, nhead=num_heads,\n",
        "                                    dim_feedforward=dim_feedforward, dropout=dropout_rate,\n",
        "                                    attention_dropout=attention_dropout, drop_path_rate=layer_dpr)\n",
        "            for layer_dpr in dpr])\n",
        "        self.norm = nn.LayerNorm(embedding_dim)\n",
        "        self.fc = nn.Linear(embedding_dim, num_classes)\n",
        "        self.apply(self.init_weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        b = x.shape[0]\n",
        "        if not exists(self.positional_emb) and x.size(1) < self.sequence_length:\n",
        "            x = F.pad(x, (0, 0, 0, self.n_channels - x.size(1)), mode='constant', value=0)\n",
        "        if not self.seq_pool:\n",
        "            cls_token = repeat(self.class_emb, '1 1 d -> b 1 d', b = b)\n",
        "            x = torch.cat((cls_token, x), dim=1)\n",
        "        if exists(self.positional_emb):\n",
        "            x += self.positional_emb\n",
        "        x = self.dropout(x)\n",
        "        for blk in self.blocks:\n",
        "            x = blk(x)\n",
        "        x = self.norm(x)\n",
        "        if self.seq_pool:\n",
        "            attn_weights = rearrange(self.attention_pool(x), 'b n 1 -> b n')\n",
        "            x = einsum('b n, b n d -> b d', attn_weights.softmax(dim = 1), x)\n",
        "        else:\n",
        "            x = x[:, 0]\n",
        "        return self.fc(x)\n",
        "\n",
        "    @staticmethod\n",
        "    def init_weight(m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            nn.init.trunc_normal_(m.weight, std=.02)\n",
        "            if isinstance(m, nn.Linear) and exists(m.bias):\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ILIdfDgmycM9"
      },
      "outputs": [],
      "source": [
        "# @title lucidrains vit.py\n",
        "# https://github.com/lucidrains/vit-pytorch/blob/main/vit_pytorch/vit.py\n",
        "import torch\n",
        "from torch import nn\n",
        "from einops import rearrange, repeat\n",
        "from einops.layers.torch import Rearrange\n",
        "\n",
        "# helpers\n",
        "def pair(t):\n",
        "    return t if isinstance(t, tuple) else (t, t)\n",
        "\n",
        "# classes\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, dim, hidden_dim, dropout = 0.):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.LayerNorm(dim), nn.Linear(dim, hidden_dim), nn.GELU(),\n",
        "            nn.Dropout(dropout), nn.Linear(hidden_dim, dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):\n",
        "        super().__init__()\n",
        "        inner_dim = dim_head *  heads\n",
        "        project_out = not (heads == 1 and dim_head == dim)\n",
        "        self.heads = heads\n",
        "        self.scale = dim_head ** -0.5\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.attend = nn.Softmax(dim = -1)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
        "        self.to_out = nn.Sequential(nn.Linear(inner_dim, dim), nn.Dropout(dropout)) if project_out else nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.norm(x)\n",
        "        qkv = self.to_qkv(x).chunk(3, dim = -1)\n",
        "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), qkv)\n",
        "        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
        "        attn = self.attend(dots)\n",
        "        attn = self.dropout(attn)\n",
        "        out = torch.matmul(attn, v)\n",
        "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
        "        return self.to_out(out)\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.):\n",
        "        super().__init__()\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.layers = nn.ModuleList([])\n",
        "        for _ in range(depth):\n",
        "            self.layers.append(nn.ModuleList([\n",
        "                Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout),\n",
        "                FeedForward(dim, mlp_dim, dropout = dropout)\n",
        "            ]))\n",
        "\n",
        "    def forward(self, x):\n",
        "        for attn, ff in self.layers:\n",
        "            x = attn(x) + x\n",
        "            x = ff(x) + x\n",
        "        return self.norm(x)\n",
        "\n",
        "class ViT(nn.Module):\n",
        "    def __init__(self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, pool = 'cls', channels = 3, dim_head = 64, dropout = 0., emb_dropout = 0.):\n",
        "        super().__init__()\n",
        "        image_height, image_width = pair(image_size)\n",
        "        patch_height, patch_width = pair(patch_size)\n",
        "        assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size.'\n",
        "        num_patches = (image_height // patch_height) * (image_width // patch_width)\n",
        "        patch_dim = channels * patch_height * patch_width\n",
        "        assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'\n",
        "        self.to_patch_embedding = nn.Sequential(\n",
        "            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_height, p2 = patch_width),\n",
        "            nn.LayerNorm(patch_dim), nn.Linear(patch_dim, dim),\n",
        "            nn.LayerNorm(dim),\n",
        "        )\n",
        "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
        "        self.dropout = nn.Dropout(emb_dropout)\n",
        "        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\n",
        "        self.pool = pool\n",
        "        self.to_latent = nn.Identity()\n",
        "        self.mlp_head = nn.Linear(dim, num_classes)\n",
        "\n",
        "    def forward(self, img):\n",
        "        x = self.to_patch_embedding(img)\n",
        "        b, n, _ = x.shape\n",
        "        cls_tokens = repeat(self.cls_token, '1 1 d -> b 1 d', b = b)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "        x += self.pos_embedding[:, :(n + 1)]\n",
        "        x = self.dropout(x)\n",
        "        x = self.transformer(x)\n",
        "        x = x.mean(dim = 1) if self.pool == 'mean' else x[:, 0]\n",
        "        x = self.to_latent(x)\n",
        "        return self.mlp_head(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        },
        "id": "3oBrwquUT1fV",
        "outputId": "7db724e6-2606-4cba-d8b5-aee9a52f258e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/timm/models/registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n"
          ]
        },
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'deit'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-8c8a004c772d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtimm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDropPath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_2tuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrunc_normal_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtimm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregistry\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mregister_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdeit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels_v2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvit_models\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLayer_scale_init_Block\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAttention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minit_random_2d_freqs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_heads\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrotate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'deit'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# @title 2D RoPE\n",
        "# https://github.com/naver-ai/rope-vit/blob/main/models/vit_rope.py\n",
        "\n",
        "\n",
        "# https://github.com/facebookresearch/deit\n",
        "# https://github.com/meta-llama/codellama/blob/main/llama/model.py\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from functools import partial\n",
        "import torch.nn.functional as F\n",
        "# from timm.models.vision_transformer import Mlp, PatchEmbed , _cfg\n",
        "from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n",
        "from timm.models.registry import register_model\n",
        "from deit.models_v2 import vit_models, Layer_scale_init_Block, Attention\n",
        "\n",
        "def init_random_2d_freqs(dim: int, num_heads: int, theta: float = 10.0, rotate: bool = True):\n",
        "    freqs_x = []\n",
        "    freqs_y = []\n",
        "    mag = 1 / (theta ** (torch.arange(0, dim, 4)[: (dim // 4)].float() / dim))\n",
        "    for i in range(num_heads):\n",
        "        angles = torch.rand(1) * 2 * torch.pi if rotate else torch.zeros(1)\n",
        "        fx = torch.cat([mag * torch.cos(angles), mag * torch.cos(torch.pi/2 + angles)], dim=-1)\n",
        "        fy = torch.cat([mag * torch.sin(angles), mag * torch.sin(torch.pi/2 + angles)], dim=-1)\n",
        "        freqs_x.append(fx)\n",
        "        freqs_y.append(fy)\n",
        "    freqs_x = torch.stack(freqs_x, dim=0)\n",
        "    freqs_y = torch.stack(freqs_y, dim=0)\n",
        "    freqs = torch.stack([freqs_x, freqs_y], dim=0)\n",
        "    return freqs\n",
        "\n",
        "def compute_mixed_cis(freqs: torch.Tensor, t_x: torch.Tensor, t_y: torch.Tensor, num_heads: int):\n",
        "    N = t_x.shape[0]\n",
        "    depth = freqs.shape[1]\n",
        "    # No float 16 for this range\n",
        "    with torch.cuda.amp.autocast(enabled=False):\n",
        "        freqs_x = (t_x.unsqueeze(-1) @ freqs[0].unsqueeze(-2)).view(depth, N, num_heads, -1).permute(0, 2, 1, 3)\n",
        "        freqs_y = (t_y.unsqueeze(-1) @ freqs[1].unsqueeze(-2)).view(depth, N, num_heads, -1).permute(0, 2, 1, 3)\n",
        "        freqs_cis = torch.polar(torch.ones_like(freqs_x), freqs_x + freqs_y)\n",
        "    return freqs_cis\n",
        "\n",
        "\n",
        "def compute_axial_cis(dim: int, end_x: int, end_y: int, theta: float = 100.0):\n",
        "    freqs_x = 1.0 / (theta ** (torch.arange(0, dim, 4)[: (dim // 4)].float() / dim))\n",
        "    freqs_y = 1.0 / (theta ** (torch.arange(0, dim, 4)[: (dim // 4)].float() / dim))\n",
        "\n",
        "    t_x, t_y = init_t_xy(end_x, end_y)\n",
        "    freqs_x = torch.outer(t_x, freqs_x)\n",
        "    freqs_y = torch.outer(t_y, freqs_y)\n",
        "    freqs_cis_x = torch.polar(torch.ones_like(freqs_x), freqs_x)\n",
        "    freqs_cis_y = torch.polar(torch.ones_like(freqs_y), freqs_y)\n",
        "    return torch.cat([freqs_cis_x, freqs_cis_y], dim=-1)\n",
        "\n",
        "def init_t_xy(end_x: int, end_y: int):\n",
        "    t = torch.arange(end_x * end_y, dtype=torch.float32)\n",
        "    t_x = (t % end_x).float()\n",
        "    t_y = torch.div(t, end_x, rounding_mode='floor').float()\n",
        "    return t_x, t_y\n",
        "\n",
        "\n",
        "def reshape_for_broadcast(freqs_cis: torch.Tensor, x: torch.Tensor):\n",
        "    ndim = x.ndim\n",
        "    assert 0 <= 1 < ndim\n",
        "    if freqs_cis.shape == (x.shape[-2], x.shape[-1]):\n",
        "        shape = [d if i >= ndim-2 else 1 for i, d in enumerate(x.shape)]\n",
        "    elif freqs_cis.shape == (x.shape[-3], x.shape[-2], x.shape[-1]):\n",
        "        shape = [d if i >= ndim-3 else 1 for i, d in enumerate(x.shape)]\n",
        "    return freqs_cis.view(*shape)\n",
        "\n",
        "def apply_rotary_emb(xq: torch.Tensor, xk: torch.Tensor, freqs_cis: torch.Tensor):\n",
        "    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))\n",
        "    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))\n",
        "    freqs_cis = reshape_for_broadcast(freqs_cis, xq_)\n",
        "    xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(3)\n",
        "    xk_out = torch.view_as_real(xk_ * freqs_cis).flatten(3)\n",
        "    return xq_out.type_as(xq).to(xq.device), xk_out.type_as(xk).to(xk.device)\n",
        "\n",
        "\n",
        "class RoPEAttention(Attention):\n",
        "    \"\"\"Multi-head Attention block with rotary position embeddings.\"\"\"\n",
        "    def forward(self, x, freqs_cis):\n",
        "        B, N, C = x.shape\n",
        "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "\n",
        "        q[:, :, 1:], k[:, :, 1:] = apply_rotary_emb(q[:, :, 1:], k[:, :, 1:], freqs_cis=freqs_cis)\n",
        "        attn = (q * self.scale) @ k.transpose(-2, -1)\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "        return x\n",
        "\n",
        "class RoPE_Layer_scale_init_Block(Layer_scale_init_Block):\n",
        "    # taken from https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py\n",
        "    # with slight modifications\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        kwargs[\"Attention_block\"] = RoPEAttention\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "    def forward(self, x, freqs_cis):\n",
        "        x = x + self.drop_path(self.gamma_1 * self.attn(self.norm1(x), freqs_cis=freqs_cis))\n",
        "        x = x + self.drop_path(self.gamma_2 * self.mlp(self.norm2(x)))\n",
        "        return x\n",
        "\n",
        "class rope_vit_models(vit_models):\n",
        "    def __init__(self, rope_theta=100.0, rope_mixed=False, use_ape=False, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        img_size = kwargs['img_size'] if 'img_size' in kwargs else 224\n",
        "        patch_size = kwargs['patch_size'] if 'patch_size' in kwargs else 16\n",
        "        num_heads = kwargs['num_heads'] if 'num_heads' in kwargs else 12\n",
        "        embed_dim = kwargs['embed_dim'] if 'embed_dim' in kwargs else 768\n",
        "        mlp_ratio = kwargs['mlp_ratio'] if 'mlp_ratio' in kwargs else 4.\n",
        "\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "        trunc_normal_(self.cls_token, std=.02)\n",
        "\n",
        "        self.use_ape = use_ape\n",
        "        if not self.use_ape:\n",
        "            self.pos_embed = None\n",
        "\n",
        "        self.rope_mixed = rope_mixed\n",
        "        self.num_heads = num_heads\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "        if self.rope_mixed:\n",
        "            self.compute_cis = partial(compute_mixed_cis, num_heads=self.num_heads)\n",
        "\n",
        "            freqs = []\n",
        "            for i, _ in enumerate(self.blocks):\n",
        "                freqs.append(\n",
        "                    init_random_2d_freqs(dim=embed_dim // num_heads, num_heads=num_heads, theta=rope_theta)\n",
        "                )\n",
        "            freqs = torch.stack(freqs, dim=1).view(2, len(self.blocks), -1)\n",
        "            self.freqs = nn.Parameter(freqs.clone(), requires_grad=True)\n",
        "\n",
        "            t_x, t_y = init_t_xy(end_x = img_size // patch_size, end_y = img_size // patch_size)\n",
        "            self.register_buffer('freqs_t_x', t_x)\n",
        "            self.register_buffer('freqs_t_y', t_y)\n",
        "        else:\n",
        "            self.compute_cis = partial(compute_axial_cis, dim=embed_dim//num_heads, theta=rope_theta)\n",
        "\n",
        "            freqs_cis = self.compute_cis(end_x = img_size // patch_size, end_y = img_size // patch_size)\n",
        "            self.freqs_cis = freqs_cis\n",
        "\n",
        "    @torch.jit.ignore\n",
        "    def no_weight_decay(self):\n",
        "        return {'pos_embed', 'cls_token', 'freqs'}\n",
        "\n",
        "    def forward_features(self, x):\n",
        "        B, C, H, W = x.shape\n",
        "        x = self.patch_embed(x)\n",
        "\n",
        "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
        "\n",
        "        if self.use_ape:\n",
        "            pos_embed = self.pos_embed\n",
        "            if pos_embed.shape[-2] != x.shape[-2]:\n",
        "                img_size = self.patch_embed.img_size\n",
        "                patch_size = self.patch_embed.patch_size\n",
        "                pos_embed = pos_embed.view(\n",
        "                    1, (img_size[1] // patch_size[1]), (img_size[0] // patch_size[0]), self.embed_dim\n",
        "                ).permute(0, 3, 1, 2)\n",
        "                pos_embed = F.interpolate(\n",
        "                    pos_embed, size=(H // patch_size[1], W // patch_size[0]), mode='bicubic', align_corners=False\n",
        "                )\n",
        "                pos_embed = pos_embed.permute(0, 2, 3, 1).flatten(1, 2)\n",
        "            x = x + pos_embed\n",
        "\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "\n",
        "        if self.rope_mixed:\n",
        "            if self.freqs_t_x.shape[0] != x.shape[1] - 1:\n",
        "                t_x, t_y = init_t_xy(end_x = W // self.patch_size, end_y = H // self.patch_size)\n",
        "                t_x, t_y = t_x.to(x.device), t_y.to(x.device)\n",
        "            else:\n",
        "                t_x, t_y = self.freqs_t_x, self.freqs_t_y\n",
        "            freqs_cis = self.compute_cis(self.freqs, t_x, t_y)\n",
        "\n",
        "            for i , blk in enumerate(self.blocks):\n",
        "                x = blk(x, freqs_cis=freqs_cis[i])\n",
        "        else:\n",
        "            if self.freqs_cis.shape[0] != x.shape[1] - 1:\n",
        "                freqs_cis = self.compute_cis(end_x = W // self.patch_size, end_y = H // self.patch_size)\n",
        "            else:\n",
        "                freqs_cis = self.freqs_cis\n",
        "            freqs_cis = freqs_cis.to(x.device)\n",
        "\n",
        "            for i , blk in enumerate(self.blocks):\n",
        "                x = blk(x, freqs_cis=freqs_cis)\n",
        "\n",
        "        x = self.norm(x)\n",
        "        x = x[:, 0]\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "def adjust_pos_embed_size(model, state_dict):\n",
        "    # interpolate position embedding\n",
        "    if 'pos_embed' in state_dict:\n",
        "        pos_embed_checkpoint = state_dict['pos_embed']\n",
        "        embedding_size = pos_embed_checkpoint.shape[-1]\n",
        "        num_patches = model.patch_embed.num_patches\n",
        "        num_extra_tokens = model.pos_embed.shape[-2] - num_patches\n",
        "        # height (== width) for the checkpoint position embedding\n",
        "        orig_size = int((pos_embed_checkpoint.shape[-2] - num_extra_tokens) ** 0.5)\n",
        "        # height (== width) for the new position embedding\n",
        "        new_size = int(num_patches ** 0.5)\n",
        "        # class_token and dist_token are kept unchanged\n",
        "        extra_tokens = pos_embed_checkpoint[:, :num_extra_tokens]\n",
        "        # only the position tokens are interpolated\n",
        "        pos_tokens = pos_embed_checkpoint[:, num_extra_tokens:]\n",
        "        pos_tokens = pos_tokens.reshape(-1, orig_size, orig_size, embedding_size).permute(0, 3, 1, 2)\n",
        "        pos_tokens = torch.nn.functional.interpolate(\n",
        "            pos_tokens, size=(new_size, new_size), mode='bicubic', align_corners=False)\n",
        "        pos_tokens = pos_tokens.permute(0, 2, 3, 1).flatten(1, 2)\n",
        "        new_pos_embed = torch.cat((extra_tokens, pos_tokens), dim=1)\n",
        "        state_dict['pos_embed'] = new_pos_embed\n",
        "\n",
        "    return state_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Mpy-XMvCCVbt"
      },
      "outputs": [],
      "source": [
        "# @title facebookresearch ViT down\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/models/vision_transformer.py\n",
        "import math\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from src.utils.tensors import trunc_normal_, repeat_interleave_batch\n",
        "\n",
        "def get_2d_sincos_pos_embed_from_grid(embed_dim, grid):\n",
        "    assert embed_dim % 2 == 0\n",
        "    # use half of dimensions to encode grid_h\n",
        "    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[0])  # (pred_emb_dim//2, 1,h,h) ->\n",
        "    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[1])  # (H*W, D/2)\n",
        "    emb = np.concatenate([emb_h, emb_w], axis=1)  # (H*W, D)\n",
        "    return emb\n",
        "\n",
        "def get_2d_sincos_pos_embed(embed_dim, grid_size, cls_token=False): # grid_size: int of the grid height and width\n",
        "    grid_h, grid_w = np.arange(grid_size, dtype=float), np.arange(grid_size, dtype=float)\n",
        "    grid = np.meshgrid(grid_w, grid_h)  # here w goes first\n",
        "    grid = np.stack(grid, axis=0)\n",
        "    grid = grid.reshape([2, 1, grid_size, grid_size])\n",
        "    pos_embed = get_2d_sincos_pos_embed_from_grid(embed_dim, grid) # (pred_emb_dim, [2,1,h,h])\n",
        "    if cls_token:\n",
        "        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0) # prepend zeros for cls token later?\n",
        "    return pos_embed # [grid_size*grid_size, embed_dim] or [1+grid_size*grid_size, embed_dim] (w/ or w/o cls_token)\n",
        "\n",
        "\n",
        "def get_1d_sincos_pos_embed_from_grid(embed_dim, pos):\n",
        "    \"\"\"\n",
        "    embed_dim: output dimension for each position\n",
        "    pos: a list of positions to be encoded: size (M,)\n",
        "    out: (M, D)\n",
        "    \"\"\"\n",
        "    assert embed_dim % 2 == 0\n",
        "    omega = np.arange(embed_dim // 2, dtype=float)\n",
        "    omega /= embed_dim / 2.\n",
        "    omega = 1. / 10000**omega   # (D/2,)\n",
        "    pos = pos.reshape(-1)   # (M,)\n",
        "    out = np.einsum('m,d->md', pos, omega)   # (M, D/2), outer product\n",
        "    emb_sin = np.sin(out)  # (M, D/2)\n",
        "    emb_cos = np.cos(out)  # (M, D/2)\n",
        "    emb = np.concatenate([emb_sin, emb_cos], axis=1)  # (M, D)\n",
        "    return emb\n",
        "\n",
        "\n",
        "def drop_path(x, drop_prob: float = 0., training: bool = False):\n",
        "    if drop_prob == 0. or not training: return x\n",
        "    keep_prob = 1 - drop_prob\n",
        "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n",
        "    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
        "    random_tensor.floor_()  # binarize\n",
        "    output = x.div(keep_prob) * random_tensor\n",
        "    return output\n",
        "\n",
        "class DropPath(nn.Module):\n",
        "    \"\"\"Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\"\"\"\n",
        "    def __init__(self, drop_prob=None):\n",
        "        super().__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "\n",
        "    def forward(self, x):\n",
        "        return drop_path(x, self.drop_prob, self.training)\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, dim, num_heads, drop=0., drop_path=0.):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(dim)\n",
        "        # self.attn = Attention(dim, num_heads=num_heads, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=drop)\n",
        "        self.attn = AttentionBlock(dim, dim//num_heads)\n",
        "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "        self.norm2 = nn.LayerNorm(dim)\n",
        "        mlp_hidden_dim = int(dim * 4)\n",
        "        self.mlp = MLP(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=nn.GELU, drop=drop)\n",
        "\n",
        "    def forward(self, x, return_attention=False):\n",
        "        y, attn = self.attn(self.norm1(x))\n",
        "        if return_attention: return attn\n",
        "        x = x + self.drop_path(y)\n",
        "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
        "        return x\n",
        "\n",
        "\n",
        "class PatchEmbed(nn.Module):\n",
        "    \"\"\" Image to Patch Embedding\"\"\"\n",
        "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n",
        "        super().__init__()\n",
        "        self.num_patches = (img_size // patch_size) * (img_size // patch_size)\n",
        "        self.img_size, self.patch_size = img_size, patch_size\n",
        "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "    def forward(self, x): # [b,c,h,w]\n",
        "        # B, C, H, W = x.shape\n",
        "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ConvEmbed(nn.Module):\n",
        "    \"\"\"3x3 Convolution stems for ViT following ViTC models\"\"\"\n",
        "    def __init__(self, channels, strides, img_size=224, in_chans=3, batch_norm=True):\n",
        "        super().__init__()\n",
        "        stem = []\n",
        "        channels = [in_chans] + channels\n",
        "        act = nn.ReLU(inplace=True)\n",
        "        for i in range(len(channels) - 2):\n",
        "            # stem += [nn.Conv2d(channels[i], channels[i+1], kernel_size=3, stride=strides[i], padding=1, bias=(not batch_norm))]\n",
        "            # if batch_norm:\n",
        "            #     stem += [nn.BatchNorm2d(channels[i+1])]\n",
        "            # stem += [nn.ReLU(inplace=True)]\n",
        "            # nn.Sequential(\n",
        "            stem += [nn.Conv2d(channels[i], channels[i+1], kernel_size=3, stride=strides[i], padding=1, bias=(not batch_norm)), nn.BatchNorm2d(channels[i+1]) if batch_norm else nn.Identity(), act,]\n",
        "            # )\n",
        "        stem += [nn.Conv2d(channels[-2], channels[-1], kernel_size=1, stride=strides[-1])]\n",
        "        self.stem = nn.Sequential(*stem)\n",
        "\n",
        "        # Comptute the number of patches\n",
        "        stride_prod = int(np.prod(strides))\n",
        "        self.num_patches = (img_size[0] // stride_prod)**2\n",
        "\n",
        "    def forward(self, x):\n",
        "        p = self.stem(x)\n",
        "        return p.flatten(2).transpose(1, 2)\n",
        "\n",
        "\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/masks/utils.py\n",
        "def apply_masks(x, masks):\n",
        "    \"\"\":param x: tensor of shape [B (batch-size), N (num-patches), D (feature-dim)]\n",
        "    :param masks: list of tensors containing indices of patches in [N] to keep\"\"\"\n",
        "    all_x = []\n",
        "    for m in masks:\n",
        "        mask_keep = m.unsqueeze(-1).repeat(1, 1, x.size(-1))\n",
        "        all_x += [torch.gather(x, dim=1, index=mask_keep)]\n",
        "    return torch.cat(all_x, dim=0)\n",
        "\n",
        "class VisionTransformerPredictor(nn.Module):\n",
        "    \"\"\" Vision Transformer \"\"\"\n",
        "    def __init__(self, num_patches, embed_dim=768, predictor_embed_dim=384,\n",
        "        depth=6, num_heads=12, drop_rate=0.0, drop_path_rate=0.0, init_std=0.02, **kwargs):\n",
        "        super().__init__()\n",
        "        self.init_std = init_std\n",
        "        self.predictor_embed = nn.Linear(embed_dim, predictor_embed_dim, bias=True)\n",
        "        self.mask_token = nn.Parameter(torch.zeros(1, 1, predictor_embed_dim))\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
        "        # --\n",
        "        self.predictor_pos_embed = nn.Parameter(torch.zeros(1, num_patches, predictor_embed_dim), requires_grad=False) # [1, (h*w), pred_emb_dim]\n",
        "        predictor_pos_embed = get_2d_sincos_pos_embed(self.predictor_pos_embed.shape[-1], int(num_patches**.5), cls_token=False) # get_2d_sincos_pos_embed(pred_emb_dim, (h)) ->\n",
        "        self.predictor_pos_embed.data.copy_(torch.from_numpy(predictor_pos_embed).float().unsqueeze(0))\n",
        "        # --\n",
        "        self.predictor_blocks = nn.ModuleList([Block(dim=predictor_embed_dim, num_heads=num_heads, drop=drop_rate, drop_path=dpr[i]) for i in range(depth)])\n",
        "        self.predictor_norm = nn.LayerNorm(predictor_embed_dim)\n",
        "        self.predictor_proj = nn.Linear(predictor_embed_dim, embed_dim, bias=True)\n",
        "        # ------\n",
        "        trunc_normal_(self.mask_token, std=self.init_std)\n",
        "\n",
        "    def forward(self, x, masks_x, masks):\n",
        "        assert (masks is not None) and (masks_x is not None), 'Cannot run predictor without mask indices'\n",
        "        if not isinstance(masks_x, list): masks_x = [masks_x]\n",
        "        if not isinstance(masks, list): masks = [masks]\n",
        "\n",
        "        # -- Batch Size\n",
        "        B = len(x) // len(masks_x) # batch*M // M\n",
        "\n",
        "        # -- map from encoder-dim to pedictor-dim\n",
        "        x = self.predictor_embed(x)\n",
        "\n",
        "        # -- add positional embedding to x tokens\n",
        "        x_pos_embed = self.predictor_pos_embed.repeat(B, 1, 1)\n",
        "        x += apply_masks(x_pos_embed, masks_x) # apply pos emb to mask tokens\n",
        "\n",
        "        _, N_ctxt, D = x.shape\n",
        "\n",
        "        # -- concat mask tokens to x\n",
        "        pos_embs = self.predictor_pos_embed.repeat(B, 1, 1)\n",
        "        pos_embs = apply_masks(pos_embs, masks)\n",
        "        pos_embs = repeat_interleave_batch(pos_embs, B, repeat=len(masks_x))\n",
        "        # --\n",
        "        pred_tokens = self.mask_token.repeat(pos_embs.size(0), pos_embs.size(1), 1)\n",
        "        # --\n",
        "        pred_tokens += pos_embs\n",
        "        x = x.repeat(len(masks), 1, 1)\n",
        "        x = torch.cat([x, pred_tokens], dim=1)\n",
        "\n",
        "        # -- fwd prop\n",
        "        for blk in self.predictor_blocks:\n",
        "            x = blk(x)\n",
        "        x = self.predictor_norm(x)\n",
        "\n",
        "        # -- return preds for mask tokens\n",
        "        x = x[:, N_ctxt:]\n",
        "        x = self.predictor_proj(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class VisionTransformer(nn.Module):\n",
        "    \"\"\" Vision Transformer \"\"\"\n",
        "    def __init__(self, img_size=[224], patch_size=16, in_chans=3, embed_dim=768, predictor_embed_dim=384, depth=12, predictor_depth=12,\n",
        "        num_heads=12, mlp_ratio=4.0, qkv_bias=True, qk_scale=None, drop_rate=0.0, attn_drop_rate=0.0, drop_path_rate=0.0, norm_layer=nn.LayerNorm, init_std=0.02, **kwargs):\n",
        "        super().__init__()\n",
        "        self.num_features = self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.init_std = init_std\n",
        "        # --\n",
        "        self.patch_embed = PatchEmbed(img_size=img_size[0], patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n",
        "        num_patches = self.patch_embed.num_patches\n",
        "        # --\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim), requires_grad=False)\n",
        "        pos_embed = get_2d_sincos_pos_embed(self.pos_embed.shape[-1], int(self.patch_embed.num_patches**.5), cls_token=False)\n",
        "        self.pos_embed.data.copy_(torch.from_numpy(pos_embed).float().unsqueeze(0))\n",
        "        # --\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
        "        self.blocks = nn.ModuleList([\n",
        "            Block(dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer)\n",
        "            for i in range(depth)])\n",
        "        self.norm = norm_layer(embed_dim)\n",
        "\n",
        "    def forward(self, x, masks=None):\n",
        "        if masks is not None:\n",
        "            if not isinstance(masks, list):\n",
        "                masks = [masks]\n",
        "\n",
        "        # -- patchify x\n",
        "        x = self.patch_embed(x)\n",
        "        B, N, D = x.shape\n",
        "\n",
        "        # -- add positional embedding to x\n",
        "        pos_embed = self.interpolate_pos_encoding(x, self.pos_embed)\n",
        "        x = x + pos_embed\n",
        "\n",
        "        # -- mask x\n",
        "        if masks is not None:\n",
        "            x = apply_masks(x, masks)\n",
        "\n",
        "        # -- fwd prop\n",
        "        for i, blk in enumerate(self.blocks):\n",
        "            x = blk(x)\n",
        "\n",
        "        if self.norm is not None:\n",
        "            x = self.norm(x)\n",
        "        return x\n",
        "\n",
        "    def interpolate_pos_encoding(self, x, pos_embed):\n",
        "        npatch = x.shape[1] - 1\n",
        "        N = pos_embed.shape[1] - 1\n",
        "        if npatch == N: return pos_embed\n",
        "        class_emb, pos_embed = pos_embed[:, 0], pos_embed[:, 1:]\n",
        "        dim = x.shape[-1]\n",
        "        pos_embed = nn.functional.interpolate(pos_embed.reshape(1, int(math.sqrt(N)), int(math.sqrt(N)), dim).permute(0, 3, 1, 2), scale_factor=math.sqrt(npatch / N), mode='bicubic',)\n",
        "        pos_embed = pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n",
        "        return torch.cat((class_emb.unsqueeze(0), pos_embed), dim=1)\n",
        "\n",
        "# from functools import partial\n",
        "# def vit_predictor(**kwargs):\n",
        "#     model = VisionTransformerPredictor(\n",
        "#         mlp_ratio=4, qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
        "#     return model\n",
        "\n",
        "def vit(patch_size=16, **kwargs):\n",
        "    model = VisionTransformer(\n",
        "        patch_size=patch_size, embed_dim=192, depth=12, num_heads=3, mlp_ratio=4, # tiny\n",
        "        # patch_size=patch_size, embed_dim=384, depth=12, num_heads=6, mlp_ratio=4, # small\n",
        "        # patch_size=patch_size, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, # base\n",
        "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
        "    return model\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "rmuh1qDo7p9n"
      },
      "outputs": [],
      "source": [
        "# @title facebookresearch ViT base\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/models/vision_transformer.py\n",
        "\n",
        "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
        "# All rights reserved.\n",
        "#\n",
        "# This source code is licensed under the license found in the\n",
        "# LICENSE file in the root directory of this source tree.\n",
        "#\n",
        "\n",
        "import math\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from src.utils.tensors import trunc_normal_, repeat_interleave_batch\n",
        "from src.masks.utils import apply_masks\n",
        "\n",
        "def get_2d_sincos_pos_embed_from_grid(embed_dim, grid):\n",
        "    assert embed_dim % 2 == 0\n",
        "    # use half of dimensions to encode grid_h\n",
        "    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[0])  # (H*W, D/2)\n",
        "    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[1])  # (H*W, D/2)\n",
        "    emb = np.concatenate([emb_h, emb_w], axis=1)  # (H*W, D)\n",
        "    return emb\n",
        "\n",
        "def get_2d_sincos_pos_embed(embed_dim, grid_size, cls_token=False): # grid_size: int of the grid height and width\n",
        "    grid_h, grid_w = np.arange(grid_size, dtype=float), np.arange(grid_size, dtype=float)\n",
        "    grid = np.meshgrid(grid_w, grid_h)  # here w goes first\n",
        "    grid = np.stack(grid, axis=0)\n",
        "    grid = grid.reshape([2, 1, grid_size, grid_size])\n",
        "    pos_embed = get_2d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
        "    if cls_token:\n",
        "        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0) # prepend zeros for cls token later?\n",
        "    return pos_embed # [grid_size*grid_size, embed_dim] or [1+grid_size*grid_size, embed_dim] (w/ or w/o cls_token)\n",
        "\n",
        "\n",
        "def get_1d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):\n",
        "    \"\"\"\n",
        "    grid_size: int of the grid length\n",
        "    return:\n",
        "    pos_embed: [grid_size, embed_dim] or [1+grid_size, embed_dim] (w/ or w/o cls_token)\n",
        "    \"\"\"\n",
        "    grid = np.arange(grid_size, dtype=float)\n",
        "    pos_embed = get_1d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
        "    if cls_token:\n",
        "        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n",
        "    return pos_embed\n",
        "\n",
        "\n",
        "def get_1d_sincos_pos_embed_from_grid(embed_dim, pos):\n",
        "    \"\"\"\n",
        "    embed_dim: output dimension for each position\n",
        "    pos: a list of positions to be encoded: size (M,)\n",
        "    out: (M, D)\n",
        "    \"\"\"\n",
        "    assert embed_dim % 2 == 0\n",
        "    omega = np.arange(embed_dim // 2, dtype=float)\n",
        "    omega /= embed_dim / 2.\n",
        "    omega = 1. / 10000**omega   # (D/2,)\n",
        "    pos = pos.reshape(-1)   # (M,)\n",
        "    out = np.einsum('m,d->md', pos, omega)   # (M, D/2), outer product\n",
        "    emb_sin = np.sin(out)  # (M, D/2)\n",
        "    emb_cos = np.cos(out)  # (M, D/2)\n",
        "    emb = np.concatenate([emb_sin, emb_cos], axis=1)  # (M, D)\n",
        "    return emb\n",
        "\n",
        "\n",
        "def drop_path(x, drop_prob: float = 0., training: bool = False):\n",
        "    if drop_prob == 0. or not training:\n",
        "        return x\n",
        "    keep_prob = 1 - drop_prob\n",
        "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n",
        "    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
        "    random_tensor.floor_()  # binarize\n",
        "    output = x.div(keep_prob) * random_tensor\n",
        "    return output\n",
        "\n",
        "\n",
        "class DropPath(nn.Module):\n",
        "    \"\"\"Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\"\"\"\n",
        "    def __init__(self, drop_prob=None):\n",
        "        super(DropPath, self).__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "\n",
        "    def forward(self, x):\n",
        "        return drop_path(x, self.drop_prob, self.training)\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
        "        super().__init__()\n",
        "        out_features = out_features or in_features\n",
        "        hidden_features = hidden_features or in_features\n",
        "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
        "        self.act = act_layer()\n",
        "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.drop(x)\n",
        "        return x\n",
        "# lin act drop lin drop\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads\n",
        "        self.scale = qk_scale or head_dim ** -0.5\n",
        "\n",
        "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "        return x, attn\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
        "                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
        "        super().__init__()\n",
        "        self.norm1 = norm_layer(dim)\n",
        "        self.attn = Attention(dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
        "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "        self.norm2 = norm_layer(dim)\n",
        "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
        "        self.mlp = MLP(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
        "\n",
        "    def forward(self, x, return_attention=False):\n",
        "        y, attn = self.attn(self.norm1(x))\n",
        "        if return_attention:\n",
        "            return attn\n",
        "        x = x + self.drop_path(y)\n",
        "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
        "        return x\n",
        "\n",
        "\n",
        "class PatchEmbed(nn.Module):\n",
        "    \"\"\" Image to Patch Embedding\"\"\"\n",
        "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n",
        "        super().__init__()\n",
        "        num_patches = (img_size // patch_size) * (img_size // patch_size)\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.num_patches = num_patches\n",
        "\n",
        "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.shape\n",
        "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ConvEmbed(nn.Module):\n",
        "    \"\"\"3x3 Convolution stems for ViT following ViTC models\"\"\"\n",
        "    def __init__(self, channels, strides, img_size=224, in_chans=3, batch_norm=True):\n",
        "        super().__init__()\n",
        "        # Build the stems\n",
        "        stem = []\n",
        "        channels = [in_chans] + channels\n",
        "        for i in range(len(channels) - 2):\n",
        "            stem += [nn.Conv2d(channels[i], channels[i+1], kernel_size=3, stride=strides[i], padding=1, bias=(not batch_norm))]\n",
        "            if batch_norm:\n",
        "                stem += [nn.BatchNorm2d(channels[i+1])]\n",
        "            stem += [nn.ReLU(inplace=True)]\n",
        "        stem += [nn.Conv2d(channels[-2], channels[-1], kernel_size=1, stride=strides[-1])]\n",
        "        self.stem = nn.Sequential(*stem)\n",
        "\n",
        "        # Comptute the number of patches\n",
        "        stride_prod = int(np.prod(strides))\n",
        "        self.num_patches = (img_size[0] // stride_prod)**2\n",
        "\n",
        "    def forward(self, x):\n",
        "        p = self.stem(x)\n",
        "        return p.flatten(2).transpose(1, 2)\n",
        "\n",
        "\n",
        "class VisionTransformerPredictor(nn.Module):\n",
        "    \"\"\" Vision Transformer \"\"\"\n",
        "    def __init__(self, num_patches, embed_dim=768, predictor_embed_dim=384,\n",
        "        depth=6, num_heads=12, mlp_ratio=4.0, qkv_bias=True, qk_scale=None,\n",
        "        drop_rate=0.0, attn_drop_rate=0.0, drop_path_rate=0.0, norm_layer=nn.LayerNorm, init_std=0.02, **kwargs):\n",
        "        super().__init__()\n",
        "        self.predictor_embed = nn.Linear(embed_dim, predictor_embed_dim, bias=True)\n",
        "        self.mask_token = nn.Parameter(torch.zeros(1, 1, predictor_embed_dim))\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
        "        # --\n",
        "        self.predictor_pos_embed = nn.Parameter(torch.zeros(1, num_patches, predictor_embed_dim), requires_grad=False)\n",
        "        predictor_pos_embed = get_2d_sincos_pos_embed(self.predictor_pos_embed.shape[-1], int(num_patches**.5), cls_token=False)\n",
        "        self.predictor_pos_embed.data.copy_(torch.from_numpy(predictor_pos_embed).float().unsqueeze(0))\n",
        "        # --\n",
        "        self.predictor_blocks = nn.ModuleList([\n",
        "            Block(dim=predictor_embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer)\n",
        "            for i in range(depth)])\n",
        "        self.predictor_norm = norm_layer(predictor_embed_dim)\n",
        "        self.predictor_proj = nn.Linear(predictor_embed_dim, embed_dim, bias=True)\n",
        "        # ------\n",
        "        self.init_std = init_std\n",
        "        trunc_normal_(self.mask_token, std=self.init_std)\n",
        "        self.apply(self._init_weights)\n",
        "        self.fix_init_weight()\n",
        "\n",
        "    def fix_init_weight(self):\n",
        "        def rescale(param, layer_id):\n",
        "            param.div_(math.sqrt(2.0 * layer_id))\n",
        "\n",
        "        for layer_id, layer in enumerate(self.predictor_blocks):\n",
        "            rescale(layer.attn.proj.weight.data, layer_id + 1)\n",
        "            rescale(layer.mlp.fc2.weight.data, layer_id + 1)\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            trunc_normal_(m.weight, std=self.init_std)\n",
        "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "        elif isinstance(m, nn.Conv2d):\n",
        "            trunc_normal_(m.weight, std=self.init_std)\n",
        "            if m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x, masks_x, masks):\n",
        "        assert (masks is not None) and (masks_x is not None), 'Cannot run predictor without mask indices'\n",
        "\n",
        "        if not isinstance(masks_x, list): masks_x = [masks_x]\n",
        "        if not isinstance(masks, list): masks = [masks]\n",
        "\n",
        "        # -- Batch Size\n",
        "        B = len(x) // len(masks_x)\n",
        "\n",
        "        # -- map from encoder-dim to pedictor-dim\n",
        "        x = self.predictor_embed(x)\n",
        "\n",
        "        # -- add positional embedding to x tokens\n",
        "        x_pos_embed = self.predictor_pos_embed.repeat(B, 1, 1)\n",
        "        x += apply_masks(x_pos_embed, masks_x)\n",
        "\n",
        "        _, N_ctxt, D = x.shape\n",
        "\n",
        "        # -- concat mask tokens to x\n",
        "        pos_embs = self.predictor_pos_embed.repeat(B, 1, 1)\n",
        "        pos_embs = apply_masks(pos_embs, masks)\n",
        "        pos_embs = repeat_interleave_batch(pos_embs, B, repeat=len(masks_x))\n",
        "        # --\n",
        "        pred_tokens = self.mask_token.repeat(pos_embs.size(0), pos_embs.size(1), 1)\n",
        "        # --\n",
        "        pred_tokens += pos_embs\n",
        "        x = x.repeat(len(masks), 1, 1)\n",
        "        x = torch.cat([x, pred_tokens], dim=1)\n",
        "\n",
        "        # -- fwd prop\n",
        "        for blk in self.predictor_blocks:\n",
        "            x = blk(x)\n",
        "        x = self.predictor_norm(x)\n",
        "\n",
        "        # -- return preds for mask tokens\n",
        "        x = x[:, N_ctxt:]\n",
        "        x = self.predictor_proj(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class VisionTransformer(nn.Module):\n",
        "    \"\"\" Vision Transformer \"\"\"\n",
        "    def __init__(self, img_size=[224], patch_size=16, in_chans=3, embed_dim=768, predictor_embed_dim=384, depth=12, predictor_depth=12,\n",
        "        num_heads=12, mlp_ratio=4.0, qkv_bias=True, qk_scale=None, drop_rate=0.0, attn_drop_rate=0.0, drop_path_rate=0.0, norm_layer=nn.LayerNorm, init_std=0.02, **kwargs):\n",
        "        super().__init__()\n",
        "        self.num_features = self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        # --\n",
        "        self.patch_embed = PatchEmbed(img_size=img_size[0], patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n",
        "        num_patches = self.patch_embed.num_patches\n",
        "        # --\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim), requires_grad=False)\n",
        "        pos_embed = get_2d_sincos_pos_embed(self.pos_embed.shape[-1], int(self.patch_embed.num_patches**.5), cls_token=False)\n",
        "        self.pos_embed.data.copy_(torch.from_numpy(pos_embed).float().unsqueeze(0))\n",
        "        # --\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
        "        self.blocks = nn.ModuleList([\n",
        "            Block(dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer)\n",
        "            for i in range(depth)])\n",
        "        self.norm = norm_layer(embed_dim)\n",
        "        # ------\n",
        "        self.init_std = init_std\n",
        "        self.apply(self._init_weights)\n",
        "        self.fix_init_weight()\n",
        "\n",
        "    def fix_init_weight(self):\n",
        "        def rescale(param, layer_id):\n",
        "            param.div_(math.sqrt(2.0 * layer_id))\n",
        "\n",
        "        for layer_id, layer in enumerate(self.blocks):\n",
        "            rescale(layer.attn.proj.weight.data, layer_id + 1)\n",
        "            rescale(layer.mlp.fc2.weight.data, layer_id + 1)\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            trunc_normal_(m.weight, std=self.init_std)\n",
        "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "        elif isinstance(m, nn.Conv2d):\n",
        "            trunc_normal_(m.weight, std=self.init_std)\n",
        "            if m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x, masks=None):\n",
        "        if masks is not None:\n",
        "            if not isinstance(masks, list):\n",
        "                masks = [masks]\n",
        "\n",
        "        # -- patchify x\n",
        "        x = self.patch_embed(x)\n",
        "        B, N, D = x.shape\n",
        "\n",
        "        # -- add positional embedding to x\n",
        "        pos_embed = self.interpolate_pos_encoding(x, self.pos_embed)\n",
        "        x = x + pos_embed\n",
        "\n",
        "        # -- mask x\n",
        "        if masks is not None:\n",
        "            x = apply_masks(x, masks)\n",
        "\n",
        "        # -- fwd prop\n",
        "        for i, blk in enumerate(self.blocks):\n",
        "            x = blk(x)\n",
        "\n",
        "        if self.norm is not None:\n",
        "            x = self.norm(x)\n",
        "        return x\n",
        "\n",
        "    def interpolate_pos_encoding(self, x, pos_embed):\n",
        "        npatch = x.shape[1] - 1\n",
        "        N = pos_embed.shape[1] - 1\n",
        "        if npatch == N:\n",
        "            return pos_embed\n",
        "        class_emb = pos_embed[:, 0]\n",
        "        pos_embed = pos_embed[:, 1:]\n",
        "        dim = x.shape[-1]\n",
        "        pos_embed = nn.functional.interpolate(\n",
        "            pos_embed.reshape(1, int(math.sqrt(N)), int(math.sqrt(N)), dim).permute(0, 3, 1, 2),\n",
        "            scale_factor=math.sqrt(npatch / N),\n",
        "            mode='bicubic',\n",
        "        )\n",
        "        pos_embed = pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n",
        "        return torch.cat((class_emb.unsqueeze(0), pos_embed), dim=1)\n",
        "\n",
        "\n",
        "from functools import partial\n",
        "def vit_predictor(**kwargs):\n",
        "    model = VisionTransformerPredictor(\n",
        "        mlp_ratio=4, qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
        "    return model\n",
        "\n",
        "def vit(patch_size=16, **kwargs):\n",
        "    model = VisionTransformer(\n",
        "        patch_size=patch_size, embed_dim=192, depth=12, num_heads=3, mlp_ratio=4, # tiny\n",
        "        # patch_size=patch_size, embed_dim=384, depth=12, num_heads=6, mlp_ratio=4, # small\n",
        "        # patch_size=patch_size, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, # base\n",
        "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
        "    return model\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zvZ80FwKjyIs",
        "outputId": "e76b76c7-43a2-42ec-a175-692efb6403cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "38602\n",
            "torch.Size([64, 10])\n"
          ]
        }
      ],
      "source": [
        "# @title lucid SimpleViT\n",
        "import torch\n",
        "from vit_pytorch import SimpleViT\n",
        "\n",
        "model = SimpleViT(\n",
        "    image_size = 32,\n",
        "    patch_size = 8,\n",
        "    num_classes = 10,\n",
        "    dim = 64,\n",
        "    depth = 1,\n",
        "    heads = 8,\n",
        "    mlp_dim = 64,\n",
        "\n",
        "    # num_heads = 8,\n",
        "    # mlp_ratio = 1.,\n",
        "    # positional_embedding = 'learnable', # ['sine', 'learnable', 'none']\n",
        "    dim_head = 8,\n",
        "\n",
        ")\n",
        "# def __init__(self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, channels = 3, dim_head = 64):\n",
        "\n",
        "# img = torch.randn(1, 3, 256, 256)\n",
        "# preds = v(img) # (1, 1000)\n",
        "optim = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 236235\n",
        "x = torch.rand(64, 3, 32, 32, device=device)\n",
        "# x = torch.rand(64, 1, 28,28, device=device)\n",
        "logits = model(x)\n",
        "print(logits.shape)\n",
        "\n",
        "# loss: 1.684826  [49920/50000]\n",
        "# Test Error:\n",
        "#  Accuracy: 40.7%, Avg loss: 1.642769\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title NVlabs afno2d.py\n",
        "# https://github.com/NVlabs/AFNO-transformer/blob/master/afno/afno2d.py\n",
        "# Copyright (c) 2020, NVIDIA CORPORATION.  All rights reserved.\n",
        "\n",
        "# import math\n",
        "import torch\n",
        "# import torch.fft\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class AFNO2D(nn.Module):\n",
        "    \"\"\"\n",
        "    hidden_size: channel dimension size\n",
        "    num_blocks: how many blocks to use in the block diagonal weight matrices (higher => less complexity but less parameters)\n",
        "    hard_thresholding_fraction: how many frequencies you want to completely mask out (lower => hard_thresholding_fraction^2 less FLOPs)\n",
        "    \"\"\"\n",
        "    def __init__(self, hidden_size, num_blocks=8, hard_thresholding_fraction=1, hidden_size_factor=1):\n",
        "        super().__init__()\n",
        "        assert hidden_size % num_blocks == 0, f\"hidden_size {hidden_size} should be divisble by num_blocks {num_blocks}\"\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_blocks = num_blocks\n",
        "        self.block_size = self.hidden_size // self.num_blocks\n",
        "        self.hard_thresholding_fraction = hard_thresholding_fraction\n",
        "        self.hidden_size_factor = hidden_size_factor\n",
        "        self.scale = 0.02\n",
        "\n",
        "        self.w1 = nn.Parameter(self.scale * torch.randn(2, self.num_blocks, self.block_size, self.block_size * self.hidden_size_factor))\n",
        "        self.b1 = nn.Parameter(self.scale * torch.randn(2, self.num_blocks, self.block_size * self.hidden_size_factor))\n",
        "        self.w2 = nn.Parameter(self.scale * torch.randn(2, self.num_blocks, self.block_size * self.hidden_size_factor, self.block_size))\n",
        "        self.b2 = nn.Parameter(self.scale * torch.randn(2, self.num_blocks, self.block_size))\n",
        "\n",
        "    def forward(self, x, spatial_size=None):\n",
        "        bias = x\n",
        "        dtype = x.dtype\n",
        "        x = x.float()\n",
        "        B, N, C = x.shape\n",
        "\n",
        "        x = x.reshape(B, H, W, C)\n",
        "        x = torch.fft.rfft2(x, dim=(1, 2), norm=\"ortho\")\n",
        "        x = x.reshape(B, x.shape[1], x.shape[2], self.num_blocks, self.block_size) # [b, h, w//2+1, n_blocks, block_size]\n",
        "        o1_real = torch.zeros([B, x.shape[1], x.shape[2], self.num_blocks, self.block_size * self.hidden_size_factor], device=x.device)\n",
        "        o1_imag = torch.zeros([B, x.shape[1], x.shape[2], self.num_blocks, self.block_size * self.hidden_size_factor], device=x.device)\n",
        "        o2_real = torch.zeros(x.shape, device=x.device)\n",
        "        o2_imag = torch.zeros(x.shape, device=x.device)\n",
        "\n",
        "        total_modes = N // 2 + 1\n",
        "        kept_modes = int(total_modes * self.hard_thresholding_fraction)\n",
        "\n",
        "        o1_real[:, :, :kept_modes] = F.relu(\n",
        "            torch.einsum('...bi,bio->...bo', x[:, :, :kept_modes].real, self.w1[0]) - \\\n",
        "            torch.einsum('...bi,bio->...bo', x[:, :, :kept_modes].imag, self.w1[1]) + \\\n",
        "            self.b1[0]\n",
        "        )\n",
        "\n",
        "        o1_imag[:, :, :kept_modes] = F.relu(\n",
        "            torch.einsum('...bi,bio->...bo', x[:, :, :kept_modes].imag, self.w1[0]) + \\\n",
        "            torch.einsum('...bi,bio->...bo', x[:, :, :kept_modes].real, self.w1[1]) + \\\n",
        "            self.b1[1]\n",
        "        )\n",
        "\n",
        "        o2_real[:, :, :kept_modes] = (\n",
        "            torch.einsum('...bi,bio->...bo', o1_real[:, :, :kept_modes], self.w2[0]) - \\\n",
        "            torch.einsum('...bi,bio->...bo', o1_imag[:, :, :kept_modes], self.w2[1]) + \\\n",
        "            self.b2[0]\n",
        "        )\n",
        "\n",
        "        o2_imag[:, :, :kept_modes] = (\n",
        "            torch.einsum('...bi,bio->...bo', o1_imag[:, :, :kept_modes], self.w2[0]) + \\\n",
        "            torch.einsum('...bi,bio->...bo', o1_real[:, :, :kept_modes], self.w2[1]) + \\\n",
        "            self.b2[1]\n",
        "        )\n",
        "\n",
        "        x = torch.stack([o2_real, o2_imag], dim=-1)\n",
        "        x = F.softshrink(x, lambd=0.01)\n",
        "        x = torch.view_as_complex(x)\n",
        "        x = x.reshape(B, x.shape[1], x.shape[2], C)\n",
        "        x = torch.fft.irfft2(x, s=(H, W), dim=(1, 2), norm=\"ortho\")\n",
        "        x = x.reshape(B, N, C)\n",
        "        x = x.type(dtype)\n",
        "        return x + bias\n",
        "\n",
        "b,c,h,w = 2,16,9,9\n",
        "# w//2+1\n",
        "# b,c,h,w = 2,3,16,16\n",
        "mix = AFNO2D(c)\n",
        "x = torch.rand(b,c,h,w)\n",
        "out = mix(x)\n",
        "print(out.shape)\n",
        "# print(out)\n",
        "\n"
      ],
      "metadata": {
        "id": "T7MfCPJiDaue",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title AFNO2D_channelfirst save\n",
        "# Adaptive Frequency Filters As Efficient Global Token Mixers\n",
        "# https://arxiv.org/pdf/2307.14008\n",
        "# https://github.com/microsoft/TokenMixers/blob/main/Adaptive%20Frequency%20Filters/affnet/modules/aff_block.py#L62\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class AFNO2D_channelfirst(nn.Module):\n",
        "    def __init__(self, hidden_size, num_blocks=8, hidden_size_factor=1):\n",
        "        super().__init__()\n",
        "        self.num_blocks = num_blocks # num_blocks: how many blocks to use in the block diagonal weight matrices (higher => less complexity but less parameters)\n",
        "        self.block_size = hidden_size // self.num_blocks\n",
        "        scale = 0.02\n",
        "\n",
        "        self.w1 = nn.Parameter(scale * torch.randn(2, self.num_blocks, self.block_size, self.block_size * hidden_size_factor))\n",
        "        self.b1 = nn.Parameter(scale * torch.randn(2, self.num_blocks, self.block_size * hidden_size_factor))\n",
        "        self.w2 = nn.Parameter(scale * torch.randn(2, self.num_blocks, self.block_size * hidden_size_factor, self.block_size))\n",
        "        self.b2 = nn.Parameter(scale * torch.randn(2, self.num_blocks, self.block_size))\n",
        "\n",
        "    @torch.cuda.amp.autocast(enabled=False)\n",
        "    def forward(self, x): # [b,c,h,w]\n",
        "        bias = x\n",
        "        dtype = x.dtype\n",
        "        x = x.float()\n",
        "        B, C, H, W = x.shape\n",
        "        x = torch.fft.rfft2(x, dim=(2, 3), norm=\"ortho\") # [b, c, h, w//2+1]\n",
        "        origin_ffted = x\n",
        "        x = x.unflatten(1, (self.num_blocks, self.block_size)) # [b, n_blocks, block_size, h, w//2+1]\n",
        "\n",
        "# [b, n_blocks, block_size, h, w//2+1] @ [n_blocks, block_size, block_size * h_factor] -> [b, n_blocks, block_size * h_factor, h, w//2+1]\n",
        "# + [n_blocks, block_size * h_factor]\n",
        "        o1_real = F.relu(torch.einsum('bkihw,kio->bkohw', x.real, self.w1[0]) - \\\n",
        "            torch.einsum('bkihw,kio->bkohw', x.imag, self.w1[1]) + self.b1[0, :, :, None, None])\n",
        "        o1_imag = F.relu(torch.einsum('bkihw,kio->bkohw', x.imag, self.w1[0]) + \\\n",
        "            torch.einsum('bkihw,kio->bkohw', x.real, self.w1[1]) + self.b1[1, :, :, None, None])\n",
        "        o2_real = (torch.einsum('bkihw,kio->bkohw', o1_real, self.w2[0]) - \\\n",
        "            torch.einsum('bkihw,kio->bkohw', o1_imag, self.w2[1]) + self.b2[0, :, :, None, None])\n",
        "        o2_imag = (torch.einsum('bkihw,kio->bkohw', o1_imag, self.w2[0]) + \\\n",
        "            torch.einsum('bkihw,kio->bkohw', o1_real, self.w2[1]) + self.b2[1, :, :, None, None])\n",
        "\n",
        "# [n_blocks, block_size * h_factor, block_size]\n",
        "\n",
        "\n",
        "        x = torch.stack([o2_real, o2_imag], dim=-1)\n",
        "        x = F.softshrink(x, lambd=0.01)\n",
        "        x = torch.view_as_complex(x)\n",
        "        x = x.flatten(1,2) # [b, c, h, w]\n",
        "\n",
        "        x = x * origin_ffted\n",
        "        x = torch.fft.irfft2(x, s=(H, W), dim=(2, 3), norm=\"ortho\")\n",
        "        x = x.type(dtype)\n",
        "        return x + bias\n",
        "\n",
        "# batch, T, dim = 2,7,5\n",
        "b,c,h,w = 2,16,9,9\n",
        "# w//2+1\n",
        "# b,c,h,w = 2,3,16,16\n",
        "mix = AFNO2D_channelfirst(c, 4)\n",
        "x = torch.rand(b,c,h,w)\n",
        "out = mix(x)\n",
        "print(out.shape)\n",
        "# print(out)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7dhOatHPQwPU",
        "outputId": "64baad83-1f4d-4412-848a-8af45f9409b8",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 16, 9, 5])\n",
            "torch.Size([2, 4, 4, 9, 5])\n",
            "torch.Size([2, 16, 9, 9])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-36-6b5c42229079>:17: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  @torch.cuda.amp.autocast(enabled=False)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNoVYun9rw7ECGgZ1B/7ylM",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}