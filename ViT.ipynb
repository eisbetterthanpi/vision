{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eisbetterthanpi/vision/blob/main/ViT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "LxACli7GdyGq"
      },
      "outputs": [],
      "source": [
        "# @title data\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "# transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "transform = transforms.Compose([transforms.ToTensor(),])\n",
        "\n",
        "# CIFAR10: 60000 32x32 color images in 10 classes, with 6000 images per class\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='data', train=True, download=True, transform=transform)\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='data', train=False, download=True, transform=transform)\n",
        "batch_size = 128 # 4\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "def imshow(img):\n",
        "    # img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "# dataiter = iter(train_loader) # get some random training images\n",
        "# images, labels = next(dataiter)\n",
        "# print(images.shape) # [batch, 3, 32, 32]\n",
        "# imshow(torchvision.utils.make_grid(images))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ResBlock\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def zero_module(module):\n",
        "    for p in module.parameters(): p.detach().zero_()\n",
        "    return module\n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch=None, kernel=3):\n",
        "        super().__init__()\n",
        "        out_ch = out_ch or in_ch\n",
        "        act = nn.SiLU() #\n",
        "        self.res_conv = nn.Conv2d(in_ch, out_ch, 1) if in_ch != out_ch else nn.Identity()\n",
        "        # self.block = nn.Sequential( # best?\n",
        "        #     nn.Conv2d(in_ch, out_ch, 3, padding=1), nn.BatchNorm2d(out_ch), act,\n",
        "        #     zero_module(nn.Conv2d(out_ch, out_ch, 3, padding=1)), nn.BatchNorm2d(out_ch), act,\n",
        "        #     )\n",
        "        self.block = nn.Sequential(\n",
        "            nn.BatchNorm2d(in_ch), act, nn.Conv2d(in_ch, out_ch, kernel, padding=kernel//2),\n",
        "            nn.BatchNorm2d(out_ch), act, zero_module(nn.Conv2d(out_ch, out_ch, kernel, padding=kernel//2)),\n",
        "            )\n",
        "\n",
        "    def forward(self, x): # [b,c,h,w]\n",
        "        return self.block(x) + self.res_conv(x)\n"
      ],
      "metadata": {
        "id": "j3-vvMS1-gVn",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title UpDownBlock_me\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class PixelShuffleConv(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch=None, kernel=1, r=1):\n",
        "        super().__init__()\n",
        "        self.r = r\n",
        "        r = max(r, int(1/r))\n",
        "        out_ch = out_ch or in_ch\n",
        "        if self.r>1: self.net = nn.Sequential(ResBlock(in_ch, out_ch*r**2, kernel), nn.PixelShuffle(r))\n",
        "        # if self.r>1: self.net = nn.Sequential(Attention(in_ch, out_ch*r**2), nn.PixelShuffle(r))\n",
        "# MaskUnitAttention(in_dim, d_model=16, n_heads=4, q_stride=None, nd=2)\n",
        "\n",
        "        elif self.r<1: self.net = nn.Sequential(nn.PixelUnshuffle(r), ResBlock(in_ch*r**2, out_ch, kernel))\n",
        "        # elif self.r<1: self.net = nn.Sequential(nn.PixelUnshuffle(r), Attention(in_ch*r**2, out_ch))\n",
        "        elif in_ch != out_ch: self.net = ResBlock(in_ch*r**2, out_ch, kernel)\n",
        "        else: self.net = lambda x: torch.zeros_like(x)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "def AdaptiveAvgPool_nd(n, *args, **kwargs): return [nn.Identity, nn.AdaptiveAvgPool1d, nn.AdaptiveAvgPool2d, nn.AdaptiveAvgPool3d][n](*args, **kwargs)\n",
        "def AdaptiveMaxPool_nd(n, *args, **kwargs): return [nn.Identity, nn.AdaptiveMaxPool1d, nn.AdaptiveMaxPool2d, nn.AdaptiveMaxPool3d][n](*args, **kwargs)\n",
        "\n",
        "def adaptive_avg_pool_nd(n, x, output_size): return [nn.Identity, F.adaptive_avg_pool1d, F.adaptive_avg_pool2d, F.adaptive_avg_pool3d][n](x, output_size)\n",
        "def adaptive_max_pool_nd(n, x, output_size): return [nn.Identity, F.adaptive_max_pool1d, F.adaptive_max_pool2d, F.adaptive_max_pool3d][n](x, output_size)\n",
        "\n",
        "class AdaptivePool_at(nn.AdaptiveAvgPool1d): # AdaptiveAvgPool1d AdaptiveMaxPool1d\n",
        "    def __init__(self, dim=1, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.dim=dim\n",
        "    def forward(self, x):\n",
        "        x = x.transpose(self.dim,-1)\n",
        "        shape = x.shape\n",
        "        return super().forward(x.flatten(0,-2)).unflatten(0, shape[:-1]).transpose(self.dim,-1)\n",
        "\n",
        "\n",
        "def adaptive_pool_at(x, dim, output_size, pool='avg'): # [b,c,h,w]\n",
        "    x = x.transpose(dim,-1)\n",
        "    shape = x.shape\n",
        "    parent={'avg':F.adaptive_avg_pool1d, 'max':F.adaptive_max_pool1d}[pool]\n",
        "    return parent(x.flatten(0,-2), output_size).unflatten(0, shape[:-1]).transpose(dim,-1)\n",
        "\n",
        "\n",
        "class ZeroExtend():\n",
        "    def __init__(self, dim=1, output_size=16):\n",
        "        self.dim, self.out = dim, output_size\n",
        "    def __call__(self, x): # [b,c,h,w]\n",
        "        return torch.cat((x, torch.zeros(*x.shape[:self.dim], self.out - x.shape[self.dim], *x.shape[self.dim+1:])), dim=self.dim)\n",
        "\n",
        "def make_pool_at(pool='avg', dim=1, output_size=5):\n",
        "    parent={'avg':nn.AdaptiveAvgPool1d, 'max':nn.AdaptiveMaxPool1d}[pool]\n",
        "    class AdaptivePool_at(parent): # AdaptiveAvgPool1d AdaptiveMaxPool1d\n",
        "        def __init__(self, dim=1, *args, **kwargs):\n",
        "            super().__init__(*args, **kwargs)\n",
        "            self.dim=dim\n",
        "        def forward(self, x):\n",
        "            x = x.transpose(self.dim,-1)\n",
        "            shape = x.shape\n",
        "            return super().forward(x.flatten(0,-2)).unflatten(0, shape[:-1]).transpose(self.dim,-1)\n",
        "    return AdaptivePool_at(dim, output_size=output_size)\n",
        "\n",
        "class Shortcut():\n",
        "    def __init__(self, dim=1, c=3, sp=(3,3), nd=2):\n",
        "        self.dim = dim\n",
        "        # self.ch_pool = make_pool_at(pool='avg', dim=dim, output_size=c)\n",
        "        self.ch_pool = make_pool_at(pool='max', dim=dim, output_size=c)\n",
        "        # self.ch_pool = ZeroExtend(dim, output_size=c) # only for out_dim>=in_dim\n",
        "        # self.sp_pool = AdaptiveAvgPool_nd(nd, sp)\n",
        "        self.sp_pool = AdaptiveMaxPool_nd(nd, sp)\n",
        "\n",
        "    def __call__(self, x): # [b,c,h,w]\n",
        "        x = self.sp_pool(x) # spatial first preserves spatial more?\n",
        "        x = self.ch_pool(x)\n",
        "        return x\n",
        "\n",
        "class UpDownBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, kernel=7, r=1):\n",
        "        super().__init__()\n",
        "        act = nn.SiLU()\n",
        "        self.r = r\n",
        "        self.block = PixelShuffleConv(in_ch, out_ch, kernel=kernel, r=r)\n",
        "        # self.block = nn.Sequential(\n",
        "        #     nn.BatchNorm2d(in_ch), act, PixelShuffleConv(in_ch, out_ch, kernel=kernel, r=r)\n",
        "        # )\n",
        "        # if self.r>1: self.res_conv = nn.Sequential(nn.ConvTranspose2d(in_ch, out_ch, kernel, 2, kernel//2, output_padding=1))\n",
        "        # if self.r>1: self.res_conv = nn.Sequential(nn.Upsample(scale_factor=2, mode='nearest'), nn.Conv2d(in_ch, out_ch, kernel, 1, kernel//2) if in_ch != out_ch else nn.Identity())\n",
        "        # if self.r>1: self.res_conv = nn.Sequential(nn.Upsample(scale_factor=2, mode='bilinear'), nn.Conv2d(in_ch, out_ch, kernel, 1, kernel//2) if in_ch != out_ch else nn.Identity())\n",
        "        # if self.r>1: self.res_conv = nn.Sequential(nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True), nn.Conv2d(in_ch, out_ch, kernel, 1, kernel//2) if in_ch != out_ch else nn.Identity())\n",
        "\n",
        "\n",
        "        # elif self.r<1: self.res_conv = nn.Sequential(nn.Conv2d(in_ch, out_ch, kernel, 2, kernel//2))\n",
        "        # elif self.r<1: self.res_conv = nn.Sequential(nn.Conv2d(in_ch, out_ch, kernel, 1, kernel//2) if in_ch != out_ch else nn.Identity(), nn.MaxPool2d(2,2))\n",
        "        # elif self.r<1: self.res_conv = nn.Sequential(nn.Conv2d(in_ch, out_ch, kernel, 1, kernel//2) if in_ch != out_ch else nn.Identity(), nn.AvgPool2d(2,2))\n",
        "        # elif self.r<1: self.res_conv = AttentionBlock(in_ch, out_ch, n_heads=4, q_stride=(2,2))\n",
        "\n",
        "        # else: self.res_conv = nn.Conv2d(in_ch, out_ch, kernel, 1, kernel//2) if in_ch != out_ch else nn.Identity()\n",
        "\n",
        "    def forward(self, x): # [b,c,h,w]\n",
        "        b, num_tok, c, *win = x.shape\n",
        "        x = x.flatten(0,1)\n",
        "        out = self.block(x)\n",
        "        # # shortcut = F.interpolate(x.unsqueeze(1), size=out.shape[1:], mode='nearest-exact').squeeze(1) # pytorch.org/docs/stable/generated/torch.nn.functional.interpolate.html\n",
        "        # shortcut = F.adaptive_avg_pool3d(x, out.shape[1:]) # https://pytorch.org/docs/stable/nn.html#pooling-layers\n",
        "        # # shortcut = F.adaptive_max_pool3d(x, out.shape[1:]) # https://pytorch.org/docs/stable/nn.html#pooling-layers\n",
        "        # # shortcut = F.adaptive_avg_pool3d(x, out.shape[1:]) if out.shape[1]>=x.shape[1] else F.adaptive_max_pool3d(x, out.shape[1:])\n",
        "        # shortcut(x)\n",
        "        shortcut = Shortcut(dim=1, c=out.shape[1], sp=out.shape[-2:], nd=2)(x)\n",
        "        out = out + shortcut\n",
        "        out = out.unflatten(0, (b, num_tok))\n",
        "        return out\n",
        "\n",
        "        # return out + shortcut + self.res_conv(x)\n",
        "        # return out + self.res_conv(x)\n",
        "        # return self.res_conv(x)\n",
        "\n",
        "# if out>in, inter=max=ave=near.\n",
        "# if out<in, inter=ave. max=max\n",
        "\n",
        "# stride2\n",
        "# interconv/convpool\n",
        "# pixelconv\n",
        "# pixeluib\n",
        "# pixelres\n",
        "# shortcut\n",
        "\n",
        "# in_ch, out_ch = 16,3\n",
        "in_ch, out_ch = 3,16\n",
        "model = UpDownBlock(in_ch, out_ch, r=1/2).to(device)\n",
        "# model = UpDownBlock(in_ch, out_ch, r=2).to(device)\n",
        "\n",
        "x = torch.rand(12, in_ch, 64,64, device=device)\n",
        "x = torch.rand(12, 2, in_ch, 64,64, device=device)\n",
        "out = model(x)\n",
        "\n",
        "print(out.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m0lclc9myo2c",
        "outputId": "6546089e-298a-436c-ec86-6ea8288b8a9e",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([12, 2, 16, 32, 32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title maxpool path bchw\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def conv_nd(n, *args, **kwargs): return [nn.Identity, nn.Conv1d, nn.Conv2d, nn.Conv3d][n](*args, **kwargs)\n",
        "def maxpool_nd(n, *args, **kwargs): return [nn.Identity, nn.MaxPool1d, nn.MaxPool2d, nn.MaxPool3d][n](*args, **kwargs)\n",
        "def avgpool_nd(n, *args, **kwargs): return [nn.Identity, nn.AvgPool1d, nn.AvgPool2d, nn.AvgPool3d][n](*args, **kwargs)\n",
        "import math\n",
        "\n",
        "class MaskUnitAttention(nn.Module):\n",
        "    # def __init__(self, d_model=16, n_heads=4, q_stride=None, nd=2):\n",
        "    def __init__(self, in_dim, d_model=16, n_heads=4, q_stride=None, nd=2):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_heads, self.d_head = n_heads, d_model // n_heads\n",
        "        self.scale = self.d_head**-.5\n",
        "        # self.qkv = conv_nd(nd, d_model, 3*d_model, 1, bias=False)\n",
        "        self.qkv = conv_nd(nd, in_dim, 3*d_model, 1, bias=False)\n",
        "        # self.out = conv_nd(nd, d_model, d_model, 1)\n",
        "        self.out = nn.Linear(d_model, d_model, 1)\n",
        "        self.q_stride = q_stride # If greater than 1, pool q with this stride. The stride should be flattened (e.g., 2x2 = 4).\n",
        "        if q_stride:\n",
        "            self.q_stride = (q_stride,)*nd if type(q_stride)==int else q_stride\n",
        "            self.q_pool = maxpool_nd(nd, self.q_stride, stride=self.q_stride)\n",
        "\n",
        "    def forward(self, x): # [b,num_tok,c,win1,win2]\n",
        "        b, num_tok, c, *win = x.shape\n",
        "        # x = x.transpose(1,2).flatten(0,1) # [b*num_tok,c,win1,win2]\n",
        "        x = x.flatten(0,1) # [b*num_tok,c,win1,win2]\n",
        "        # print(x.shape)\n",
        "        # q, k, v = self.qkv(x).reshape(b, 3, self.n_heads, self.d_head, num_tok, win*win).permute(1,0,2,4,5,3) # [b,3*d_model,num_tok*win,win] -> 3* [b, n_heads, num_tok, win*win, d_head]\n",
        "        # self.qkv(x).flatten(1,-2).unflatten(-1, (self.heads,-1)).chunk(3, dim=-1) # [b,sp,n_heads,d_head]\n",
        "        q,k,v = self.qkv(x).chunk(3, dim=1) # [b*num_tok,d,win,win]\n",
        "        if self.q_stride:\n",
        "            q = self.q_pool(q)\n",
        "            win=[w//s for w,s in zip(win, self.q_stride)] # win = win/q_stride\n",
        "        if math.prod(win) >= 200:\n",
        "            print('MUattn', math.prod(win))\n",
        "            q, k, v = map(lambda t: t.reshape(b*num_tok, self.n_heads, self.d_head, -1).transpose(-2,-1), (q,k,v)) # [b*num_tok, n_heads, win*win, d_head]\n",
        "        else: q, k, v = map(lambda t: t.reshape(b, num_tok, self.n_heads, self.d_head, -1).permute(0,2,1,4,3).flatten(2,3), (q,k,v)) # [b, n_heads, num_tok*win*win, d_head]\n",
        "\n",
        "        q, k = q.softmax(dim=-1)*self.scale, k.softmax(dim=-2) # [b, n_heads, t(/pool), d_head]\n",
        "        context = k.transpose(-2,-1) @ v # [b, n_heads, d_head, d_head]\n",
        "        x = q @ context # [b, n_heads, t(/pool), d_head]\n",
        "\n",
        "        # print('attn fwd 2',x.shape)\n",
        "        # x = x.transpose(1, 3).reshape(B, -1, self.d_model)\n",
        "        x = x.transpose(1,2).reshape(b, -1, self.d_model) # [b,t,d]\n",
        "        # x = x.transpose(-2,-1).reshape(x.shape[0], self.d_model, ) # [b,t,d]\n",
        "        # [b, n_heads, num_tok, win*win, d_head] -> [b, n_heads, d_head, num_tok, win*win] -> [b,c,num_tok*win,win]\n",
        "        # x = x.permute(0,1,4,2,3).reshape(b, self.d_model, num_tok*win,win) # [b, n_heads, num_tok, win*win, d_head]\n",
        "        # x = x.transpose(-2,-1).reshape(b, self.d_model, num_tok, *win) # [b*num_tok, n_heads, win*win, d_head]\n",
        "        x = self.out(x)\n",
        "        L=len(win)\n",
        "        x = x.reshape(b, num_tok, *win, self.d_model).permute(0,1,L+2,*range(2,L+2)) # [b, num_tok, out_dim, *win]\n",
        "        # x = x.unflatten(0, (b, num_tok))\n",
        "        return x # [b,num_tok,c,win1,win2]\n",
        "\n",
        "d_model=16\n",
        "model = MaskUnitAttention(d_model, n_heads=4, q_stride=2)\n",
        "# MaskUnitAttention(d_model=16, n_heads=4, q_stride=None, nd=2)\n",
        "# x=torch.randn(2,4,4,3)\n",
        "# x=torch.randn(2,3,d_model,8,8)\n",
        "x=torch.randn(2,3,d_model,32,32)\n",
        "# [b*num_tok,c,win,win]\n",
        "\n",
        "out = model(x)\n",
        "print(out.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d14bad2-7039-4855-b7e8-b698d6028835",
        "id": "ZAyKHKivc0j7",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MUattn 256\n",
            "torch.Size([2, 3, 16, 16, 16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title hiera vit me\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def zero_module(module):\n",
        "    for p in module.parameters():\n",
        "        p.detach().zero_()\n",
        "    return module\n",
        "\n",
        "class LayerNorm_at(nn.RMSNorm): # LayerNorm RMSNorm\n",
        "    def __init__(self, dim=1, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.dim=dim\n",
        "    def forward(self, x):\n",
        "        return super().forward(x.transpose(self.dim,-1)).transpose(self.dim,-1)\n",
        "\n",
        "def unshuffle(x, window_shape): # [b,c,h,w] -> [b, h/win1* w/win2, c, win1,win2]\n",
        "    new_shape = list(x.shape[:2]) + [val for xx, win in zip(list(x.shape[2:]), window_shape) for val in [xx//win, win]] # [h,w]->[h/win1, win1, w/win2, win2]\n",
        "    x = x.reshape(new_shape) # [b, c, h/win1, win1, w/win2, win2]\n",
        "    # print('unsh',x.shape, window_shape, new_shape)\n",
        "    L = len(new_shape)\n",
        "    permute = ([0] + list(range(2, L - 1, 2)) + [1] + list(range(3, L, 2))) # [0,2,4,1,3,5] / [0,2,4,6,1,3,5,6]\n",
        "    return x.permute(permute).flatten(1, L//2-1) # [b, h/win1* w/win2, c, win1,win2]\n",
        "\n",
        "class AttentionBlock(nn.Module):\n",
        "    # def __init__(self, d_model, n_heads, q_stride=None, mult=4, drop=0, nd=2):\n",
        "    def __init__(self, in_dim, d_model, n_heads, q_stride=None, mult=4, drop=0, nd=2):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        # self.norm = LayerNorm_at(2, d_model) # LayerNorm RMSNorm\n",
        "        self.norm = LayerNorm_at(2, in_dim) # LayerNorm RMSNorm\n",
        "        self.drop = nn.Dropout(drop)\n",
        "        # self.attn = MaskUnitAttention(d_model, n_heads, q_stride)\n",
        "        self.attn = MaskUnitAttention(in_dim, d_model, n_heads, q_stride)\n",
        "        ff_dim=d_model*mult\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.RMSNorm(d_model), nn.Linear(d_model, ff_dim), nn.ReLU(), # ReLU GELU\n",
        "            nn.RMSNorm(ff_dim), nn.Dropout(drop), nn.Linear(ff_dim, d_model)\n",
        "            # nn.RMSNorm(d_model), act, nn.Linear(d_model, ff_dim),\n",
        "            # nn.RMSNorm(ff_dim), act, nn.Linear(ff_dim, d_model)\n",
        "            # nn.RMSNorm(d_model), nn.Linear(d_model, ff_dim), nn.ReLU(), nn.Dropout(dropout), # ReLU GELU\n",
        "            # nn.Linear(ff_dim, d_model), nn.Dropout(dropout),\n",
        "        )\n",
        "        # self.res = conv_nd(nd, d_model, d_model, q_stride, q_stride) if q_stride else nn.Identity()\n",
        "\n",
        "        # self.res = nn.Sequential(\n",
        "        #     conv_nd(nd, in_dim, d_model, 1, 1) if in_dim!=d_model else nn.Identity(),\n",
        "        #     # maxpool_nd(nd, q_stride, q_stride) if q_stride else nn.Identity(),\n",
        "        #     avgpool_nd(nd, q_stride, q_stride) if q_stride else nn.Identity(),\n",
        "        # )\n",
        "        self.res = UpDownBlock(in_dim, d_model, kernel=1, r=1/2 if q_stride else 1)\n",
        "\n",
        "        # x = self.proj(x_norm).unflatten(1, (self.attn.q_stride, -1)).max(dim=1).values # pooling res # [b, (Sy, Sx, h/Sy, w/Sx), c] -> [b, (Sy, Sx), (h/Sy, w/Sx), c] -> [b, (h/Sy, w/Sx), c]\n",
        "\n",
        "\n",
        "    def forward(self, x): # [b, num_tok, c, *win]\n",
        "        b, num_tok, c, *win = x.shape\n",
        "        # print('attnblk fwd',x.shape)\n",
        "        # x = x + self.drop(self.self(self.norm(x)))\n",
        "        # print('attnblk fwd',self.res(x.flatten(0,1)).shape, self.drop(self.attn(self.norm(x))).flatten(0,1).shape)\n",
        "        # x = self.res(x.flatten(0,1)) + self.drop(self.attn(self.norm(x))).flatten(0,1) # [b*num_tok,c,win1,win2,win3]\n",
        "        x = self.res(x) + self.drop(self.attn(self.norm(x))) # [b*num_tok,c,win1,win2,win3]\n",
        "        # x = x + self.ff(x)\n",
        "        # x = x + self.ff(x.transpose(1,-1)).transpose(1,-1)\n",
        "        x = x + self.ff(x.transpose(2,-1)).transpose(2,-1)\n",
        "        # x = self.ff(x)\n",
        "        # x = x.unflatten(0, (b, num_tok))\n",
        "        return x\n",
        "\n",
        "    # def forward(self, x): # [b,t,c] # [b, (Sy, Sx, h/Sy, w/Sx), c]\n",
        "    #     # Attention + Q Pooling\n",
        "    #     x_norm = self.norm1(x)\n",
        "    #     if self.dim != self.dim_out:\n",
        "    #         x = self.proj(x_norm).unflatten(1, (self.attn.q_stride, -1)).max(dim=1).values # pooling res # [b, (Sy, Sx, h/Sy, w/Sx), c] -> [b, (Sy, Sx), (h/Sy, w/Sx), c] -> [b, (h/Sy, w/Sx), c]\n",
        "    #     x = x + self.drop_path(self.attn(x_norm))\n",
        "    #     x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
        "    #     return x\n",
        "\n",
        "\n",
        "class levelBlock(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim, n_heads=None, depth=1, r=1):\n",
        "        super().__init__()\n",
        "        self.seq = nn.Sequential(\n",
        "            UpDownBlock(in_ch, out_ch, r=min(1,r)) if in_ch != out_ch or r<1 else nn.Identity(),\n",
        "            # AttentionBlock(d_model, d_model, n_heads, q_stride) if in_ch != out_ch or r<1 else nn.Identity(),\n",
        "            # AttentionBlock(d_model, d_model, n_heads, q_stride=(2,2)),\n",
        "            *[AttentionBlock(d_model, d_model, n_heads) for i in range(1)],\n",
        "            # UpDownBlock(out_ch, out_ch, r=r) if r>1 else nn.Identity(),\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.seq(x)\n",
        "\n",
        "class SimpleViT(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim, d_model, n_heads, depth):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Sequential( # in, out, kernel, stride, pad\n",
        "            # nn.Conv2d(in_dim, d_model, 7, 2, 7//2, bias=False), nn.MaxPool2d(3,2,1), # nn.MaxPool2d(2,2)\n",
        "            nn.Conv2d(in_dim, d_model, 7, 1, 7//2, bias=False),\n",
        "            # UpDownBlock(in_dim, dim, r=1/2, kernel=3), UpDownBlock(dim, dim, r=1/2, kernel=3)\n",
        "            # nn.PixelUnshuffle(2), nn.Conv2d(in_dim*2**2, dim, 1, bias=False),\n",
        "            # ResBlock(in_dim, d_model, kernel),\n",
        "            )\n",
        "        # # self.pos_emb = LearnedRoPE2D(dim) # LearnedRoPE2D, RoPE2D\n",
        "        # self.pos_emb = nn.Parameter(torch.zeros(1, d_model, 32,32)) # positional_embedding == 'learnable'\n",
        "\n",
        "        emb_shape = (32,32)\n",
        "        # emb_shape = (8,32,32)\n",
        "        if len(emb_shape) == 3: # for video\n",
        "            pos_spatial = nn.Parameter(torch.randn(1, emb_shape[1]*emb_shape[2], d_model)*.02)\n",
        "            pos_temporal = nn.Parameter(torch.randn(1, emb_shape[0], d_model)*.02)\n",
        "            self.pos_emb = pos_spatial.repeat(1, emb_shape[0], 1) + torch.repeat_interleave(pos_temporal, emb_shape[1] * emb_shape[2], dim=1)\n",
        "        elif len(emb_shape) == 2: # for img\n",
        "            self.pos_emb = nn.Parameter(torch.randn(1, math.prod(emb_shape), d_model)*.02) # 56*56=3136\n",
        "\n",
        "\n",
        "        # self.blocks = nn.Sequential(*[AttentionBlock(d_model, n_heads, q_stride=(2,2) if i in [1,3] else None) for i in range(depth)])\n",
        "        # mult = [1,1,1,1]\n",
        "        mult = [1,2,4,8] # [1,2,3,4] [1,2,2,2]\n",
        "        ch_list = [d_model * m for m in mult] # [128, 256, 384, 512]\n",
        "\n",
        "        self.blocks = nn.Sequential(\n",
        "            # AttentionBlock(ch_list[0], ch_list[1], n_heads, q_stride=(2,2)),\n",
        "            # UpDownBlock(ch_list[0], ch_list[1], kernel=1, r=1/2),\n",
        "            UpDownBlock(ch_list[0], ch_list[1], kernel=4, r=1/2),\n",
        "            AttentionBlock(ch_list[1], ch_list[1], n_heads),\n",
        "            # AttentionBlock(ch_list[1], ch_list[2], n_heads, q_stride=(2,2)),\n",
        "            # UpDownBlock(ch_list[1], ch_list[2], kernel=1, r=1/2),\n",
        "            UpDownBlock(ch_list[1], ch_list[2], kernel=2, r=1/2),\n",
        "            AttentionBlock(ch_list[2], ch_list[2], n_heads),\n",
        "            )\n",
        "        self.attn_pool = nn.Linear(ch_list[2], 1)\n",
        "        self.out = nn.Linear(ch_list[2], out_dim, bias=False)\n",
        "\n",
        "    def forward(self, img): # [b,c,h,w]\n",
        "        x = self.embed(img)\n",
        "        # print('vit fwd', x.shape)\n",
        "        x = x + self.pos_emb\n",
        "        x = unshuffle(x, (4,4)) # [b,num_tok,c,win1,win2,win3] or [b,1,c,f,h,w]\n",
        "        # mask # [b, num_tok]\n",
        "        # x=x[:,:7]\n",
        "        # print('vit fwd1', x.shape)\n",
        "\n",
        "        x = self.blocks(x) # [b,num_tok,c,1,1,1]\n",
        "        # print('vit fwd2', x.shape)\n",
        "        x = x.squeeze() # [b,num_tok,d]\n",
        "        attn = self.attn_pool(x).squeeze(-1) # [batch, (h,w)] # seq_pool\n",
        "        x = (attn.softmax(dim=1).unsqueeze(1) @ x).squeeze(1) # [batch, 1, (h,w)] @ [batch, (h,w), dim] -> [batch, dim]\n",
        "        return self.out(x)\n",
        "\n",
        "\n",
        "# pos_emb rope < learn < learned\n",
        "# conv > pixel?\n",
        "# droppath not required\n",
        "\n",
        "# norm,act,conv < conv,norm,act\n",
        "# 2*s1 < uib < resblock\n",
        "# gatedadaln 3 < 2 = 1 < ffmult4 = 2*gatedadaln\n",
        "# MaxPool2d(2,2) < MaxPool2d(3,2,3//2)\n",
        "\n",
        "# patchattn only for\n",
        "\n",
        "# multiendfusion negligible diff\n",
        "\n",
        "d_model = 64\n",
        "dim_head = 8\n",
        "heads = d_model // dim_head\n",
        "num_classes = 10\n",
        "# model = SimpleViT(image_size=32, patch_size=4, num_classes=num_classes, dim=dim, depth=1, heads=heads, mlp_dim=dim*4, channels = 3, dim_head = dim_head)\n",
        "model = SimpleViT(in_dim=3, out_dim=num_classes, d_model=d_model, depth=5, n_heads=heads).to(device)\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 59850\n",
        "optim = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "\n",
        "# print(images.shape) # [batch, 3, 32, 32]\n",
        "x = torch.rand(25, 3, 32, 32, device=device)\n",
        "# x = torch.rand(64, 3, 28,28, device=device)\n",
        "logits = model(x)\n",
        "print(logits.shape)\n",
        "# print(logits[0])\n",
        "# print(logits[0].argmax(1))\n",
        "pred_probab = nn.Softmax(dim=1)(logits)\n",
        "y_pred = pred_probab.argmax(1)\n",
        "# print(f\"Predicted class: {y_pred}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "lec1Nq7nkbgt",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Gated Linear Unit\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def zero_module(module):\n",
        "    for p in module.parameters():\n",
        "        p.detach().zero_()\n",
        "    return module\n",
        "\n",
        "class GLU(nn.Module): # https://arxiv.org/pdf/2002.05202\n",
        "    def __init__(self, in_dim, d_model):\n",
        "        super().__init__()\n",
        "        self.lin0 = nn.Sequential(\n",
        "            nn.SiLU(), zero_module(nn.Linear(in_dim, 2*d_model))\n",
        "            nn.LayerNorm(in_dim), nn.SiLU(), zero_module(nn.Linear(in_dim, 2*d_model, bias=False))\n",
        "            nn.LayerNorm(in_dim), zero_module(nn.Linear(in_dim, 2*d_model, bias=False))\n",
        "            zero_module(nn.Linear(in_dim, 2*d_model, bias=False))\n",
        "        )\n",
        "        self.lin1 = nn.Sequential(\n",
        "            nn.SiLU(), zero_module(nn.Linear(d_model, in_dim))\n",
        "            zero_module(nn.Linear(d_model, in_dim, bias=False))\n",
        "        )\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x): # [b,t,d]\n",
        "        x0, x1 = self.lin0(x).chunk(2, dim=-1)\n",
        "        x = x + x0 * self.norm(x) + x1 # AdaLN\n",
        "        # x0, x1, x2 = self.lin0(x).chunk(3, dim=-1)\n",
        "        # x = x + x0 * x1 # Bilinear\n",
        "        # x = x + x0 * x1 + x2\n",
        "        # x = x + x0.exp() * x1 + x2\n",
        "\n",
        "        # x = self.lin1(x0*F.sigmoid(x1)) # GLU\n",
        "        # x = self.lin1(x0*F.gelu(x1)) # GEGLU\n",
        "        # x = self.lin1(x0*F.silu(x1)) # SwiGLU\n",
        "        # x = self.lin1(x0*F.relu(x1)) # ReGLU\n",
        "        # x = self.lin1(x0*x1.exp()) #\n",
        "        # x = self.lin1(x0*x1) # Bilinear\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "class SwiGLU(nn.Module): # https://arxiv.org/pdf/2002.05202\n",
        "    def __init__(self, in_dim, d_model):\n",
        "        super().__init__()\n",
        "        self.lin0 = zero_module(nn.Linear(in_dim, 2*d_model, bias=False))\n",
        "        self.lin1 = zero_module(nn.Linear(d_model, in_dim, bias=False))\n",
        "\n",
        "    def forward(self, x): # [b,t,d]\n",
        "        x0, x1 = self.lin0(x).chunk(2, dim=-1)\n",
        "        return self.lin1(x0*F.silu(x1)) # SwiGLU\n",
        "\n",
        "\n",
        "class AGeLU(nn.Module): # https://openreview.net/pdf?id=I8pdQLfR77\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.act = nn.GELU() # GELU SiLU\n",
        "        self.coef = nn.Parameter(torch.randn(4))\n",
        "\n",
        "    def forward(self, x): # [b,t,d]\n",
        "        return self.coef[0] * self.act(self.coef[1] * x + self.coef[2]) + self.coef[3]\n",
        "\n",
        "\n",
        "class AMLP(nn.Module):\n",
        "    def __init__(self, d_model):\n",
        "        super().__init__()\n",
        "        self.lin0 = nn.Sequential(\n",
        "            nn.SiLU(), nn.Linear(d_model, 2*d_model)\n",
        "        )\n",
        "        self.lin1 = nn.Sequential(\n",
        "            zero_module(nn.Linear(2*d_model, d_model))\n",
        "        )\n",
        "        self.act = nn.GELU() # GELU SiLU\n",
        "        self.agelu0, self.agelu1 = AGeLU(), AGeLU()\n",
        "\n",
        "    def forward(self, x): # [b,t,d]\n",
        "        x0, x1 = self.lin0(x).chunk(2, dim=-1) # for btd\n",
        "        x = torch.cat([self.agelu0(x0), self.agelu1(x1)], dim=-1)\n",
        "        return self.lin1(x)\n",
        "\n",
        "dim = 64\n",
        "# model = GLU(dim, int(3.5*dim))\n",
        "model = AMLP(dim)\n",
        "\n",
        "x = torch.randn(2, dim)\n",
        "out = model(x)\n",
        "print(out.shape)  # Should be (32, 64)\n"
      ],
      "metadata": {
        "id": "rdkxgQD3Sq4A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ViT me more\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def zero_module(module):\n",
        "    for p in module.parameters():\n",
        "        p.detach().zero_()\n",
        "    return module\n",
        "\n",
        "class SelfAttn(nn.Module):\n",
        "    def __init__(self, dim, n_heads):\n",
        "        super().__init__()\n",
        "        self.dim, self.heads = dim, n_heads\n",
        "        d_head = dim//n_heads\n",
        "        self.qkv = nn.Linear(dim, dim*3, bias=False)\n",
        "        # self.lin = nn.Linear(dim, dim)\n",
        "        self.lin = zero_module(nn.Linear(dim, dim))\n",
        "        # self.rope = RoPE(d_head, seq_len=512, base=10000)\n",
        "        # # self.rope = RoPE2D(d_head, h=64, w=64, base=100)\n",
        "        self.scale = d_head**-.5\n",
        "\n",
        "    def forward(self, x): # [b,t,d]\n",
        "        # print('satt',x.shape)\n",
        "        q,k,v = self.qkv(x).unflatten(-1, (self.heads,-1)).transpose(-3,-2).chunk(3, dim=-1) # [b, r^2, h/r*w/r, dim] # [b*r^2, h/r*w/r, n_heads, d_head]?\n",
        "        # q, k = self.rope(q), self.rope(k)\n",
        "\n",
        "        q, k = q.softmax(dim=-1)*self.scale, k.softmax(dim=-2)\n",
        "        context = k.transpose(-2,-1) @ v # [batch, n_heads, d_head, d_head]\n",
        "        x = q @ context # [batch, n_heads, T/num_tok, d_head]\n",
        "\n",
        "        # print('satt',x.shape)\n",
        "        x = x.transpose(1,2).flatten(2)\n",
        "        # print('satt',x.shape)\n",
        "        return self.lin(x)\n",
        "\n",
        "\n",
        "class AttentionBlock(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, drop=0):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.norm = nn.RMSNorm(d_model) # LayerNorm RMSNorm\n",
        "        self.drop = nn.Dropout(drop)\n",
        "        self.attn = SelfAttn(d_model, n_heads) # 16448\n",
        "        # self.attn = GLAblock(hidden_size=d_model, expand_k=1, expand_v=1, num_heads=n_heads)\n",
        "        # self.self = Pooling()\n",
        "        # act = nn.GELU() # ReLU GELU\n",
        "        # self.ff = nn.Sequential(\n",
        "        #     *[nn.BatchNorm2d(d_model), act, SeparableConv2d(d_model, d_model),]*3\n",
        "        #     )\n",
        "        # self.ff = ResBlock(d_model) # 74112\n",
        "        # self.ff = UIB(d_model, mult=4) # uib m4 36992, m2 18944\n",
        "        self.ff = GLU(d_model, int(2*d_model)) # 3.5\n",
        "        ff_dim=d_model*4#mult\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.RMSNorm(d_model), nn.Linear(d_model, ff_dim), nn.ReLU(), # ReLU GELU\n",
        "            nn.RMSNorm(ff_dim), nn.Dropout(drop), nn.Linear(ff_dim, d_model)\n",
        "            # nn.RMSNorm(d_model), act, nn.Linear(d_model, ff_dim),\n",
        "            # nn.RMSNorm(ff_dim), act, nn.Linear(ff_dim, d_model)\n",
        "            # nn.RMSNorm(d_model), nn.Linear(d_model, ff_dim), nn.ReLU(), nn.Dropout(dropout), # ReLU GELU\n",
        "            # nn.Linear(ff_dim, d_model), nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x, mask=None): # [b,c,h,w], [batch, num_tok, cond_dim], [batch,T]\n",
        "        bchw = x.shape\n",
        "        x = x.flatten(2).transpose(1,2) # [b,h*w,c]\n",
        "        # print('attnblk fwd',x.shape)\n",
        "\n",
        "        x = x + self.drop(self.attn(self.norm(x)))\n",
        "        x = x + self.ff(x)\n",
        "        x = x.transpose(1,2).reshape(*bchw)\n",
        "        # x = self.ff(x)\n",
        "        # x = x + self.drop(self.norm2(self.ff(x)))\n",
        "        return x\n",
        "\n",
        "\n",
        "class SimpleViT(nn.Module):\n",
        "    def __init__(self, in_dim, d_model, out_dim=None, n_heads=4, nlyrs=1):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Sequential( # in, out, kernel, stride, pad\n",
        "            nn.Conv2d(in_dim, d_model, 7, 2, 7//2, bias=False), nn.MaxPool2d(3,2,1), # nn.MaxPool2d(2,2)\n",
        "            # nn.Conv2d(in_dim, d_model,3,2,3//2), nn.BatchNorm1d(d_model), nn.ReLU(), nn.MaxPool1d(2,2),\n",
        "            # nn.Conv2d(d_model, d_model,3,2,3//2),\n",
        "            # UpDownBlock(in_dim, d_model, r=1/2, kernel=3), UpDownBlock(d_model, d_model, r=1/2, kernel=3)\n",
        "            # nn.PixelUnshuffle(2), nn.Conv2d(in_dim*2**2, d_model, 1, bias=False),\n",
        "            )\n",
        "        # self.pos_emb = LearnedRoPE2D(dim) # LearnedRoPE2D, RoPE2D\n",
        "        self.pos_emb = nn.Parameter(torch.zeros(1, dim, 8,8)) # positional_embedding == 'learnable'\n",
        "        # self.transformer = nn.Sequential(*[AttentionBlock(dim, n_heads) for _ in range(nlyrs)])\n",
        "        self.transformer = nn.ModuleList([AttentionBlock(dim, n_heads) for i in range(2)])\n",
        "\n",
        "        self.attn_pool = nn.Linear(d_model, 1)\n",
        "        self.out = nn.Linear(d_model, out_dim or d_model, bias=False)\n",
        "\n",
        "    def forward(self, img, mask=None):\n",
        "        # device = img.device\n",
        "        x = self.embed(img)\n",
        "        # x = self.pos_emb(x)\n",
        "        bchw = x.shape\n",
        "        x = x + self.pos_emb\n",
        "        # for blk in self.transformer: x = blk(x)\n",
        "        # x = self.transformer(x)\n",
        "        x = self.transformer[0](x) + self.transformer[1](x)\n",
        "        x = x.flatten(2).transpose(1,2) # [b,h*w,c]\n",
        "        attn = self.attn_pool(x).squeeze(-1) # [batch, (h,w)] # seq_pool\n",
        "        x = (attn.softmax(dim=1).unsqueeze(1) @ x).squeeze(1) # [batch, 1, (h,w)] @ [batch, (h,w), dim] -> [batch, dim]\n",
        "        return self.out(x)\n",
        "\n",
        "\n",
        "# pos_emb rope < learn < learned\n",
        "# conv > pixel?\n",
        "# droppath not required\n",
        "\n",
        "# norm,act,conv < conv,norm,act\n",
        "# 2*s1 < uib < resblock\n",
        "# gatedadaln 3 < 2 = 1 < ffmult4 = 2*gatedadaln\n",
        "# MaxPool2d(2,2) < MaxPool2d(3,2,3//2)\n",
        "\n",
        "# 32^2->8^2=64\n",
        "\n",
        "# fla 4lyr 234449params 22.56sec\n",
        "# selfattn 4lyr 213889\n",
        "\n",
        "dim = 64#64\n",
        "in_dim=3\n",
        "out_dim = 10\n",
        "# model = SimpleViT(image_size=32, patch_size=4, num_classes=num_classes, dim=dim, depth=1, heads=heads, mlp_dim=dim*4, channels = 3, dim_head = dim_head)\n",
        "model = SimpleViT(in_dim, 64, out_dim, nlyrs=1, n_heads=4).to(device)\n",
        "model = SimpleViT(in_dim, 32, out_dim, nlyrs=1, n_heads=2).to(device)\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 59850\n",
        "print(sum(p.numel() for p in model.transformer[0].attn.parameters() if p.requires_grad)) # 59850\n",
        "print(sum(p.numel() for p in model.transformer[0].ff.parameters() if p.requires_grad)) # 59850\n",
        "optim = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "\n",
        "# print(images.shape) # [batch, 3, 32, 32]\n",
        "x = torch.rand(24, 3, 32,32, device=device)\n",
        "# x = torch.rand(64, 3, 28,28, device=device)\n",
        "logits = model(x)\n",
        "print(logits.shape)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e773612-25fc-47a0-ba14-bd0419ce339d",
        "id": "M-zdjdJixtOu"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "213889\n",
            "torch.Size([24, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(sum(p.numel() for p in model.transformer_blocks[0].self.parameters() if p.requires_grad)) # 59850\n",
        "print(sum(p.numel() for p in model.transformer_blocks[0].ff.parameters() if p.requires_grad)) # 59850\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1hq7DQhft6l",
        "outputId": "b9c448ef-6efc-4b5b-f87c-bde0bcc40479"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16448\n",
            "18944\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "2Nd-sGe6Ku4S",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "outputId": "ad8040f1-adc8-4dc2-b67c-c0ab369fc627"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbobdole\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.9"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250412_130900-n9b510br</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/bobdole/vit/runs/n9b510br' target=\"_blank\">copper-thunder-72</a></strong> to <a href='https://wandb.ai/bobdole/vit' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/bobdole/vit' target=\"_blank\">https://wandb.ai/bobdole/vit</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/bobdole/vit/runs/n9b510br' target=\"_blank\">https://wandb.ai/bobdole/vit/runs/n9b510br</a>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# @title wandb\n",
        "!pip install -q wandb\n",
        "import wandb # https://docs.wandb.ai/quickstart\n",
        "wandb.login(key='487a2109e55dce4e13fc70681781de9f50f27be7')\n",
        "try: run.finish()\n",
        "except NameError: pass\n",
        "run = wandb.init(project=\"vit\", config={\"model\": \"res18\",})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fsealXK3OPQa",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9271fee8-e290-46e7-f045-eae750283599"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.355891  [    0/50000]\n",
            "loss: 2.305916  [ 4992/50000]\n",
            "loss: 2.321788  [ 9984/50000]\n",
            "loss: 2.261930  [14976/50000]\n",
            "loss: 2.274566  [19968/50000]\n",
            "loss: 2.172393  [24960/50000]\n",
            "loss: 2.116346  [29952/50000]\n",
            "loss: 2.008737  [34944/50000]\n",
            "loss: 1.983347  [39936/50000]\n",
            "loss: 2.011741  [44928/50000]\n",
            "Test Error: \n",
            " Accuracy: 25.5%, Avg loss: 1.959480 \n",
            "\n",
            "time: 16.358219861984253 16.35822081565857\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 2.155640  [    0/50000]\n",
            "loss: 1.917851  [ 4992/50000]\n",
            "loss: 2.031436  [ 9984/50000]\n",
            "loss: 1.979323  [14976/50000]\n",
            "loss: 1.898517  [19968/50000]\n",
            "loss: 2.331604  [24960/50000]\n",
            "loss: 2.288532  [29952/50000]\n",
            "loss: 2.258516  [34944/50000]\n",
            "loss: 2.237510  [39936/50000]\n",
            "loss: 2.207013  [44928/50000]\n",
            "Test Error: \n",
            " Accuracy: 18.3%, Avg loss: 2.200488 \n",
            "\n",
            "time: 31.57065463066101 15.785327672958374\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 2.222084  [    0/50000]\n",
            "loss: 2.162521  [ 4992/50000]\n",
            "loss: 2.242355  [ 9984/50000]\n",
            "loss: 2.188580  [14976/50000]\n",
            "loss: 2.201468  [19968/50000]\n",
            "loss: 2.156187  [24960/50000]\n",
            "loss: 2.193287  [29952/50000]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-4e043cbbe779>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {t+1}\\n-------------------------------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time:'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-4e043cbbe779>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(dataloader, model, loss_fn, optim)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0msx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# reset gradients of model parameters, to prevent double-counting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-5fff6b2bbb9b>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, img, mask)\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpositional_emb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;31m# for blk in self.transformer_blocks: x = blk(x)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# [b,h*w,c]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0mattn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn_pool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# [batch, (h,w)] # seq_pool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-5fff6b2bbb9b>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;31m# print('attnblk fwd',x.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbchw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-5fff6b2bbb9b>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# [b,t,d]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m# print('satt',x.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqkv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# [b, r^2, h/r*w/r, dim] # [b*r^2, h/r*w/r, n_heads, d_head]?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;31m# q, k = self.rope(q), self.rope(k)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1913\u001b[0m     \u001b[0;31m# on `torch.nn.Module` and all its subclasses is largely disabled as a result. See:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1914\u001b[0m     \u001b[0;31m# https://github.com/pytorch/pytorch/pull/115074\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1915\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Module\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1916\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"_parameters\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1917\u001b[0m             \u001b[0m_parameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_parameters\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# @title train test function\n",
        "\n",
        "def train(dataloader, model, loss_fn, optim):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, (x, y) in enumerate(dataloader):\n",
        "        sx, sy = x.to(device), y.to(device)\n",
        "        pred = model(sx)\n",
        "        loss = loss_fn(pred, sy)\n",
        "        optim.zero_grad() # reset gradients of model parameters, to prevent double-counting\n",
        "        loss.backward() # Backpropagate gradients\n",
        "        optim.step() # adjust the parameters by the gradients\n",
        "        if (batch) % (size//(10* len(x))) == 0:\n",
        "            loss, current = loss.item(), batch * len(x)\n",
        "            # loss_list.append(loss)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "        try: wandb.log({\"loss\": loss.item()})\n",
        "        except: pass\n",
        "\n",
        "def test(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    for X, y in dataloader:\n",
        "        x, y = X.to(device), y.to(device)\n",
        "        with torch.no_grad():\n",
        "            pred = model(x)\n",
        "        # loss = loss_fn(pred, y)\n",
        "        test_loss += loss_fn(pred, y).item()\n",
        "        correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "    return correct\n",
        "\n",
        "import time\n",
        "start = time.time()\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "for t in range(10):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(train_loader, model, loss_fn, optim)\n",
        "    test(test_loader, model, loss_fn)\n",
        "    print('time:',time.time() - start, (time.time()-start)/(t+1))\n",
        "    end = time.time()\n",
        "print(\"Done!\")\n",
        "\n",
        "end = time.time()\n",
        "print(\"time: \",end - start)\n",
        "\n",
        "torch.save(model.state_dict(), \"model.pth\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gBVIQsv4E2XB"
      },
      "outputs": [],
      "source": [
        "# @title save/load\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "folder='/content/drive/MyDrive/jepa/'\n",
        "\n",
        "modelsd, optimsd = torch.load(folder+'vit.pkl', map_location=device).values()\n",
        "model.load_state_dict(modelsd, strict=False)\n",
        "optim.load_state_dict(optimsd)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aC1TLfccE3W5"
      },
      "outputs": [],
      "source": [
        "checkpoint = {'model': model.state_dict(), 'optimizer': optim.state_dict()}\n",
        "# torch.save(checkpoint, folder+'vit.pkl')\n",
        "torch.save(checkpoint, 'cct.pkl')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## drawer"
      ],
      "metadata": {
        "id": "VeMRpak44QHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Shortcut\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def make_pool_at(pool='avg', dim=1, output_size=5):\n",
        "    parent={'avg':nn.AdaptiveAvgPool1d, 'max':nn.AdaptiveMaxPool1d}[pool]\n",
        "    class AdaptivePool_at(parent): # AdaptiveAvgPool1d AdaptiveMaxPool1d\n",
        "        def __init__(self, dim=1, *args, **kwargs):\n",
        "            super().__init__(*args, **kwargs)\n",
        "            self.dim=dim\n",
        "        def forward(self, x):\n",
        "            x = x.transpose(self.dim,-1)\n",
        "            shape = x.shape\n",
        "            return super().forward(x.flatten(0,-2)).unflatten(0, shape[:-1]).transpose(self.dim,-1)\n",
        "    return AdaptivePool_at(dim, output_size=output_size)\n",
        "\n",
        "\n",
        "# class Shortcut(nn.Module):\n",
        "class Shortcut():\n",
        "    def __init__(self, dim=1, c=3, sp=(3,3), nd=2):\n",
        "        # super().__init__()\n",
        "        self.dim = dim\n",
        "        self.ch_pool = make_pool_at(pool='avg', dim=dim, output_size=c)\n",
        "        self.ch_pool = make_pool_at(pool='max', dim=dim, output_size=c)\n",
        "        # self.ch_pool = ZeroExtend(dim, output_size=c) # only for out_dim>=in_dim\n",
        "        self.sp_pool = AdaptiveAvgPool_nd(nd, sp)\n",
        "        # self.sp_pool = AdaptiveMaxPool_nd(nd, sp)\n",
        "\n",
        "    # def forward(self, x): # [b,c,h,w]\n",
        "    def __call__(self, x): # [b,c,h,w]\n",
        "        x = self.ch_pool(x)\n",
        "        print('aft ch',x)\n",
        "        x = self.sp_pool(x)\n",
        "        return x\n",
        "\n",
        "class Shortcut2(Shortcut):\n",
        "    def __init__(self, dim=1, c=5, sp=(4,4), nd=2):\n",
        "        super().__init__()\n",
        "    def __call__(self, x): # [b,c,h,w]\n",
        "        x = self.sp_pool(x)\n",
        "        print('aft sp',x)\n",
        "        x = self.ch_pool(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# mmT identical, close to AdaptiveAvgPool3d\n",
        "# maF diff; down is identical # max can copy adjacent channels\n",
        "# aaF close\n",
        "# amFF diff\n",
        "\n",
        "# shortcut = F.interpolate(x.unsqueeze(1), size=out.shape[1:], mode='nearest-exact').squeeze(1) # pytorch.org/docs/stable/generated/torch.nn.functional.interpolate.html\n",
        "# shortcut = F.adaptive_avg_pool3d(x, out.shape[1:]) # https://pytorch.org/docs/stable/nn.html#pooling-layers\n",
        "# shortcut = F.adaptive_max_pool3d(x, out.shape[1:]) # https://pytorch.org/docs/stable/nn.html#pooling-layers\n",
        "# pool3 = nn.AdaptiveAvgPool3d((3,3,3))\n",
        "\n",
        "pool1 = Shortcut()\n",
        "pool2 = Shortcut2()\n",
        "x0 = torch.rand(1, 2, 4,4)\n",
        "x1 = torch.rand(1, 2, 2,2)\n",
        "x2 = torch.rand(1, 4, 4,4)\n",
        "x3 = torch.rand(1, 4, 2,2)\n",
        "print(x2)\n",
        "# for x in [x0,x1,x2,x3]:\n",
        "for x in [x2]:\n",
        "    out1 = pool1(x)\n",
        "    out2 = pool2(x)\n",
        "    # out3 = pool3(x)\n",
        "    # # print(out.shape)\n",
        "    # print((out1==out2).all())\n",
        "    print(out1-out2)\n",
        "\n",
        "# print(out1)\n",
        "# print(out2)\n",
        "# print(out3)\n",
        "print(out1-out2)\n",
        "print(out1-out3)\n",
        "print(out2-out3)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "SYE33NCnHGpK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title maxpool path bchw\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from timm.models.layers import DropPath, Mlp\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def conv_nd(n, *args, **kwargs): return [nn.Identity, nn.Conv1d, nn.Conv2d, nn.Conv3d][n](*args, **kwargs)\n",
        "def maxpool_nd(n, *args, **kwargs): return [nn.Identity, nn.MaxPool1d, nn.MaxPool2d, nn.MaxPool3d][n](*args, **kwargs)\n",
        "\n",
        "class MaskUnitAttention(nn.Module):\n",
        "    def __init__(self, d_model=16, n_heads=4, q_stride=None):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_heads, self.d_head = n_heads, d_model // n_heads\n",
        "        self.scale = self.d_head**-.5\n",
        "        nd = 2\n",
        "        self.qkv = conv_nd(nd, d_model, 3*d_model, 1)\n",
        "        self.out = conv_nd(nd, d_model, d_model, 1)\n",
        "        self.q_stride = q_stride # If greater than 1, pool q with this stride. The stride should be flattened (e.g., 2x2 = 4).\n",
        "        if q_stride:\n",
        "            self.q_stride = (q_stride,)*nd if type(q_stride)==int else q_stride\n",
        "            self.q_pool = maxpool_nd(nd, self.q_stride, stride=self.q_stride)\n",
        "\n",
        "    def forward(self, x): # [b,c,num_tok,win1,win2,win3] [b,num_tok,c,win1,win2,win3]\n",
        "        b, num_tok, c, *win = x.shape\n",
        "        # x = x.transpose(1,2).flatten(0,1) # [b*num_tok,c,win1,win2,win3]\n",
        "        x = x.flatten(0,1) # [b*num_tok,c,win1,win2,win3]\n",
        "        print(x.shape)\n",
        "        # q, k, v = self.qkv(x).reshape(b, 3, self.n_heads, self.d_head, num_tok, win*win).permute(1,0,2,4,5,3) # [b,3*d_model,num_tok*win,win] -> 3* [b, n_heads, num_tok, win*win, d_head]\n",
        "        # self.qkv(x).flatten(1,-2).unflatten(-1, (self.heads,-1)).chunk(3, dim=-1) # [b,sp,n_heads,d_head]\n",
        "        q,k,v = self.qkv(x).chunk(3, dim=1) # [b,d,num_tok*win,win]\n",
        "        if self.q_stride:\n",
        "            q = self.q_pool(q)\n",
        "            win=[w//s for w,s in zip(win, self.q_stride)] # win = win//self.q_stride\n",
        "        # q, k, v = map(lambda t: t.reshape(b, self.n_heads, self.d_head, num_tok, -1).permute(0,1,3,4,2), (q,k,v)) # [b,d,num_tok*win(/pool),win(/pool)] -> [b, n_heads, num_tok, win*win, d_head]\n",
        "        q, k, v = map(lambda t: t.reshape(b*num_tok, self.n_heads, self.d_head, -1).transpose(-2,-1), (q,k,v)) # [b,d,num_tok*win(/pool),win(/pool)] -> [b, n_heads, num_tok, win*win, d_head]\n",
        "\n",
        "        q, k = q.softmax(dim=-1)*self.scale, k.softmax(dim=-2) # [b, n_heads, t(/pool), d_head]\n",
        "        context = k.transpose(-2,-1) @ v # [b, n_heads, d_head, d_head]\n",
        "        x = q @ context # [b, n_heads, t(/pool), d_head]\n",
        "\n",
        "        # # x = F.scaled_dot_product_attention(q, k, v)\n",
        "        # attn = (q * self.scale) @ k.transpose(-2,-1) # [b, n_heads, num_windows, window_size, d_head] @ [b, n_heads, num_windows, d_head, q_stride*window_size] = [b, n_heads, num_windows, window_size, q_stride*window_size]\n",
        "        # x = attn.softmax(dim=-1) @ v # [b, n_heads, num_windows, window_size, d_head]\n",
        "        print('attn fwd 2',x.shape)\n",
        "        # x = x.transpose(1, 3).reshape(B, -1, self.d_model)\n",
        "        # x = x.transpose(1, 2).reshape(x.shape[0], -1, self.d_model) # [b,t,d]\n",
        "        # x = x.transpose(-2,-1).reshape(x.shape[0], self.d_model, ) # [b,t,d]\n",
        "        # [b, n_heads, num_tok, win*win, d_head] -> [b, n_heads, d_head, num_tok, win*win] -> [b,c,num_tok*win,win]\n",
        "        # x = x.permute(0,1,4,2,3).reshape(b, self.d_model, num_tok*win,win) # [b, n_heads, num_tok, win*win, d_head]\n",
        "        x = x.transpose(-2,-1).reshape(b*num_tok, self.d_model, *win) # [b*num_tok, n_heads, win*win, d_head]\n",
        "        x = self.out(x)\n",
        "        x = x.unflatten(0, (b, num_tok))\n",
        "        return x\n",
        "\n",
        "d_model=16\n",
        "model = MaskUnitAttention(d_model, n_heads=4, q_stride=2)\n",
        "# x=torch.randn(2,4,4,3)\n",
        "x=torch.randn(2,3,d_model,8,8)\n",
        "# [b*num_tok,c,win,win]\n",
        "\n",
        "out = model(x)\n",
        "print(out.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06077b51-5549-4a40-bc83-a41930912701",
        "cellView": "form",
        "id": "d98WYm1bZJsM"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([6, 16, 8, 8])\n",
            "attn fwd 2 torch.Size([6, 4, 16, 4])\n",
            "torch.Size([2, 3, 16, 4, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title facebookresearch hiera_utils.py\n",
        "# https://github.com/facebookresearch/hiera/blob/main/hiera/hiera_utils.py\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def conv_nd(n, *args, **kwargs): return [nn.Identity, nn.Conv1d, nn.Conv2d, nn.Conv3d][n](*args, **kwargs)\n",
        "\n",
        "# [B, (H, W), C] -> [B, (Sy, Sx, H // Sy, W // Sx), C]; stride (Sy, Sx)\n",
        "def Unroll(x, tokens_spatial_shape, unroll_schedule): # [56, 56] [(2, 2), (2, 2), (2, 2)]\n",
        "    B, _, C = shape = x.shape\n",
        "    x = x.view(B, *tokens_spatial_shape, C)\n",
        "    # print('unroll', x.shape, tokens_spatial_shape, unroll_schedule)\n",
        "    for strides in unroll_schedule: # q_stride (2, 2)\n",
        "        tokens_spatial_shape = [i//s for i, s in zip(tokens_spatial_shape, strides)] # [28, 28], [14, 14], [7, 7]\n",
        "        new_shape = [B] + [val for tup in zip(tokens_spatial_shape, strides) for val in tup] + [C]\n",
        "        x = x.view(new_shape) # 2d: [B, H/Sy, Sy, W/Sx, Sx, C] # [24, 28, 2, 28, 2, 16], [96, 14, 2, 14, 2, 16], [384, 7, 2, 7, 2, 16]\n",
        "        L = len(new_shape)\n",
        "        permute = ([0] + list(range(2, L - 1, 2)) + list(range(1, L - 1, 2)) + [L - 1]) # [0, 2, 4, 1, 3, 5] / [0, 2, 4, 6, 1, 3, 5, 7]\n",
        "        x = x.permute(permute).flatten(0, len(strides)) # 2d: [B, Sy, Sx, H/Sy, W/Sx, C] -> [B*Sy*Sx, H/Sy, W/Sx, C]\n",
        "        B *= math.prod(strides)\n",
        "    # print('unroll out', x.shape)\n",
        "    x = x.reshape(*shape) # 2d: [B*Sy*Sx, H/Sy* W/Sx, C] # [24, 3136, 16]\n",
        "    return x # then x.max(1) == MaxPoolNd\n",
        "\n",
        "\n",
        "\n",
        "# stages=(2, 2, 3, 2)\n",
        "# input_size = (224, 224)\n",
        "# q_stride = (2, 2)\n",
        "# patch_stride = (4, 4)\n",
        "\n",
        "# x = torch.rand(24, 16, 56, 56)\n",
        "# x = x.flatten(2).transpose(1, 2) # [b,t,c]\n",
        "# stage_ends = [sum(stages[:i]) - 1 for i in range(1, len(stages) + 1)]\n",
        "# tokens_spatial_shape = [i//s for i, s in zip(input_size, patch_stride)] # list [224/4, 224/4] = [56, 56]\n",
        "# x = Unroll(x, tokens_spatial_shape, [q_stride]*len(stage_ends[:-1]))\n",
        "# print(x.shape)\n",
        "# print(x)\n",
        "\n",
        "\n",
        "# [24, 49, 8, 8, 16] [56, 56] [8, 8] -> [24, 56, 56, 16]\n",
        "# [24, 49, 4, 4, 32] [28, 28] [4, 4] -> [24, 28, 28, 16]\n",
        "# [24, 49, 2, 2, 64] [14, 14] [2, 2] -> [24, 14, 14, 16]\n",
        "# [24, 49, 1, 1, 128] [7, 7] [1, 1] -> [24, 7, 7, 16]\n",
        "# [B, #MUy*#MUx, MUy, MUx, C] -> [B, #MUy*MUy, #MUx*MUx, C]\n",
        "def undo_windowing(x, shape, mu_shape): # x [B, #MUy*#MUx, MUy, MUx, C] # shape: desired spatial shape. 2d: [B, #MUy*MUy, #MUx*MUx, C] # mu_shape: current mask unit shape. 2d: [MUy, MUx]\n",
        "    # print(\"undo_windowing\", x.shape, shape, mu_shape)\n",
        "    B, C = x.shape[0], x.shape[-1]\n",
        "    num_MUs = [s//mu for s, mu in zip(shape, mu_shape)] # [7,7]\n",
        "    x = x.view(B, *num_MUs, *mu_shape, C) # [B, #MUy*#MUx, MUy, MUx, C] -> [B, #MUy, #MUx, MUy, MUx, C]\n",
        "    D = len(shape)\n",
        "    # permute = ([0] + sum([list(p) for p in zip(range(1, 1 + D), range(1 + D, 1 + 2 * D))], []) + [len(x.shape) - 1]) # [0, 1, 3, 2, 4, 5]\n",
        "    permute = ([0] + [val for tup in zip(range(1, 1+D), range(1+D, 1+2*D)) for val in tup] + [len(x.shape)-1]) # [0, 1, 3, 2, 4, 5] / [0, 1, 4, 2, 5, 3, 6, 7]\n",
        "    x = x.permute(permute).reshape(B, *shape, C) # [B, #MUy, #MUx, MUy, MUx, C] -> [B, #MUy*MUy, #MUx*MUx, C]\n",
        "    return x # [B, #MUy*MUy, #MUx*MUx, C]\n",
        "\n",
        "# ss=2 # 8 4 2 1\n",
        "# x = torch.rand(24, 49, 2**ss,2**ss, 16)\n",
        "# shape, mu_shape = [7*2**ss, 7*2**ss], [2**ss, 2**ss]\n",
        "# o = undo_windowing(x, shape, mu_shape)\n",
        "# print(o.shape) # [24, 56, 56, 16]\n",
        "\n",
        "# [q_stride] * len(self.stage_ends[:-1]), self.stage_ends, q_pool\n",
        "\n",
        "def Reroll(x, size, schedule, mask=None): # [24, 256, 16], [56, 56], list[(2, 2), (2, 2), (2, 2)]\n",
        "    # print('Reroll', schedule, size)\n",
        "    B, N, C = x.shape\n",
        "    # print('in', x.shape)\n",
        "    D = len(size)\n",
        "    cur_mu_shape = [1] * D\n",
        "    for strides in schedule:\n",
        "        x = x.view(B, *strides, N // math.prod(strides), *cur_mu_shape, C) # 2d: [B, Sy, Sx, N//(Sy*Sx), MUy, MUx, C]\n",
        "        L = len(x.shape)\n",
        "        permute = ([0, 1+D] + [val for tup in zip(range(1, 1+D), range(1+D+1, L-1)) for val in tup] + [L-1]) # [0, 1, 3, 2, 4, 5] / [0, 1, 4, 2, 5, 3, 6, 7]\n",
        "        x = x.permute(permute) # 2d: [B, Sy, Sx, N//(Sy*Sx), MUy, MUx, C] -> [B, N//(Sy*Sx), Sy, MUy, Sx, MUx, C]\n",
        "        for i in range(D):\n",
        "            cur_mu_shape[i] *= strides[i]\n",
        "        # print('cur_mu_shape',cur_mu_shape)\n",
        "        x = x.reshape(B, -1, *cur_mu_shape, C) # [B, N//(Sy*Sx), *MU, C]\n",
        "        N = x.shape[1]\n",
        "        # print('in stride', x.shape)\n",
        "    # print('final cur_mu_shape',cur_mu_shape)\n",
        "    x = x.view(B, N, *cur_mu_shape, C) # 2d: [B, #MUy*#MUx, MUy, MUx, C]\n",
        "    # if mask != None: return x # decoder # 2d: [B, #MUs, MUy, MUx, C]\n",
        "    # print(size, cur_mu_shape)\n",
        "    # x = undo_windowing(x, size, cur_mu_shape) # encoder [B, H, W, C] / [B, T, H, W, C]\n",
        "    # print('mask', x.shape)\n",
        "    return x\n",
        "\n",
        "                # intermediate = Reroll(x, size, schedule)\n",
        "                # print(intermediate.shape)\n",
        "                # intermediates.append(intermediate)\n",
        "                # if len(schedule) > 0: size = [n//s for n, s in zip(size, schedule[0])]\n",
        "                # schedule = schedule[1:]\n",
        "\n",
        "# stages=(2, 2, 3, 2)\n",
        "# input_size = (224, 224)\n",
        "# q_stride = (2, 2)\n",
        "# patch_stride = (4, 4)\n",
        "# q_pool = 3\n",
        "# x = torch.rand(24, 16, 56, 56)\n",
        "# x = x.flatten(2).transpose(1, 2) # [b,t,c]\n",
        "# stage_ends = [sum(stages[:i]) - 1 for i in range(1, len(stages) + 1)] # [1, 3, 6, 8]\n",
        "# tokens_spatial_shape = [i//s for i, s in zip(input_size, patch_stride)] # list [224/4, 224/4] = [56, 56]\n",
        "# reroll = Reroll(x, tokens_spatial_shape, [q_stride] * len(stage_ends[:-1]))\n",
        "\n",
        "# torch.Size([24, 256, 16]) 1 torch.Size([24, 49]) # [24, 4, 8, 8, 16] 1: ([(2, 2), (2, 2), (2, 2)], [56, 56])\n",
        "# torch.Size([24, 64, 32]) 3 torch.Size([24, 49]) # [24, 4, 4, 4, 32] 3: ([(2, 2), (2, 2)], [28, 28])\n",
        "# torch.Size([24, 16, 64]) 6 torch.Size([24, 49]) # [24, 4, 2, 2, 64] 6: ([(2, 2)], [14, 14])\n",
        "# torch.Size([24, 4, 128]) 8 torch.Size([24, 49]) # [24, 4, 1, 1, 128]  8: ([], [7, 7])\n",
        "\n",
        "def randpatch(length, B, gamma=0.9): # num patches of seq, mask patch size, masking ratio\n",
        "    g = torch.normal(gamma, std=.1, size=(1,)).clamp(.5,.9)\n",
        "    # g = gamma\n",
        "    noise = torch.rand(B, length, device=device)\n",
        "    ids = torch.argsort(noise, dim=1)  # ascend: small is keep, large is remove\n",
        "\n",
        "    mask = torch.zeros((B, length), dtype=bool, device=device)\n",
        "    mask[:, :int(length*(1-gamma))] = True\n",
        "    mask = torch.gather(mask, dim=1, index=ids)\n",
        "\n",
        "    # idx = torch.randperm(length)[:int(length*gamma)]\n",
        "    # mask = torch.zeros((B, length), dtype=bool)\n",
        "    # mask[:,idx] = True\n",
        "    return mask\n"
      ],
      "metadata": {
        "id": "cYn9qqvVIfSc",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title facebookresearch hiera.py\n",
        "# https://github.com/facebookresearch/hiera/blob/main/hiera/hiera.py\n",
        "# Hiera: A Hierarchical Vision Transformer without the Bells-and-Whistles https://arxiv.org/abs/2306.00989/\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from timm.models.layers import DropPath, Mlp\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def zero_module(module):\n",
        "    for p in module.parameters():\n",
        "        p.detach().zero_()\n",
        "    return module\n",
        "\n",
        "class MaskUnitAttention(nn.Module):\n",
        "    def __init__(self, in_dim, d_model, n_heads, q_stride=1, window_size=0, use_mask_unit_attn=False):\n",
        "        super().__init__()\n",
        "        self.in_dim, self.d_model = in_dim, d_model\n",
        "        self.n_heads, self.d_head = n_heads, d_model // n_heads\n",
        "        self.scale = self.d_head**-.5\n",
        "        self.qkv = nn.Linear(in_dim, 3*d_model)\n",
        "        self.lin = nn.Linear(d_model, d_model)\n",
        "        self.q_stride = q_stride # If greater than 1, pool q with this stride. The stride should be flattened (e.g., 2x2 = 4).\n",
        "        self.window_size = window_size # The current (flattened) size of a mask unit *after* pooling (if any).\n",
        "        self.use_mask_unit_attn = use_mask_unit_attn # Use Mask Unit or Global Attention.\n",
        "        # within each window, pool exery q_stride\n",
        "\n",
        "    def forward(self, x): # [b,t,c] # x must be already been flattened and unrolled into mask units\n",
        "        B, N, _ = x.shape\n",
        "        # print('attn N,q,win', N,self.q_stride, self.window_size, self.use_mask_unit_attn)\n",
        "        num_windows = ((N // (self.q_stride * self.window_size)) if self.use_mask_unit_attn else 1) # for Mask Unit else Global Attention\n",
        "        q, k, v = self.qkv(x).reshape(B, -1, num_windows, 3, self.n_heads, self.d_head).permute(3, 0, 4, 2, 1, 5) # [b,t,3*d_model] -> [b, t/num_windows = q_stride*window_size, num_windows, 3, n_heads, d_head] -> [3, b, n_heads, num_windows, q_stride*window_size, d_head]\n",
        "        if self.q_stride > 1: q = q.view(B, self.n_heads, num_windows, self.q_stride, -1, self.d_head).max(dim=3).values # q pooling # [b, n_heads, num_windows, q_stride,window_size, d_head]-> [b, n_heads, num_windows, window_size, d_head]\n",
        "\n",
        "        q, k = q.softmax(dim=-1)*self.scale, k.softmax(dim=-2)\n",
        "        context = k.transpose(-2,-1) @ v # [b, n_heads, num_windows, d_head, d_head]\n",
        "        x = q @ context # [b, n_heads, num_windows, window_size, d_head]\n",
        "\n",
        "        # # x = F.scaled_dot_product_attention(q, k, v)\n",
        "        # attn = (q * self.scale) @ k.transpose(-1, -2) # [b, n_heads, num_windows, window_size, d_head] @ [b, n_heads, num_windows, d_head, q_stride*window_size] = [b, n_heads, num_windows, window_size, q_stride*window_size]\n",
        "        # x = attn.softmax(dim=-1) @ v # [b, n_heads, num_windows, window_size, d_head]\n",
        "\n",
        "        x = x.transpose(1, 3).reshape(B, -1, self.d_model)\n",
        "        x = self.lin(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class HieraBlock(nn.Module):\n",
        "    def __init__(self, dim, dim_out, heads, drop_path = 0,\n",
        "        q_stride = 1, window_size = 0, use_mask_unit_attn = False):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.dim_out = dim_out\n",
        "        self.norm1 = nn.LayerNorm(dim)\n",
        "        print('hiera init',dim, dim_out)\n",
        "        self.attn = MaskUnitAttention(dim, dim_out, heads, q_stride, window_size, use_mask_unit_attn)\n",
        "        self.norm2 = nn.LayerNorm(dim_out)\n",
        "        self.mlp = Mlp(dim_out, int(dim_out * 4), act_layer=nn.GELU)\n",
        "        self.drop_path = DropPath(drop_path) if drop_path > 0 else nn.Identity()\n",
        "        if dim != dim_out:\n",
        "            self.proj = nn.Linear(dim, dim_out)\n",
        "\n",
        "    def forward(self, x): # [b,t,c] # [b, (Sy, Sx, h/Sy, w/Sx), c]\n",
        "        # Attention + Q Pooling\n",
        "        x_norm = self.norm1(x)\n",
        "        if self.dim != self.dim_out:\n",
        "            x = self.proj(x_norm).unflatten(1, (self.attn.q_stride, -1)).max(dim=1).values # pooling res # [b, (Sy, Sx, h/Sy, w/Sx), c] -> [b, (Sy, Sx), (h/Sy, w/Sx), c] -> [b, (h/Sy, w/Sx), c]\n",
        "        print('HieraBlock fwd', x.shape, self.attn(x_norm).shape, x_norm.shape)\n",
        "        x = x + self.drop_path(self.attn(x_norm))\n",
        "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
        "        return x\n",
        "\n",
        "# class levelBlock(nn.Module):\n",
        "#     def __init__(self, in_dim, out_dim, n_heads=None, depth=1, r=1, drop):\n",
        "#         super().__init__()\n",
        "#         n_heads = n_heads or out_ch//d_head\n",
        "#         self.seq = nn.Sequential(\n",
        "#             # UpDownBlock(in_ch, out_ch, r=min(1,r)) if in_ch != out_ch or r<1 else nn.Identity(),\n",
        "#             # *[U_DiTBlock(out_ch, cond_dim, n_heads) for i in range(1)],\n",
        "#             # UpDownBlock(out_ch, out_ch, r=r) if r>1 else nn.Identity(),\n",
        "#             block = HieraBlock(dim=in_dim, dim_out=out_dim, heads=n_heads,\n",
        "#                 q_stride=(flat_q_stride if i in q_pool_blocks else 1),\n",
        "#                 window_size=flat_mu_size,\n",
        "#                 use_mask_unit_attn=use_mask_unit_attn,\n",
        "#                 drop_path=drop\n",
        "#             )\n",
        "#         )\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         return self.seq(x)\n",
        "\n",
        "\n",
        "def apply_fusion_head(head, x): # [B, #MUs, My, Mx, C]\n",
        "    if isinstance(head, nn.Identity): return x\n",
        "    B, num_mask_units = x.shape[0:2]\n",
        "    permute = [0] + [len(x.shape) - 2] + list(range(1, len(x.shape) - 2))\n",
        "    x = head(x.reshape(B * num_mask_units, *x.shape[2:]).permute(permute)) # [B, #MUs, My, Mx, C] -> head([B * #MUs, C, My, Mx])\n",
        "    permute = [0] + list(range(2, len(x.shape))) + [1] # [0, *2345.., 1]\n",
        "    x = x.permute(permute).reshape(B, num_mask_units, *x.shape[2:], x.shape[1]) # [B * #MUs, C', My', Mx'] -> [B, #MUs, My', Mx', C']\n",
        "    return x\n",
        "\n",
        "\n",
        "class Hiera(nn.Module):\n",
        "    def __init__(self, input_size = (224, 224), in_dim = 3, d_model = 16, n_heads = 1,\n",
        "        stages = (2, 2, 3, 2),\n",
        "        q_pool = 3,  # number of q_pool stages\n",
        "        q_stride = (2, 2),\n",
        "        mask_unit_size = (8, 8), # must divide q_stride**(num_stages-1)\n",
        "        mask_unit_attn = (True, True, False, False), # which stages use mask unit attention\n",
        "        patch_kernel = (7, 7), patch_stride = (4, 4), patch_padding = (3, 3),\n",
        "        drop_path_rate = 0):\n",
        "        super().__init__()\n",
        "        self.patch_stride = patch_stride\n",
        "        self.tokens_spatial_shape = [i//s for i, s in zip(input_size, patch_stride)] # list [224/4, 224/4] = [56, 56]\n",
        "        flat_mu_size = math.prod(mask_unit_size)\n",
        "\n",
        "        # assert q_pool < len(stages)\n",
        "        self.q_pool, self.q_stride = q_pool, q_stride\n",
        "        self.mu_size, self.mask_unit_size = flat_mu_size, mask_unit_size\n",
        "        self.mask_spatial_shape = [i//s for i, s in zip(self.tokens_spatial_shape, self.mask_unit_size)]\n",
        "        self.stage_ends = [sum(stages[:i]) - 1 for i in range(1, len(stages) + 1)] # [1, 3, 6, 8]\n",
        "        self.proj = conv_nd(len(patch_kernel), in_dim, d_model, patch_kernel, patch_stride, patch_padding)\n",
        "\n",
        "        if len(patch_kernel) == 3: # for video\n",
        "            pos_embed_spatial = nn.Parameter(torch.randn(1, self.tokens_spatial_shape[1]*self.tokens_spatial_shape[2], d_model)*0.02)\n",
        "            pos_embed_temporal = nn.Parameter(torch.randn(1, self.tokens_spatial_shape[0], d_model)*0.02)\n",
        "            self.pos_embed = pos_embed_spatial.repeat(1, self.tokens_spatial_shape[0], 1) + torch.repeat_interleave(pos_embed_temporal, self.tokens_spatial_shape[1] * self.tokens_spatial_shape[2], dim=1)\n",
        "        elif len(patch_kernel) == 2: # for img\n",
        "            self.pos_embed = nn.Parameter(torch.randn(1, math.prod(self.tokens_spatial_shape), d_model)*0.02) # 56*56=3136\n",
        "\n",
        "        depth = sum(stages)\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]\n",
        "\n",
        "        flat_q_stride = math.prod(q_stride)\n",
        "        q_pool_blocks = [x + 1 for x in self.stage_ends[:q_pool]] # list[2, 4, 7] # q_pool locations. the first q_pool num of stage_end loc + 1\n",
        "        # Transformer blocks\n",
        "        cur_stage = 0\n",
        "        self.blocks = nn.ModuleList()\n",
        "        for i in range(depth):\n",
        "            dim_out = d_model\n",
        "            use_mask_unit_attn = mask_unit_attn[cur_stage] # Lag by 1 block, so that global attention, applied post pooling on lower resolution\n",
        "\n",
        "            if i - 1 in self.stage_ends:\n",
        "                # dim_out, n_heads = d_model * 2, n_heads * 2\n",
        "                dim_out, n_heads = d_model * 1, n_heads * 2\n",
        "                cur_stage += 1\n",
        "                if i in q_pool_blocks: flat_mu_size //= flat_q_stride\n",
        "\n",
        "            print(i, \"q_pool, mask_unit, mu_attn\", (flat_q_stride if i in q_pool_blocks else 1), flat_mu_size, use_mask_unit_attn)\n",
        "            block = HieraBlock(dim=d_model, dim_out=dim_out, heads=n_heads,\n",
        "                q_stride=(flat_q_stride if i in q_pool_blocks else 1),\n",
        "                window_size=flat_mu_size,\n",
        "                use_mask_unit_attn=use_mask_unit_attn,\n",
        "                drop_path=dpr[i],\n",
        "            )\n",
        "            d_model = dim_out\n",
        "            self.blocks.append(block)\n",
        "\n",
        "        # encoder_dim_out = self.blocks[-1].dim_out\n",
        "        # self.encoder_norm = nn.LayerNorm(encoder_dim_out)\n",
        "        self.encoder_norm = nn.LayerNorm(dim_out)\n",
        "        self.mask_unit_spatial_shape_final = [i//s**self.q_pool for i, s in zip(self.mask_unit_size, self.q_stride)]\n",
        "        self.tokens_spatial_shape_final = [i//s**self.q_pool for i, s in zip(self.tokens_spatial_shape, self.q_stride)]\n",
        "        # --------------------------------------------------------------------------\n",
        "        # Multi-scale fusion heads\n",
        "        curr_mu_size = self.mask_unit_size\n",
        "        self.multi_scale_fusion_heads = nn.ModuleList()\n",
        "        for i in self.stage_ends[: self.q_pool]:  # resolution constant after q_pool # [1,3,6]\n",
        "            kernel = [i//s for i, s in zip(curr_mu_size, self.mask_unit_spatial_shape_final)]\n",
        "            curr_mu_size = [i//s for i, s in zip(curr_mu_size, self.q_stride)]\n",
        "            self.multi_scale_fusion_heads.append(\n",
        "                # conv_nd(len(self.q_stride), self.blocks[i].dim_out, dim_out, kernel_size=kernel, stride=kernel)\n",
        "                zero_module(conv_nd(len(self.q_stride), self.blocks[i].dim_out, dim_out, kernel_size=kernel, stride=kernel))\n",
        "            )\n",
        "            print('multi_scale_fusion_heads',len(self.q_stride), self.blocks[i].dim_out, dim_out, kernel)\n",
        "            # 2 16 64 [4, 4] # conv2d, indim to final dim, kernel=MU_size\n",
        "            # 2 32 64 [2, 2]\n",
        "        self.multi_scale_fusion_heads.append(nn.Identity())  # final stage, no transform\n",
        "        # print('multi_scale_fusion_heads', self.multi_scale_fusion_heads)\n",
        "\n",
        "\n",
        "    def forward(self, x, mask=None, return_intermediates=False): # [b,c,h,w], [b,(f*)h*w]\n",
        "        # print('fwd0', x.shape, self.mask_spatial_shape)\n",
        "        # [24, 3, 32, 32], [24, 4], [4, 4]\n",
        "        if mask != None: x = x * F.interpolate(mask.unflatten(-1, (1, *self.mask_spatial_shape)).float(), size=x.shape[2:], mode='nearest-exact').bool() # zero masked locations\n",
        "        x = self.proj(x) # [24, 3, 224, 224]->[24, 16, 56, 56]\n",
        "        x = x.flatten(2).transpose(1, 2) # [b,t,c]\n",
        "        x = x + self.pos_embed\n",
        "        print('fwd1', x.shape)\n",
        "\n",
        "        size, schedule = self.tokens_spatial_shape, [self.q_stride] * len(self.stage_ends[:-1])\n",
        "        x = Unroll(x, size, schedule) # [B, (H, W), C] -> [B, (Sy, Sx, H // Sy, W // Sx), C] ; stride of (Sy, Sx)\n",
        "\n",
        "        # try: print('fwd2', x.shape, mask.shape, self.mu_size)\n",
        "        # except: print('fwd2', x.shape, self.mu_size)\n",
        "        if mask is not None: x = x[mask[..., None].tile(1, self.mu_size, x.shape[-1])].view(x.shape[0], -1, x.shape[-1]) # Discard masked tokens\n",
        "\n",
        "        print('fwd3', x.shape)\n",
        "        intermediates = []\n",
        "        for i, blk in enumerate(self.blocks):\n",
        "            x = blk(x)\n",
        "            if return_intermediates and i in self.stage_ends:#[:self.q_pool]: # [1,3,6]\n",
        "                # print(i, x.shape, self.stage_ends)\n",
        "                intermediate = Reroll(x, size, schedule)\n",
        "                # intermediates.appen(Reroll(x, size, schedule))\n",
        "                print('intermediate',i,intermediate.shape)\n",
        "                intermediates.append(intermediate)\n",
        "                if i in self.stage_ends[:self.q_pool]:\n",
        "                    if len(schedule) > 0: size = [n//s for n, s in zip(size, schedule[0])]\n",
        "                    schedule = schedule[1:]\n",
        "\n",
        "        if return_intermediates:\n",
        "            intermediates = intermediates[:self.q_pool] + intermediates[-1:] # take only last of each resolution\n",
        "            x = sum([apply_fusion_head(head, interm_x) for head, interm_x in zip(self.multi_scale_fusion_heads, intermediates)])\n",
        "        # print('fwd out', x.shape)\n",
        "        x = self.encoder_norm(x) # [b, num_tok, 1, 1, dim_out]\n",
        "\n",
        "        return x.flatten(1,-2).mean(1)\n",
        "\n",
        "\n",
        "input_size = (32,32)\n",
        "# input_size = (64,64)\n",
        "# model = Hiera(d_model=16, n_heads=1, stages=(1, 1, 1, 1))\n",
        "# model = Hiera(input_size, d_model=16, n_heads=1, stages=(2, 2, 3, 2))\n",
        "# model = Hiera(input_size, d_model=16, n_heads=1, stages=(2, 3, 2),\n",
        "model = Hiera(input_size, d_model=16, n_heads=4, stages=(1, 2,1),\n",
        "        # patch_stride = (2,2),\n",
        "        # patch_kernel = (3, 3), patch_stride = (1, 1), patch_padding = (1, 1),\n",
        "        patch_kernel = (7, 7), patch_stride = (2, 2), patch_padding = (3, 3),\n",
        "        # q_pool = 2, q_stride = (2, 2),\n",
        "        q_pool = 2, q_stride = (2, 2),\n",
        "        # mask_unit_size = (4,4), mask_unit_attn = (True, True, False)).to(device)\n",
        "        mask_unit_size = (4,4), mask_unit_attn = (False, False, False)).to(device)\n",
        "        # mask_unit_size = (1,1), mask_unit_attn = (False, False, False)).to(device)\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 59850\n",
        "optim = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "\n",
        "batch=24\n",
        "# x = torch.randn(batch, 3, 224, 224)\n",
        "\n",
        "# mask should be a boolean tensor of shape [B, #MUt*#MUy*#MUx] where #MU are the number of mask units in that dim.\n",
        "# Note: 1 in mask is *keep*, 0 is *remove*; mask.sum(dim=-1) should be the same across the batch.\n",
        "# mask = (torch.randn(batch, (224//4)*(224//4))>0)\n",
        "# print(model.mask_spatial_shape)\n",
        "gamma=0.8\n",
        "length = 16**2\n",
        "# (input_size/patch_stride/q_stride^q_pool)^2\n",
        "\n",
        "mask = randpatch(length, batch, gamma=0.9)\n",
        "mask=None\n",
        "# print(mask)\n",
        "# print(mask.sum(-1))\n",
        "\n",
        "x = torch.randn(batch, 3, *input_size, device=device)\n",
        "# print(mask)\n",
        "# print(mask.shape)\n",
        "y = model(x, mask, return_intermediates=True)\n",
        "# y = model(x, mask)\n",
        "# y, intermediates = model(x, return_intermediates=True)\n",
        "# y = model(x)\n",
        "print(y.shape)\n",
        "# print(y)\n",
        "\n",
        "# [batch, (224/patch_stride/q_stride^q_pool)^2=(224/32)^2=7^2, embed_dim^num_stages]\n",
        "\n",
        "\n",
        "# # Image hiera_tiny_224(embed_dim=96, num_heads=1, stages=(1, 2, 7, 2))\n",
        "# # Video # def hiera_base_16x224(num_classes=400,\n",
        "        # input_size=(16, 224, 224), q_stride=(1, 2, 2), mask_unit_size=(1, 8, 8),\n",
        "#         patch_kernel=(3, 7, 7), patch_stride=(2, 4, 4), patch_padding=(1, 3, 3))\n",
        "# def hiera_base_plus_16x224(hiera_base_16x224(embed_dim=112, num_heads=2, stages=(2, 3, 16, 3), **kwdargs\n",
        "\n"
      ],
      "metadata": {
        "id": "TAqNeCVTOrgf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 713
        },
        "outputId": "2a546b0b-169c-4848-d913-02b69d5f1d9b",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 q_pool, mask_unit, mu_attn 1 16 False\n",
            "hiera init 16 16\n",
            "1 q_pool, mask_unit, mu_attn 4 4 False\n",
            "hiera init 16 16\n",
            "2 q_pool, mask_unit, mu_attn 1 4 False\n",
            "hiera init 16 16\n",
            "3 q_pool, mask_unit, mu_attn 4 1 False\n",
            "hiera init 16 16\n",
            "multi_scale_fusion_heads 2 16 16 [4, 4]\n",
            "multi_scale_fusion_heads 2 16 16 [2, 2]\n",
            "24768\n",
            "fwd1 torch.Size([24, 256, 16])\n",
            "fwd3 torch.Size([24, 256, 16])\n",
            "HieraBlock fwd torch.Size([24, 256, 16]) torch.Size([24, 256, 16]) torch.Size([24, 256, 16])\n",
            "intermediate 0 torch.Size([24, 16, 4, 4, 16])\n",
            "HieraBlock fwd torch.Size([24, 256, 16]) torch.Size([24, 64, 16]) torch.Size([24, 256, 16])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "The size of tensor a (256) must match the size of tensor b (64) at non-singleton dimension 1",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-66-7745be162829>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[0;31m# print(mask)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[0;31m# print(mask.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_intermediates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m \u001b[0;31m# y = model(x, mask)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[0;31m# y, intermediates = model(x, return_intermediates=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-66-7745be162829>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, mask, return_intermediates)\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0mintermediates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mreturn_intermediates\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstage_ends\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m#[:self.q_pool]: # [1,3,6]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m                 \u001b[0;31m# print(i, x.shape, self.stage_ends)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-66-7745be162829>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_norm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_stride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[0;31m# pooling res # [b, (Sy, Sx, h/Sy, w/Sx), c] -> [b, (Sy, Sx), (h/Sy, w/Sx), c] -> [b, (h/Sy, w/Sx), c]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'HieraBlock fwd'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_norm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_norm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_norm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (256) must match the size of tensor b (64) at non-singleton dimension 1"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title hiera vit me\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def zero_module(module):\n",
        "    for p in module.parameters():\n",
        "        p.detach().zero_()\n",
        "    return module\n",
        "\n",
        "# class LayerNorm2d(nn.LayerNorm):\n",
        "class LayerNorm2d(nn.RMSNorm):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "    def forward(self, x):\n",
        "        x = x.permute(0, 2, 3, 1)\n",
        "        x = super().forward(x)\n",
        "        x = x.permute(0, 3, 1, 2)\n",
        "        return x\n",
        "\n",
        "def unshuffle(x, window_shape): # [b,c,h,w] -> [b, h/win1* w/win2, c, win1,win2]\n",
        "    new_shape = list(x.shape[:2]) + [val for xx,win in zip(list(x.shape[2:]), window_shape) for val in [win,xx//win]]\n",
        "    # x = x.reshape(new_shape) # [b, c, h/win1, win1, w/win2, win2]\n",
        "    print('unsh',x.shape, window_shape, new_shape)\n",
        "    x = x.reshape(new_shape) # [b, c, win1, h/win1, win2, w/win2]\n",
        "    L = len(new_shape)\n",
        "    permute = ([0] + list(range(3, L, 2)) + [1] + list(range(2, L - 1, 2))) # [0,3,5,1,2,4] / [0,3,5,7,1,2,4,6]\n",
        "    x = x.permute(permute).flatten(1, L//2-1) # [b, h/win1* w/win2, c, win1,win2]\n",
        "    return x\n",
        "\n",
        "def shuffle(x, window_shape): # [b,win*win, c, h/win, w/win] -> [b,1,c,h,w]\n",
        "    out_shape = [x.shape[0], -1, x.shape[2]] + [x*w for x,w in zip(x.shape[3:], window_shape)] # [b,1,c,h/win*wim, w/win*wim]\n",
        "    x = x.unflatten(1, (-1, *window_shape)) # [b,num_tok,win,win, c, h/win, w/win]\n",
        "    D=x.dim()+1\n",
        "    permute = [0,1,D//2] + [val for tup in zip(range(1+D//2, D), range(2, D//2)) for val in tup]\n",
        "    x = x.permute(permute).reshape(out_shape)\n",
        "    return x\n",
        "\n",
        "\n",
        "class AttentionBlock(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, cond_dim=None, mult=4, drop=0.1, q_stride=None):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        # self.norm = nn.RMSNorm(d_model) # LayerNorm RMSNorm\n",
        "        self.norm = LayerNorm_at(2, d_model) # LayerNorm RMSNorm\n",
        "        self.drop = nn.Dropout(drop)\n",
        "        self.cond_dim = cond_dim\n",
        "        # self.self = MultiHeadAttention(d_model, d_head=d_head, dropout=0) # 16448\n",
        "        self.self = MaskUnitAttention(d_model, n_heads, q_stride)\n",
        "        # in_dim, d_model, n_heads, q_stride=(2,2)\n",
        "        # self.self = Pooling()\n",
        "        # act = nn.GELU() # ReLU GELU\n",
        "        # self.ff = nn.Sequential(\n",
        "        #     *[nn.BatchNorm2d(d_model), act, SeparableConv2d(d_model, d_model),]*3\n",
        "        #     )\n",
        "        # self.ff = ResBlock(d_model) # 74112\n",
        "        self.ff = UIB(d_model, mult=4) # uib m4 36992, m2 18944\n",
        "        # self.q_stride = q_stride\n",
        "        self.res = zero_module(conv_nd(2, d_model, d_model, q_stride, stride=q_stride)) if q_stride else nn.Identity()\n",
        "\n",
        "\n",
        "    def forward(self, x, cond=None, mask=None): # [b,c,h,w], [batch, num_tok, cond_dim], [batch,T]\n",
        "        b, num_tok, c, *win = x.shape\n",
        "        # bchw = x.shape\n",
        "        # [b,c,num_tok*win,win] # [b,c,num_tok,win1,win2,win3]\n",
        "        # x = x.flatten(2).transpose(1,2) # [b,h*w,c]\n",
        "        # print('attnblk fwd',x.shape)\n",
        "\n",
        "        # if self.cond_dim==None: cond=None # is self attn\n",
        "        # x = x + self.drop(self.self(self.norm(x)))\n",
        "        x = self.res(x.flatten(0,1)) + self.drop(self.self(self.norm(x))).flatten(0,1)\n",
        "        # x = x.transpose(1,2).reshape(*bchw)\n",
        "        # [b*num_tok,c,win1,win2,win3]\n",
        "\n",
        "        # x = x.flatten(0,1) # [b*num_tok,c,win1,win2,win3]\n",
        "        x = x + self.ff(x)\n",
        "        # x = self.ff(x)\n",
        "        # x = x + self.drop(self.norm2(self.ff(x)))\n",
        "        x = x.unflatten(0, (b, num_tok))\n",
        "        return x\n",
        "\n",
        "\n",
        "class SimpleViT(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim, d_model, depth, n_heads, mlp_dim, channels=3, dim_head=8):\n",
        "        super().__init__()\n",
        "        self.to_patch_embedding = nn.Sequential( # in, out, kernel, stride, pad\n",
        "            # nn.Conv2d(in_dim, d_model, kernel_size=7, stride=2, padding=7//2, bias=False), nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "            nn.Conv2d(in_dim, d_model, kernel_size=7, stride=1, padding=7//2, bias=False),\n",
        "            # nn.Conv2d(in_dim, dim, kernel_size=7, stride=2, padding=7//2, bias=False), nn.MaxPool2d(2,2)\n",
        "            # UpDownBlock(in_dim, dim, r=1/2, kernel=3), UpDownBlock(dim, dim, r=1/2, kernel=3)\n",
        "            # nn.PixelUnshuffle(2), nn.Conv2d(in_dim*2**2, dim, 1, bias=False),\n",
        "            )\n",
        "        # self.pos_embedding = LearnedRoPE2D(dim) # LearnedRoPE2D, RoPE2D\n",
        "        # self.positional_emb = nn.Parameter(torch.zeros(1, d_model, 8,8)) # positional_embedding == 'learnable'\n",
        "        self.positional_emb = nn.Parameter(torch.zeros(1, d_model, 32,32)) # positional_embedding == 'learnable'\n",
        "        # self.blocks = nn.ModuleList([AttentionBlock(d_model=dim, d_head=dim_head) for i in range(depth)])\n",
        "        # self.blocks = nn.Sequential(*[AttentionBlock(d_model, n_heads, q_stride) for _ in range(depth)])\n",
        "        # self.blocks = nn.Sequential(*[AttentionBlock(d_model, n_heads, q_stride=(2,2) if i in [1,3] else None) for i in range(depth)])\n",
        "        self.blocks = nn.ModuleList([\n",
        "            AttentionBlock(d_model, n_heads, q_stride=(2,2)),\n",
        "            AttentionBlock(d_model, n_heads),\n",
        "            AttentionBlock(d_model, n_heads, q_stride=(2,2)),\n",
        "            AttentionBlock(d_model, n_heads),\n",
        "            ])\n",
        "\n",
        "        self.attn_pool = nn.Linear(d_model, 1)\n",
        "        self.out = nn.Linear(d_model, out_dim, bias=False)\n",
        "\n",
        "    def forward(self, img): # [b,c,h,w]\n",
        "        # [b,c,num_tok,win1,win2,win3]\n",
        "        x = self.to_patch_embedding(img)\n",
        "        # x = self.pos_embedding(x)\n",
        "        bchw = x.shape\n",
        "        print('vit fwd', x.shape)\n",
        "        x = x + self.positional_emb\n",
        "        # mask\n",
        "        x = unshuffle(x, (16,16)) # [b,num_tok,c,win1,win2,win3] or [b,1,c,f,h,w]\n",
        "        print('vit fwd', x.shape)\n",
        "\n",
        "        for i, blk in enumerate(self.blocks):\n",
        "            print(i, x.shape)\n",
        "            x = blk(x)\n",
        "            print('bef shf',i, x.shape)\n",
        "            # if i==0: x = shuffle(x, (16,16))\n",
        "            if i==0: x = shuffle(x, (2,2))\n",
        "        # x = self.blocks(x) # [b,num_tok,c,1,1,1]\n",
        "        # x = x.flatten(2).transpose(1,2) # [b,h*w,c]\n",
        "        print('vit fwd2', x.shape)\n",
        "        x = x.squeeze() # [b,num_tok,d]\n",
        "        attn = self.attn_pool(x).squeeze(-1) # [batch, (h,w)] # seq_pool\n",
        "        x = (attn.softmax(dim=1).unsqueeze(1) @ x).squeeze(1) # [batch, 1, (h,w)] @ [batch, (h,w), dim] -> [batch, dim]\n",
        "        return self.out(x)\n",
        "\n",
        "\n",
        "\n",
        "        # self.multi_scale_fusion_heads = nn.ModuleList([\n",
        "        #         zero_module(conv_nd(2, d_model, 4*d_model, kernel_size=(4,4), stride=(4,4))),\n",
        "        #         zero_module(conv_nd(2, 2*d_model, 4*d_model, kernel_size=(2,2), stride=(2,2))),\n",
        "        # ])\n",
        "\n",
        "\n",
        "    def forward(self, img): # [b,c,h,w]\n",
        "\n",
        "        # intermediates = []\n",
        "        for i, blk in enumerate(self.blocks):\n",
        "            # print(i, x.shape)\n",
        "            x = blk(x)\n",
        "            # if i in [0,2]:\n",
        "                # intermediates.append(x)\n",
        "        # x = x + sum([conv(interm_x.flatten(0,1)) for conv, interm_x in zip(self.multi_scale_fusion_heads, intermediates)]).unflatten(0, x.shape[:2])\n",
        "\n",
        "            # print('bef shf',i, x.shape)\n",
        "        # x = self.blocks(x) # [b,num_tok,c,1,1,1]\n",
        "        # x = x.flatten(2).transpose(1,2) # [b,h*w,c]\n",
        "        # print('vit fwd2', x.shape)\n",
        "        x = x.squeeze() # [b,num_tok,d]\n",
        "        attn = self.attn_pool(x).squeeze(-1) # [batch, (h,w)] # seq_pool\n",
        "        x = (attn.softmax(dim=1).unsqueeze(1) @ x).squeeze(1) # [batch, 1, (h,w)] @ [batch, (h,w), dim] -> [batch, dim]\n",
        "        return self.out(x)\n",
        "\n",
        "\n",
        "\n",
        "# pos_emb rope < learn < learned\n",
        "# conv > pixel?\n",
        "# droppath not required\n",
        "\n",
        "# norm,act,conv < conv,norm,act\n",
        "# 2*s1 < uib < resblock\n",
        "# gatedadaln 3 < 2 = 1 < ffmult4 = 2*gatedadaln\n",
        "# MaxPool2d(2,2) < MaxPool2d(3,2,3//2)\n",
        "\n",
        "\n",
        "d_model = 64\n",
        "dim_head = 8\n",
        "heads = d_model // dim_head\n",
        "num_classes = 10\n",
        "# model = SimpleViT(image_size=32, patch_size=4, num_classes=num_classes, dim=dim, depth=1, heads=heads, mlp_dim=dim*4, channels = 3, dim_head = dim_head)\n",
        "model = SimpleViT(in_dim=3, out_dim=num_classes, d_model=d_model, depth=5, n_heads=heads, mlp_dim=dim*4, channels = 3, dim_head = dim_head).to(device)\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 59850\n",
        "optim = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "\n",
        "# print(images.shape) # [batch, 3, 32, 32]\n",
        "x = torch.rand(64, 3, 32, 32, device=device)\n",
        "# x = torch.rand(64, 3, 28,28, device=device)\n",
        "logits = model(x)\n",
        "print(logits.shape)\n",
        "# print(logits[0])\n",
        "# print(logits[0].argmax(1))\n",
        "pred_probab = nn.Softmax(dim=1)(logits)\n",
        "y_pred = pred_probab.argmax(1)\n",
        "# print(f\"Predicted class: {y_pred}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9cca7567-06b3-41bd-b8bd-63540c90ab2b",
        "cellView": "form",
        "id": "cfWOTdswXceO"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "323073\n",
            "vit fwd torch.Size([64, 64, 32, 32])\n",
            "unsh torch.Size([64, 64, 32, 32]) (16, 16) [64, 64, 16, 2, 16, 2]\n",
            "vit fwd torch.Size([64, 4, 64, 16, 16])\n",
            "0 torch.Size([64, 4, 64, 16, 16])\n",
            "torch.Size([256, 64, 16, 16])\n",
            "attn fwd 2 torch.Size([256, 8, 64, 8])\n",
            "bef shf 0 torch.Size([64, 4, 64, 8, 8])\n",
            "1 torch.Size([64, 1, 64, 16, 16])\n",
            "torch.Size([64, 64, 16, 16])\n",
            "attn fwd 2 torch.Size([64, 8, 256, 8])\n",
            "bef shf 1 torch.Size([64, 1, 64, 16, 16])\n",
            "2 torch.Size([64, 1, 64, 16, 16])\n",
            "torch.Size([64, 64, 16, 16])\n",
            "attn fwd 2 torch.Size([64, 8, 64, 8])\n",
            "bef shf 2 torch.Size([64, 1, 64, 8, 8])\n",
            "3 torch.Size([64, 1, 64, 8, 8])\n",
            "torch.Size([64, 64, 8, 8])\n",
            "attn fwd 2 torch.Size([64, 8, 64, 8])\n",
            "bef shf 3 torch.Size([64, 1, 64, 8, 8])\n",
            "vit fwd2 torch.Size([64, 1, 64, 8, 8])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "mat1 and mat2 shapes cannot be multiplied (32768x8 and 64x1)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-c53c030b16b2>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;31m# x = torch.rand(64, 3, 28,28, device=device)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;31m# print(logits[0])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-43-c53c030b16b2>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'vit fwd2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# [b,num_tok,d]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m         \u001b[0mattn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn_pool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# [batch, (h,w)] # seq_pool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# [batch, 1, (h,w)] @ [batch, (h,w), dim] -> [batch, dim]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (32768x8 and 64x1)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title get_random_mask\n",
        "def get_random_mask(x, mask_ratio=.9, mask_spatial_shape=(3,8,8)):\n",
        "    \"\"\"\n",
        "    Generates a random mask, mask_ratio fraction are dropped.\n",
        "    1 is *keep*, 0 is *remove*. Useful for MAE, FLIP, etc.\n",
        "    \"\"\"\n",
        "    B = x.shape[0]\n",
        "    # Tokens selected for masking at mask unit level\n",
        "    num_windows = math.prod(mask_spatial_shape)  # num_mask_units\n",
        "    len_keep = int(num_windows * (1 - mask_ratio))\n",
        "    noise = torch.rand(B, num_windows, device=x.device)\n",
        "    # Sort noise for each sample\n",
        "    ids_shuffle = torch.argsort(noise, dim=1)  # ascend: small is keep, large is remove\n",
        "    ids_restore = torch.argsort(ids_shuffle, dim=1)\n",
        "    # Generate the binary mask: 1 is *keep*, 0 is *remove*\n",
        "    # Note this is opposite to original MAE\n",
        "    mask = torch.zeros([B, num_windows], device=x.device)\n",
        "    mask[:, :len_keep] = 1\n",
        "    # Unshuffle to get the binary mask\n",
        "    mask = torch.gather(mask, dim=1, index=ids_restore)\n",
        "    return mask.bool()\n",
        "\n",
        "x = torch.rand(2,3,4,4)\n",
        "mask = get_random_mask(x)\n",
        "print(mask.shape)\n",
        "print(mask)\n",
        "\n",
        "mask = torch.rand((x.shape[0],8*8))>.9 # True->keep\n",
        "# mask = torch.rand((x.shape[0],math.prod(mask_spatial_shape)))>mask_ratio # True->keep\n",
        "print(mask.shape)\n",
        "print(mask)\n"
      ],
      "metadata": {
        "id": "MAM10UwF45-I",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "B8Pm-Fw6jn4A"
      },
      "outputs": [],
      "source": [
        "# @title DropPath\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def zero_module(module):\n",
        "    \"\"\"Zero out the parameters of a module and return it.\"\"\"\n",
        "    for p in module.parameters():\n",
        "        p.detach().zero_()\n",
        "    return module\n",
        "\n",
        "# https://github.com/osmr/imgclsmob/blob/master/pytorch/pytorchcv/models/fractalnet_cifar.py\n",
        "    def calc_drop_mask(batch_size,\n",
        "                       glob_num_columns,\n",
        "                       curr_num_columns,\n",
        "                       max_num_columns,\n",
        "                       loc_drop_prob):\n",
        "        \"\"\"\n",
        "        Calculate drop path mask.\n",
        "\n",
        "        Parameters:\n",
        "        ----------\n",
        "        batch_size\n",
        "            Size of batch.\n",
        "        glob_num_columns\n",
        "            Number of columns in global drop path mask.\n",
        "        curr_num_columns\n",
        "            Number of active columns in the current level of block.\n",
        "        max_num_columns\n",
        "            Number of columns for all network.\n",
        "        loc_drop_prob\n",
        "            Local drop path probability.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        Tensor\n",
        "            Resulted mask.\n",
        "        \"\"\"\n",
        "        glob_batch_size = glob_num_columns.shape[0]\n",
        "        glob_drop_mask = np.zeros((curr_num_columns, glob_batch_size), dtype=np.float32)\n",
        "        glob_drop_num_columns = glob_num_columns - (max_num_columns - curr_num_columns)\n",
        "        glob_drop_indices = np.where(glob_drop_num_columns >= 0)[0]\n",
        "        glob_drop_mask[glob_drop_num_columns[glob_drop_indices], glob_drop_indices] = 1.0\n",
        "\n",
        "        loc_batch_size = batch_size - glob_batch_size\n",
        "        loc_drop_mask = np.random.binomial(\n",
        "            n=1,\n",
        "            p=(1.0 - loc_drop_prob),\n",
        "            size=(curr_num_columns, loc_batch_size)).astype(np.float32)\n",
        "        alive_count = loc_drop_mask.sum(axis=0)\n",
        "        dead_indices = np.where(alive_count == 0.0)[0]\n",
        "        loc_drop_mask[np.random.randint(0, curr_num_columns, size=dead_indices.shape), dead_indices] = 1.0\n",
        "\n",
        "        drop_mask = np.concatenate((glob_drop_mask, loc_drop_mask), axis=1)\n",
        "        return torch.from_numpy(drop_mask)\n",
        "\n",
        "# https://github.com/FrancescoSaverioZuppichini/DropPath\n",
        "\n",
        "\n",
        "class DropPath(nn.Module):\n",
        "    def __init__(self, drop_prob=None):\n",
        "        super().__init__()\n",
        "        self.drop_prob = float(drop_prob)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch, drop_prob, device, dtype = x.shape[0], self.drop_prob, x.device, x.dtype\n",
        "        if drop_prob <= 0. or not self.training: return x\n",
        "        keep_prob = 1 - self.drop_prob\n",
        "        shape = (batch, *((1,) * (x.ndim - 1)))\n",
        "        keep_mask = torch.zeros(shape, device = device).float().uniform_(0, 1) < keep_prob\n",
        "        output = x.div(keep_prob) * keep_mask.float()\n",
        "        return output\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# class DropPath(nn.Module):\n",
        "#     \"\"\"Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\"\"\"\n",
        "#     def __init__(self, drop_prob=0.1):\n",
        "#         super().__init__()\n",
        "#         self.drop_prob = drop_prob\n",
        "\n",
        "#     # def forward(self, x):\n",
        "#     # def drop_path(x, drop_prob = 0., training = False):\n",
        "#     def forward(x, drop_prob = 0., training = False):\n",
        "#         if drop_prob == 0. or not training: return x\n",
        "#         keep_prob = 1 - drop_prob\n",
        "#         shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n",
        "#         random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
        "#         random_tensor.floor_()  # binarize\n",
        "#         output = x.div(keep_prob) * random_tensor\n",
        "#         # return output\n",
        "#         return drop_path(x, self.drop_prob, self.training)\n",
        "\n",
        "\n",
        "\n",
        "# d_model=8\n",
        "# d_head=4\n",
        "# batch=4\n",
        "# h,w=5,6\n",
        "# x=torch.rand(batch,d_model,h,w)\n",
        "# cond_dim=10\n",
        "# cross = AttentionBlock(d_model=d_model, d_head=d_head,cond_dim=cond_dim)\n",
        "# num_tok=1\n",
        "# cond=torch.rand(batch,num_tok,cond_dim)\n",
        "# mask=torch.rand(batch,h*w)>0.5\n",
        "# out = cross(x, cond)\n",
        "# print(out.shape)\n",
        "# print(out)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "xTuZNWMXqNXP"
      },
      "outputs": [],
      "source": [
        "# @title test pos emb, polar\n",
        "import torch\n",
        "\n",
        "h,w = 3,5\n",
        "batch, dim = 2,8\n",
        "img = torch.rand(batch, dim, h, w)\n",
        "\n",
        "y, x = torch.meshgrid(torch.arange(h), torch.arange(w), indexing=\"ij\") # print(y,x) # [h,w], y:row_num, x:col_num\n",
        "# y, x = torch.meshgrid(torch.arange(h), torch.arange(w)) # print(y,x) # [h,w], y:row_num, x:col_num\n",
        "# print(y,x)\n",
        "\n",
        "# self.weights = nn.Parameter(torch.randn(1, dim//2))\n",
        "weight_x, weight_y = torch.arange(dim//2).unsqueeze(0), torch.arange(dim//2).unsqueeze(0)/10\n",
        "# weight_x, weight_y = torch.randn(1, dim//2), torch.randn(1, dim//2)\n",
        "# print(weight_x, weight_y)\n",
        "# out = weight_x * x + weight_y * y # [1, dim//2] * [h,w] = [h,w, dim//2]\n",
        "out = weight_x * x.reshape(-1,1) + weight_y * y.reshape(-1,1) # [1, dim//2] * [h*w] = [h*w, dim//2]\n",
        "# print(out)\n",
        "\n",
        "\n",
        "# y, x = torch.meshgrid(torch.arange(h), torch.arange(w), indexing=\"ij\") # print(y,x) # [h,w], y:row_num, x:col_num\n",
        "# (self.weight_x * x + self.weight_y * y) # [b,dim,h,w] * [1,h,w,]\n",
        "\n",
        "# angles = (self.weights * pos * 2*math.pi).unsqueeze(-1) # [seq_len, 1] * [dim // 2] -> [seq_len, dim // 2, 1]\n",
        "# angles = (weight_x * x + weight_y * y * 2*math.pi)[None,...,None] #.unsqueeze(-1) # [1,h,w,1] ; # [seq_len, 1] * [dim // 2] -> [seq_len, dim // 2, 1]\n",
        "angles = (out * 2*torch.pi)[None,...,None] #.unsqueeze(-1) # [1,h*w, dim//2,1] ; # [seq_len, 1] * [dim // 2] -> [seq_len, dim // 2, 1]\n",
        "rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1) # [1,h*w, dim//2,2]\n",
        "# return x * rot_emb.flatten(-2).unsqueeze(0)\n",
        "# return x * rot_emb.flatten(-2).transpose(-2,-1).reshape(1,dim,h,w)\n",
        "out = img * rot_emb.flatten(-2).transpose(-2,-1).reshape(1,dim,h,w)\n",
        "# print(out)\n",
        "\n",
        "\n",
        "pe = posemb_sincos_2d(h,w,dim)\n",
        "print(pe)\n",
        "\n",
        "\n",
        "# x = torch.randn(2,2, dtype=torch.cfloat)\n",
        "# x = torch.randn(3, 2)\n",
        "# y = torch.view_as_complex(x)\n",
        "# torch.view_as_real(y)\n",
        "# torch.polar(abs, angle)\n",
        "# torch.real(x)\n",
        "# torch.complex(x)\n",
        "# x.real\n",
        "# x.imag\n",
        "# x1=torch.tensor([3j, 4+4j])\n",
        "# x1.abs()\n",
        "# x1.angle()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !git clone https://github.com/fla-org/flash-bidirectional-linear-attention.git\n",
        "%pip install -e flash-bidirectional-linear-attention/.\n",
        "import fbi_la\n",
        "from fbi_la.ops.linear_attn.attention import linear_attention\n",
        "x = linear_attention(q,k,v)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6A-bSQMWGR78",
        "outputId": "ed47fc4c-4961-409d-9ef5-4b18039d5d62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Obtaining file:///content/flash-bidirectional-linear-attention\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Installing collected packages: fbi_la\n",
            "  Attempting uninstall: fbi_la\n",
            "    Found existing installation: fbi_la 0.1\n",
            "    Uninstalling fbi_la-0.1:\n",
            "      Successfully uninstalled fbi_la-0.1\n",
            "  Running setup.py develop for fbi_la\n",
            "Successfully installed fbi_la-0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title fla-org simple_gla\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from einops import rearrange, repeat\n",
        "\n",
        "# https://github.com/fla-org/flash-linear-attention/blob/main/fla/ops/simple_gla/naive.py#L7\n",
        "def torch_simple_gla(q, k, v, g, chunk_size=64, scale=None):\n",
        "    scale = scale or q.shape[-1]**-.5\n",
        "    q = rearrange(q, 'b h (n c) d -> b h n c d', c=chunk_size) * scale # [b, n_heads, t/c, c, key_dim]\n",
        "    k = rearrange(k, 'b h (n c) d -> b h n c d', c=chunk_size)\n",
        "    v = rearrange(v, 'b h (n c) d -> b h n c d', c=chunk_size)\n",
        "    g = rearrange(g, 'b h (n c) -> b h n c', c=chunk_size) # [b,n_heads,t/c,c]\n",
        "    g = g.cumsum(-1)\n",
        "    # kv = k.transpose(-1, -2) @ (v * (-g + g[:, :, :, -1, None]).exp()[..., None])\n",
        "    kv = k.transpose(-1, -2) @ (v * (-g + g[:, :, :, -1:]).exp()[..., None])\n",
        "    S = torch.zeros_like(kv)\n",
        "\n",
        "    for i in range(1, g.shape[-2]):\n",
        "        # S[:, :, i] = S[:, :, i-1].clone() * g[:, :, i-1, -1, None, None].exp() + kv[:, :, i-1]\n",
        "        S[:, :, i] = S[:, :, i-1].clone() * g[:, :, i-1, -1:, None].exp() + kv[:, :, i-1]\n",
        "\n",
        "    # print('torch_simple_gla 1',q.shape, k.shape, v.shape, g.shape, S.shape)\n",
        "    inter = (q * g[..., None].exp()) @ S # [2, 4, 64, 16, 32]\n",
        "    attn = q @ k.transpose(-1, -2)\n",
        "    attn = attn * (g[..., None] - g[..., None, :]).exp()\n",
        "    # attn = attn * (g.unsqueeze(-1) - g.unsqueeze(-2)).exp()\n",
        "    attn = attn.masked_fill(torch.triu(torch.ones(chunk_size, chunk_size, dtype=bool, device=q.device), diagonal=1), 0)\n",
        "    intra = attn @ v\n",
        "    # print('torch_simple_gla 3',inter.shape, intra.shape)#[2, 10, 1, 16, 8]) torch.Size([2, 10, 1, 16, 8][2, 4, 1024, 32]) torch.Size([2, 1024, 128]\n",
        "    #     [2, 10, 8, 16, 8]) torch.Size([2, 10, 8, 16, 8]\n",
        "    o = inter + intra\n",
        "    return rearrange(o, 'b h n c d -> b h (n c) d') # [b, n_heads, t, key_dim]\n",
        "\n",
        "# https://github.com/fla-org/flash-linear-attention/blob/main/fla/layers/simple_gla.py#L21\n",
        "class GLAblock(nn.Module):\n",
        "    def __init__(self, hidden_size, expand_k=.5, expand_v=1, num_heads=4, num_kv_heads=None):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.num_kv_heads = num_kv_heads if num_kv_heads is not None else num_heads\n",
        "        self.num_kv_groups = self.num_heads // self.num_kv_heads\n",
        "\n",
        "        self.key_dim = int(hidden_size * expand_k)\n",
        "        self.value_dim = int(hidden_size * expand_v)\n",
        "        self.key_dim_per_group = self.key_dim // self.num_kv_groups\n",
        "        self.value_dim_per_group = self.value_dim // self.num_kv_groups\n",
        "        self.head_k_dim = self.key_dim // num_heads\n",
        "        self.head_v_dim = self.value_dim // num_heads\n",
        "\n",
        "        self.q_proj = nn.Linear(hidden_size, self.key_dim, bias=False)\n",
        "        self.k_proj = nn.Linear(hidden_size, self.key_dim_per_group, bias=False)\n",
        "        self.v_proj = nn.Linear(hidden_size, self.value_dim_per_group, bias=False)\n",
        "        self.g_proj = nn.Linear(hidden_size, self.value_dim, bias=False)\n",
        "        gate_low_rank_dim=16\n",
        "        self.gk_proj = nn.Sequential(nn.Linear(hidden_size, gate_low_rank_dim, bias=False),\n",
        "                                nn.Linear(gate_low_rank_dim, self.num_heads, bias=True))\n",
        "        self.o_proj = nn.Linear(self.value_dim, hidden_size, bias=False)\n",
        "        self.g_norm = nn.RMSNorm(self.head_v_dim)\n",
        "        self.gate_fn = nn.SiLU()\n",
        "\n",
        "    def forward(self, x): # [b,t,d]\n",
        "        q, k, v = self.q_proj(x), self.k_proj(x), self.v_proj(x) # [b,t,key_dim/key_dim_per_group/value_dim_per_group]\n",
        "        gk = self.gk_proj(x) # [b,t,n_heads]\n",
        "        # if attention_mask is not None: v = v.mul_(attention_mask[:, -v.shape[-2]:, None]) # 0-1 matrix with shape [batch_size, seq_len]\n",
        "        gk = gk.transpose(-2,-1)\n",
        "        # q = rearrange(q, '... (h d) -> ... h d', d=self.head_k_dim)\n",
        "        # q = rearrange(q, '... (h d) -> ... h d', h=self.num_heads) # [b,t,n_heads,key_dim/n_heads]\n",
        "        q = rearrange(q, 'b t (h d) -> b h t d', h=self.num_heads) # [b,t,n_heads,key_dim/n_heads]\n",
        "        if self.num_kv_groups > 1:\n",
        "            print(self.num_kv_groups)\n",
        "            k, gk = (repeat(x, '... (h d) -> ... (h g) d', g=self.num_kv_groups, d=self.head_k_dim) for x in (k, gk))\n",
        "            # k = repeat(k, '... (h d) -> ... (h g) d', g=self.num_kv_groups, d=self.head_k_dim)\n",
        "            v = repeat(v, '... (h d) -> ... (h g) d', g=self.num_kv_groups, d=self.head_v_dim)\n",
        "            # k, v = (repeat(x, '... (h d) -> ... (h g) d', h=self.num_kv_heads, g=self.num_kv_groups) for x in (k, v))\n",
        "        else:\n",
        "            # k = rearrange(k, '... (h d) -> ... h d', d=self.head_k_dim)\n",
        "            # v = rearrange(v, '... (h d) -> ... h d', d=self.head_v_dim)\n",
        "            # k, v = (rearrange(x, '... (h d) -> ... h d', h=self.num_kv_heads) for x in (k, v)) # [b,t,num_kv_heads,head_k/v_dim]\n",
        "            k, v = (rearrange(x, 'b t (h d) -> b h t d', h=self.num_kv_heads) for x in (k, v)) # [b,t,num_kv_heads,head_k/v_dim]\n",
        "        gk = F.logsigmoid(gk) / 16 # gate_logit_normalizer\n",
        "\n",
        "        # print('fwd 1',q.shape, k.shape, v.shape, gk.shape)\n",
        "        o = torch_simple_gla(q, k, v, gk, chunk_size=16) # [b, n_heads, t, key_dim]\n",
        "\n",
        "        g = self.g_proj(x) # [b,t,value_dim]\n",
        "        # print(o.shape, g.shape)\n",
        "        # o = rearrange(self.g_norm(o), '... h d -> ... (h d)')\n",
        "        o = rearrange(self.g_norm(o), 'b h t d -> b t (h d)')\n",
        "        # print(o.shape, g.shape)\n",
        "        o = o * self.gate_fn(g)\n",
        "        o = self.o_proj(o)\n",
        "        return o\n",
        "\n",
        "\n",
        "import torch\n",
        "d_model=128\n",
        "# attn = GatedLinearAttention(mode='chunk', hidden_size=d_model, expand_k=.5, expand_v=1, num_heads=4)\n",
        "# attn = GLAblock(hidden_size=d_model, expand_k=.5, expand_v=1, num_heads=16, num_kv_heads=2)\n",
        "attn = GLAblock(hidden_size=d_model, expand_k=1, expand_v=1, num_heads=4)\n",
        "# torch_simple_gla(q, k, v, g, chunk_size=64)\n",
        "# n_heads must be mul of chunk\n",
        "\n",
        "x = torch.randn(2, 512, d_model)\n",
        "out = attn(x)\n",
        "print(out.shape)\n",
        "\n",
        "# GLAblock fwd 1 torch.Size([2, 10, 64]) torch.Size([2, 10, 64]) torch.Size([2, 10, 128]) torch.Size([2, 10, 16])\n",
        "# GLAblock fwd 2 torch.Size([2, 10, 16, 4]) torch.Size([2, 10, 16, 4]) torch.Size([2, 10, 16, 8]) torch.Size([2, 10, 16])\n",
        "# torch_simple_gla 1 torch.Size([2, 10, 1, 16, 4]) torch.Size([2, 10, 1, 16, 4]) torch.Size([2, 10, 1, 16, 8]) torch.Size([2, 10, 1, 16])\n",
        "# torch_simple_gla 3 torch.Size([2, 10, 1, 16, 8]) torch.Size([2, 10, 1, 16, 8])\n",
        "# torch.Size([2, 10, 16, 8]) torch.Size([2, 10, 128])\n",
        "# torch.Size([2, 10, 128]) torch.Size([2, 10, 128])\n",
        "# torch.Size([2, 10, 128])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fA0jpysWODOh",
        "outputId": "c7048868-f4b1-475d-f2a7-df54c10589f3",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 512, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title GlobalContext AttentionPool\n",
        "        # x = x.flatten(-2).mean(dim=-1) # mean pool\n",
        "        attn_weights = self.attention_pool(x).squeeze(-1) # [batch, (h,w)] # seq_pool\n",
        "        x = (attn_weights.softmax(dim = 1).unsqueeze(1) @ x).squeeze(1) # [batch, 1, (h,w)] @ [batch, (h,w), dim]\n",
        "        # cls_token = repeat(self.class_emb, '1 1 d -> b 1 d', b = b)\n",
        "        # x = torch.cat((cls_token, x), dim=1)\n",
        "        # x = x[:, 0] # first token\n",
        "\n",
        "# https://github.com/lucidrains/imagen-pytorch/blob/main/imagen_pytorch/imagen_pytorch.py#L969\n",
        "class GlobalContext(nn.Module):\n",
        "    \"\"\" basically a superior form of squeeze-excitation that is attention-esque \"\"\"\n",
        "    def __init__(self, *, dim_in, dim_out):\n",
        "        super().__init__()\n",
        "        self.to_k = nn.Conv2d(dim_in, 1, 1)\n",
        "        hidden_dim = max(3, dim_out // 2)\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(dim_in, hidden_dim, 1), nn.SiLU(),\n",
        "            nn.Conv2d(hidden_dim, dim_out, 1), nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        context = self.to_k(x)\n",
        "        x, context = map(lambda t: rearrange(t, 'b n ... -> b n (...)'), (x, context))\n",
        "        out = einsum('b i n, b c n -> b c i', context.softmax(dim = -1), x)\n",
        "        out = rearrange(out, '... -> ... 1')\n",
        "        return self.net(out)\n",
        "\n",
        "\n",
        "# https://github.com/lucidrains/enformer-pytorch/blob/main/enformer_pytorch/modeling_enformer.py#L159\n",
        "class AttentionPool(nn.Module):\n",
        "    def __init__(self, dim, pool_size = 2):\n",
        "        super().__init__()\n",
        "        self.pool_size = pool_size\n",
        "        self.pool_fn = Rearrange('b d (n p) -> b d n p', p = pool_size)\n",
        "        self.to_attn_logits = nn.Conv2d(dim, dim, 1, bias = False)\n",
        "        nn.init.dirac_(self.to_attn_logits.weight)\n",
        "        with torch.no_grad():\n",
        "            self.to_attn_logits.weight.mul_(2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, _, n = x.shape\n",
        "        remainder = n % self.pool_size\n",
        "        needs_padding = remainder > 0\n",
        "        if needs_padding:\n",
        "            x = F.pad(x, (0, remainder), value = 0)\n",
        "            mask = torch.zeros((b, 1, n), dtype = torch.bool, device = x.device)\n",
        "            mask = F.pad(mask, (0, remainder), value = True)\n",
        "        x = self.pool_fn(x)\n",
        "        logits = self.to_attn_logits(x)\n",
        "        if needs_padding:\n",
        "            mask_value = -torch.finfo(logits.dtype).max\n",
        "            logits = logits.masked_fill(self.pool_fn(mask), mask_value)\n",
        "        attn = logits.softmax(dim = -1)\n",
        "        return (x * attn).sum(dim = -1)\n"
      ],
      "metadata": {
        "id": "taw6x-Nyxai3",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title PixelShuffle init_conv\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def init_conv(conv, out_r=1, in_r=1):\n",
        "    o, i, h, w = conv.weight.shape\n",
        "    conv_weight = torch.empty(o//out_r**2, i//in_r**2, h, w)\n",
        "    nn.init.kaiming_uniform_(conv_weight)\n",
        "    conv.weight.data.copy_(conv_weight.repeat_interleave(out_r**2, dim=0).repeat_interleave(in_r**2, dim=1))\n",
        "    if conv.bias is not None: nn.init.zeros_(conv.bias)\n",
        "    return conv\n",
        "\n",
        "# Inline model initialization\n",
        "net = nn.Sequential(\n",
        "    # nn.PixelUnshuffle(2), nn.Conv2d(3 * 2**2, 4, 3, 1, padding=3//2),\n",
        "    # nn.Conv2d(4, 3 * 2**2, 3, 1, padding=3//2), nn.PixelShuffle(2)\n",
        "    # nn.PixelUnshuffle(2), init_conv(nn.Conv2d(3 * 2**2, 4, 3, 1, padding=3//2), down=2),\n",
        "    init_conv(nn.Conv2d(3, 3 * 2**2, 3, 1, padding=3//2), out_r=2), nn.PixelShuffle(2), # good\n",
        "    # nn.PixelShuffle(2), init_conv(nn.Conv2d(3* 2**2, 3, 3, 1, padding=3//2), in_r=2),\n",
        "\n",
        "    # nn.PixelUnshuffle(2), init_conv(nn.Conv2d(3 * 2**2, 2* 2**2, 3, 1, padding=3//2), up=2, down=2), nn.PixelUnshuffle(2),\n",
        "    # nn.PixelShuffle(2), init_conv(nn.Conv2d(2* 2**2, 3 * 2**2, 3, 1, padding=3//2), up=2, down=2), nn.PixelShuffle(2),\n",
        "\n",
        "    nn.PixelUnshuffle(2), init_conv(nn.Conv2d(3 * 2**2, 3, 3, 1, padding=3//2), in_r=2), # good\n",
        "    # init_conv(nn.Conv2d(3, 3, 3, 1, padding=3//2))\n",
        ")\n",
        "# self.net = nn.Sequential(nn.Conv2d(in_ch, out_ch * r**2, kernel_size, 1, padding=kernel_size//2), nn.PixelShuffle(r)) # PixelShuffle: [b,c*r^2,h,w] -> [b,c,h*r,w*r] # upscale by upscale factor r # https://arxiv.org/pdf/1609.05158v2\n",
        "# self.net = nn.Sequential(nn.PixelUnshuffle(r), nn.Conv2d(in_ch * r**2, out_ch, kernel_size, 1, padding=kernel_size//2)) # PixelUnshuffle: [b,c,h*r,w*r] -> [b,c*r^2,h,w]\n",
        "\n",
        "\n",
        "# for n, p in net.named_parameters():\n",
        "#     print(n, p)\n",
        "\n",
        "\n",
        "dataiter = iter(train_loader) # get some random training images\n",
        "images, labels = next(dataiter)\n",
        "out = net(images[0])\n",
        "# out = net(images[0].repeat_interleave(4, dim=0))\n",
        "\n",
        "print(out.shape)\n",
        "imshow(out.detach().cpu())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 486
        },
        "id": "QHhXxS4K2Xye",
        "outputId": "49bbc237-4cba-4dca-b23b-9a6b1bfb2105",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-13.823969..10.89641].\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 32, 32])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# x = torch.rand(1,2,4,4)\n",
        "# print(x)\n",
        "# x = nn.PixelUnshuffle(2)(x)\n",
        "# print(x) # [1,8,2,2]\n",
        "# print(x.shape) # [1,2,4,4]\n",
        "x = torch.rand(1,2,2,2).repeat(1,4,1,1)\n",
        "print(x)\n",
        "x = torch.rand(1,2,2,2).repeat_interleave(4,dim=1)\n",
        "print(x)\n",
        "\n",
        "# x = torch.rand(1,4,2,2)\n",
        "# print(x)\n",
        "# x = nn.PixelShuffle(2)(x)\n",
        "# print(x)\n"
      ],
      "metadata": {
        "id": "tW4kWJ71Aon0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GxePWzy1aTWt",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title ViT me\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class SimpleViT(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim, dim, depth, heads, mlp_dim, channels=3, dim_head=8):\n",
        "        super().__init__()\n",
        "\n",
        "        self.to_patch_embedding = nn.Sequential( # in, out, kernel, stride, pad\n",
        "            # nn.Conv2d(in_dim, dim, kernel_size=7, stride=2, padding=7//2, bias=False), nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "            # PixelShuffleConv(3, dim, 1, r=1/4),\n",
        "            nn.PixelUnshuffle(2), nn.Conv2d(in_dim * 2**2, dim//2**2, 7, 1, padding=7//2), nn.PixelUnshuffle(2),\n",
        "            )\n",
        "\n",
        "\n",
        "        # dim = h/2^2 * w/2^2\n",
        "        # self.pos_embedding = LearnedRoPE2D(dim)\n",
        "        # self.pos_embedding = RoPE2D(dim)\n",
        "        self.positional_emb = nn.Parameter(torch.zeros(1, 8*8, dim), requires_grad=True) # positional_embedding == 'learnable'\n",
        "        # nn.init.trunc_normal_(self.positional_emb, std=0.2)\n",
        "\n",
        "        # self.dropout = nn.Dropout(p=0.1)\n",
        "        self.transformer = AttentionBlock(d_model=dim, d_head=dim_head)\n",
        "        # self.norm = nn.LayerNorm(dim)\n",
        "\n",
        "        self.attention_pool = nn.Linear(dim, 1)\n",
        "        self.out = nn.Linear(dim, out_dim, bias=False)\n",
        "\n",
        "    def forward(self, img):\n",
        "        device = img.device\n",
        "        x = self.to_patch_embedding(img)\n",
        "        # x = self.pos_embedding(x)\n",
        "        x = x.flatten(-2).transpose(-2,-1) # b c h w -> b (h w) c\n",
        "        x = x + self.positional_emb\n",
        "        # x = self.dropout(x)\n",
        "\n",
        "        x = self.transformer(x)\n",
        "        # x = self.norm(x)\n",
        "\n",
        "        # x = x.flatten(-2).mean(dim=-1) # meal pool\n",
        "        attn_weights = self.attention_pool(x).squeeze(-1) # [batch, (h,w)] # seq_pool\n",
        "        x = (attn_weights.softmax(dim = 1).unsqueeze(1) @ x).squeeze(1) # [batch, 1, (h,w)] @ [batch, (h,w), dim]\n",
        "\n",
        "        # cls_token = repeat(self.class_emb, '1 1 d -> b 1 d', b = b)\n",
        "        # x = torch.cat((cls_token, x), dim=1)\n",
        "        # x = x[:, 0] # first token\n",
        "        return self.out(x)\n",
        "\n",
        "\n",
        "# pos_emb rope < learn < learned\n",
        "# conv > pixel?\n",
        "# droppath not required\n",
        "\n",
        "dim = 64\n",
        "dim_head = 8\n",
        "heads = dim // dim_head\n",
        "num_classes = 10\n",
        "# model = SimpleViT(image_size=32, patch_size=4, num_classes=num_classes, dim=dim, depth=1, heads=heads, mlp_dim=dim*4, channels = 3, dim_head = dim_head)\n",
        "model = SimpleViT(in_dim=3, out_dim=num_classes, dim=dim, depth=1, heads=heads, mlp_dim=dim*4, channels = 3, dim_head = dim_head).to(device)\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 59850\n",
        "optim = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "\n",
        "# print(images.shape) # [batch, 3, 32, 32]\n",
        "x = torch.rand(64, 3, 32, 32, device=device)\n",
        "# x = torch.rand(64, 3, 28,28, device=device)\n",
        "logits = model(x)\n",
        "print(logits.shape)\n",
        "# print(logits[0])\n",
        "# print(logits[0].argmax(1))\n",
        "pred_probab = nn.Softmax(dim=1)(logits)\n",
        "y_pred = pred_probab.argmax(1)\n",
        "# print(f\"Predicted class: {y_pred}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ViT me simple\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, n_heads=None, d_head=8, cond_dim=None, dropout=0.): # .1\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.d_head = d_head\n",
        "        self.n_heads = d_model // d_head\n",
        "        # self.d_head = d_model // n_heads\n",
        "        self.cond_dim = cond_dim\n",
        "        self.q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.kv = nn.Linear(cond_dim or d_model, 2*d_model, bias=False)\n",
        "        # self.k = nn.Sequential(nn.Dropout(dropout), nn.Linear(cond_dim, d_model, bias=False))\n",
        "        self.lin = nn.Linear(d_model, d_model)\n",
        "        # self.lin = zero_module(nn.Linear(d_model, d_model))\n",
        "        # self.lin = nn.Sequential(nn.Dropout(dropout), zero_module(nn.Linear(d_model, d_model)))\n",
        "        self.drop = nn.Dropout(dropout) # indp before q,k,v; after linout\n",
        "        self.scale = self.d_head ** -.5\n",
        "\n",
        "    def forward(self, x, cond=None, mask=None): # [batch, T, d_model]=[batch, h*w, c], [batch, num_tok, cond_dim], [batch,T]\n",
        "        batch = x.shape[0]\n",
        "        if self.cond_dim==None: cond=x # is self attn\n",
        "        Q = self.q(x).view(batch, -1, self.n_heads, self.d_head).transpose(1, 2) # [batch, T, d_model] -> [batch, n_heads, T, d_head]\n",
        "        # K = self.k(x).view(batch, -1, self.n_heads, self.d_head).transpose(1, 2)\n",
        "        K, V = self.kv(cond).view(batch, -1, self.n_heads, 2*self.d_head).transpose(1, 2).chunk(2, dim=-1) # [batch, n_heads, T/num_tok, d_head]\n",
        "\n",
        "        # # linear attention # Softmax(Q) @ (Softmax(K).T @ V)\n",
        "        if mask != None:\n",
        "            mask = mask[:, None, :, None] # [batch,T] -> [batch,1,T,1]\n",
        "            K, V = K.masked_fill(mask, -torch.finfo(x.dtype).max), V.masked_fill(mask, -torch.finfo(x.dtype).max)\n",
        "        Q, K = Q.softmax(dim=-1)*self.scale, K.softmax(dim=-2)\n",
        "        context = K.transpose(-2,-1) @ V # [batch, n_heads, d_head, d_head]\n",
        "        out = Q @ context # [batch, n_heads, T/num_tok, d_head]\n",
        "\n",
        "        # # (quadratic) attention # Softmax(Q @ K.T) @ V\n",
        "        # attn = Q @ K.transpose(-2,-1) * self.scale # [batch, n_heads, T] # [batch, n_heads, T, T/num_tok]\n",
        "        # if mask != None: attn = attn.masked_fill(mask[:, None, :, None], -torch.finfo(attn.dtype).max) # [batch,T]->[batch,1,T,1]\n",
        "        # attention = torch.softmax(attn, dim=-1)\n",
        "        # out = self.drop(attention) @ V # [batch, n_heads, T, d_head]\n",
        "\n",
        "        out = out.transpose(1, 2).flatten(2)\n",
        "        return self.drop(self.lin(out)) # [batch, T, d_model]\n",
        "\n",
        "# if self, dont pass cond_dim in init, dont pass cond in fwd\n",
        "# Softmax(Q @ K.T) @ V ~ Softmax(Q) @ Softmax(K).T @ V\n",
        "\n",
        "# https://github.com/lucidrains/x-transformers/blob/main/x_transformers/x_transformers.py#L1855\n",
        "\n",
        "class AttentionBlock(nn.Module):\n",
        "    def __init__(self, d_model, d_head, cond_dim=None, ff_mult=1, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.norm1 = nn.RMSNorm(d_model) # LayerNorm RMSNorm\n",
        "        self.norm2 = nn.RMSNorm(d_model)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.cond_dim = cond_dim\n",
        "        self.self = MultiHeadAttention(d_model, d_head=d_head, dropout=0)\n",
        "        if self.cond_dim!=None: self.cross = MultiHeadAttention(d_model, d_head=d_head, cond_dim=cond_dim, dropout=0)\n",
        "        act = nn.GELU() # ReLU GELU\n",
        "        ff_dim=d_model*ff_mult\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.RMSNorm(d_model), nn.Dropout(dropout), nn.Linear(d_model, ff_dim), act, # ReLU GELU\n",
        "            nn.RMSNorm(d_model), nn.Dropout(dropout), nn.Linear(ff_dim, d_model)\n",
        "            # nn.RMSNorm(d_model), act, nn.Dropout(dropout), nn.Linear(d_model, ff_dim),\n",
        "            # nn.RMSNorm(ff_dim), act, nn.Dropout(dropout), nn.Linear(ff_dim, d_model)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, cond=None, mask=None): # [b,c,h,w], [batch, num_tok, cond_dim], [batch,T]\n",
        "        # bchw = x.shape\n",
        "        # x = x.flatten(2).transpose(1,2) # [b,h*w,c]\n",
        "\n",
        "        if self.cond_dim==None: cond=None # is self attn\n",
        "        x = x + self.drop(self.self(self.norm1(x)))\n",
        "        if self.cond_dim!=None: x = x + self.cross(self.norm2(x), cond, mask)\n",
        "        x = x + self.ff(x)\n",
        "        # x = x + self.drop(self.ff(x))\n",
        "\n",
        "        # return x.transpose(1,2).reshape(*bchw)\n",
        "        return x\n",
        "\n",
        "\n",
        "class SimpleViT(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim, dim, depth, heads, mlp_dim, channels=3, dim_head=8):\n",
        "        super().__init__()\n",
        "        self.to_patch_embedding = nn.Sequential( # in, out, kernel, stride, pad\n",
        "            nn.Conv2d(in_dim, dim, kernel_size=7, stride=2, padding=7//2, bias=False), nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "            # nn.Conv2d(in_dim, dim, kernel_size=7, stride=2, padding=7//2, bias=False), nn.MaxPool2d(2,2)\n",
        "            )\n",
        "        # self.pos_embedding = LearnedRoPE2D(dim) # LearnedRoPE2D, RoPE2D\n",
        "        self.positional_emb = nn.Parameter(torch.zeros(1, 8*8, dim)) # positional_embedding == 'learnable'\n",
        "        self.transformer = AttentionBlock(d_model=dim, d_head=dim_head)\n",
        "        # self.transformer_blocks = nn.ModuleList([AttentionBlock(d_model=dim, d_head=dim_head) for i in range(depth)])\n",
        "\n",
        "        self.attention_pool = nn.Linear(dim, 1)\n",
        "        self.out = nn.Linear(dim, out_dim, bias=False)\n",
        "\n",
        "    def forward(self, img):\n",
        "        device = img.device\n",
        "        x = self.to_patch_embedding(img)\n",
        "        # x = self.pos_embedding(x)\n",
        "        x = x.flatten(-2).transpose(-2,-1) # b c h w -> b (h w) c\n",
        "        x = x + self.positional_emb\n",
        "        x = self.transformer(x)\n",
        "        # for blk in self.predictor_blocks: x = blk(x)\n",
        "\n",
        "        # x = x.flatten(-2).mean(dim=-1) # mean pool\n",
        "        attn_weights = self.attention_pool(x).squeeze(-1) # [batch, (h,w)] # seq_pool\n",
        "        x = (attn_weights.softmax(dim = 1).unsqueeze(1) @ x).squeeze(1) # [batch, 1, (h,w)] @ [batch, (h,w), dim]\n",
        "        # cls_token = repeat(self.class_emb, '1 1 d -> b 1 d', b = b)\n",
        "        # x = torch.cat((cls_token, x), dim=1)\n",
        "        # x = x[:, 0] # first token\n",
        "        return self.out(x)\n",
        "\n",
        "\n",
        "# pos_emb rope < learn < learned\n",
        "# conv > pixel?\n",
        "# droppath not required\n",
        "\n",
        "# dim = 64\n",
        "# dim_head = 8\n",
        "# heads = dim // dim_head\n",
        "# num_classes = 10\n",
        "# # model = SimpleViT(image_size=32, patch_size=4, num_classes=num_classes, dim=dim, depth=1, heads=heads, mlp_dim=dim*4, channels = 3, dim_head = dim_head)\n",
        "# model = SimpleViT(in_dim=3, out_dim=num_classes, dim=dim, depth=1, heads=heads, mlp_dim=dim*4, channels = 3, dim_head = dim_head).to(device)\n",
        "# print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 59850\n",
        "# optim = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "\n",
        "# # print(images.shape) # [batch, 3, 32, 32]\n",
        "# x = torch.rand(64, 3, 32, 32, device=device)\n",
        "# # x = torch.rand(64, 3, 28,28, device=device)\n",
        "# logits = model(x)\n",
        "# print(logits.shape)\n",
        "# pred_probab = nn.Softmax(dim=1)(logits)\n",
        "# y_pred = pred_probab.argmax(1)\n",
        "# # print(y_pred)\n"
      ],
      "metadata": {
        "id": "GHzr3NVs2EcG",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "H13D4xpGzHI3"
      },
      "outputs": [],
      "source": [
        "# @title lucidrains simple_vit.py\n",
        "# https://github.com/lucidrains/vit-pytorch/blob/main/vit_pytorch/simple_vit.py\n",
        "import torch\n",
        "from torch import nn\n",
        "from einops import rearrange\n",
        "from einops.layers.torch import Rearrange\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def posemb_sincos_2d(h, w, dim, temperature = 10000, dtype = torch.float32):\n",
        "    y, x = torch.meshgrid(torch.arange(h), torch.arange(w), indexing=\"ij\")\n",
        "    assert (dim % 4) == 0, \"feature dimension must be multiple of 4 for sincos emb\"\n",
        "    omega = torch.arange(dim//4) / (dim//4 - 1)\n",
        "    omega = 1.0 / (temperature ** omega)\n",
        "    y = y.flatten()[:, None] * omega[None, :]\n",
        "    x = x.flatten()[:, None] * omega[None, :]\n",
        "    pe = torch.cat((x.sin(), x.cos(), y.sin(), y.cos()), dim=1)\n",
        "    return pe.type(dtype)\n",
        "# pos_embedding = posemb_sincos_2d(h = image_height // patch_height, w = image_width // patch_width, dim = dim)\n",
        "# x = x+pos_embedding\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.LayerNorm(dim), nn.Linear(dim, hidden_dim), nn.GELU(),\n",
        "            nn.Linear(hidden_dim, dim),\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, heads = 8, dim_head = 64):\n",
        "        super().__init__()\n",
        "        inner_dim = dim_head *  heads\n",
        "        self.heads = heads\n",
        "        self.scale = dim_head ** -0.5\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.attend = nn.Softmax(dim = -1)\n",
        "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
        "        self.to_out = nn.Linear(inner_dim, dim, bias = False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.norm(x)\n",
        "        qkv = self.to_qkv(x).chunk(3, dim = -1)\n",
        "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), qkv)\n",
        "        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
        "        attn = self.attend(dots)\n",
        "        out = torch.matmul(attn, v)\n",
        "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
        "        return self.to_out(out)\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, dim, depth, heads, dim_head, mlp_dim):\n",
        "        super().__init__()\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.layers = nn.ModuleList([])\n",
        "        for _ in range(depth):\n",
        "            self.layers.append(nn.ModuleList([\n",
        "                Attention(dim, heads = heads, dim_head = dim_head),\n",
        "                FeedForward(dim, mlp_dim)\n",
        "            ]))\n",
        "    def forward(self, x):\n",
        "        for attn, ff in self.layers:\n",
        "            x = attn(x) + x\n",
        "            x = ff(x) + x\n",
        "        return self.norm(x)\n",
        "\n",
        "\n",
        "def pair(t): return t if isinstance(t, tuple) else (t, t)\n",
        "class SimpleViT(nn.Module):\n",
        "    def __init__(self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, channels = 3, dim_head = 64):\n",
        "        super().__init__()\n",
        "        image_height, image_width = pair(image_size)\n",
        "        patch_height, patch_width = pair(patch_size)\n",
        "        assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size.'\n",
        "        patch_dim = channels * patch_height * patch_width\n",
        "        self.to_patch_embedding = nn.Sequential(\n",
        "            Rearrange(\"b c (h p1) (w p2) -> b (h w) (p1 p2 c)\", p1 = patch_height, p2 = patch_width),\n",
        "            nn.LayerNorm(patch_dim), nn.Linear(patch_dim, dim),\n",
        "            nn.LayerNorm(dim),\n",
        "        )\n",
        "        self.pos_embedding = posemb_sincos_2d(h = image_height // patch_height, w = image_width // patch_width, dim = dim)\n",
        "        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim)\n",
        "        self.to_latent = nn.Identity()\n",
        "        self.linear_head = nn.Linear(dim, num_classes)\n",
        "\n",
        "    def forward(self, img):\n",
        "        device = img.device\n",
        "        x = self.to_patch_embedding(img)\n",
        "        x += self.pos_embedding.to(device, dtype=x.dtype)\n",
        "        x = self.transformer(x)\n",
        "        x = x.mean(dim = 1)\n",
        "        x = self.to_latent(x)\n",
        "        return self.linear_head(x)\n",
        "\n",
        "\n",
        "# dim = 64\n",
        "# dim_head = 8\n",
        "# heads = dim // dim_head\n",
        "# num_classes = 10\n",
        "# # model = SimpleViT(image_size=32, patch_size=4, num_classes=num_classes, dim=dim, depth=1, heads=heads, mlp_dim=dim*4, channels = 3, dim_head = dim_head)\n",
        "# model = SimpleViT(num_classes=num_classes, dim=dim, depth=1, heads=heads, mlp_dim=dim*4, channels = 3, dim_head = dim_head)\n",
        "# optim = torch.optim.AdamW(model.parameters(), lr=1e-3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vtKPXZWBAEjn",
        "outputId": "ced7d1f1-5bd9-4210-af16-e061e3ec3bc3",
        "cellView": "form"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "39371\n",
            "torch.Size([64, 10])\n"
          ]
        }
      ],
      "source": [
        "# @title lucid CCT\n",
        "# !pip install -q vit-pytorch\n",
        "\n",
        "import torch\n",
        "from vit_pytorch.cct import CCT\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "model = CCT(\n",
        "    img_size = (32,32),\n",
        "    embedding_dim = 64,\n",
        "    n_conv_layers = 1,\n",
        "    kernel_size = 7,\n",
        "    stride = 2,\n",
        "    padding = 3,\n",
        "    pooling_kernel_size = 3,\n",
        "    pooling_stride = 2,\n",
        "    # pooling_padding = 1,\n",
        "    num_layers = 1,\n",
        "    num_heads = 8,\n",
        "    mlp_ratio = 1.,\n",
        "    num_classes = 10,\n",
        "    positional_embedding = 'learnable', # ['sine', 'learnable', 'none']\n",
        "    dim_head = 8,\n",
        "    # pool='mean', # ['mean', 'cls', 'max']\n",
        ")\n",
        "\n",
        "\n",
        "# def __init__(self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, pool = 'cls', channels = 3, dim_head = 64, dropout = 0., emb_dropout = 0.):\n",
        "\n",
        "optim = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 236235\n",
        "\n",
        "\n",
        "x = torch.rand(64, 3, 32, 32, device=device)\n",
        "# x = torch.rand(64, 1, 28,28, device=device)\n",
        "logits = model(x)\n",
        "print(logits.shape)\n",
        "pred_probab = nn.Softmax(dim=1)(logits)\n",
        "y_pred = pred_probab.argmax(1)\n",
        "\n",
        "# loss: 1.057389  [49920/50000]\n",
        "# Test Error:\n",
        "#  Accuracy: 58.5%, Avg loss: 1.146490\n",
        "# 8: loss: 0.801141  [49920/50000]\n",
        "# Test Error:\n",
        "#  Accuracy: 69.9%, Avg loss: 0.885053\n",
        "\n",
        "# print(model)\n",
        "\n",
        "# # cct 39371 learn\n",
        "# loss: 1.426435  [49920/50000]\n",
        "# Test Error:\n",
        "#  Accuracy: 50.0%, Avg loss: 1.355331\n",
        "# 5: loss: 0.954245  [49920/50000]\n",
        "# Test Error:\n",
        "#  Accuracy: 64.1%, Avg loss: 1.000242"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "DAOLtmMKZMQR"
      },
      "outputs": [],
      "source": [
        "# @title sincos_pos_embed\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/models/vision_transformer.py\n",
        "import numpy as np\n",
        "\n",
        "def get_2d_sincos_pos_embed_from_grid(embed_dim, grid):\n",
        "    assert embed_dim % 2 == 0\n",
        "    # use half of dimensions to encode grid_h\n",
        "    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[0])  # (H*W, D/2)\n",
        "    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[1])  # (H*W, D/2)\n",
        "    emb = np.concatenate([emb_h, emb_w], axis=1)  # (H*W, D)\n",
        "    return emb\n",
        "\n",
        "def get_2d_sincos_pos_embed(embed_dim, grid_size, cls_token=False): # grid_size of the grid height and width\n",
        "    grid_h, grid_w = np.arange(grid_size, dtype=float), np.arange(grid_size, dtype=float)\n",
        "    grid = np.meshgrid(grid_w, grid_h)  # here w goes first\n",
        "    grid = np.stack(grid, axis=0)\n",
        "    grid = grid.reshape([2, 1, grid_size, grid_size])\n",
        "    pos_embed = get_2d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
        "    if cls_token:\n",
        "        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0) # prepend zeros for cls token later?\n",
        "    return pos_embed # [grid_size*grid_size, embed_dim] or [1+grid_size*grid_size, embed_dim] (w/ or w/o cls_token)\n",
        "\n",
        "\n",
        "def get_1d_sincos_pos_embed_from_grid(embed_dim, pos):\n",
        "    \"\"\"\n",
        "    embed_dim: output dimension for each position\n",
        "    pos: a list of positions to be encoded: size (M,)\n",
        "    out: (M, D)\n",
        "    \"\"\"\n",
        "    assert embed_dim % 2 == 0\n",
        "    omega = np.arange(embed_dim // 2, dtype=float)\n",
        "    omega /= embed_dim / 2.\n",
        "    omega = 1. / 10000**omega   # (D/2,)\n",
        "    pos = pos.reshape(-1)   # (M,)\n",
        "    out = np.einsum('m,d->md', pos, omega)   # (M, D/2), outer product\n",
        "    emb_sin = np.sin(out)  # (M, D/2)\n",
        "    emb_cos = np.cos(out)  # (M, D/2)\n",
        "    emb = np.concatenate([emb_sin, emb_cos], axis=1)  # (M, D)\n",
        "    return emb\n",
        "\n",
        "import math\n",
        "def interpolate_pos_encoding(x, pos_embed):\n",
        "    npatch = x.shape[1] - 1\n",
        "    N = pos_embed.shape[1] - 1\n",
        "    if npatch == N: return pos_embed\n",
        "    class_emb, pos_embed = pos_embed[:, 0], pos_embed[:, 1:]\n",
        "    dim = x.shape[-1]\n",
        "    pos_embed = nn.functional.interpolate(pos_embed.reshape(1, int(math.sqrt(N)), int(math.sqrt(N)), dim).permute(0, 3, 1, 2), scale_factor=math.sqrt(npatch / N), mode='bicubic',)\n",
        "    pos_embed = pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n",
        "    return torch.cat((class_emb.unsqueeze(0), pos_embed), dim=1)\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "class PatchEmbed(nn.Module):\n",
        "    \"\"\" Image to Patch Embedding\"\"\"\n",
        "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n",
        "        super().__init__()\n",
        "        self.num_patches = (img_size // patch_size) * (img_size // patch_size)\n",
        "        self.img_size, self.patch_size = img_size, patch_size\n",
        "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "    def forward(self, x): # [b,c,h,w]\n",
        "        # B, C, H, W = x.shape\n",
        "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
        "        return x\n",
        "\n",
        "num_patches = 16\n",
        "embed_dim = 8\n",
        "# pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim), requires_grad=False)\n",
        "pos_embed = torch.zeros(1, num_patches, embed_dim)\n",
        "# patch_embed = PatchEmbed(img_size=img_size[0], patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n",
        "patch_embed = PatchEmbed()\n",
        "# print(pos_embed)\n",
        "# patch_embed.num_patches = (224/16)^2 = 14^2 = 196\n",
        "# pos_embed = get_2d_sincos_pos_embed(pos_embed.shape[-1], int(patch_embed.num_patches**.5), cls_token=False)\n",
        "pos_embed = get_2d_sincos_pos_embed(embed_dim, 14)\n",
        "print(pos_embed)\n",
        "print(pos_embed.shape) # 14^2, embed_dim\n",
        "\n",
        "# x = torch.rand(4,224*224,embed_dim)\n",
        "# pos_embed.data.copy_(torch.from_numpy(pos_embed).float().unsqueeze(0))\n",
        "\n",
        "# pos_embed = interpolate_pos_encoding(x, pos_embed)\n",
        "# print(pos_embed)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.imshow(pos_embed)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "3CA-Eu6eGzVQ"
      },
      "outputs": [],
      "source": [
        "# @title posemb_sincos_2d\n",
        "def posemb_sincos_2d(h, w, dim, temperature = 10000, dtype = torch.float32):\n",
        "    print(h,w)\n",
        "    y, x = torch.meshgrid(torch.arange(h), torch.arange(w), indexing=\"ij\")\n",
        "    # print(y,x) # [h,w], y:row_num, x:col_num\n",
        "    # assert (dim % 4) == 0, \"feature dimension must be multiple of 4 for sincos emb\"\n",
        "    omega = 1. / (temperature**torch.linspace(0,1,dim//4))\n",
        "    # print(omega)\n",
        "    y = y.reshape(-1,1) * omega.unsqueeze(0) # [h*w,1]*[1,dim//4] = [h*w,dim//4]\n",
        "    x = x.reshape(-1,1) * omega.unsqueeze(0)\n",
        "    print(y.shape,x.shape) # [h,w], y:row num, x:col num\n",
        "    pe = torch.cat((x.sin(), x.cos(), y.sin(), y.cos()), dim=1) # [h*w,dim]\n",
        "    return pe.type(dtype)\n",
        "\n",
        "# image_height, image_width = 64, 64\n",
        "# patch_height, patch_width = 8, 8\n",
        "h=3\n",
        "w=5\n",
        "dim=18\n",
        "# pos_embedding = posemb_sincos_2d(h = image_height // patch_height, w = image_width // patch_width, dim = dim)\n",
        "pos_embedding = posemb_sincos_2d(h=h, w=w, dim=dim)\n",
        "# x = x+pos_embedding\n",
        "# print(pos_embedding)\n",
        "# print(pos_embedding.shape) # [h*w, dim]\n",
        "# for i,x in enumerate(pos_embedding):\n",
        "#     if i%h==0: print('### ', i, ' ###')\n",
        "#     print(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "XwpnHW4wn9S1"
      },
      "outputs": [],
      "source": [
        "# @title data\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "train_data = torchvision.datasets.MNIST(root=\"data\", train=True, download=True,transform=transforms.ToTensor(),) # do not normalise! want img in [0,1)\n",
        "batch_size = 512 # 64 512\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "test_data = torchvision.datasets.MNIST(root=\"data\", train=False, download=True, transform=transforms.ToTensor(),) #opt no download\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# dataiter = iter(train_data)\n",
        "# x,y = next(dataiter)\n",
        "# print(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "2p0-65KlpqyB"
      },
      "outputs": [],
      "source": [
        "# @title lucidrains cct.py\n",
        "# https://github.com/lucidrains/vit-pytorch/blob/main/vit_pytorch/cct.py\n",
        "\n",
        "def sinusoidal_embedding(n_channels, dim):\n",
        "    pe = torch.FloatTensor([[p / (10000 ** (2 * (i // 2) / dim)) for i in range(dim)]\n",
        "                            for p in range(n_channels)])\n",
        "    pe[:, 0::2] = torch.sin(pe[:, 0::2])\n",
        "    pe[:, 1::2] = torch.cos(pe[:, 1::2])\n",
        "    return rearrange(pe, '... -> 1 ...')\n",
        "\n",
        "# modules\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, num_heads=8, attention_dropout=0.1, projection_dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.heads = num_heads\n",
        "        head_dim = dim // self.heads\n",
        "        self.scale = head_dim ** -0.5\n",
        "        self.qkv = nn.Linear(dim, dim * 3, bias=False)\n",
        "        self.attn_drop = nn.Dropout(attention_dropout)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(projection_dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "        qkv = self.qkv(x).chunk(3, dim = -1)\n",
        "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), qkv)\n",
        "        q = q * self.scale\n",
        "        attn = einsum('b h i d, b h j d -> b h i j', q, k)\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "        x = einsum('b h i j, b h j d -> b h i d', attn, v)\n",
        "        x = rearrange(x, 'b h n d -> b n (h d)')\n",
        "        return self.proj_drop(self.proj(x))\n",
        "\n",
        "\n",
        "class TransformerEncoderLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Inspired by torch.nn.TransformerEncoderLayer and\n",
        "    rwightman's timm package.\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1,attention_dropout=0.1, drop_path_rate=0.1):\n",
        "        super().__init__()\n",
        "        self.pre_norm = nn.LayerNorm(d_model)\n",
        "        self.self_attn = Attention(dim=d_model, num_heads=nhead, attention_dropout=attention_dropout, projection_dropout=dropout)\n",
        "        self.linear1  = nn.Linear(d_model, dim_feedforward)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.norm1    = nn.LayerNorm(d_model)\n",
        "        self.linear2  = nn.Linear(dim_feedforward, d_model)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.drop_path = DropPath(drop_path_rate)\n",
        "        self.activation = F.gelu\n",
        "\n",
        "    def forward(self, src, *args, **kwargs):\n",
        "        src = src + self.drop_path(self.self_attn(self.pre_norm(src)))\n",
        "        src = self.norm1(src)\n",
        "        src2 = self.linear2(self.dropout1(self.activation(self.linear1(src))))\n",
        "        src = src + self.drop_path(self.dropout2(src2))\n",
        "        return src\n",
        "\n",
        "class DropPath(nn.Module):\n",
        "    def __init__(self, drop_prob=None):\n",
        "        super().__init__()\n",
        "        self.drop_prob = float(drop_prob)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch, drop_prob, device, dtype = x.shape[0], self.drop_prob, x.device, x.dtype\n",
        "\n",
        "        if drop_prob <= 0. or not self.training:\n",
        "            return x\n",
        "\n",
        "        keep_prob = 1 - self.drop_prob\n",
        "        shape = (batch, *((1,) * (x.ndim - 1)))\n",
        "\n",
        "        keep_mask = torch.zeros(shape, device = device).float().uniform_(0, 1) < keep_prob\n",
        "        output = x.div(keep_prob) * keep_mask.float()\n",
        "        return output\n",
        "\n",
        "\n",
        "class Tokenizer(nn.Module):\n",
        "    def __init__(self, kernel_size, stride, padding, pooling_kernel_size=3, pooling_stride=2, pooling_padding=1, n_conv_layers=1,\n",
        "                 n_input_channels=3, n_output_channels=64, in_planes=64,\n",
        "                 activation=None, max_pool=True, conv_bias=False):\n",
        "        super().__init__()\n",
        "        n_filter_list = [n_input_channels] + [in_planes for _ in range(n_conv_layers - 1)] + [n_output_channels]\n",
        "        n_filter_list_pairs = zip(n_filter_list[:-1], n_filter_list[1:])\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            *[nn.Sequential(\n",
        "                nn.Conv2d(chan_in, chan_out, kernel_size=(kernel_size, kernel_size), stride=(stride, stride), padding=(padding, padding), bias=conv_bias),\n",
        "                nn.Identity() if not exists(activation) else activation(),\n",
        "                nn.MaxPool2d(kernel_size=pooling_kernel_size, stride=pooling_stride, padding=pooling_padding) if max_pool else nn.Identity())\n",
        "                for chan_in, chan_out in n_filter_list_pairs\n",
        "            ])\n",
        "        self.apply(self.init_weight)\n",
        "\n",
        "    def sequence_length(self, n_channels=3, height=224, width=224):\n",
        "        return self.forward(torch.zeros((1, n_channels, height, width))).shape[1]\n",
        "\n",
        "    def forward(self, x):\n",
        "        return rearrange(self.conv_layers(x), 'b c h w -> b (h w) c')\n",
        "\n",
        "    @staticmethod\n",
        "    def init_weight(m):\n",
        "        if isinstance(m, nn.Conv2d):\n",
        "            nn.init.kaiming_normal_(m.weight)\n",
        "\n",
        "class Tokenizer(nn.Module):\n",
        "    def __init__(self, n_conv_layers=1, n_input_channels=3, n_output_channels=64, in_planes=64,):\n",
        "        super().__init__()\n",
        "        n_filter_list = [n_input_channels] + [in_planes for _ in range(n_conv_layers - 1)] + [n_output_channels]\n",
        "        # print(n_filter_list)\n",
        "        n_filter_list_pairs = zip(n_filter_list[:-1], n_filter_list[1:])\n",
        "        # print([x for x in n_filter_list_pairs])\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            *[nn.Sequential(\n",
        "                nn.Conv2d(chan_in, chan_out, kernel_size=7, stride=2, padding=7//2, bias=False),\n",
        "                # nn.Identity() if not exists(activation) else activation(), # None\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "                ) for chan_in, chan_out in n_filter_list_pairs\n",
        "            ])\n",
        "\n",
        "    # def sequence_length(self, n_channels=3, height=224, width=224):\n",
        "    #     return self.forward(torch.zeros((1, n_channels, height, width))).shape[1]\n",
        "\n",
        "    def forward(self, x):\n",
        "        # return rearrange(self.conv_layers(x), 'b c h w -> b (h w) c')\n",
        "        return self.conv_layers(x)\n",
        "\n",
        "class TransformerClassifier(nn.Module):\n",
        "    def __init__(self,\n",
        "                 seq_pool=True,\n",
        "                 embedding_dim=768,\n",
        "                 num_layers=12,\n",
        "                 num_heads=12,\n",
        "                 mlp_ratio=4.0,\n",
        "                 num_classes=1000,\n",
        "                 dropout_rate=0.1,\n",
        "                 attention_dropout=0.1,\n",
        "                 stochastic_depth_rate=0.1,\n",
        "                 positional_embedding='sine',\n",
        "                 sequence_length=None,\n",
        "                 *args, **kwargs):\n",
        "        super().__init__()\n",
        "        assert positional_embedding in {'sine', 'learnable', 'none'}\n",
        "\n",
        "        dim_feedforward = int(embedding_dim * mlp_ratio)\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.sequence_length = sequence_length\n",
        "        self.seq_pool = seq_pool\n",
        "\n",
        "        assert exists(sequence_length) or positional_embedding == 'none', \\\n",
        "            f\"Positional embedding is set to {positional_embedding} and\" \\\n",
        "            f\" the sequence length was not specified.\"\n",
        "        if not seq_pool:\n",
        "            sequence_length += 1\n",
        "            self.class_emb = nn.Parameter(torch.zeros(1, 1, self.embedding_dim), requires_grad=True)\n",
        "        else:\n",
        "            self.attention_pool = nn.Linear(self.embedding_dim, 1)\n",
        "        if positional_embedding == 'none':\n",
        "            self.positional_emb = None\n",
        "        elif positional_embedding == 'learnable':\n",
        "            self.positional_emb = nn.Parameter(torch.zeros(1, sequence_length, embedding_dim),\n",
        "                                               requires_grad=True)\n",
        "            nn.init.trunc_normal_(self.positional_emb, std=0.2)\n",
        "        else:\n",
        "            self.positional_emb = nn.Parameter(sinusoidal_embedding(sequence_length, embedding_dim),\n",
        "                                               requires_grad=False)\n",
        "        self.dropout = nn.Dropout(p=dropout_rate)\n",
        "        dpr = [x.item() for x in torch.linspace(0, stochastic_depth_rate, num_layers)]\n",
        "        self.blocks = nn.ModuleList([\n",
        "            TransformerEncoderLayer(d_model=embedding_dim, nhead=num_heads,\n",
        "                                    dim_feedforward=dim_feedforward, dropout=dropout_rate,\n",
        "                                    attention_dropout=attention_dropout, drop_path_rate=layer_dpr)\n",
        "            for layer_dpr in dpr])\n",
        "        self.norm = nn.LayerNorm(embedding_dim)\n",
        "        self.fc = nn.Linear(embedding_dim, num_classes)\n",
        "        self.apply(self.init_weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        b = x.shape[0]\n",
        "        if not exists(self.positional_emb) and x.size(1) < self.sequence_length:\n",
        "            x = F.pad(x, (0, 0, 0, self.n_channels - x.size(1)), mode='constant', value=0)\n",
        "        if not self.seq_pool:\n",
        "            cls_token = repeat(self.class_emb, '1 1 d -> b 1 d', b = b)\n",
        "            x = torch.cat((cls_token, x), dim=1)\n",
        "        if exists(self.positional_emb):\n",
        "            x += self.positional_emb\n",
        "        x = self.dropout(x)\n",
        "        for blk in self.blocks:\n",
        "            x = blk(x)\n",
        "        x = self.norm(x)\n",
        "        if self.seq_pool:\n",
        "            attn_weights = rearrange(self.attention_pool(x), 'b n 1 -> b n')\n",
        "            x = einsum('b n, b n d -> b d', attn_weights.softmax(dim = 1), x)\n",
        "        else:\n",
        "            x = x[:, 0]\n",
        "        return self.fc(x)\n",
        "\n",
        "    @staticmethod\n",
        "    def init_weight(m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            nn.init.trunc_normal_(m.weight, std=.02)\n",
        "            if isinstance(m, nn.Linear) and exists(m.bias):\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ILIdfDgmycM9"
      },
      "outputs": [],
      "source": [
        "# @title lucidrains vit.py\n",
        "# https://github.com/lucidrains/vit-pytorch/blob/main/vit_pytorch/vit.py\n",
        "import torch\n",
        "from torch import nn\n",
        "from einops import rearrange, repeat\n",
        "from einops.layers.torch import Rearrange\n",
        "\n",
        "# helpers\n",
        "def pair(t):\n",
        "    return t if isinstance(t, tuple) else (t, t)\n",
        "\n",
        "# classes\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, dim, hidden_dim, dropout = 0.):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.LayerNorm(dim), nn.Linear(dim, hidden_dim), nn.GELU(),\n",
        "            nn.Dropout(dropout), nn.Linear(hidden_dim, dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):\n",
        "        super().__init__()\n",
        "        inner_dim = dim_head *  heads\n",
        "        project_out = not (heads == 1 and dim_head == dim)\n",
        "        self.heads = heads\n",
        "        self.scale = dim_head ** -0.5\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.attend = nn.Softmax(dim = -1)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
        "        self.to_out = nn.Sequential(nn.Linear(inner_dim, dim), nn.Dropout(dropout)) if project_out else nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.norm(x)\n",
        "        qkv = self.to_qkv(x).chunk(3, dim = -1)\n",
        "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), qkv)\n",
        "        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
        "        attn = self.attend(dots)\n",
        "        attn = self.dropout(attn)\n",
        "        out = torch.matmul(attn, v)\n",
        "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
        "        return self.to_out(out)\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.):\n",
        "        super().__init__()\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.layers = nn.ModuleList([])\n",
        "        for _ in range(depth):\n",
        "            self.layers.append(nn.ModuleList([\n",
        "                Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout),\n",
        "                FeedForward(dim, mlp_dim, dropout = dropout)\n",
        "            ]))\n",
        "\n",
        "    def forward(self, x):\n",
        "        for attn, ff in self.layers:\n",
        "            x = attn(x) + x\n",
        "            x = ff(x) + x\n",
        "        return self.norm(x)\n",
        "\n",
        "class ViT(nn.Module):\n",
        "    def __init__(self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, pool = 'cls', channels = 3, dim_head = 64, dropout = 0., emb_dropout = 0.):\n",
        "        super().__init__()\n",
        "        image_height, image_width = pair(image_size)\n",
        "        patch_height, patch_width = pair(patch_size)\n",
        "        assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size.'\n",
        "        num_patches = (image_height // patch_height) * (image_width // patch_width)\n",
        "        patch_dim = channels * patch_height * patch_width\n",
        "        assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'\n",
        "        self.to_patch_embedding = nn.Sequential(\n",
        "            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_height, p2 = patch_width),\n",
        "            nn.LayerNorm(patch_dim), nn.Linear(patch_dim, dim),\n",
        "            nn.LayerNorm(dim),\n",
        "        )\n",
        "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
        "        self.dropout = nn.Dropout(emb_dropout)\n",
        "        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\n",
        "        self.pool = pool\n",
        "        self.to_latent = nn.Identity()\n",
        "        self.mlp_head = nn.Linear(dim, num_classes)\n",
        "\n",
        "    def forward(self, img):\n",
        "        x = self.to_patch_embedding(img)\n",
        "        b, n, _ = x.shape\n",
        "        cls_tokens = repeat(self.cls_token, '1 1 d -> b 1 d', b = b)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "        x += self.pos_embedding[:, :(n + 1)]\n",
        "        x = self.dropout(x)\n",
        "        x = self.transformer(x)\n",
        "        x = x.mean(dim = 1) if self.pool == 'mean' else x[:, 0]\n",
        "        x = self.to_latent(x)\n",
        "        return self.mlp_head(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "3oBrwquUT1fV"
      },
      "outputs": [],
      "source": [
        "# @title 2D RoPE\n",
        "# https://github.com/naver-ai/rope-vit/blob/main/models/vit_rope.py\n",
        "\n",
        "\n",
        "# https://github.com/facebookresearch/deit\n",
        "# https://github.com/meta-llama/codellama/blob/main/llama/model.py\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from functools import partial\n",
        "import torch.nn.functional as F\n",
        "# from timm.models.vision_transformer import Mlp, PatchEmbed , _cfg\n",
        "from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n",
        "from timm.models.registry import register_model\n",
        "from deit.models_v2 import vit_models, Layer_scale_init_Block, Attention\n",
        "\n",
        "def init_random_2d_freqs(dim, num_heads, theta = 10.0, rotate = True):\n",
        "    freqs_x = []\n",
        "    freqs_y = []\n",
        "    mag = 1 / (theta ** (torch.arange(0, dim, 4)[: (dim // 4)].float() / dim))\n",
        "    for i in range(num_heads):\n",
        "        angles = torch.rand(1) * 2 * torch.pi if rotate else torch.zeros(1)\n",
        "        fx = torch.cat([mag * torch.cos(angles), mag * torch.cos(torch.pi/2 + angles)], dim=-1)\n",
        "        fy = torch.cat([mag * torch.sin(angles), mag * torch.sin(torch.pi/2 + angles)], dim=-1)\n",
        "        freqs_x.append(fx)\n",
        "        freqs_y.append(fy)\n",
        "    freqs_x = torch.stack(freqs_x, dim=0)\n",
        "    freqs_y = torch.stack(freqs_y, dim=0)\n",
        "    freqs = torch.stack([freqs_x, freqs_y], dim=0)\n",
        "    return freqs\n",
        "\n",
        "def compute_mixed_cis(freqs, t_x, t_y, num_heads):\n",
        "    N = t_x.shape[0]\n",
        "    depth = freqs.shape[1]\n",
        "    # No float 16 for this range\n",
        "    with torch.cuda.amp.autocast(enabled=False):\n",
        "        freqs_x = (t_x.unsqueeze(-1) @ freqs[0].unsqueeze(-2)).view(depth, N, num_heads, -1).permute(0, 2, 1, 3)\n",
        "        freqs_y = (t_y.unsqueeze(-1) @ freqs[1].unsqueeze(-2)).view(depth, N, num_heads, -1).permute(0, 2, 1, 3)\n",
        "        freqs_cis = torch.polar(torch.ones_like(freqs_x), freqs_x + freqs_y)\n",
        "    return freqs_cis\n",
        "\n",
        "\n",
        "def compute_axial_cis(dim, end_x, end_y, theta = 100.0):\n",
        "    freqs_x = 1.0 / (theta ** (torch.arange(0, dim, 4)[: (dim // 4)].float() / dim))\n",
        "    freqs_y = 1.0 / (theta ** (torch.arange(0, dim, 4)[: (dim // 4)].float() / dim))\n",
        "\n",
        "    t_x, t_y = init_t_xy(end_x, end_y)\n",
        "    freqs_x = torch.outer(t_x, freqs_x)\n",
        "    freqs_y = torch.outer(t_y, freqs_y)\n",
        "    freqs_cis_x = torch.polar(torch.ones_like(freqs_x), freqs_x)\n",
        "    freqs_cis_y = torch.polar(torch.ones_like(freqs_y), freqs_y)\n",
        "    return torch.cat([freqs_cis_x, freqs_cis_y], dim=-1)\n",
        "\n",
        "def init_t_xy(end_x, end_y):\n",
        "    t = torch.arange(end_x * end_y, dtype=torch.float32)\n",
        "    t_x = (t % end_x).float()\n",
        "    t_y = torch.div(t, end_x, rounding_mode='floor').float()\n",
        "    return t_x, t_y\n",
        "\n",
        "\n",
        "def reshape_for_broadcast(freqs_cis, x):\n",
        "    ndim = x.ndim\n",
        "    assert 0 <= 1 < ndim\n",
        "    if freqs_cis.shape == (x.shape[-2], x.shape[-1]):\n",
        "        shape = [d if i >= ndim-2 else 1 for i, d in enumerate(x.shape)]\n",
        "    elif freqs_cis.shape == (x.shape[-3], x.shape[-2], x.shape[-1]):\n",
        "        shape = [d if i >= ndim-3 else 1 for i, d in enumerate(x.shape)]\n",
        "    return freqs_cis.view(*shape)\n",
        "\n",
        "def apply_rotary_emb(xq, xk, freqs_cis):\n",
        "    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))\n",
        "    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))\n",
        "    freqs_cis = reshape_for_broadcast(freqs_cis, xq_)\n",
        "    xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(3)\n",
        "    xk_out = torch.view_as_real(xk_ * freqs_cis).flatten(3)\n",
        "    return xq_out.type_as(xq).to(xq.device), xk_out.type_as(xk).to(xk.device)\n",
        "\n",
        "\n",
        "class RoPEAttention(Attention):\n",
        "    \"\"\"Multi-head Attention block with rotary position embeddings.\"\"\"\n",
        "    def forward(self, x, freqs_cis):\n",
        "        B, N, C = x.shape\n",
        "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "\n",
        "        q[:, :, 1:], k[:, :, 1:] = apply_rotary_emb(q[:, :, 1:], k[:, :, 1:], freqs_cis=freqs_cis)\n",
        "        attn = (q * self.scale) @ k.transpose(-2, -1)\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "        return x\n",
        "\n",
        "class RoPE_Layer_scale_init_Block(Layer_scale_init_Block):\n",
        "    # taken from https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py\n",
        "    # with slight modifications\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        kwargs[\"Attention_block\"] = RoPEAttention\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "    def forward(self, x, freqs_cis):\n",
        "        x = x + self.drop_path(self.gamma_1 * self.attn(self.norm1(x), freqs_cis=freqs_cis))\n",
        "        x = x + self.drop_path(self.gamma_2 * self.mlp(self.norm2(x)))\n",
        "        return x\n",
        "\n",
        "class rope_vit_models(vit_models):\n",
        "    def __init__(self, rope_theta=100.0, rope_mixed=False, use_ape=False, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        img_size = kwargs['img_size'] if 'img_size' in kwargs else 224\n",
        "        patch_size = kwargs['patch_size'] if 'patch_size' in kwargs else 16\n",
        "        num_heads = kwargs['num_heads'] if 'num_heads' in kwargs else 12\n",
        "        embed_dim = kwargs['embed_dim'] if 'embed_dim' in kwargs else 768\n",
        "        mlp_ratio = kwargs['mlp_ratio'] if 'mlp_ratio' in kwargs else 4.\n",
        "\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "        trunc_normal_(self.cls_token, std=.02)\n",
        "\n",
        "        self.use_ape = use_ape\n",
        "        if not self.use_ape:\n",
        "            self.pos_embed = None\n",
        "\n",
        "        self.rope_mixed = rope_mixed\n",
        "        self.num_heads = num_heads\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "        if self.rope_mixed:\n",
        "            self.compute_cis = partial(compute_mixed_cis, num_heads=self.num_heads)\n",
        "\n",
        "            freqs = []\n",
        "            for i, _ in enumerate(self.blocks):\n",
        "                freqs.append(\n",
        "                    init_random_2d_freqs(dim=embed_dim // num_heads, num_heads=num_heads, theta=rope_theta)\n",
        "                )\n",
        "            freqs = torch.stack(freqs, dim=1).view(2, len(self.blocks), -1)\n",
        "            self.freqs = nn.Parameter(freqs.clone(), requires_grad=True)\n",
        "\n",
        "            t_x, t_y = init_t_xy(end_x = img_size // patch_size, end_y = img_size // patch_size)\n",
        "            self.register_buffer('freqs_t_x', t_x)\n",
        "            self.register_buffer('freqs_t_y', t_y)\n",
        "        else:\n",
        "            self.compute_cis = partial(compute_axial_cis, dim=embed_dim//num_heads, theta=rope_theta)\n",
        "\n",
        "            freqs_cis = self.compute_cis(end_x = img_size // patch_size, end_y = img_size // patch_size)\n",
        "            self.freqs_cis = freqs_cis\n",
        "\n",
        "    @torch.jit.ignore\n",
        "    def no_weight_decay(self):\n",
        "        return {'pos_embed', 'cls_token', 'freqs'}\n",
        "\n",
        "    def forward_features(self, x):\n",
        "        B, C, H, W = x.shape\n",
        "        x = self.patch_embed(x)\n",
        "\n",
        "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
        "\n",
        "        if self.use_ape:\n",
        "            pos_embed = self.pos_embed\n",
        "            if pos_embed.shape[-2] != x.shape[-2]:\n",
        "                img_size = self.patch_embed.img_size\n",
        "                patch_size = self.patch_embed.patch_size\n",
        "                pos_embed = pos_embed.view(\n",
        "                    1, (img_size[1] // patch_size[1]), (img_size[0] // patch_size[0]), self.embed_dim\n",
        "                ).permute(0, 3, 1, 2)\n",
        "                pos_embed = F.interpolate(\n",
        "                    pos_embed, size=(H // patch_size[1], W // patch_size[0]), mode='bicubic', align_corners=False\n",
        "                )\n",
        "                pos_embed = pos_embed.permute(0, 2, 3, 1).flatten(1, 2)\n",
        "            x = x + pos_embed\n",
        "\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "\n",
        "        if self.rope_mixed:\n",
        "            if self.freqs_t_x.shape[0] != x.shape[1] - 1:\n",
        "                t_x, t_y = init_t_xy(end_x = W // self.patch_size, end_y = H // self.patch_size)\n",
        "                t_x, t_y = t_x.to(x.device), t_y.to(x.device)\n",
        "            else:\n",
        "                t_x, t_y = self.freqs_t_x, self.freqs_t_y\n",
        "            freqs_cis = self.compute_cis(self.freqs, t_x, t_y)\n",
        "\n",
        "            for i , blk in enumerate(self.blocks):\n",
        "                x = blk(x, freqs_cis=freqs_cis[i])\n",
        "        else:\n",
        "            if self.freqs_cis.shape[0] != x.shape[1] - 1:\n",
        "                freqs_cis = self.compute_cis(end_x = W // self.patch_size, end_y = H // self.patch_size)\n",
        "            else:\n",
        "                freqs_cis = self.freqs_cis\n",
        "            freqs_cis = freqs_cis.to(x.device)\n",
        "\n",
        "            for i , blk in enumerate(self.blocks):\n",
        "                x = blk(x, freqs_cis=freqs_cis)\n",
        "\n",
        "        x = self.norm(x)\n",
        "        x = x[:, 0]\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "def adjust_pos_embed_size(model, state_dict):\n",
        "    # interpolate position embedding\n",
        "    if 'pos_embed' in state_dict:\n",
        "        pos_embed_checkpoint = state_dict['pos_embed']\n",
        "        embedding_size = pos_embed_checkpoint.shape[-1]\n",
        "        num_patches = model.patch_embed.num_patches\n",
        "        num_extra_tokens = model.pos_embed.shape[-2] - num_patches\n",
        "        # height (== width) for the checkpoint position embedding\n",
        "        orig_size = int((pos_embed_checkpoint.shape[-2] - num_extra_tokens) ** 0.5)\n",
        "        # height (== width) for the new position embedding\n",
        "        new_size = int(num_patches ** 0.5)\n",
        "        # class_token and dist_token are kept unchanged\n",
        "        extra_tokens = pos_embed_checkpoint[:, :num_extra_tokens]\n",
        "        # only the position tokens are interpolated\n",
        "        pos_tokens = pos_embed_checkpoint[:, num_extra_tokens:]\n",
        "        pos_tokens = pos_tokens.reshape(-1, orig_size, orig_size, embedding_size).permute(0, 3, 1, 2)\n",
        "        pos_tokens = torch.nn.functional.interpolate(\n",
        "            pos_tokens, size=(new_size, new_size), mode='bicubic', align_corners=False)\n",
        "        pos_tokens = pos_tokens.permute(0, 2, 3, 1).flatten(1, 2)\n",
        "        new_pos_embed = torch.cat((extra_tokens, pos_tokens), dim=1)\n",
        "        state_dict['pos_embed'] = new_pos_embed\n",
        "\n",
        "    return state_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Mpy-XMvCCVbt"
      },
      "outputs": [],
      "source": [
        "# @title facebookresearch ViT down\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/models/vision_transformer.py\n",
        "import math\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from src.utils.tensors import trunc_normal_, repeat_interleave_batch\n",
        "\n",
        "def get_2d_sincos_pos_embed_from_grid(embed_dim, grid):\n",
        "    assert embed_dim % 2 == 0\n",
        "    # use half of dimensions to encode grid_h\n",
        "    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[0])  # (pred_emb_dim//2, 1,h,h) ->\n",
        "    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[1])  # (H*W, D/2)\n",
        "    emb = np.concatenate([emb_h, emb_w], axis=1)  # (H*W, D)\n",
        "    return emb\n",
        "\n",
        "def get_2d_sincos_pos_embed(embed_dim, grid_size, cls_token=False): # grid_size of the grid height and width\n",
        "    grid_h, grid_w = np.arange(grid_size, dtype=float), np.arange(grid_size, dtype=float)\n",
        "    grid = np.meshgrid(grid_w, grid_h)  # here w goes first\n",
        "    grid = np.stack(grid, axis=0)\n",
        "    grid = grid.reshape([2, 1, grid_size, grid_size])\n",
        "    pos_embed = get_2d_sincos_pos_embed_from_grid(embed_dim, grid) # (pred_emb_dim, [2,1,h,h])\n",
        "    if cls_token:\n",
        "        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0) # prepend zeros for cls token later?\n",
        "    return pos_embed # [grid_size*grid_size, embed_dim] or [1+grid_size*grid_size, embed_dim] (w/ or w/o cls_token)\n",
        "\n",
        "\n",
        "def get_1d_sincos_pos_embed_from_grid(embed_dim, pos):\n",
        "    \"\"\"\n",
        "    embed_dim: output dimension for each position\n",
        "    pos: a list of positions to be encoded: size (M,)\n",
        "    out: (M, D)\n",
        "    \"\"\"\n",
        "    assert embed_dim % 2 == 0\n",
        "    omega = np.arange(embed_dim // 2, dtype=float)\n",
        "    omega /= embed_dim / 2.\n",
        "    omega = 1. / 10000**omega   # (D/2,)\n",
        "    pos = pos.reshape(-1)   # (M,)\n",
        "    out = np.einsum('m,d->md', pos, omega)   # (M, D/2), outer product\n",
        "    emb_sin = np.sin(out)  # (M, D/2)\n",
        "    emb_cos = np.cos(out)  # (M, D/2)\n",
        "    emb = np.concatenate([emb_sin, emb_cos], axis=1)  # (M, D)\n",
        "    return emb\n",
        "\n",
        "\n",
        "def drop_path(x, drop_prob = 0., training = False):\n",
        "    if drop_prob == 0. or not training: return x\n",
        "    keep_prob = 1 - drop_prob\n",
        "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n",
        "    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
        "    random_tensor.floor_()  # binarize\n",
        "    output = x.div(keep_prob) * random_tensor\n",
        "    return output\n",
        "\n",
        "class DropPath(nn.Module):\n",
        "    \"\"\"Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\"\"\"\n",
        "    def __init__(self, drop_prob=None):\n",
        "        super().__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "\n",
        "    def forward(self, x):\n",
        "        return drop_path(x, self.drop_prob, self.training)\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, dim, num_heads, drop=0., drop_path=0.):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(dim)\n",
        "        # self.attn = Attention(dim, num_heads=num_heads, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=drop)\n",
        "        self.attn = AttentionBlock(dim, dim//num_heads)\n",
        "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "        self.norm2 = nn.LayerNorm(dim)\n",
        "        mlp_hidden_dim = int(dim * 4)\n",
        "        self.mlp = MLP(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=nn.GELU, drop=drop)\n",
        "\n",
        "    def forward(self, x, return_attention=False):\n",
        "        y, attn = self.attn(self.norm1(x))\n",
        "        if return_attention: return attn\n",
        "        x = x + self.drop_path(y)\n",
        "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
        "        return x\n",
        "\n",
        "\n",
        "class PatchEmbed(nn.Module):\n",
        "    \"\"\" Image to Patch Embedding\"\"\"\n",
        "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n",
        "        super().__init__()\n",
        "        self.num_patches = (img_size // patch_size) * (img_size // patch_size)\n",
        "        self.img_size, self.patch_size = img_size, patch_size\n",
        "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "    def forward(self, x): # [b,c,h,w]\n",
        "        # B, C, H, W = x.shape\n",
        "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ConvEmbed(nn.Module):\n",
        "    \"\"\"3x3 Convolution stems for ViT following ViTC models\"\"\"\n",
        "    def __init__(self, channels, strides, img_size=224, in_chans=3, batch_norm=True):\n",
        "        super().__init__()\n",
        "        stem = []\n",
        "        channels = [in_chans] + channels\n",
        "        act = nn.ReLU(inplace=True)\n",
        "        for i in range(len(channels) - 2):\n",
        "            # stem += [nn.Conv2d(channels[i], channels[i+1], kernel_size=3, stride=strides[i], padding=1, bias=(not batch_norm))]\n",
        "            # if batch_norm:\n",
        "            #     stem += [nn.BatchNorm2d(channels[i+1])]\n",
        "            # stem += [nn.ReLU(inplace=True)]\n",
        "            # nn.Sequential(\n",
        "            stem += [nn.Conv2d(channels[i], channels[i+1], kernel_size=3, stride=strides[i], padding=1, bias=(not batch_norm)), nn.BatchNorm2d(channels[i+1]) if batch_norm else nn.Identity(), act,]\n",
        "            # )\n",
        "        stem += [nn.Conv2d(channels[-2], channels[-1], kernel_size=1, stride=strides[-1])]\n",
        "        self.stem = nn.Sequential(*stem)\n",
        "\n",
        "        # Comptute the number of patches\n",
        "        stride_prod = int(np.prod(strides))\n",
        "        self.num_patches = (img_size[0] // stride_prod)**2\n",
        "\n",
        "    def forward(self, x):\n",
        "        p = self.stem(x)\n",
        "        return p.flatten(2).transpose(1, 2)\n",
        "\n",
        "\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/masks/utils.py\n",
        "def apply_masks(x, masks):\n",
        "    \"\"\":param x: tensor of shape [B (batch-size), N (num-patches), D (feature-dim)]\n",
        "    :param masks: list of tensors containing indices of patches in [N] to keep\"\"\"\n",
        "    all_x = []\n",
        "    for m in masks:\n",
        "        mask_keep = m.unsqueeze(-1).repeat(1, 1, x.size(-1))\n",
        "        all_x += [torch.gather(x, dim=1, index=mask_keep)]\n",
        "    return torch.cat(all_x, dim=0)\n",
        "\n",
        "class VisionTransformerPredictor(nn.Module):\n",
        "    \"\"\" Vision Transformer \"\"\"\n",
        "    def __init__(self, num_patches, embed_dim=768, predictor_embed_dim=384,\n",
        "        depth=6, num_heads=12, drop_rate=0.0, drop_path_rate=0.0, init_std=0.02, **kwargs):\n",
        "        super().__init__()\n",
        "        self.init_std = init_std\n",
        "        self.predictor_embed = nn.Linear(embed_dim, predictor_embed_dim, bias=True)\n",
        "        self.mask_token = nn.Parameter(torch.zeros(1, 1, predictor_embed_dim))\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
        "        # --\n",
        "        self.predictor_pos_embed = nn.Parameter(torch.zeros(1, num_patches, predictor_embed_dim), requires_grad=False) # [1, (h*w), pred_emb_dim]\n",
        "        predictor_pos_embed = get_2d_sincos_pos_embed(self.predictor_pos_embed.shape[-1], int(num_patches**.5), cls_token=False) # get_2d_sincos_pos_embed(pred_emb_dim, (h)) ->\n",
        "        self.predictor_pos_embed.data.copy_(torch.from_numpy(predictor_pos_embed).float().unsqueeze(0))\n",
        "        # --\n",
        "        self.predictor_blocks = nn.ModuleList([Block(dim=predictor_embed_dim, num_heads=num_heads, drop=drop_rate, drop_path=dpr[i]) for i in range(depth)])\n",
        "        self.predictor_norm = nn.LayerNorm(predictor_embed_dim)\n",
        "        self.predictor_proj = nn.Linear(predictor_embed_dim, embed_dim, bias=True)\n",
        "        # ------\n",
        "        trunc_normal_(self.mask_token, std=self.init_std)\n",
        "\n",
        "    def forward(self, x, masks_x, masks):\n",
        "        assert (masks is not None) and (masks_x is not None), 'Cannot run predictor without mask indices'\n",
        "        if not isinstance(masks_x, list): masks_x = [masks_x]\n",
        "        if not isinstance(masks, list): masks = [masks]\n",
        "\n",
        "        # -- Batch Size\n",
        "        B = len(x) // len(masks_x) # batch*M // M\n",
        "\n",
        "        # -- map from encoder-dim to pedictor-dim\n",
        "        x = self.predictor_embed(x)\n",
        "\n",
        "        # -- add positional embedding to x tokens\n",
        "        x_pos_embed = self.predictor_pos_embed.repeat(B, 1, 1)\n",
        "        x += apply_masks(x_pos_embed, masks_x) # apply pos emb to mask tokens\n",
        "\n",
        "        _, N_ctxt, D = x.shape\n",
        "\n",
        "        # -- concat mask tokens to x\n",
        "        pos_embs = self.predictor_pos_embed.repeat(B, 1, 1)\n",
        "        pos_embs = apply_masks(pos_embs, masks)\n",
        "        pos_embs = repeat_interleave_batch(pos_embs, B, repeat=len(masks_x))\n",
        "        # --\n",
        "        pred_tokens = self.mask_token.repeat(pos_embs.size(0), pos_embs.size(1), 1)\n",
        "        # --\n",
        "        pred_tokens += pos_embs\n",
        "        x = x.repeat(len(masks), 1, 1)\n",
        "        x = torch.cat([x, pred_tokens], dim=1)\n",
        "\n",
        "        # -- fwd prop\n",
        "        for blk in self.predictor_blocks:\n",
        "            x = blk(x)\n",
        "        x = self.predictor_norm(x)\n",
        "\n",
        "        # -- return preds for mask tokens\n",
        "        x = x[:, N_ctxt:]\n",
        "        x = self.predictor_proj(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class VisionTransformer(nn.Module):\n",
        "    \"\"\" Vision Transformer \"\"\"\n",
        "    def __init__(self, img_size=[224], patch_size=16, in_chans=3, embed_dim=768, predictor_embed_dim=384, depth=12, predictor_depth=12,\n",
        "        num_heads=12, mlp_ratio=4.0, qkv_bias=True, qk_scale=None, drop_rate=0.0, attn_drop_rate=0.0, drop_path_rate=0.0, norm_layer=nn.LayerNorm, init_std=0.02, **kwargs):\n",
        "        super().__init__()\n",
        "        self.num_features = self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.init_std = init_std\n",
        "        # --\n",
        "        self.patch_embed = PatchEmbed(img_size=img_size[0], patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n",
        "        num_patches = self.patch_embed.num_patches\n",
        "        # --\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim), requires_grad=False)\n",
        "        pos_embed = get_2d_sincos_pos_embed(self.pos_embed.shape[-1], int(self.patch_embed.num_patches**.5), cls_token=False)\n",
        "        self.pos_embed.data.copy_(torch.from_numpy(pos_embed).float().unsqueeze(0))\n",
        "        # --\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
        "        self.blocks = nn.ModuleList([\n",
        "            Block(dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer)\n",
        "            for i in range(depth)])\n",
        "        self.norm = norm_layer(embed_dim)\n",
        "\n",
        "    def forward(self, x, masks=None):\n",
        "        if masks is not None:\n",
        "            if not isinstance(masks, list):\n",
        "                masks = [masks]\n",
        "\n",
        "        # -- patchify x\n",
        "        x = self.patch_embed(x)\n",
        "        B, N, D = x.shape\n",
        "\n",
        "        # -- add positional embedding to x\n",
        "        pos_embed = self.interpolate_pos_encoding(x, self.pos_embed)\n",
        "        x = x + pos_embed\n",
        "\n",
        "        # -- mask x\n",
        "        if masks is not None:\n",
        "            x = apply_masks(x, masks)\n",
        "\n",
        "        # -- fwd prop\n",
        "        for i, blk in enumerate(self.blocks):\n",
        "            x = blk(x)\n",
        "\n",
        "        if self.norm is not None:\n",
        "            x = self.norm(x)\n",
        "        return x\n",
        "\n",
        "    def interpolate_pos_encoding(self, x, pos_embed):\n",
        "        npatch = x.shape[1] - 1\n",
        "        N = pos_embed.shape[1] - 1\n",
        "        if npatch == N: return pos_embed\n",
        "        class_emb, pos_embed = pos_embed[:, 0], pos_embed[:, 1:]\n",
        "        dim = x.shape[-1]\n",
        "        pos_embed = nn.functional.interpolate(pos_embed.reshape(1, int(math.sqrt(N)), int(math.sqrt(N)), dim).permute(0, 3, 1, 2), scale_factor=math.sqrt(npatch / N), mode='bicubic',)\n",
        "        pos_embed = pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n",
        "        return torch.cat((class_emb.unsqueeze(0), pos_embed), dim=1)\n",
        "\n",
        "# from functools import partial\n",
        "# def vit_predictor(**kwargs):\n",
        "#     model = VisionTransformerPredictor(\n",
        "#         mlp_ratio=4, qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
        "#     return model\n",
        "\n",
        "def vit(patch_size=16, **kwargs):\n",
        "    model = VisionTransformer(\n",
        "        patch_size=patch_size, embed_dim=192, depth=12, num_heads=3, mlp_ratio=4, # tiny\n",
        "        # patch_size=patch_size, embed_dim=384, depth=12, num_heads=6, mlp_ratio=4, # small\n",
        "        # patch_size=patch_size, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, # base\n",
        "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
        "    return model\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "rmuh1qDo7p9n"
      },
      "outputs": [],
      "source": [
        "# @title facebookresearch ViT base\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/models/vision_transformer.py\n",
        "\n",
        "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
        "# All rights reserved.\n",
        "#\n",
        "# This source code is licensed under the license found in the\n",
        "# LICENSE file in the root directory of this source tree.\n",
        "#\n",
        "\n",
        "import math\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from src.utils.tensors import trunc_normal_, repeat_interleave_batch\n",
        "from src.masks.utils import apply_masks\n",
        "\n",
        "def get_2d_sincos_pos_embed_from_grid(embed_dim, grid):\n",
        "    assert embed_dim % 2 == 0\n",
        "    # use half of dimensions to encode grid_h\n",
        "    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[0])  # (H*W, D/2)\n",
        "    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[1])  # (H*W, D/2)\n",
        "    emb = np.concatenate([emb_h, emb_w], axis=1)  # (H*W, D)\n",
        "    return emb\n",
        "\n",
        "def get_2d_sincos_pos_embed(embed_dim, grid_size, cls_token=False): # grid_size of the grid height and width\n",
        "    grid_h, grid_w = np.arange(grid_size, dtype=float), np.arange(grid_size, dtype=float)\n",
        "    grid = np.meshgrid(grid_w, grid_h)  # here w goes first\n",
        "    grid = np.stack(grid, axis=0)\n",
        "    grid = grid.reshape([2, 1, grid_size, grid_size])\n",
        "    pos_embed = get_2d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
        "    if cls_token:\n",
        "        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0) # prepend zeros for cls token later?\n",
        "    return pos_embed # [grid_size*grid_size, embed_dim] or [1+grid_size*grid_size, embed_dim] (w/ or w/o cls_token)\n",
        "\n",
        "\n",
        "def get_1d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):\n",
        "    \"\"\"\n",
        "    grid_size: int of the grid length\n",
        "    return:\n",
        "    pos_embed: [grid_size, embed_dim] or [1+grid_size, embed_dim] (w/ or w/o cls_token)\n",
        "    \"\"\"\n",
        "    grid = np.arange(grid_size, dtype=float)\n",
        "    pos_embed = get_1d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
        "    if cls_token:\n",
        "        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n",
        "    return pos_embed\n",
        "\n",
        "\n",
        "def get_1d_sincos_pos_embed_from_grid(embed_dim, pos):\n",
        "    \"\"\"\n",
        "    embed_dim: output dimension for each position\n",
        "    pos: a list of positions to be encoded: size (M,)\n",
        "    out: (M, D)\n",
        "    \"\"\"\n",
        "    assert embed_dim % 2 == 0\n",
        "    omega = np.arange(embed_dim // 2, dtype=float)\n",
        "    omega /= embed_dim / 2.\n",
        "    omega = 1. / 10000**omega   # (D/2,)\n",
        "    pos = pos.reshape(-1)   # (M,)\n",
        "    out = np.einsum('m,d->md', pos, omega)   # (M, D/2), outer product\n",
        "    emb_sin = np.sin(out)  # (M, D/2)\n",
        "    emb_cos = np.cos(out)  # (M, D/2)\n",
        "    emb = np.concatenate([emb_sin, emb_cos], axis=1)  # (M, D)\n",
        "    return emb\n",
        "\n",
        "\n",
        "def drop_path(x, drop_prob = 0., training = False):\n",
        "    if drop_prob == 0. or not training:\n",
        "        return x\n",
        "    keep_prob = 1 - drop_prob\n",
        "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n",
        "    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
        "    random_tensor.floor_()  # binarize\n",
        "    output = x.div(keep_prob) * random_tensor\n",
        "    return output\n",
        "\n",
        "\n",
        "class DropPath(nn.Module):\n",
        "    \"\"\"Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\"\"\"\n",
        "    def __init__(self, drop_prob=None):\n",
        "        super(DropPath, self).__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "\n",
        "    def forward(self, x):\n",
        "        return drop_path(x, self.drop_prob, self.training)\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
        "        super().__init__()\n",
        "        out_features = out_features or in_features\n",
        "        hidden_features = hidden_features or in_features\n",
        "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
        "        self.act = act_layer()\n",
        "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.drop(x)\n",
        "        return x\n",
        "# lin act drop lin drop\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads\n",
        "        self.scale = qk_scale or head_dim ** -0.5\n",
        "\n",
        "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "        return x, attn\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
        "                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
        "        super().__init__()\n",
        "        self.norm1 = norm_layer(dim)\n",
        "        self.attn = Attention(dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
        "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "        self.norm2 = norm_layer(dim)\n",
        "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
        "        self.mlp = MLP(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
        "\n",
        "    def forward(self, x, return_attention=False):\n",
        "        y, attn = self.attn(self.norm1(x))\n",
        "        if return_attention:\n",
        "            return attn\n",
        "        x = x + self.drop_path(y)\n",
        "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
        "        return x\n",
        "\n",
        "\n",
        "class PatchEmbed(nn.Module):\n",
        "    \"\"\" Image to Patch Embedding\"\"\"\n",
        "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n",
        "        super().__init__()\n",
        "        num_patches = (img_size // patch_size) * (img_size // patch_size)\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.num_patches = num_patches\n",
        "\n",
        "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.shape\n",
        "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ConvEmbed(nn.Module):\n",
        "    \"\"\"3x3 Convolution stems for ViT following ViTC models\"\"\"\n",
        "    def __init__(self, channels, strides, img_size=224, in_chans=3, batch_norm=True):\n",
        "        super().__init__()\n",
        "        # Build the stems\n",
        "        stem = []\n",
        "        channels = [in_chans] + channels\n",
        "        for i in range(len(channels) - 2):\n",
        "            stem += [nn.Conv2d(channels[i], channels[i+1], kernel_size=3, stride=strides[i], padding=1, bias=(not batch_norm))]\n",
        "            if batch_norm:\n",
        "                stem += [nn.BatchNorm2d(channels[i+1])]\n",
        "            stem += [nn.ReLU(inplace=True)]\n",
        "        stem += [nn.Conv2d(channels[-2], channels[-1], kernel_size=1, stride=strides[-1])]\n",
        "        self.stem = nn.Sequential(*stem)\n",
        "\n",
        "        # Comptute the number of patches\n",
        "        stride_prod = int(np.prod(strides))\n",
        "        self.num_patches = (img_size[0] // stride_prod)**2\n",
        "\n",
        "    def forward(self, x):\n",
        "        p = self.stem(x)\n",
        "        return p.flatten(2).transpose(1, 2)\n",
        "\n",
        "\n",
        "class VisionTransformerPredictor(nn.Module):\n",
        "    \"\"\" Vision Transformer \"\"\"\n",
        "    def __init__(self, num_patches, embed_dim=768, predictor_embed_dim=384,\n",
        "        depth=6, num_heads=12, mlp_ratio=4.0, qkv_bias=True, qk_scale=None,\n",
        "        drop_rate=0.0, attn_drop_rate=0.0, drop_path_rate=0.0, norm_layer=nn.LayerNorm, init_std=0.02, **kwargs):\n",
        "        super().__init__()\n",
        "        self.predictor_embed = nn.Linear(embed_dim, predictor_embed_dim, bias=True)\n",
        "        self.mask_token = nn.Parameter(torch.zeros(1, 1, predictor_embed_dim))\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
        "        # --\n",
        "        self.predictor_pos_embed = nn.Parameter(torch.zeros(1, num_patches, predictor_embed_dim), requires_grad=False)\n",
        "        predictor_pos_embed = get_2d_sincos_pos_embed(self.predictor_pos_embed.shape[-1], int(num_patches**.5), cls_token=False)\n",
        "        self.predictor_pos_embed.data.copy_(torch.from_numpy(predictor_pos_embed).float().unsqueeze(0))\n",
        "        # --\n",
        "        self.predictor_blocks = nn.ModuleList([\n",
        "            Block(dim=predictor_embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer)\n",
        "            for i in range(depth)])\n",
        "        self.predictor_norm = norm_layer(predictor_embed_dim)\n",
        "        self.predictor_proj = nn.Linear(predictor_embed_dim, embed_dim, bias=True)\n",
        "        # ------\n",
        "        self.init_std = init_std\n",
        "        trunc_normal_(self.mask_token, std=self.init_std)\n",
        "        self.apply(self._init_weights)\n",
        "        self.fix_init_weight()\n",
        "\n",
        "    def fix_init_weight(self):\n",
        "        def rescale(param, layer_id):\n",
        "            param.div_(math.sqrt(2.0 * layer_id))\n",
        "\n",
        "        for layer_id, layer in enumerate(self.predictor_blocks):\n",
        "            rescale(layer.attn.proj.weight.data, layer_id + 1)\n",
        "            rescale(layer.mlp.fc2.weight.data, layer_id + 1)\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            trunc_normal_(m.weight, std=self.init_std)\n",
        "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "        elif isinstance(m, nn.Conv2d):\n",
        "            trunc_normal_(m.weight, std=self.init_std)\n",
        "            if m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x, masks_x, masks):\n",
        "        assert (masks is not None) and (masks_x is not None), 'Cannot run predictor without mask indices'\n",
        "\n",
        "        if not isinstance(masks_x, list): masks_x = [masks_x]\n",
        "        if not isinstance(masks, list): masks = [masks]\n",
        "\n",
        "        # -- Batch Size\n",
        "        B = len(x) // len(masks_x)\n",
        "\n",
        "        # -- map from encoder-dim to pedictor-dim\n",
        "        x = self.predictor_embed(x)\n",
        "\n",
        "        # -- add positional embedding to x tokens\n",
        "        x_pos_embed = self.predictor_pos_embed.repeat(B, 1, 1)\n",
        "        x += apply_masks(x_pos_embed, masks_x)\n",
        "\n",
        "        _, N_ctxt, D = x.shape\n",
        "\n",
        "        # -- concat mask tokens to x\n",
        "        pos_embs = self.predictor_pos_embed.repeat(B, 1, 1)\n",
        "        pos_embs = apply_masks(pos_embs, masks)\n",
        "        pos_embs = repeat_interleave_batch(pos_embs, B, repeat=len(masks_x))\n",
        "        # --\n",
        "        pred_tokens = self.mask_token.repeat(pos_embs.size(0), pos_embs.size(1), 1)\n",
        "        # --\n",
        "        pred_tokens += pos_embs\n",
        "        x = x.repeat(len(masks), 1, 1)\n",
        "        x = torch.cat([x, pred_tokens], dim=1)\n",
        "\n",
        "        # -- fwd prop\n",
        "        for blk in self.predictor_blocks:\n",
        "            x = blk(x)\n",
        "        x = self.predictor_norm(x)\n",
        "\n",
        "        # -- return preds for mask tokens\n",
        "        x = x[:, N_ctxt:]\n",
        "        x = self.predictor_proj(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class VisionTransformer(nn.Module):\n",
        "    \"\"\" Vision Transformer \"\"\"\n",
        "    def __init__(self, img_size=[224], patch_size=16, in_chans=3, embed_dim=768, predictor_embed_dim=384, depth=12, predictor_depth=12,\n",
        "        num_heads=12, mlp_ratio=4.0, qkv_bias=True, qk_scale=None, drop_rate=0.0, attn_drop_rate=0.0, drop_path_rate=0.0, norm_layer=nn.LayerNorm, init_std=0.02, **kwargs):\n",
        "        super().__init__()\n",
        "        self.num_features = self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        # --\n",
        "        self.patch_embed = PatchEmbed(img_size=img_size[0], patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n",
        "        num_patches = self.patch_embed.num_patches\n",
        "        # --\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim), requires_grad=False)\n",
        "        pos_embed = get_2d_sincos_pos_embed(self.pos_embed.shape[-1], int(self.patch_embed.num_patches**.5), cls_token=False)\n",
        "        self.pos_embed.data.copy_(torch.from_numpy(pos_embed).float().unsqueeze(0))\n",
        "        # --\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
        "        self.blocks = nn.ModuleList([\n",
        "            Block(dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer)\n",
        "            for i in range(depth)])\n",
        "        self.norm = norm_layer(embed_dim)\n",
        "        # ------\n",
        "        self.init_std = init_std\n",
        "        self.apply(self._init_weights)\n",
        "        self.fix_init_weight()\n",
        "\n",
        "    def fix_init_weight(self):\n",
        "        def rescale(param, layer_id):\n",
        "            param.div_(math.sqrt(2.0 * layer_id))\n",
        "\n",
        "        for layer_id, layer in enumerate(self.blocks):\n",
        "            rescale(layer.attn.proj.weight.data, layer_id + 1)\n",
        "            rescale(layer.mlp.fc2.weight.data, layer_id + 1)\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            trunc_normal_(m.weight, std=self.init_std)\n",
        "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "        elif isinstance(m, nn.Conv2d):\n",
        "            trunc_normal_(m.weight, std=self.init_std)\n",
        "            if m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x, masks=None):\n",
        "        if masks is not None:\n",
        "            if not isinstance(masks, list):\n",
        "                masks = [masks]\n",
        "\n",
        "        # -- patchify x\n",
        "        x = self.patch_embed(x)\n",
        "        B, N, D = x.shape\n",
        "\n",
        "        # -- add positional embedding to x\n",
        "        pos_embed = self.interpolate_pos_encoding(x, self.pos_embed)\n",
        "        x = x + pos_embed\n",
        "\n",
        "        # -- mask x\n",
        "        if masks is not None:\n",
        "            x = apply_masks(x, masks)\n",
        "\n",
        "        # -- fwd prop\n",
        "        for i, blk in enumerate(self.blocks):\n",
        "            x = blk(x)\n",
        "\n",
        "        if self.norm is not None:\n",
        "            x = self.norm(x)\n",
        "        return x\n",
        "\n",
        "    def interpolate_pos_encoding(self, x, pos_embed):\n",
        "        npatch = x.shape[1] - 1\n",
        "        N = pos_embed.shape[1] - 1\n",
        "        if npatch == N:\n",
        "            return pos_embed\n",
        "        class_emb = pos_embed[:, 0]\n",
        "        pos_embed = pos_embed[:, 1:]\n",
        "        dim = x.shape[-1]\n",
        "        pos_embed = nn.functional.interpolate(\n",
        "            pos_embed.reshape(1, int(math.sqrt(N)), int(math.sqrt(N)), dim).permute(0, 3, 1, 2),\n",
        "            scale_factor=math.sqrt(npatch / N),\n",
        "            mode='bicubic',\n",
        "        )\n",
        "        pos_embed = pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n",
        "        return torch.cat((class_emb.unsqueeze(0), pos_embed), dim=1)\n",
        "\n",
        "\n",
        "from functools import partial\n",
        "def vit_predictor(**kwargs):\n",
        "    model = VisionTransformerPredictor(\n",
        "        mlp_ratio=4, qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
        "    return model\n",
        "\n",
        "def vit(patch_size=16, **kwargs):\n",
        "    model = VisionTransformer(\n",
        "        patch_size=patch_size, embed_dim=192, depth=12, num_heads=3, mlp_ratio=4, # tiny\n",
        "        # patch_size=patch_size, embed_dim=384, depth=12, num_heads=6, mlp_ratio=4, # small\n",
        "        # patch_size=patch_size, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, # base\n",
        "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
        "    return model\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zvZ80FwKjyIs",
        "outputId": "e76b76c7-43a2-42ec-a175-692efb6403cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "38602\n",
            "torch.Size([64, 10])\n"
          ]
        }
      ],
      "source": [
        "# @title lucid SimpleViT\n",
        "import torch\n",
        "from vit_pytorch import SimpleViT\n",
        "\n",
        "model = SimpleViT(\n",
        "    image_size = 32,\n",
        "    patch_size = 8,\n",
        "    num_classes = 10,\n",
        "    dim = 64,\n",
        "    depth = 1,\n",
        "    heads = 8,\n",
        "    mlp_dim = 64,\n",
        "\n",
        "    # num_heads = 8,\n",
        "    # mlp_ratio = 1.,\n",
        "    # positional_embedding = 'learnable', # ['sine', 'learnable', 'none']\n",
        "    dim_head = 8,\n",
        "\n",
        ")\n",
        "# def __init__(self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, channels = 3, dim_head = 64):\n",
        "\n",
        "# img = torch.randn(1, 3, 256, 256)\n",
        "# preds = v(img) # (1, 1000)\n",
        "optim = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 236235\n",
        "x = torch.rand(64, 3, 32, 32, device=device)\n",
        "# x = torch.rand(64, 1, 28,28, device=device)\n",
        "logits = model(x)\n",
        "print(logits.shape)\n",
        "\n",
        "# loss: 1.684826  [49920/50000]\n",
        "# Test Error:\n",
        "#  Accuracy: 40.7%, Avg loss: 1.642769\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title NVlabs afno2d.py\n",
        "# https://github.com/NVlabs/AFNO-transformer/blob/master/afno/afno2d.py\n",
        "# Copyright (c) 2020, NVIDIA CORPORATION.  All rights reserved.\n",
        "\n",
        "# import math\n",
        "import torch\n",
        "# import torch.fft\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class AFNO2D(nn.Module):\n",
        "    \"\"\"\n",
        "    hidden_size: channel dimension size\n",
        "    num_blocks: how many blocks to use in the block diagonal weight matrices (higher => less complexity but less parameters)\n",
        "    hard_thresholding_fraction: how many frequencies you want to completely mask out (lower => hard_thresholding_fraction^2 less FLOPs)\n",
        "    \"\"\"\n",
        "    def __init__(self, hidden_size, num_blocks=8, hard_thresholding_fraction=1, hidden_size_factor=1):\n",
        "        super().__init__()\n",
        "        assert hidden_size % num_blocks == 0, f\"hidden_size {hidden_size} should be divisble by num_blocks {num_blocks}\"\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_blocks = num_blocks\n",
        "        self.block_size = self.hidden_size // self.num_blocks\n",
        "        self.hard_thresholding_fraction = hard_thresholding_fraction\n",
        "        self.hidden_size_factor = hidden_size_factor\n",
        "        self.scale = 0.02\n",
        "\n",
        "        self.w1 = nn.Parameter(self.scale * torch.randn(2, self.num_blocks, self.block_size, self.block_size * self.hidden_size_factor))\n",
        "        self.b1 = nn.Parameter(self.scale * torch.randn(2, self.num_blocks, self.block_size * self.hidden_size_factor))\n",
        "        self.w2 = nn.Parameter(self.scale * torch.randn(2, self.num_blocks, self.block_size * self.hidden_size_factor, self.block_size))\n",
        "        self.b2 = nn.Parameter(self.scale * torch.randn(2, self.num_blocks, self.block_size))\n",
        "\n",
        "    def forward(self, x, spatial_size=None):\n",
        "        bias = x\n",
        "        dtype = x.dtype\n",
        "        x = x.float()\n",
        "        B, N, C = x.shape\n",
        "\n",
        "        x = x.reshape(B, H, W, C)\n",
        "        x = torch.fft.rfft2(x, dim=(1, 2), norm=\"ortho\")\n",
        "        x = x.reshape(B, x.shape[1], x.shape[2], self.num_blocks, self.block_size) # [b, h, w//2+1, n_blocks, block_size]\n",
        "        o1_real = torch.zeros([B, x.shape[1], x.shape[2], self.num_blocks, self.block_size * self.hidden_size_factor], device=x.device)\n",
        "        o1_imag = torch.zeros([B, x.shape[1], x.shape[2], self.num_blocks, self.block_size * self.hidden_size_factor], device=x.device)\n",
        "        o2_real = torch.zeros(x.shape, device=x.device)\n",
        "        o2_imag = torch.zeros(x.shape, device=x.device)\n",
        "\n",
        "        total_modes = N // 2 + 1\n",
        "        kept_modes = int(total_modes * self.hard_thresholding_fraction)\n",
        "\n",
        "        o1_real[:, :, :kept_modes] = F.relu(\n",
        "            torch.einsum('...bi,bio->...bo', x[:, :, :kept_modes].real, self.w1[0]) - \\\n",
        "            torch.einsum('...bi,bio->...bo', x[:, :, :kept_modes].imag, self.w1[1]) + \\\n",
        "            self.b1[0]\n",
        "        )\n",
        "\n",
        "        o1_imag[:, :, :kept_modes] = F.relu(\n",
        "            torch.einsum('...bi,bio->...bo', x[:, :, :kept_modes].imag, self.w1[0]) + \\\n",
        "            torch.einsum('...bi,bio->...bo', x[:, :, :kept_modes].real, self.w1[1]) + \\\n",
        "            self.b1[1]\n",
        "        )\n",
        "\n",
        "        o2_real[:, :, :kept_modes] = (\n",
        "            torch.einsum('...bi,bio->...bo', o1_real[:, :, :kept_modes], self.w2[0]) - \\\n",
        "            torch.einsum('...bi,bio->...bo', o1_imag[:, :, :kept_modes], self.w2[1]) + \\\n",
        "            self.b2[0]\n",
        "        )\n",
        "\n",
        "        o2_imag[:, :, :kept_modes] = (\n",
        "            torch.einsum('...bi,bio->...bo', o1_imag[:, :, :kept_modes], self.w2[0]) + \\\n",
        "            torch.einsum('...bi,bio->...bo', o1_real[:, :, :kept_modes], self.w2[1]) + \\\n",
        "            self.b2[1]\n",
        "        )\n",
        "\n",
        "        x = torch.stack([o2_real, o2_imag], dim=-1)\n",
        "        x = F.softshrink(x, lambd=0.01)\n",
        "        x = torch.view_as_complex(x)\n",
        "        x = x.reshape(B, x.shape[1], x.shape[2], C)\n",
        "        x = torch.fft.irfft2(x, s=(H, W), dim=(1, 2), norm=\"ortho\")\n",
        "        x = x.reshape(B, N, C)\n",
        "        x = x.type(dtype)\n",
        "        return x + bias\n",
        "\n",
        "b,c,h,w = 2,16,9,9\n",
        "# w//2+1\n",
        "# b,c,h,w = 2,3,16,16\n",
        "mix = AFNO2D(c)\n",
        "x = torch.rand(b,c,h,w)\n",
        "out = mix(x)\n",
        "print(out.shape)\n",
        "# print(out)\n",
        "\n"
      ],
      "metadata": {
        "id": "T7MfCPJiDaue",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title AFNO2D_channelfirst save\n",
        "# Adaptive Frequency Filters As Efficient Global Token Mixers\n",
        "# https://arxiv.org/pdf/2307.14008\n",
        "# https://github.com/microsoft/TokenMixers/blob/main/Adaptive%20Frequency%20Filters/affnet/modules/aff_block.py#L62\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class AFNO2D_channelfirst(nn.Module):\n",
        "    def __init__(self, hidden_size, num_blocks=8, hidden_size_factor=1):\n",
        "        super().__init__()\n",
        "        self.num_blocks = num_blocks # num_blocks: how many blocks to use in the block diagonal weight matrices (higher => less complexity but less parameters)\n",
        "        self.block_size = hidden_size // self.num_blocks\n",
        "        scale = 0.02\n",
        "\n",
        "        self.w1 = nn.Parameter(scale * torch.randn(2, self.num_blocks, self.block_size, self.block_size * hidden_size_factor))\n",
        "        self.b1 = nn.Parameter(scale * torch.randn(2, self.num_blocks, self.block_size * hidden_size_factor))\n",
        "        self.w2 = nn.Parameter(scale * torch.randn(2, self.num_blocks, self.block_size * hidden_size_factor, self.block_size))\n",
        "        self.b2 = nn.Parameter(scale * torch.randn(2, self.num_blocks, self.block_size))\n",
        "\n",
        "    @torch.cuda.amp.autocast(enabled=False)\n",
        "    def forward(self, x): # [b,c,h,w]\n",
        "        bias = x\n",
        "        dtype = x.dtype\n",
        "        x = x.float()\n",
        "        B, C, H, W = x.shape\n",
        "        x = torch.fft.rfft2(x, dim=(2, 3), norm=\"ortho\") # [b, c, h, w//2+1]\n",
        "        origin_ffted = x\n",
        "        x = x.unflatten(1, (self.num_blocks, self.block_size)) # [b, n_blocks, block_size, h, w//2+1]\n",
        "\n",
        "# [b, n_blocks, block_size, h, w//2+1] @ [n_blocks, block_size, block_size * h_factor] -> [b, n_blocks, block_size * h_factor, h, w//2+1]\n",
        "# + [n_blocks, block_size * h_factor]\n",
        "        o1_real = F.relu(torch.einsum('bkihw,kio->bkohw', x.real, self.w1[0]) - \\\n",
        "            torch.einsum('bkihw,kio->bkohw', x.imag, self.w1[1]) + self.b1[0, :, :, None, None])\n",
        "        o1_imag = F.relu(torch.einsum('bkihw,kio->bkohw', x.imag, self.w1[0]) + \\\n",
        "            torch.einsum('bkihw,kio->bkohw', x.real, self.w1[1]) + self.b1[1, :, :, None, None])\n",
        "        o2_real = (torch.einsum('bkihw,kio->bkohw', o1_real, self.w2[0]) - \\\n",
        "            torch.einsum('bkihw,kio->bkohw', o1_imag, self.w2[1]) + self.b2[0, :, :, None, None])\n",
        "        o2_imag = (torch.einsum('bkihw,kio->bkohw', o1_imag, self.w2[0]) + \\\n",
        "            torch.einsum('bkihw,kio->bkohw', o1_real, self.w2[1]) + self.b2[1, :, :, None, None])\n",
        "\n",
        "# [n_blocks, block_size * h_factor, block_size]\n",
        "\n",
        "\n",
        "        x = torch.stack([o2_real, o2_imag], dim=-1)\n",
        "        x = F.softshrink(x, lambd=0.01)\n",
        "        x = torch.view_as_complex(x)\n",
        "        x = x.flatten(1,2) # [b, c, h, w]\n",
        "\n",
        "        x = x * origin_ffted\n",
        "        x = torch.fft.irfft2(x, s=(H, W), dim=(2, 3), norm=\"ortho\")\n",
        "        x = x.type(dtype)\n",
        "        return x + bias\n",
        "\n",
        "# batch, T, dim = 2,7,5\n",
        "b,c,h,w = 2,16,9,9\n",
        "# w//2+1\n",
        "# b,c,h,w = 2,3,16,16\n",
        "mix = AFNO2D_channelfirst(c, 4)\n",
        "x = torch.rand(b,c,h,w)\n",
        "out = mix(x)\n",
        "print(out.shape)\n",
        "# print(out)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7dhOatHPQwPU",
        "outputId": "64baad83-1f4d-4412-848a-8af45f9409b8",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 16, 9, 5])\n",
            "torch.Size([2, 4, 4, 9, 5])\n",
            "torch.Size([2, 16, 9, 9])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-36-6b5c42229079>:17: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  @torch.cuda.amp.autocast(enabled=False)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## trash"
      ],
      "metadata": {
        "id": "3TZPNk-vBQtG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title hiera_mae.py\n",
        "# https://github.com/facebookresearch/hiera/blob/main/hiera/hiera_mae.py\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def apply_fusion_head(head, x):\n",
        "    if isinstance(head, nn.Identity):\n",
        "        return x\n",
        "    B, num_mask_units = x.shape[0:2]\n",
        "    # Apply head, e.g [B, #MUs, My, Mx, C] -> head([B * #MUs, C, My, Mx])\n",
        "    permute = [0] + [len(x.shape) - 2] + list(range(1, len(x.shape) - 2))\n",
        "    x = head(x.reshape(B * num_mask_units, *x.shape[2:]).permute(permute))\n",
        "\n",
        "    # Restore original layout, e.g. [B * #MUs, C', My', Mx'] -> [B, #MUs, My', Mx', C']\n",
        "    permute = [0] + list(range(2, len(x.shape))) + [1]\n",
        "    x = x.permute(permute).reshape(B, num_mask_units, *x.shape[2:], x.shape[1])\n",
        "    return x\n",
        "\n",
        "\n",
        "class MaskedAutoencoderHiera(Hiera):\n",
        "    def __init__(self, in_chans = 3, patch_stride = (4, 4),\n",
        "        decoder_embed_dim = 512, decoder_depth = 8, decoder_num_heads = 16, **kwdargs,):\n",
        "        super().__init__(in_chans=in_chans, patch_stride=patch_stride, **kwdargs)\n",
        "        del self.head\n",
        "        encoder_dim_out = self.blocks[-1].dim_out\n",
        "        self.encoder_norm = nn.LayerNorm(encoder_dim_out)\n",
        "        self.mask_unit_spatial_shape_final = [i // s ** (self.q_pool) for i, s in zip(self.mask_unit_size, self.q_stride)]\n",
        "        self.tokens_spatial_shape_final = [i // s ** (self.q_pool) for i, s in zip(self.tokens_spatial_shape, self.q_stride)]\n",
        "        # --------------------------------------------------------------------------\n",
        "        # Multi-scale fusion heads\n",
        "        curr_mu_size = self.mask_unit_size\n",
        "        self.multi_scale_fusion_heads = nn.ModuleList()\n",
        "        for i in self.stage_ends[: self.q_pool]:  # resolution constant after q_pool\n",
        "            kernel = [i // s for i, s in zip(curr_mu_size, self.mask_unit_spatial_shape_final)]\n",
        "            curr_mu_size = [i // s for i, s in zip(curr_mu_size, self.q_stride)]\n",
        "            self.multi_scale_fusion_heads.append(\n",
        "                conv_nd(len(self.q_stride))(self.blocks[i].dim_out, encoder_dim_out, kernel_size=kernel, stride=kernel)\n",
        "            )\n",
        "        self.multi_scale_fusion_heads.append(nn.Identity())  # final stage, no transform\n",
        "\n",
        "        # --------------------------------------------------------------------------\n",
        "        # MAE decoder specifics\n",
        "        self.decoder_embed = nn.Linear(encoder_dim_out, decoder_embed_dim)\n",
        "        self.mask_token = nn.Parameter(torch.randn(1, 1, decoder_embed_dim)*0.02)\n",
        "        self.decoder_pos_embed = nn.Parameter(torch.randn(1, math.prod(self.tokens_spatial_shape_final), decoder_embed_dim)*0.02)\n",
        "        self.decoder_blocks = nn.ModuleList([HieraBlock(dim=decoder_embed_dim, dim_out=decoder_embed_dim, heads=decoder_num_heads) for i in range(decoder_depth)])\n",
        "        self.pred_stride = patch_stride[-1] * (self.q_stride[-1] ** self.q_pool)\n",
        "        self.decoder_out = nn.Sequential(nn.LayerNorm(decoder_embed_dim), nn.Linear(decoder_embed_dim, (self.pred_stride**min(2, len(self.q_stride)))*in_chans))\n",
        "\n",
        "    def get_pixel_label_2d(self, input_img, mask, norm=True): # mask: bool, True -> mask\n",
        "        input_img = input_img.permute(0, 2, 3, 1)\n",
        "\n",
        "        size = self.pred_stride\n",
        "        label = input_img.unfold(1, size, size).unfold(2, size, size)\n",
        "        label = label.flatten(1, 2).flatten(2)\n",
        "        label = label[mask]\n",
        "        if norm:\n",
        "            label = F.layer_norm(label, label.shape[-1:]) # l = F.layer_norm(label, (label.size(-1),))\n",
        "        return label\n",
        "\n",
        "    def get_pixel_label_3d(self, input_vid, mask, norm = True): # [b,c,f,h,w]?\n",
        "        # mask (boolean tensor): True must correspond to *masked*\n",
        "        # We use time strided loss, only take the first frame from each token\n",
        "        input_vid = input_vid[:, :, ::self.patch_stride[0], :, :]\n",
        "        size = self.pred_stride\n",
        "        label = input_vid.unfold(3, size, size).unfold(4, size, size)\n",
        "        label = label.permute(0, 2, 3, 4, 5, 6, 1)  # Different from 2d, mistake during training lol\n",
        "        label = label.flatten(1, 3).flatten(2)\n",
        "        label = label[mask]\n",
        "        if norm:\n",
        "            label = F.layer_norm(label, label.shape[-1:])\n",
        "        return label\n",
        "\n",
        "    def forward_encoder(self, x, mask=None):\n",
        "        # Get multi-scale representations from encoder\n",
        "        _, intermediates = super().forward(x, mask, return_intermediates=True)\n",
        "        # Resolution unchanged after q_pool stages, so skip those features\n",
        "        intermediates = intermediates[: self.q_pool] + intermediates[-1:]\n",
        "\n",
        "        # Multi-scale fusion\n",
        "        x = sum([apply_fusion_head(head, interm_x) for head, interm_x in zip(self.multi_scale_fusion_heads, intermediates)])\n",
        "        x = self.encoder_norm(x)\n",
        "        return x, mask\n",
        "\n",
        "    def forward_decoder(self, x, mask):\n",
        "        x = self.decoder_embed(x)\n",
        "\n",
        "        # Combine visible and mask tokens\n",
        "\n",
        "        # x: [B, #MUs, *mask_unit_spatial_shape_final, encoder_dim_out]\n",
        "        # mask: [B, #MUs_all]\n",
        "        x_dec = torch.zeros(*mask.shape, *x.shape[2:], device=x.device, dtype=x.dtype)\n",
        "        mask_tokens = self.mask_token.view((1,) * (len(mask.shape) + len(x.shape[2:-1])) + (-1,))\n",
        "        mask = mask.reshape(mask.shape + (1,) * len(x.shape[2:]))\n",
        "        mask = mask.expand((-1,) * 2 + x.shape[2:]).bool()\n",
        "        x_dec[mask] = x.flatten()\n",
        "        x_dec = ~mask * mask_tokens + mask * x_dec\n",
        "\n",
        "        # Get back spatial order\n",
        "        x = undo_windowing(x_dec, self.tokens_spatial_shape_final, self.mask_unit_spatial_shape_final)\n",
        "        mask = undo_windowing(mask[..., 0:1], self.tokens_spatial_shape_final, self.mask_unit_spatial_shape_final)\n",
        "\n",
        "        # Flatten\n",
        "        x = x.reshape(x.shape[0], -1, x.shape[-1])\n",
        "        mask = mask.view(x.shape[0], -1)\n",
        "\n",
        "        x = x + self.decoder_pos_embed\n",
        "        for blk in self.decoder_blocks:\n",
        "            x = blk(x)\n",
        "        x = self.decoder_out(x)\n",
        "        return x, mask\n",
        "\n",
        "    def forward_loss(self, x, pred, mask):\n",
        "        \"\"\"\n",
        "        Note: in mask, 0 is *visible*, 1 is *masked*\n",
        "\n",
        "        x: e.g. [B, 3, H, W]\n",
        "        pred: [B * num_pred_tokens, num_pixels_in_pred_patch * in_chans]\n",
        "        label: [B * num_pred_tokens, num_pixels_in_pred_patch * in_chans]\n",
        "        \"\"\"\n",
        "        if len(self.q_stride) == 2:\n",
        "            label = self.get_pixel_label_2d(x, mask)\n",
        "        elif len(self.q_stride) == 3:\n",
        "            label = self.get_pixel_label_3d(x, mask)\n",
        "        pred = pred[mask]\n",
        "        loss = (pred - label) ** 2\n",
        "        return loss.mean(), pred, label\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        latent, mask = self.forward_encoder(x, mask=mask)\n",
        "        pred, pred_mask = self.forward_decoder(latent, mask)  # pred_mask is mask at resolution of *prediction*\n",
        "        # Toggle mask, to generate labels for *masked* tokens\n",
        "        return *self.forward_loss(x, pred, ~pred_mask), mask\n",
        "\n",
        "\n",
        "# model = MaskedAutoencoderHiera(embed_dim=16, num_heads=1, stages=(1, 2, 2, 2), q_pool=2)\n",
        "mask_ratio=.8\n",
        "# x = torch.randn((1, 3, 224, 224))\n",
        "# mask = randpatch(math.prod(model.mask_spatial_shape), x.shape[0], mask_ratio)  # [B, #MUs_all]\n",
        "# # out = model(x, mask_ratio=0.6, mask=mask)\n",
        "out = model(x, mask=mask)\n",
        "# out = model(x)\n",
        "print(out)\n",
        "print(len(out))\n",
        "for o in out:\n",
        "    print(o.shape)\n",
        "\n",
        "# [], [116, 768], [116, 768], [1, 49]\n",
        "\n",
        "# # Image Models\n",
        "# def mae_hiera_tiny_224(**kwargs):\n",
        "#     return MaskedAutoencoderHiera(embed_dim=96, num_heads=1, stages=(1, 2, 7, 2), q_pool=2, **kwargs,\n",
        "# def mae_hiera_small_224(embed_dim=96, num_heads=1, stages=(1, 2, 11, 2), q_pool=2, **kwargs,\n",
        "# def mae_hiera_base_224(embed_dim=96, num_heads=1, stages=(2, 3, 16, 3), q_pool=2, **kwargs,\n",
        "# def mae_hiera_base_plus_224(embed_dim=112, num_heads=2, stages=(2, 3, 16, 3), q_pool=2, **kwargs,\n",
        "# def mae_hiera_large_224(embed_dim=144, num_heads=2, stages=(2, 6, 36, 4), q_pool=2, **kwargs,\n",
        "# def mae_hiera_huge_224(embed_dim=256, num_heads=4, stages=(2, 6, 36, 4), q_pool=2, **kwargs,\n",
        "\n",
        "# # Video Models\n",
        "# def mae_hiera_base_16x224(num_classes = 400, **kwdargs):\n",
        "#     return MaskedAutoencoderHiera(num_classes=num_classes,  # K400 has 400 classes\n",
        "#         input_size=(16, 224, 224), q_stride=(1, 2, 2), mask_unit_size=(1, 8, 8),\n",
        "#         patch_kernel=(3, 7, 7), patch_stride=(2, 4, 4), patch_padding=(1, 3, 3),\n",
        "#         sep_pos_embed=True, q_pool=2, **kwdargs)\n",
        "# def mae_hiera_base_plus_16x224(embed_dim=112, num_heads=2, stages=(2, 3, 16, 3), **kwdargs\n",
        "# def mae_hiera_large_16x224(embed_dim=144, num_heads=2, stages=(2, 6, 36, 4), **kwdargs\n",
        "# def mae_hiera_huge_16x224(embed_dim=256, num_heads=4, stages=(2, 6, 36, 4), **kwdargs\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "ibbgw_YVHadz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title hiera_mae.py\n",
        "# https://github.com/facebookresearch/hiera/blob/main/hiera/hiera_mae.py\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def apply_fusion_head(head, x): # [B, #MUs, My, Mx, C]\n",
        "    if isinstance(head, nn.Identity): return x\n",
        "    B, num_mask_units = x.shape[0:2]\n",
        "    permute = [0] + [len(x.shape) - 2] + list(range(1, len(x.shape) - 2))\n",
        "    x = head(x.reshape(B * num_mask_units, *x.shape[2:]).permute(permute)) # [B, #MUs, My, Mx, C] -> head([B * #MUs, C, My, Mx])\n",
        "    permute = [0] + list(range(2, len(x.shape))) + [1] # [0, *2345.., 1]\n",
        "    x = x.permute(permute).reshape(B, num_mask_units, *x.shape[2:], x.shape[1]) # [B * #MUs, C', My', Mx'] -> [B, #MUs, My', Mx', C']\n",
        "    return x\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, in_dim=16*2**3, d_model = 512, out_dim=3,depth = 8, n_heads=16, patch_stride = (4, 4),tokens_spatial_shape=(56,56),q_pool=3 , q_stride=(2, 2),\n",
        "        mask_unit_size = (8, 8), # must divide q_stride**(num_stages-1)\n",
        "        ):\n",
        "        super().__init__()\n",
        "        self.mask_unit_spatial_shape_final = [i//s**q_pool for i, s in zip(mask_unit_size, q_stride)] # [1, 1]\n",
        "        self.tokens_spatial_shape_final = [i//s**q_pool for i, s in zip(tokens_spatial_shape, q_stride)] # [7, 7]\n",
        "\n",
        "        self.embed = nn.Linear(in_dim, d_model)\n",
        "        self.mask_token = nn.Parameter(torch.randn(1, 1, d_model)*0.02)\n",
        "        self.pos_embed = nn.Parameter(torch.randn(1, math.prod(self.tokens_spatial_shape_final), d_model)*0.02)\n",
        "        self.blocks = nn.ModuleList([HieraBlock(dim=d_model, dim_out=d_model, heads=n_heads) for i in range(depth)])\n",
        "        self.pred_stride = patch_stride[-1] * (q_stride[-1] ** q_pool)\n",
        "        self.out = nn.Sequential(nn.LayerNorm(d_model), nn.Linear(d_model, (self.pred_stride**min(2, len(q_stride)))*out_dim))\n",
        "\n",
        "    def forward(self, x, mask): # [24, 9, 2, 2, 128], [24, 49] # [24, 9, 128], [24, 49]\n",
        "        # print('fwd dec1', x.shape)\n",
        "        print('fwd dec1', x.shape, mask.shape)\n",
        "        x = self.embed(x) # [24, 9, 2, 2, 512]\n",
        "\n",
        "        # # x: [B, #MUs, *mask_unit_spatial_shape_final, encoder_dim_out]\n",
        "        # # mask: [B, #MUs_all]\n",
        "        # x_dec = torch.zeros(*mask.shape, *x.shape[2:], device=x.device, dtype=x.dtype) # [24, 49, 2, 2, 512]\n",
        "        # mask_tokens = self.mask_token.view((1,) * (mask.dim() + x.dim()-3) + (-1,)) # (1, 1, 1, 1, -1) # [1, 1, 1, 1, 512]\n",
        "        # mask = mask[...,*[None for _ in range(x.dim()-2)]] # [24, 49, 1, 1, 1] # ([24, 49, 1, 1, 1]\n",
        "        # mask = mask.expand((-1,) * 2 + x.shape[2:]).bool() # (-1, -1, 2, 2, 512) # [24, 49, 2, 2, 512]\n",
        "\n",
        "        x_dec = torch.zeros(*mask.shape, x.shape[-1], device=x.device, dtype=x.dtype) # [24, 49, 512]\n",
        "\n",
        "        # print('fwd dec3', x_dec.shape, mask.shape, mask_tokens.shape)\n",
        "        print('fwd dec3', x_dec.shape, mask.shape, self.mask_token.shape) # [24, 49, 512], [24, 49], [1, 1, 512]\n",
        "        mask = mask.unsqueeze(-1).repeat(1,1, x.shape[-1])\n",
        "\n",
        "        x_dec[mask] = x.flatten()\n",
        "        # x_dec = ~mask * mask_tokens + mask * x_dec # [24, 49, 2, 2, 512]\n",
        "        x_dec = ~mask * self.mask_token + mask * x_dec # [24, 49, 2, 2, 512]\n",
        "        # print('fwd dec3', x_dec.shape)\n",
        "\n",
        "        # Get back spatial order\n",
        "        # print(self.tokens_spatial_shape_final, self.mask_unit_spatial_shape_final) # [24, 14, 14, 512]\n",
        "        x = undo_windowing(x_dec, self.tokens_spatial_shape_final, self.mask_unit_spatial_shape_final) # [24, 14, 14, 512]\n",
        "        # mask = undo_windowing(mask[..., 0:1], self.tokens_spatial_shape_final, self.mask_unit_spatial_shape_final) # [24, 14, 14, 1]\n",
        "        # print('fwd dec4', x.shape, mask[..., 0].shape) # ,\n",
        "\n",
        "        x = x.reshape(x.shape[0], -1, x.shape[-1])\n",
        "        # mask = mask.view(x.shape[0], -1)\n",
        "\n",
        "        x = x + self.pos_embed\n",
        "        for blk in self.blocks:\n",
        "            x = blk(x)\n",
        "        x = self.out(x)\n",
        "        return x#, mask\n",
        "\n",
        "encoder = Hiera(d_model=16, n_heads=1, stages=(2, 2, 3, 2), q_pool=3)\n",
        "decoder = Decoder(in_dim=16*2**3, n_heads=1, out_dim=3, tokens_spatial_shape=(56,56))\n",
        "# self.decoder_blocks = nn.ModuleList([HieraBlock(dim=decoder_embed_dim, dim_out=decoder_embed_dim, heads=decoder_num_heads) for i in range(decoder_depth)])\n",
        "\n",
        "mask_ratio=.8\n",
        "x = torch.randn((24, 3, 224, 224))\n",
        "mask = randpatch(math.prod(encoder.mask_spatial_shape), x.shape[0], mask_ratio)  # [B, #MUs_all]\n",
        "latent = encoder(x, mask, return_intermediates=True)\n",
        "print('latent',latent.shape)\n",
        "pred = decoder(latent, mask)\n",
        "\n",
        "# print(out.shape)\n",
        "print(pred.shape)\n",
        "\n",
        "# # Image Models\n",
        "# mae_hiera_tiny_224(embed_dim=96, num_heads=1, stages=(1, 2, 7, 2), q_pool=2)\n",
        "# # Video Models\n",
        "# def mae_hiera_base_16x224(num_classes = 400, **kwdargs):\n",
        "#     return MaskedAutoencoderHiera(num_classes=num_classes,  # K400 has 400 classes\n",
        "#         input_size=(16, 224, 224), q_stride=(1, 2, 2), mask_unit_size=(1, 8, 8),\n",
        "#         patch_kernel=(3, 7, 7), patch_stride=(2, 4, 4), patch_padding=(1, 3, 3),\n",
        "#         sep_pos_embed=True, q_pool=2, **kwdargs)\n",
        "# def mae_hiera_base_plus_16x224(embed_dim=112, num_heads=2, stages=(2, 3, 16, 3)\n",
        "\n"
      ],
      "metadata": {
        "id": "aaGz4BrU9C1L",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "outputId": "58b62c26-1851-49a4-b23f-32bfd5dbbf05",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'Hiera' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-56aa435bf16a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;31m#, mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m \u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHiera\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_heads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_pool\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0mdecoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_heads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens_spatial_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m56\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m56\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;31m# self.decoder_blocks = nn.ModuleList([HieraBlock(dim=decoder_embed_dim, dim_out=decoder_embed_dim, heads=decoder_num_heads) for i in range(decoder_depth)])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Hiera' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title hiera_mae.py\n",
        "# https://github.com/facebookresearch/hiera/blob/main/hiera/hiera_mae.py\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def apply_fusion_head(head, x): # [B, #MUs, My, Mx, C]\n",
        "    if isinstance(head, nn.Identity): return x\n",
        "    B, num_mask_units = x.shape[0:2]\n",
        "    permute = [0] + [len(x.shape) - 2] + list(range(1, len(x.shape) - 2))\n",
        "    x = head(x.reshape(B * num_mask_units, *x.shape[2:]).permute(permute)) # [B, #MUs, My, Mx, C] -> head([B * #MUs, C, My, Mx])\n",
        "    permute = [0] + list(range(2, len(x.shape))) + [1] # [0, *2345.., 1]\n",
        "    x = x.permute(permute).reshape(B, num_mask_units, *x.shape[2:], x.shape[1]) # [B * #MUs, C', My', Mx'] -> [B, #MUs, My', Mx', C']\n",
        "    return x\n",
        "\n",
        "# class MaskedAutoencoderHiera(Hiera):\n",
        "class MaskedAutoencoderHiera(nn.Module):\n",
        "    def __init__(self, in_dim=16*2**3, d_model = 512, out_dim=3,depth = 8, n_heads=16, patch_stride = (4, 4),tokens_spatial_shape=(56,56),q_pool=3 , q_stride=(2, 2),\n",
        "        mask_unit_size = (8, 8), # must divide q_stride**(num_stages-1)\n",
        "        ):\n",
        "        super().__init__()\n",
        "        self.mask_unit_spatial_shape_final = [i//s**q_pool for i, s in zip(mask_unit_size, q_stride)] # [1, 1]\n",
        "        self.tokens_spatial_shape_final = [i//s**q_pool for i, s in zip(tokens_spatial_shape, q_stride)] # [7, 7]\n",
        "\n",
        "        self.embed = nn.Linear(in_dim, d_model)\n",
        "        self.mask_token = nn.Parameter(torch.randn(1, 1, d_model)*0.02)\n",
        "        self.pos_embed = nn.Parameter(torch.randn(1, math.prod(self.tokens_spatial_shape_final), d_model)*0.02)\n",
        "        self.blocks = nn.ModuleList([HieraBlock(dim=d_model, dim_out=d_model, heads=n_heads) for i in range(depth)])\n",
        "        self.pred_stride = patch_stride[-1] * (q_stride[-1] ** q_pool)\n",
        "        self.out = nn.Sequential(nn.LayerNorm(d_model), nn.Linear(d_model, (self.pred_stride**min(2, len(q_stride)))*out_dim))\n",
        "\n",
        "    # def get_pixel_label_2d(self, input_img, mask, norm=True): # mask: bool, True -> mask\n",
        "    #     # [b,c,h,w], [B * num_pred_tokens, num_pixels_in_pred_patch * in_dim], 1->mask\n",
        "    #     input_img = input_img.permute(0, 2, 3, 1) # [b,h,w,c]\n",
        "    #     size = self.pred_stride\n",
        "    #     label = input_img.unfold(1, size, size).unfold(2, size, size) # [b,h/s,w/s,c,s,s]\n",
        "    #     label = label.flatten(1, 2).flatten(2) # [b, h/s* w/s, s*s*c]\n",
        "    #     label = label[mask]\n",
        "    #     if norm: label = F.layer_norm(label, label.shape[-1:]) # l = F.layer_norm(label, (label.size(-1),))\n",
        "    #     return label\n",
        "\n",
        "    # def get_pixel_label_3d(self, input_vid, mask, norm = True): # [b,c,f,h,w]?\n",
        "    #     # mask (boolean tensor): True must correspond to *masked*\n",
        "    #     # We use time strided loss, only take the first frame from each token\n",
        "    #     input_vid = input_vid[:, :, ::self.patch_stride[0], :, :]\n",
        "    #     # input_img = input_img.permute(0, 2, 3, 4, 1) # [b,f,h,w,c]\n",
        "    #     size = self.pred_stride\n",
        "    #     label = input_vid.unfold(3, size, size).unfold(4, size, size) # [b,c,patch_stride[0],h/s,w/s,s,s]\n",
        "    #     label = label.permute(0, 2, 3, 4, 5, 6, 1) # [b,patch_stride[0],h/s,w/s,s,s,c] # Different from 2d, mistake during training lol\n",
        "    #     label = label.flatten(1, 3).flatten(2) # [b,patch_stride[0]* h/s* w/s,s*s*c]\n",
        "    #     label = label[mask]\n",
        "    #     if norm: label = F.layer_norm(label, label.shape[-1:])\n",
        "    #     return label\n",
        "\n",
        "    def forward_decoder(self, x, mask): # [24, 9, 2, 2, 128], [24, 49]\n",
        "        print('fwd dec1', x.shape)\n",
        "        x = self.embed(x) # [24, 9, 2, 2, 512]\n",
        "\n",
        "        # x: [B, #MUs, *mask_unit_spatial_shape_final, encoder_dim_out]\n",
        "        # mask: [B, #MUs_all]\n",
        "        x_dec = torch.zeros(*mask.shape, *x.shape[2:], device=x.device, dtype=x.dtype) # [24, 49, 2, 2, 512]\n",
        "        mask_tokens = self.mask_token.view((1,) * (mask.dim() + x.dim()-3) + (-1,)) # (1, 1, 1, 1, -1) # [1, 1, 1, 1, 512]\n",
        "\n",
        "        mask = mask[...,*[None for _ in range(x.dim()-2)]] # [24, 49, 1, 1, 1] # ([24, 49, 1, 1, 1]\n",
        "        # print('fwd dec3', x_dec.shape, mask.shape, mask_tokens.shape)\n",
        "        mask = mask.expand((-1,) * 2 + x.shape[2:]).bool() # (-1, -1, 2, 2, 512) # [24, 49, 2, 2, 512]\n",
        "        # print((-1,) * 2 + x.shape[2:])\n",
        "        x_dec[mask] = x.flatten()\n",
        "        x_dec = ~mask * mask_tokens + mask * x_dec # [24, 49, 2, 2, 512]\n",
        "        print('fwd dec3', x_dec.shape)\n",
        "\n",
        "        # Get back spatial order\n",
        "        print(self.tokens_spatial_shape_final, self.mask_unit_spatial_shape_final) # [24, 14, 14, 512]\n",
        "        x = undo_windowing(x_dec, self.tokens_spatial_shape_final, self.mask_unit_spatial_shape_final) # [24, 14, 14, 512]\n",
        "        # mask = undo_windowing(mask[..., 0:1], self.tokens_spatial_shape_final, self.mask_unit_spatial_shape_final) # [24, 14, 14, 1]\n",
        "        # print('fwd dec4', x.shape, mask[..., 0].shape) # ,\n",
        "\n",
        "        x = x.reshape(x.shape[0], -1, x.shape[-1])\n",
        "        # mask = mask.view(x.shape[0], -1)\n",
        "\n",
        "        x = x + self.pos_embed\n",
        "        for blk in self.blocks:\n",
        "            x = blk(x)\n",
        "        x = self.out(x)\n",
        "        return x#, mask\n",
        "\n",
        "    # def forward_loss(self, x, pred, mask): # [b,c,(f,)h,w], [B * num_pred_tokens, num_pixels_in_pred_patch * in_chans], 1->mask\n",
        "    #     if len(self.q_stride) == 2: label = self.get_pixel_label_2d(x, mask) # [B * num_pred_tokens, num_pixels_in_pred_patch * in_chans]\n",
        "    #     elif len(self.q_stride) == 3: label = self.get_pixel_label_3d(x, mask)\n",
        "    #     pred = pred[mask]\n",
        "    #     loss = F.mse_loss(pred, label) # ((pred-label)**2).mean()\n",
        "    #     return loss, pred, label\n",
        "\n",
        "    # def forward(self, x, mask=None):\n",
        "    #     latent, mask = self.forward_encoder(x, mask=mask)\n",
        "    #     # latent = self.forward_encoder(x, mask=mask)\n",
        "    #     # print('mse fwd', latent.shape, mask.shape)\n",
        "    #     # pred, pred_mask = self.forward_decoder(latent, mask)  # pred_mask is mask at resolution of *prediction*\n",
        "    #     pred = self.forward_decoder(latent, mask)  # pred_mask is mask at resolution of *prediction*\n",
        "    #     # Toggle mask, to generate labels for *masked* tokens\n",
        "    #     # return *self.forward_loss(x, pred, ~pred_mask), mask\n",
        "    #     return pred\n",
        "\n",
        "\n",
        "# model = MaskedAutoencoderHiera(embed_dim=16, num_heads=1, stages=(1, 2, 2, 2), q_pool=2)\n",
        "encoder = Hiera(embed_dim=16, num_heads=1, stages=(1, 2, 2, 2), q_pool=2)\n",
        "# # decoder = MaskedAutoencoderHiera(embed_dim=16, num_heads=1, stages=(1, 2, 2, 2), q_pool=2)\n",
        "decoder = MaskedAutoencoderHiera(in_dim=16*2**3, n_heads=1, out_dim=3, tokens_spatial_shape=(56,56))\n",
        "\n",
        "# self.decoder_blocks = nn.ModuleList([HieraBlock(dim=decoder_embed_dim, dim_out=decoder_embed_dim, heads=decoder_num_heads) for i in range(decoder_depth)])\n",
        "\n",
        "\n",
        "mask_ratio=.8\n",
        "# x = torch.randn((1, 3, 224, 224))\n",
        "# mask = randpatch(math.prod(model.mask_spatial_shape), x.shape[0], mask_ratio)  # [B, #MUs_all]\n",
        "mask = randpatch(math.prod(encoder.mask_spatial_shape), x.shape[0], mask_ratio)  # [B, #MUs_all]\n",
        "# # out = model(x, mask_ratio=0.6, mask=mask)\n",
        "out = model(x, mask=mask)\n",
        "# latent = encoder(x, mask, return_intermediates=True)\n",
        "# print(latent.shape)\n",
        "# pred = decoder.forward_decoder(latent, mask)\n",
        "\n",
        "# pred =\n",
        "# out = model.forward_loss(x, pred, mask)\n",
        "# out = model(x)\n",
        "# loss, pred, label, msk = out\n",
        "# for o in out:\n",
        "#     print(o.shape)\n",
        "# print(label.shape, pred.shape)\n",
        "print(out.shape)\n",
        "\n",
        "# [], [116, 768], [116, 768], [1, 49]\n",
        "\n",
        "# # Image Models\n",
        "# def mae_hiera_tiny_224(**kwargs):\n",
        "#     return MaskedAutoencoderHiera(embed_dim=96, num_heads=1, stages=(1, 2, 7, 2), q_pool=2, **kwargs,\n",
        "# def mae_hiera_small_224(embed_dim=96, num_heads=1, stages=(1, 2, 11, 2), q_pool=2, **kwargs,\n",
        "# def mae_hiera_base_224(embed_dim=96, num_heads=1, stages=(2, 3, 16, 3), q_pool=2, **kwargs,\n",
        "# def mae_hiera_base_plus_224(embed_dim=112, num_heads=2, stages=(2, 3, 16, 3), q_pool=2, **kwargs,\n",
        "# def mae_hiera_large_224(embed_dim=144, num_heads=2, stages=(2, 6, 36, 4), q_pool=2, **kwargs,\n",
        "# def mae_hiera_huge_224(embed_dim=256, num_heads=4, stages=(2, 6, 36, 4), q_pool=2, **kwargs,\n",
        "\n",
        "# # Video Models\n",
        "# def mae_hiera_base_16x224(num_classes = 400, **kwdargs):\n",
        "#     return MaskedAutoencoderHiera(num_classes=num_classes,  # K400 has 400 classes\n",
        "#         input_size=(16, 224, 224), q_stride=(1, 2, 2), mask_unit_size=(1, 8, 8),\n",
        "#         patch_kernel=(3, 7, 7), patch_stride=(2, 4, 4), patch_padding=(1, 3, 3),\n",
        "#         sep_pos_embed=True, q_pool=2, **kwdargs)\n",
        "# def mae_hiera_base_plus_16x224(embed_dim=112, num_heads=2, stages=(2, 3, 16, 3), **kwdargs\n",
        "# def mae_hiera_large_16x224(embed_dim=144, num_heads=2, stages=(2, 6, 36, 4), **kwdargs\n",
        "# def mae_hiera_huge_16x224(embed_dim=256, num_heads=4, stages=(2, 6, 36, 4), **kwdargs\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d6f16ce-c1fc-4afc-ecc1-7a0f78549636",
        "cellView": "form",
        "id": "sQvDhA8S75DX"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fwd1 torch.Size([24, 3136, 96])\n",
            "1 torch.Size([24, 576, 96]) [1, 4, 20, 23]\n",
            "4 torch.Size([24, 144, 192]) [1, 4, 20, 23]\n",
            "20 torch.Size([24, 36, 384]) [1, 4, 20, 23]\n",
            "23 torch.Size([24, 9, 768]) [1, 4, 20, 23]\n",
            "fwd dec1 torch.Size([24, 9, 1, 1, 768])\n",
            "fwd dec3 torch.Size([24, 49, 1, 1, 512])\n",
            "[7, 7] [1, 1]\n",
            "torch.Size([24, 49, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title conv_nd maxpool_nd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# if self.q_stride > 1: q = q.view(B, self.n_heads, num_windows, self.q_stride, -1, self.d_head).max(dim=3).values # q pooling # [b, n_heads, num_windows, q_stride,window_size, d_head]-> [b, n_heads, num_windows, window_size, d_head]\n",
        "\n",
        "# def conv_nd(n): return [nn.Identity, nn.Conv1d, nn.Conv2d, nn.Conv3d][n]\n",
        "def conv_nd(n, *args, **kwargs): return [nn.Identity, nn.Conv1d, nn.Conv2d, nn.Conv3d][n](*args, **kwargs)\n",
        "def maxpool_nd(n, *args, **kwargs): return [nn.Identity, nn.MaxPool1d, nn.MaxPool2d, nn.MaxPool3d][n](*args, **kwargs)\n",
        "# conv = conv_nd(2, 3,16,(3,3), 1,3//2)\n",
        "conv = conv_nd(2, 3,16,(3,3), 1,padding=3//2)\n",
        "# conv = nn.Conv2d(3,16,(3,3), 1,3//2) # Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "pool = maxpool_nd(3, (3, 2, 2), stride=(2, 1, 2))\n",
        "print(conv)\n",
        "print(pool)\n",
        "\n",
        "# [b,c,f,h,w]\n",
        "b,c,f,h,w = 2,3,5,8,8\n",
        "x = torch.rand(b,c,f,h,w)\n",
        "# pool of square window of size=3, stride=2\n",
        "pool = nn.MaxPool3d((2,4,4), stride=(2,4,4))\n",
        "# # pool of non-square window\n",
        "# m = nn.MaxPool3d((3, 2, 2), stride=(2, 1, 2))\n",
        "# input = torch.randn(20, 16, 50, 44, 31)\n",
        "out = pool(x)\n",
        "print(out.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oRVMQIh0gPAi",
        "outputId": "6247541a-a66f-48fb-9f2d-82d0cf3a63d7",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "MaxPool3d(kernel_size=(3, 2, 2), stride=(2, 1, 2), padding=0, dilation=1, ceil_mode=False)\n",
            "torch.Size([2, 3, 2, 2, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title facebookresearch hiera_utils.py\n",
        "# https://github.com/facebookresearch/hiera/blob/main/hiera/hiera_utils.py\n",
        "\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# def conv_nd(n): return [nn.Identity, nn.Conv1d, nn.Conv2d, nn.Conv3d][n]\n",
        "\n",
        "# [B, (H, W), C] -> [B, (Sy, Sx, H // Sy, W // Sx), C]; stride (Sy, Sx)\n",
        "def Unroll(x, tokens_spatial_shape, unroll_schedule): # [56, 56] [(2, 2), (2, 2), (2, 2)]\n",
        "    B, _, C = shape = x.shape\n",
        "    x = x.view(B, *tokens_spatial_shape, C)\n",
        "    for strides in unroll_schedule: # q_stride (2, 2)\n",
        "        tokens_spatial_shape = [i//s for i, s in zip(tokens_spatial_shape, strides)] # [28, 28], [14, 14], [7, 7]\n",
        "        new_shape = [B] + [val for tup in zip(tokens_spatial_shape, strides) for val in tup] + [C]\n",
        "        x = x.view(new_shape) # 2d: [B, H/Sy, Sy, W/Sx, Sx, C] # [24, 28, 2, 28, 2, 16], [96, 14, 2, 14, 2, 16], [384, 7, 2, 7, 2, 16]\n",
        "        L = len(new_shape)\n",
        "        permute = ([0] + list(range(2, L - 1, 2)) + list(range(1, L - 1, 2)) + [L - 1]) # [0, 2, 4, 1, 3, 5] / [0, 2, 4, 6, 1, 3, 5, 7]\n",
        "        x = x.permute(permute).flatten(0, len(strides)) # 2d: [B, Sy, Sx, H/Sy, W/Sx, C] -> [B*Sy*Sx, H/Sy, W/Sx, C]\n",
        "        B *= math.prod(strides)\n",
        "    x = x.reshape(*shape) # 2d: [B*Sy*Sx, H/Sy* W/Sx, C] # [24, 3136, 16]\n",
        "    return x # then x.max(1) == MaxPoolNd\n",
        "\n",
        "\n",
        "\n",
        "# stages=(2, 2, 3, 2)\n",
        "# input_size = (224, 224)\n",
        "# q_stride = (2, 2)\n",
        "# patch_stride = (4, 4)\n",
        "\n",
        "# x = torch.rand(24, 16, 56, 56)\n",
        "# x = x.flatten(2).transpose(1, 2) # [b,t,c]\n",
        "# stage_ends = [sum(stages[:i]) - 1 for i in range(1, len(stages) + 1)]\n",
        "# tokens_spatial_shape = [i//s for i, s in zip(input_size, patch_stride)] # list [224/4, 224/4] = [56, 56]\n",
        "# x = Unroll(x, tokens_spatial_shape, [q_stride]*len(stage_ends[:-1]))\n",
        "# print(x.shape)\n",
        "# print(x)\n",
        "\n",
        "\n",
        "# [24, 49, 8, 8, 16] [56, 56] [8, 8] -> [24, 56, 56, 16]\n",
        "# [24, 49, 4, 4, 32] [28, 28] [4, 4] -> [24, 28, 28, 16]\n",
        "# [24, 49, 2, 2, 64] [14, 14] [2, 2] -> [24, 14, 14, 16]\n",
        "# [24, 49, 1, 1, 128] [7, 7] [1, 1] -> [24, 7, 7, 16]\n",
        "\n",
        "def undo_windowing(x, shape, mu_shape): # x [B, #MUy*#MUx, MUy, MUx, C] # shape: desired spatial shape. 2d: [B, #MUy*MUy, #MUx*MUx, C] # mu_shape: current mask unit shape. 2d: [MUy, MUx]\n",
        "    # print(\"undo_windowing\", x.shape, shape, mu_shape)\n",
        "    B, C = x.shape[0], x.shape[-1]\n",
        "    num_MUs = [s//mu for s, mu in zip(shape, mu_shape)] # [7,7]\n",
        "    x = x.view(B, *num_MUs, *mu_shape, C) # [B, #MUy*#MUx, MUy, MUx, C] -> [B, #MUy, #MUx, MUy, MUx, C]\n",
        "    D = len(shape)\n",
        "    # permute = ([0] + sum([list(p) for p in zip(range(1, 1 + D), range(1 + D, 1 + 2 * D))], []) + [len(x.shape) - 1]) # [0, 1, 3, 2, 4, 5]\n",
        "    permute = ([0] + [val for tup in zip(range(1, 1+D), range(1+D, 1+2*D)) for val in tup] + [len(x.shape)-1]) # [0, 1, 3, 2, 4, 5] / [0, 1, 4, 2, 5, 3, 6, 7]\n",
        "    x = x.permute(permute).reshape(B, *shape, C) # [B, #MUy, #MUx, MUy, MUx, C] -> [B, #MUy*MUy, #MUx*MUx, C]\n",
        "    return x # [B, #MUy*MUy, #MUx*MUx, C]\n",
        "\n",
        "# ss=2 # 8 4 2 1\n",
        "# x = torch.rand(24, 49, 2**ss,2**ss, 16)\n",
        "# shape, mu_shape = [7*2**ss, 7*2**ss], [2**ss, 2**ss]\n",
        "# o = undo_windowing(x, shape, mu_shape)\n",
        "# print(o.shape) # [24, 56, 56, 16]\n",
        "\n",
        "# [q_stride] * len(self.stage_ends[:-1]), self.stage_ends, q_pool\n",
        "class Reroll(nn.Module):\n",
        "    \"\"\"Undos the \"unroll\" operation so that you can use intermediate features.\"\"\"\n",
        "    def __init__(self, tokens_spatial_shape, unroll_schedule, stage_ends, q_pool):\n",
        "        # print(unroll_schedule, stage_ends, q_pool)\n",
        "        super().__init__()\n",
        "        # The first stage has to reverse everything, The next stage has to reverse all but the first unroll, etc.\n",
        "        # self.schedule = {}\n",
        "        # for i in range(stage_ends[-1] + 1):\n",
        "        #     self.schedule[i] = unroll_schedule, tokens_spatial_shape\n",
        "        #     # schedule unchanged if no pooling at a stage end\n",
        "        #     if i in stage_ends[:q_pool]:\n",
        "        #         if len(unroll_schedule) > 0:\n",
        "        #             tokens_spatial_shape = [n//s for n, s in zip(tokens_spatial_shape, unroll_schedule[0])]\n",
        "        #         unroll_schedule = unroll_schedule[1:]\n",
        "        # print(self.schedule)\n",
        "# {0: ([(2, 2), (2, 2), (2, 2)], [56, 56]), 1: ([(2, 2), (2, 2), (2, 2)], [56, 56]),\n",
        "# 2: ([(2, 2), (2, 2)], [28, 28]), 3: ([(2, 2), (2, 2)], [28, 28]),\n",
        "# 4: ([(2, 2)], [14, 14]), 5: ([(2, 2)], [14, 14]), 6: ([(2, 2)], [14, 14]),\n",
        "# 7: ([], [7, 7]), 8: ([], [7, 7])}\n",
        "\n",
        "    # if i in stage_ends[:q_pool]:\n",
        "        depth = sum([i<1 for i in stage_ends[:q_pool]])\n",
        "        print(depth)\n",
        "        for _ in range(depth):\n",
        "                if len(unroll_schedule) > 0:\n",
        "                    tokens_spatial_shape = [n//s for n, s in zip(tokens_spatial_shape, unroll_schedule[0])]\n",
        "                unroll_schedule = unroll_schedule[1:]\n",
        "        print(unroll_schedule, tokens_spatial_shape)\n",
        "    # def forward(self, x, block_idx, mask=None):\n",
        "    # def forward(self, x, block_idx):\n",
        "    def forward(self, x, size, schedule, stage_ends, q_pool): # [24, 256, 16], list[(2, 2), (2, 2), (2, 2)], [56, 56]\n",
        "    # def forward(x, schedule, size): # [24, 256, 16], list[(2, 2), (2, 2), (2, 2)], [56, 56]\n",
        "        \"\"\"Roll the given tensor back up to spatial order assuming it's from the given block.\"\"\"\n",
        "        # schedule, size = self.schedule[block_idx]\n",
        "        print(schedule, size)\n",
        "        depth = sum([i<1 for i in stage_ends[:q_pool]])\n",
        "        print(depth)\n",
        "        for _ in range(depth):\n",
        "                if len(schedule) > 0:\n",
        "                    size = [n//s for n, s in zip(size, schedule[0])]\n",
        "                schedule = schedule[1:]\n",
        "\n",
        "        B, N, C = x.shape\n",
        "        D = len(size)\n",
        "        cur_mu_shape = [1] * D\n",
        "        for strides in schedule:\n",
        "            x = x.view(B, *strides, N // math.prod(strides), *cur_mu_shape, C) # 2d: [B, Sy, Sx, N//(Sy*Sx), MUy, MUx, C]\n",
        "            L = len(x.shape)\n",
        "            permute = ([0, 1+D] + [val for tup in zip(range(1, 1+D), range(1+D+1, L-1)) for val in tup] + [L-1]) # [0, 1, 3, 2, 4, 5] / [0, 1, 4, 2, 5, 3, 6, 7]\n",
        "            x = x.permute(permute) # 2d: [B, Sy, Sx, N//(Sy*Sx), MUy, MUx, C] -> [B, N//(Sy*Sx), Sy, MUy, Sx, MUx, C]\n",
        "\n",
        "            for i in range(D):\n",
        "                cur_mu_shape[i] *= strides[i]\n",
        "            print('cur_mu_shape',cur_mu_shape)\n",
        "            x = x.reshape(B, -1, *cur_mu_shape, C) # [B, N//(Sy*Sx), *MU, C]\n",
        "            N = x.shape[1]\n",
        "\n",
        "        print('final cur_mu_shape',cur_mu_shape)\n",
        "        x = x.view(B, N, *cur_mu_shape, C) # 2d: [B, #MUy*#MUx, MUy, MUx, C]\n",
        "        # if mask != None: return x # 2d: [B, #MUs, MUy, MUx, C]\n",
        "        # x = undo_windowing(x, size, cur_mu_shape) # decoder [B, H, W, C] / [B, T, H, W, C]\n",
        "        return x\n",
        "\n",
        "# def Reroll(x, idx, size, schedule, stage_ends, q_pool): # [24, 256, 16], [56, 56], list[(2, 2), (2, 2), (2, 2)]\n",
        "# # def Reroll(x, size, schedule, stage_ends): # [24, 256, 16], list[(2, 2), (2, 2), (2, 2)], [56, 56]\n",
        "#     \"\"\"Roll the given tensor back up to spatial order assuming it's from the given block.\"\"\"\n",
        "#     # schedule, size = self.schedule[block_idx]\n",
        "#     # print(stage_ends)\n",
        "#     depth = sum([i<idx for i in stage_ends[:q_pool]])\n",
        "#     print(depth)\n",
        "#     for _ in range(depth):\n",
        "#             if len(schedule) > 0:\n",
        "#                 size = [n//s for n, s in zip(size, schedule[0])]\n",
        "#             schedule = schedule[1:]\n",
        "    # print(idx, schedule, size)\n",
        "def Reroll(x, size, schedule): # [24, 256, 16], [56, 56], list[(2, 2), (2, 2), (2, 2)]\n",
        "    print(schedule, size)\n",
        "    B, N, C = x.shape\n",
        "    D = len(size)\n",
        "    cur_mu_shape = [1] * D\n",
        "    for strides in schedule:\n",
        "        x = x.view(B, *strides, N // math.prod(strides), *cur_mu_shape, C) # 2d: [B, Sy, Sx, N//(Sy*Sx), MUy, MUx, C]\n",
        "        L = len(x.shape)\n",
        "        permute = ([0, 1+D] + [val for tup in zip(range(1, 1+D), range(1+D+1, L-1)) for val in tup] + [L-1]) # [0, 1, 3, 2, 4, 5] / [0, 1, 4, 2, 5, 3, 6, 7]\n",
        "        x = x.permute(permute) # 2d: [B, Sy, Sx, N//(Sy*Sx), MUy, MUx, C] -> [B, N//(Sy*Sx), Sy, MUy, Sx, MUx, C]\n",
        "\n",
        "        for i in range(D):\n",
        "            cur_mu_shape[i] *= strides[i]\n",
        "        print('cur_mu_shape',cur_mu_shape)\n",
        "        x = x.reshape(B, -1, *cur_mu_shape, C) # [B, N//(Sy*Sx), *MU, C]\n",
        "        N = x.shape[1]\n",
        "\n",
        "    print('final cur_mu_shape',cur_mu_shape)\n",
        "    x = x.view(B, N, *cur_mu_shape, C) # 2d: [B, #MUy*#MUx, MUy, MUx, C]\n",
        "    # if mask != None: return x # 2d: [B, #MUs, MUy, MUx, C]\n",
        "    # x = undo_windowing(x, size, cur_mu_shape) # decoder [B, H, W, C] / [B, T, H, W, C]\n",
        "    return x\n",
        "\n",
        "stages=(2, 2, 3, 2)\n",
        "input_size = (224, 224)\n",
        "q_stride = (2, 2)\n",
        "patch_stride = (4, 4)\n",
        "q_pool = 3\n",
        "x = torch.rand(24, 16, 56, 56)\n",
        "x = x.flatten(2).transpose(1, 2) # [b,t,c]\n",
        "stage_ends = [sum(stages[:i]) - 1 for i in range(1, len(stages) + 1)] # [1, 3, 6, 8]\n",
        "tokens_spatial_shape = [i//s for i, s in zip(input_size, patch_stride)] # list [224/4, 224/4] = [56, 56]\n",
        "reroll = Reroll(x, 1, tokens_spatial_shape, [q_stride] * len(stage_ends[:-1]), stage_ends, q_pool)\n",
        "# reroll = Reroll(x, tokens_spatial_shape, [q_stride] * len(stage_ends[:-1]), stage_ends)\n",
        "\n",
        "\n",
        "# torch.Size([24, 256, 16]) 1 torch.Size([24, 49]) # [24, 4, 8, 8, 16] 1: ([(2, 2), (2, 2), (2, 2)], [56, 56])\n",
        "# torch.Size([24, 64, 32]) 3 torch.Size([24, 49]) # [24, 4, 4, 4, 32] 3: ([(2, 2), (2, 2)], [28, 28])\n",
        "# torch.Size([24, 16, 64]) 6 torch.Size([24, 49]) # [24, 4, 2, 2, 64] 6: ([(2, 2)], [14, 14])\n",
        "# torch.Size([24, 4, 128]) 8 torch.Size([24, 49]) # [24, 4, 1, 1, 128]  8: ([], [7, 7])\n",
        "x, i = torch.rand(24, 256, 16), 1\n",
        "# x, i = torch.rand(24, 64, 32), 3\n",
        "# x, i = torch.rand(24, 16, 64), 6\n",
        "# x, i = torch.rand(24, 4, 128), 8\n",
        "# mask = torch.rand(24, 49)\n",
        "# x = reroll(x, i, mask=mask)\n",
        "\n",
        "# i\n",
        "# stage_ends.index(1)\n",
        "\n",
        "# x = reroll(x, i)\n",
        "# print(x.shape)\n",
        "# print(x)\n",
        "\n",
        "def randpatch(length, B, gamma=0.9): # num patches of seq, mask patch size, masking ratio\n",
        "    g = torch.normal(gamma, std=.1, size=(1,)).clamp(.5,.9)\n",
        "    # g = gamma\n",
        "    noise = torch.rand(B, length, device=device)\n",
        "    ids = torch.argsort(noise, dim=1)  # ascend: small is keep, large is remove\n",
        "\n",
        "    mask = torch.zeros((B, length), dtype=bool, device=device)\n",
        "    mask[:, :int(length*(1-gamma))] = True\n",
        "    mask = torch.gather(mask, dim=1, index=ids)\n",
        "\n",
        "    # idx = torch.randperm(length)[:int(length*gamma)]\n",
        "    # mask = torch.zeros((B, length), dtype=bool)\n",
        "    # mask[:,idx] = True\n",
        "    return mask\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "ST8CkJsY_3dn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title hiera_mae.py\n",
        "# https://github.com/facebookresearch/hiera/blob/main/hiera/hiera_mae.py\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def apply_fusion_head(head, x): # [B, #MUs, My, Mx, C]\n",
        "    if isinstance(head, nn.Identity): return x\n",
        "    B, num_mask_units = x.shape[0:2]\n",
        "    permute = [0] + [len(x.shape) - 2] + list(range(1, len(x.shape) - 2))\n",
        "    x = head(x.reshape(B * num_mask_units, *x.shape[2:]).permute(permute)) # [B, #MUs, My, Mx, C] -> head([B * #MUs, C, My, Mx])\n",
        "    permute = [0] + list(range(2, len(x.shape))) + [1] # [0, *2345.., 1]\n",
        "    x = x.permute(permute).reshape(B, num_mask_units, *x.shape[2:], x.shape[1]) # [B * #MUs, C', My', Mx'] -> [B, #MUs, My', Mx', C']\n",
        "    return x\n",
        "\n",
        "class MaskedAutoencoderHiera(Hiera):\n",
        "    def __init__(self, in_chans = 3, patch_stride = (4, 4),\n",
        "        decoder_embed_dim = 512, decoder_depth = 8, decoder_num_heads = 16, **kwdargs,):\n",
        "        super().__init__(in_chans=in_chans, patch_stride=patch_stride, **kwdargs)\n",
        "        # del self.head\n",
        "        encoder_dim_out = self.blocks[-1].dim_out\n",
        "        self.encoder_norm = nn.LayerNorm(encoder_dim_out)\n",
        "        self.mask_unit_spatial_shape_final = [i//s**self.q_pool for i, s in zip(self.mask_unit_size, self.q_stride)]\n",
        "        self.tokens_spatial_shape_final = [i//s**self.q_pool for i, s in zip(self.tokens_spatial_shape, self.q_stride)]\n",
        "        # --------------------------------------------------------------------------\n",
        "        # Multi-scale fusion heads\n",
        "        curr_mu_size = self.mask_unit_size\n",
        "        self.multi_scale_fusion_heads = nn.ModuleList()\n",
        "        for i in self.stage_ends[: self.q_pool]:  # resolution constant after q_pool\n",
        "            kernel = [i//s for i, s in zip(curr_mu_size, self.mask_unit_spatial_shape_final)]\n",
        "            curr_mu_size = [i//s for i, s in zip(curr_mu_size, self.q_stride)]\n",
        "            self.multi_scale_fusion_heads.append(\n",
        "                conv_nd(len(self.q_stride), self.blocks[i].dim_out, encoder_dim_out, kernel_size=kernel, stride=kernel)\n",
        "            )\n",
        "        self.multi_scale_fusion_heads.append(nn.Identity())  # final stage, no transform\n",
        "        # print('multi_scale_fusion_heads', self.multi_scale_fusion_heads)\n",
        "\n",
        "        # --------------------------------------------------------------------------\n",
        "        # MAE decoder specifics\n",
        "        self.decoder_embed = nn.Linear(encoder_dim_out, decoder_embed_dim)\n",
        "        self.mask_token = nn.Parameter(torch.randn(1, 1, decoder_embed_dim)*0.02)\n",
        "        self.decoder_pos_embed = nn.Parameter(torch.randn(1, math.prod(self.tokens_spatial_shape_final), decoder_embed_dim)*0.02)\n",
        "        self.decoder_blocks = nn.ModuleList([HieraBlock(dim=decoder_embed_dim, dim_out=decoder_embed_dim, heads=decoder_num_heads) for i in range(decoder_depth)])\n",
        "        self.pred_stride = patch_stride[-1] * (self.q_stride[-1] ** self.q_pool)\n",
        "        self.decoder_out = nn.Sequential(nn.LayerNorm(decoder_embed_dim), nn.Linear(decoder_embed_dim, (self.pred_stride**min(2, len(self.q_stride)))*in_chans))\n",
        "\n",
        "    def get_pixel_label_2d(self, input_img, mask, norm=True): # mask: bool, True -> mask\n",
        "        input_img = input_img.permute(0, 2, 3, 1)\n",
        "        size = self.pred_stride\n",
        "        label = input_img.unfold(1, size, size).unfold(2, size, size)\n",
        "        label = label.flatten(1, 2).flatten(2)\n",
        "        label = label[mask]\n",
        "        if norm: label = F.layer_norm(label, label.shape[-1:]) # l = F.layer_norm(label, (label.size(-1),))\n",
        "        return label\n",
        "\n",
        "    def get_pixel_label_3d(self, input_vid, mask, norm = True): # [b,c,f,h,w]?\n",
        "        # mask (boolean tensor): True must correspond to *masked*\n",
        "        # We use time strided loss, only take the first frame from each token\n",
        "        input_vid = input_vid[:, :, ::self.patch_stride[0], :, :]\n",
        "        size = self.pred_stride\n",
        "        label = input_vid.unfold(3, size, size).unfold(4, size, size)\n",
        "        label = label.permute(0, 2, 3, 4, 5, 6, 1)  # Different from 2d, mistake during training lol\n",
        "        label = label.flatten(1, 3).flatten(2)\n",
        "        label = label[mask]\n",
        "        if norm: label = F.layer_norm(label, label.shape[-1:])\n",
        "        return label\n",
        "\n",
        "    def forward_encoder(self, x, mask=None):\n",
        "        # Get multi-scale representations from encoder\n",
        "        # _, intermediates = super().forward(x, mask, return_intermediates=True)\n",
        "        x = super().forward(x, mask, return_intermediates=True)\n",
        "        # # Resolution unchanged after q_pool stages, so skip those features\n",
        "        # print('fwd enc', self.q_pool, len(intermediates), [i.shape for i in intermediates])\n",
        "        # intermediates = intermediates[:self.q_pool] + intermediates[-1:]\n",
        "\n",
        "        # # Multi-scale fusion\n",
        "        # x = sum([apply_fusion_head(head, interm_x) for head, interm_x in zip(self.multi_scale_fusion_heads, intermediates)])\n",
        "        # x = self.encoder_norm(x)\n",
        "        return x#, mask\n",
        "\n",
        "    def forward_decoder(self, x, mask):\n",
        "        x = self.decoder_embed(x)\n",
        "\n",
        "        # Combine visible and mask tokens\n",
        "\n",
        "        # x: [B, #MUs, *mask_unit_spatial_shape_final, encoder_dim_out]\n",
        "        # mask: [B, #MUs_all]\n",
        "        x_dec = torch.zeros(*mask.shape, *x.shape[2:], device=x.device, dtype=x.dtype)\n",
        "        mask_tokens = self.mask_token.view((1,) * (len(mask.shape) + len(x.shape[2:-1])) + (-1,))\n",
        "        mask = mask.reshape(mask.shape + (1,) * len(x.shape[2:]))\n",
        "        mask = mask.expand((-1,) * 2 + x.shape[2:]).bool()\n",
        "        x_dec[mask] = x.flatten()\n",
        "        x_dec = ~mask * mask_tokens + mask * x_dec\n",
        "\n",
        "        # Get back spatial order\n",
        "        x = undo_windowing(x_dec, self.tokens_spatial_shape_final, self.mask_unit_spatial_shape_final)\n",
        "        mask = undo_windowing(mask[..., 0:1], self.tokens_spatial_shape_final, self.mask_unit_spatial_shape_final)\n",
        "\n",
        "        # Flatten\n",
        "        x = x.reshape(x.shape[0], -1, x.shape[-1])\n",
        "        mask = mask.view(x.shape[0], -1)\n",
        "\n",
        "        x = x + self.decoder_pos_embed\n",
        "        for blk in self.decoder_blocks:\n",
        "            x = blk(x)\n",
        "        x = self.decoder_out(x)\n",
        "        return x, mask\n",
        "\n",
        "    def forward_loss(self, x, pred, mask): # [b,c,(f,)h,w], [B * num_pred_tokens, num_pixels_in_pred_patch * in_chans], 1->mask\n",
        "        if len(self.q_stride) == 2: label = self.get_pixel_label_2d(x, mask) # [B * num_pred_tokens, num_pixels_in_pred_patch * in_chans]\n",
        "        elif len(self.q_stride) == 3: label = self.get_pixel_label_3d(x, mask)\n",
        "        pred = pred[mask]\n",
        "        loss = (pred - label) ** 2\n",
        "        return loss.mean(), pred, label\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        latent, mask = self.forward_encoder(x, mask=mask)\n",
        "        pred, pred_mask = self.forward_decoder(latent, mask)  # pred_mask is mask at resolution of *prediction*\n",
        "        # Toggle mask, to generate labels for *masked* tokens\n",
        "        return *self.forward_loss(x, pred, ~pred_mask), mask\n",
        "\n",
        "\n",
        "model = MaskedAutoencoderHiera(embed_dim=16, num_heads=1, stages=(1, 2, 2, 2), q_pool=2)\n",
        "mask_ratio=.8\n",
        "# x = torch.randn((1, 3, 224, 224))\n",
        "mask = randpatch(math.prod(model.mask_spatial_shape), x.shape[0], mask_ratio)  # [B, #MUs_all]\n",
        "# # out = model(x, mask_ratio=0.6, mask=mask)\n",
        "out = model(x, mask=mask)\n",
        "# out = model(x)\n",
        "# print(out)\n",
        "# print(len(out))\n",
        "# for o in out:\n",
        "#     print(o.shape)\n",
        "\n",
        "# [], [116, 768], [116, 768], [1, 49]\n",
        "\n",
        "# # Image Models\n",
        "# def mae_hiera_tiny_224(**kwargs):\n",
        "#     return MaskedAutoencoderHiera(embed_dim=96, num_heads=1, stages=(1, 2, 7, 2), q_pool=2, **kwargs,\n",
        "# def mae_hiera_small_224(embed_dim=96, num_heads=1, stages=(1, 2, 11, 2), q_pool=2, **kwargs,\n",
        "# def mae_hiera_base_224(embed_dim=96, num_heads=1, stages=(2, 3, 16, 3), q_pool=2, **kwargs,\n",
        "# def mae_hiera_base_plus_224(embed_dim=112, num_heads=2, stages=(2, 3, 16, 3), q_pool=2, **kwargs,\n",
        "# def mae_hiera_large_224(embed_dim=144, num_heads=2, stages=(2, 6, 36, 4), q_pool=2, **kwargs,\n",
        "# def mae_hiera_huge_224(embed_dim=256, num_heads=4, stages=(2, 6, 36, 4), q_pool=2, **kwargs,\n",
        "\n",
        "# # Video Models\n",
        "# def mae_hiera_base_16x224(num_classes = 400, **kwdargs):\n",
        "#     return MaskedAutoencoderHiera(num_classes=num_classes,  # K400 has 400 classes\n",
        "#         input_size=(16, 224, 224), q_stride=(1, 2, 2), mask_unit_size=(1, 8, 8),\n",
        "#         patch_kernel=(3, 7, 7), patch_stride=(2, 4, 4), patch_padding=(1, 3, 3),\n",
        "#         sep_pos_embed=True, q_pool=2, **kwdargs)\n",
        "# def mae_hiera_base_plus_16x224(embed_dim=112, num_heads=2, stages=(2, 3, 16, 3), **kwdargs\n",
        "# def mae_hiera_large_16x224(embed_dim=144, num_heads=2, stages=(2, 6, 36, 4), **kwdargs\n",
        "# def mae_hiera_huge_16x224(embed_dim=256, num_heads=4, stages=(2, 6, 36, 4), **kwdargs\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "1tpPDU-6L161"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title maxpool path need shuffle kv everytime\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from timm.models.layers import DropPath, Mlp\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "def conv_nd(n, *args, **kwargs): return [nn.Identity, nn.Conv1d, nn.Conv2d, nn.Conv3d][n](*args, **kwargs)\n",
        "def maxpool_nd(n, *args, **kwargs): return [nn.Identity, nn.MaxPool1d, nn.MaxPool2d, nn.MaxPool3d][n](*args, **kwargs)\n",
        "\n",
        "class MaskUnitAttention(nn.Module):\n",
        "    def __init__(self, in_dim, d_model, n_heads, q_stride=1, window_size=0, use_mask_unit_attn=False):\n",
        "        super().__init__()\n",
        "        self.in_dim, self.d_model = in_dim, d_model\n",
        "        self.n_heads, self.d_head = n_heads, d_model // n_heads\n",
        "        self.scale = self.d_head**-.5\n",
        "        self.qkv = nn.Linear(in_dim, 3*d_model)\n",
        "        # self.qkv = conv_nd(in_dim, 3*d_model, 1)\n",
        "        self.lin = nn.Linear(d_model, d_model)\n",
        "        self.q_stride = q_stride # If greater than 1, pool q with this stride. The stride should be flattened (e.g., 2x2 = 4).\n",
        "        # self.window_size = window_size # The current (flattened) size of a mask unit *after* pooling (if any).\n",
        "        # self.use_mask_unit_attn = use_mask_unit_attn # Use Mask Unit or Global Attention.\n",
        "        # # within each window, pool exery q_stride\n",
        "        self.q_pool = maxpool_nd(3, q_stride, stride=q_stride)\n",
        "\n",
        "\n",
        "    def forward(self, x): # [b,*sp,c]\n",
        "        # B, N, _ = x.shape\n",
        "        # print('attn N,q,win', N,self.q_stride, self.window_size, self.use_mask_unit_attn)\n",
        "        # num_windows = ((N // (self.q_stride * self.window_size)) if self.use_mask_unit_attn else 1) # for Mask Unit else Global Attention\n",
        "        # [b,*sp,3d]\n",
        "        # q, k, v = self.qkv(x).reshape(B, -1, num_windows, 3, self.n_heads, self.d_head).permute(3, 0, 4, 2, 1, 5) # [b,t,3*d_model] -> [b, t/num_windows = q_stride*window_size, num_windows, 3, n_heads, d_head] -> [3, b, n_heads, num_windows, q_stride*window_size, d_head]\n",
        "        # self.qkv(x).flatten(1,-2).unflatten(-1, (self.heads,-1)).chunk(3, dim=-1) # [b,sp,n_heads,d_head]\n",
        "        q,k,v = self.qkv(x).chunk(3, dim=-1) # [b,*sp,c]\n",
        "        # pool = maxpool_nd(3, (3, 2, 2), stride=(2, 1, 2))\n",
        "        pool = maxpool_nd(2, (2, 2), stride=(2, 2))\n",
        "        L = x.dim()\n",
        "        q = pool(q.permute(0,L-1,*list(range(1,L-1)))).permute(0,*list(range(2,L)),1) # q pooling # [b,*sp,c]->[b,c,*sp]->[b,*sp,c]\n",
        "        q, k, v = map(lambda t: t.flatten(1,-2).unflatten(-1, (self.n_heads,-1)).transpose(1,2), (q,k,v)) # [b, n_heads, t(/pool), d_head]\n",
        "        print(q.shape, k.shape, v.shape)\n",
        "\n",
        "        # want k,v [b, n_heads, num_windows, q_stride*window_size, d_head]\n",
        "        # q [b, n_heads, num_windows, window_size, d_head]\n",
        "\n",
        "        # want k,v [b, n_heads, num_windows, window_size, d_head]\n",
        "        # q [b, n_heads, num_windows, window_size/pool, d_head]\n",
        "\n",
        "        # if self.q_stride > 1: q = q.view(B, self.n_heads, num_windows, self.q_stride, -1, self.d_head).max(dim=3).values # q pooling # [b, n_heads, num_windows, q_stride,window_size, d_head]-> [b, n_heads, num_windows, window_size, d_head]\n",
        "        # q = self.q_pool(q)\n",
        "        q, k = q.softmax(dim=-1)*self.scale, k.softmax(dim=-2) # [b, n_heads, t(/pool), d_head]\n",
        "        context = k.transpose(-2,-1) @ v # [b, n_heads, d_head, d_head]\n",
        "        x = q @ context # [b, n_heads, t(/pool), d_head]\n",
        "\n",
        "        # # x = F.scaled_dot_product_attention(q, k, v)\n",
        "        # attn = (q * self.scale) @ k.transpose(-1, -2) # [b, n_heads, num_windows, window_size, d_head] @ [b, n_heads, num_windows, d_head, q_stride*window_size] = [b, n_heads, num_windows, window_size, q_stride*window_size]\n",
        "        # x = attn.softmax(dim=-1) @ v # [b, n_heads, num_windows, window_size, d_head]\n",
        "        print(x.shape)\n",
        "        # x = x.transpose(1, 3).reshape(B, -1, self.d_model)\n",
        "        x = x.transpose(1, 2).reshape(x.shape[0], -1, self.d_model)\n",
        "        x = self.lin(x)\n",
        "        return x\n",
        "\n",
        "in_dim=3\n",
        "model = MaskUnitAttention(in_dim, d_model=16, n_heads=4, q_stride=1)\n",
        "x=torch.randn(2,4,4,3)\n",
        "out = model(x)\n",
        "print(out.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8d0ca2b-1927-48b1-b005-38722d61d45c",
        "cellView": "form",
        "id": "OzymxbTLSgIv"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 4, 4, 4]) torch.Size([2, 4, 16, 4]) torch.Size([2, 4, 16, 4])\n",
            "torch.Size([2, 4, 4, 4])\n",
            "torch.Size([2, 4, 16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title maxpool path need shuffle kv everytime\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from timm.models.layers import DropPath, Mlp\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "class MaskUnitAttention(nn.Module):\n",
        "    def __init__(self, in_dim, d_model, n_heads, q_stride=1, window_size=0, use_mask_unit_attn=False):\n",
        "        super().__init__()\n",
        "        self.in_dim, self.d_model = in_dim, d_model\n",
        "        self.n_heads, self.d_head = n_heads, d_model // n_heads\n",
        "        self.scale = self.d_head**-.5\n",
        "        self.qkv = nn.Linear(in_dim, 3*d_model)\n",
        "        self.lin = nn.Linear(d_model, d_model)\n",
        "        self.q_stride = q_stride # If greater than 1, pool q with this stride. The stride should be flattened (e.g., 2x2 = 4).\n",
        "        # self.window_size = window_size # The current (flattened) size of a mask unit *after* pooling (if any).\n",
        "        # self.use_mask_unit_attn = use_mask_unit_attn # Use Mask Unit or Global Attention.\n",
        "        # # within each window, pool exery q_stride\n",
        "        self.q_pool = maxpool_nd(3, q_stride, stride=q_stride)\n",
        "\n",
        "\n",
        "    def forward(self, x): # [b,*sp,c]\n",
        "        # B, N, _ = x.shape\n",
        "        # print('attn N,q,win', N,self.q_stride, self.window_size, self.use_mask_unit_attn)\n",
        "        # num_windows = ((N // (self.q_stride * self.window_size)) if self.use_mask_unit_attn else 1) # for Mask Unit else Global Attention\n",
        "        # [b,*sp,3d]\n",
        "        # q, k, v = self.qkv(x).reshape(B, -1, num_windows, 3, self.n_heads, self.d_head).permute(3, 0, 4, 2, 1, 5) # [b,t,3*d_model] -> [b, t/num_windows = q_stride*window_size, num_windows, 3, n_heads, d_head] -> [3, b, n_heads, num_windows, q_stride*window_size, d_head]\n",
        "        # self.qkv(x).flatten(1,-2).unflatten(-1, (self.heads,-1)).chunk(3, dim=-1) # [b,sp,n_heads,d_head]\n",
        "        L = x.dim()\n",
        "        q,k,v = self.qkv(x).chunk(3, dim=-1) # [b,c,*sp]\n",
        "        # pool = maxpool_nd(3, (3, 2, 2), stride=(2, 1, 2))\n",
        "        pool = maxpool_nd(2, (2, 2), stride=(2, 2))\n",
        "        print(0,L-1,*list(range(1,L-1)))\n",
        "        print(0,*list(range(2,L)),1)\n",
        "        q = pool(q.permute(0,L-1,*list(range(1,L-1)))).permute(0,*list(range(2,L)),1)\n",
        "        q, k, v = q.flatten(1,-2).unflatten(-1, (self.n_heads,-1)).transpose(1,2), k.flatten(1,-2).unflatten(-1, (self.n_heads,-1)).transpose(1,2), v.flatten(1,-2).unflatten(-1, (self.n_heads,-1)).transpose(1,2)\n",
        "        print(q.shape, k.shape, v.shape)\n",
        "\n",
        "        # want k,v [b, n_heads, num_windows, q_stride*window_size, d_head]\n",
        "        # q [b, n_heads, num_windows, window_size, d_head]\n",
        "\n",
        "        # if self.q_stride > 1: q = q.view(B, self.n_heads, num_windows, self.q_stride, -1, self.d_head).max(dim=3).values # q pooling # [b, n_heads, num_windows, q_stride,window_size, d_head]-> [b, n_heads, num_windows, window_size, d_head]\n",
        "        # q = self.q_pool(q)\n",
        "        q, k = q.softmax(dim=-1)*self.scale, k.softmax(dim=-2) # [b, n_heads, t(/pool), d_head]\n",
        "        context = k.transpose(-2,-1) @ v # [b, n_heads, d_head, d_head]\n",
        "        x = q @ context # [b, n_heads, t(/pool), d_head]\n",
        "\n",
        "        # # x = F.scaled_dot_product_attention(q, k, v)\n",
        "        # attn = (q * self.scale) @ k.transpose(-1, -2) # [b, n_heads, num_windows, window_size, d_head] @ [b, n_heads, num_windows, d_head, q_stride*window_size] = [b, n_heads, num_windows, window_size, q_stride*window_size]\n",
        "        # x = attn.softmax(dim=-1) @ v # [b, n_heads, num_windows, window_size, d_head]\n",
        "        print(x.shape)\n",
        "        # x = x.transpose(1, 3).reshape(B, -1, self.d_model)\n",
        "        x = x.transpose(1, 2).reshape(x.shape[0], -1, self.d_model)\n",
        "        x = self.lin(x)\n",
        "        return x\n",
        "\n",
        "in_dim=3\n",
        "model = MaskUnitAttention(in_dim, d_model=16, n_heads=4, q_stride=1)\n",
        "x=torch.randn(2,4,4,3)\n",
        "out = model(x)\n",
        "print(out.shape)\n"
      ],
      "metadata": {
        "id": "h7PzNQBYrGhL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b8bc4d2-6c0c-4a2c-c4a2-34f62c3eca5a",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 3 1 2\n",
            "0 2 3 1\n",
            "torch.Size([2, 4, 4, 4]) torch.Size([2, 4, 16, 4]) torch.Size([2, 4, 16, 4])\n",
            "torch.Size([2, 4, 4, 4])\n",
            "torch.Size([2, 4, 16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title test shuffle unshuffle\n",
        "b,c,h,w = 2,3,8,8\n",
        "f=4\n",
        "# x = torch.randn(b,c,h,w)\n",
        "# window_shape=(2,2)\n",
        "x = torch.randn(b,c,f,h,w)\n",
        "inn=x\n",
        "window_shape=(2,2,2)\n",
        "# print(x.shape[2:], window_shape)\n",
        "new_shape = list(x.shape[:2]) + [val for xx,win in zip(list(x.shape[2:]), window_shape) for val in [xx//win,win]]\n",
        "print(new_shape)\n",
        "x = x.reshape(new_shape)\n",
        "L = len(new_shape)\n",
        "permute = ([0] + list(range(3, L, 2)) + [1] + list(range(2, L - 1, 2))) # [0,3,5,1,2,4] / [0,3,5,7,1,2,4,6]\n",
        "print(permute)\n",
        "print(x.shape)\n",
        "x = x.permute(permute).flatten(end_dim=L//2-1) # [b*win*win, c, h/win, w/win]\n",
        "print(x.shape)\n",
        "\n",
        "print('#######')\n",
        "\n",
        "# new_shape = [-1] + window_shape +\n",
        "# print(new_shape)\n",
        "# x = x.reshape(new_shape)\n",
        "out_shape = [x*w for x,w in zip(x.shape[2:], window_shape)] # [h/win*wim, w/win*wim]\n",
        "x = x.unflatten(0, (-1, *window_shape))\n",
        "print(x.shape)\n",
        "D=x.dim()\n",
        "# [0,4,5,1,6,2,7,3]\n",
        "permute = [0,D//2] + [val for tup in zip(range(1+D//2, D+1), range(1, D//2)) for val in tup]\n",
        "print(permute)\n",
        "x = x.permute(permute)\n",
        "print(x.shape)\n",
        "print(out_shape)\n",
        "x = x.reshape(*x.shape[:2], *out_shape)\n",
        "print(x.shape)\n",
        "print((x==inn).all())\n",
        "\n",
        "\n",
        "def unshuffle(x, window_shape): # [b,c,h,w] -> [b*win*win, c, h/win, w/win]\n",
        "    new_shape = list(x.shape[:2]) + [val for xx,win in zip(list(x.shape[2:]), window_shape) for val in [xx//win,win]]\n",
        "    x = x.reshape(new_shape)\n",
        "    L = len(new_shape)\n",
        "    permute = ([0] + list(range(3, L, 2)) + [1] + list(range(2, L - 1, 2))) # [0,3,5,1,2,4] / [0,3,5,7,1,2,4,6]\n",
        "    x = x.permute(permute).flatten(end_dim=L//2-1) # [b*win*win, c, h/win, w/win]\n",
        "    return x\n",
        "\n",
        "def shuffle(x, window_shape): # [b*win*win, c, h/win, w/win] -> [b,c,h,w]\n",
        "    out_shape = [x*w for x,w in zip(x.shape[2:], window_shape)] # [h/win*wim, w/win*wim]\n",
        "    x = x.unflatten(0, (-1, *window_shape)) # [b,win,win, c, h/win, w/win]\n",
        "    D=x.dim()\n",
        "    permute = [0,D//2] + [val for tup in zip(range(1+D//2, D+1), range(1, D//2)) for val in tup]\n",
        "    x = x.permute(permute).reshape(*x.shape[:2], *out_shape)\n",
        "    return x\n",
        "\n",
        "\n",
        "# tokens_spatial_shape = [i//s for i, s in zip(tokens_spatial_shape, strides)] # [28, 28], [14, 14], [7, 7]\n",
        "# new_shape = [B] + [val for tup in zip(tokens_spatial_shape, strides) for val in tup] + [C]\n",
        "\n",
        "# x = x.view(new_shape) # 2d: [B, H/Sy, Sy, W/Sx, Sx, C] # [24, 28, 2, 28, 2, 16], [96, 14, 2, 14, 2, 16], [384, 7, 2, 7, 2, 16]\n",
        "# L = len(new_shape)\n",
        "# permute = ([0] + list(range(2, L - 1, 2)) + list(range(1, L - 1, 2)) + [L - 1]) # [0, 2, 4, 1, 3, 5] / [0, 2, 4, 6, 1, 3, 5, 7]\n",
        "# x = x.permute(permute).flatten(0, len(strides)) # 2d: [B, Sy, Sx, H/Sy, W/Sx, C] -> [B*Sy*Sx, H/Sy, W/Sx, C]\n",
        "\n",
        "# num_MUs = [s//mu for s, mu in zip(shape, mu_shape)] # [7,7]\n",
        "# x = x.view(B, *num_MUs, *mu_shape, C) # [B, #MUy*#MUx, MUy, MUx, C] -> [B, #MUy, #MUx, MUy, MUx, C]\n",
        "# D = len(shape)\n",
        "# # permute = ([0] + sum([list(p) for p in zip(range(1, 1 + D), range(1 + D, 1 + 2 * D))], []) + [len(x.shape) - 1]) # [0, 1, 3, 2, 4, 5]\n",
        "# permute = ([0] + [val for tup in zip(range(1, 1+D), range(1+D, 1+2*D)) for val in tup] + [len(x.shape)-1]) # [0, 1, 3, 2, 4, 5] / [0, 1, 4, 2, 5, 3, 6, 7]\n",
        "# x = x.permute(permute).reshape(B, *shape, C) # [B, #MUy, #MUx, MUy, MUx, C] -> [B, #MUy*MUy, #MUx*MUx, C]\n",
        "\n",
        "# y = torch.randn(batch, 3, 2,2, 128)\n",
        "# print(y.flatten(1,-2).mean(1).shape)\n",
        "# print(y.flatten(1,-2).shape)\n",
        "# print(y.squeeze(-3,-2).shape)\n",
        "\n",
        "class PixelShuffle3d(nn.Module):\n",
        "    def __init__(self, scale):\n",
        "        super().__init__()\n",
        "        self.scale = scale\n",
        "\n",
        "    def forward(self, input):\n",
        "        batch_size, channels, in_depth, in_height, in_width = input.size()\n",
        "        nOut = channels // self.scale ** 3\n",
        "        out_depth = in_depth * self.scale\n",
        "        out_height = in_height * self.scale\n",
        "        out_width = in_width * self.scale\n",
        "        input_view = input.contiguous().view(batch_size, nOut, self.scale, self.scale, self.scale, in_depth, in_height, in_width)\n",
        "        output = input_view.permute(0, 1, 5, 2, 6, 3, 7, 4).contiguous()\n",
        "        return output.view(batch_size, nOut, out_depth, out_height, out_width)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p125DyJ0fUCJ",
        "outputId": "5aa9e510-cc58-4223-82b3-0ccd65162f89",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2, 3, 2, 2, 4, 2, 4, 2]\n",
            "[0, 3, 5, 7, 1, 2, 4, 6]\n",
            "torch.Size([2, 3, 2, 2, 4, 2, 4, 2])\n",
            "torch.Size([16, 3, 2, 4, 4])\n",
            "#######\n",
            "torch.Size([2, 2, 2, 2, 3, 2, 4, 4])\n",
            "[0, 4, 5, 1, 6, 2, 7, 3]\n",
            "torch.Size([2, 3, 2, 2, 4, 2, 4, 2])\n",
            "[4, 8, 8]\n",
            "torch.Size([2, 3, 4, 8, 8])\n",
            "tensor(True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title facebookresearch hiera_utils.py\n",
        "# https://github.com/facebookresearch/hiera/blob/main/hiera/hiera_utils.py\n",
        "\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def conv_nd(n): return [nn.Identity, nn.Conv1d, nn.Conv2d, nn.Conv3d][n]\n",
        "\n",
        "def undo_windowing(x, shape, mu_shape): # x [B, #MUy*#MUx, MUy, MUx, C] # shape: desired spatial shape. 2d: [B, #MUy*MUy, #MUx*MUx, C] # mu_shape: current mask unit shape. 2d: [MUy, MUx]\n",
        "    D = len(shape)\n",
        "    B, C = x.shape[0], x.shape[-1]\n",
        "    num_MUs = [s//mu for s, mu in zip(shape, mu_shape)]\n",
        "    x = x.view(B, *num_MUs, *mu_shape, C) # [B, #MUy*#MUx, MUy, MUx, C] -> [B, #MUy, #MUx, MUy, MUx, C]\n",
        "    permute = ([0] + sum([list(p) for p in zip(range(1, 1 + D), range(1 + D, 1 + 2 * D))], []) + [len(x.shape) - 1])\n",
        "    x = x.permute(permute).reshape(B, *shape, C) # [B, #MUy, #MUx, MUy, MUx, C] -> [B, #MUy*MUy, #MUx*MUx, C]\n",
        "    return x # [B, #MUy*MUy, #MUx*MUx, C]\n",
        "\n",
        "\n",
        "class Unroll(nn.Module):\n",
        "    \"\"\"\n",
        "    Reorders the tokens such that patches are contiguous in memory.\n",
        "    E.g., given [B, (H, W), C] and stride of (Sy, Sx), this will re-order the tokens as\n",
        "                           [B, (Sy, Sx, H // Sy, W // Sx), C]\n",
        "\n",
        "    This allows operations like Max2d to be computed as x.view(B, Sx*Sy, -1, C).max(dim=1).\n",
        "    Not only is this faster, but it also makes it easy to support inputs of arbitrary\n",
        "    dimensions in addition to patch-wise sparsity.\n",
        "\n",
        "    Performing this operation multiple times in sequence puts entire windows as contiguous\n",
        "    in memory. For instance, if you applied the stride (2, 2) 3 times, entire windows of\n",
        "    size 8x8 would be contiguous in memory, allowing operations like mask unit attention\n",
        "    computed easily and efficiently, while also allowing max to be applied sequentially.\n",
        "\n",
        "    Note: This means that intermediate values of the model are not in HxW order, so they\n",
        "    need to be re-rolled if you want to use the intermediate values as a HxW feature map.\n",
        "    The last block of the network is fine though, since by then the strides are all consumed.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size, patch_stride, unroll_schedule): # (224, 224) (4, 4) [(2, 2), (2, 2), (2, 2)]\n",
        "        super().__init__()\n",
        "        # print('input_size, patch_stride, unroll_schedule', input_size, patch_stride, unroll_schedule)\n",
        "        self.size = [i//s for i, s in zip(input_size, patch_stride)] # [56, 56]\n",
        "        # print('self.size', self.size)\n",
        "        self.schedule = unroll_schedule\n",
        "\n",
        "    def forward(self, x): # [b,t,c]\n",
        "        B, _, C = shape = x.shape\n",
        "        cur_size = self.size\n",
        "        x = x.view(B, *cur_size, C)\n",
        "\n",
        "        for strides in self.schedule: # q_stride (2, 2)\n",
        "            cur_size = [i//s for i, s in zip(cur_size, strides)] # [28, 28], [14, 14], [7, 7]\n",
        "            new_shape = [B] + [val for tup in zip(cur_size, strides) for val in tup] + [C]\n",
        "            x = x.view(new_shape) # 2d: [B, H/Sy, Sy, W/Sx, Sx, C] # [24, 28, 2, 28, 2, 16], [96, 14, 2, 14, 2, 16], [384, 7, 2, 7, 2, 16]\n",
        "\n",
        "            L = len(new_shape)\n",
        "            permute = ([0] + list(range(2, L - 1, 2)) + list(range(1, L - 1, 2)) + [L - 1]) # [0, 2, 4, 1, 3, 5] / [0, 2, 4, 6, 1, 3, 5, 7]\n",
        "            x = x.permute(permute).flatten(0, len(strides)) # 2d: [B, Sy, Sx, H/Sy, W/Sx, C] -> [B*Sy*Sx, H/Sy, W/Sx, C]\n",
        "            B *= math.prod(strides)\n",
        "        # x = x.reshape(-1, math.prod(self.size), C) # 2d: [B*Sy*Sx, H/Sy* W/Sx, C] # [24, 3136, 16]\n",
        "        x = x.reshape(*shape) # 2d: [B*Sy*Sx, H/Sy* W/Sx, C] # [24, 3136, 16]\n",
        "        return x # then x.max(1) == MaxPoolNd\n",
        "\n",
        "stages=(2, 2, 3, 2)\n",
        "input_size = (224, 224)\n",
        "q_stride = (2, 2)\n",
        "patch_stride = (4, 4)\n",
        "stage_ends = [sum(stages[:i]) - 1 for i in range(1, len(stages) + 1)]\n",
        "unroll = Unroll(input_size, patch_stride, [q_stride] * len(stage_ends[:-1]))\n",
        "\n",
        "\n",
        "# [B, (H, W), C] -> [B, (Sy, Sx, H // Sy, W // Sx), C]; stride (Sy, Sx)\n",
        "def Unroll(x, tokens_spatial_shape, unroll_schedule): # [56, 56] [(2, 2), (2, 2), (2, 2)]\n",
        "    B, _, C = shape = x.shape\n",
        "    x = x.view(B, *tokens_spatial_shape, C)\n",
        "    for strides in unroll_schedule: # q_stride (2, 2)\n",
        "        tokens_spatial_shape = [i//s for i, s in zip(tokens_spatial_shape, strides)] # [28, 28], [14, 14], [7, 7]\n",
        "        new_shape = [B] + [val for tup in zip(tokens_spatial_shape, strides) for val in tup] + [C]\n",
        "        x = x.view(new_shape) # 2d: [B, H/Sy, Sy, W/Sx, Sx, C] # [24, 28, 2, 28, 2, 16], [96, 14, 2, 14, 2, 16], [384, 7, 2, 7, 2, 16]\n",
        "        L = len(new_shape)\n",
        "        permute = ([0] + list(range(2, L - 1, 2)) + list(range(1, L - 1, 2)) + [L - 1]) # [0, 2, 4, 1, 3, 5] / [0, 2, 4, 6, 1, 3, 5, 7]\n",
        "        x = x.permute(permute).flatten(0, len(strides)) # 2d: [B, Sy, Sx, H/Sy, W/Sx, C] -> [B*Sy*Sx, H/Sy, W/Sx, C]\n",
        "        B *= math.prod(strides)\n",
        "    x = x.reshape(*shape) # 2d: [B*Sy*Sx, H/Sy* W/Sx, C] # [24, 3136, 16]\n",
        "    return x # then x.max(1) == MaxPoolNd\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "x = torch.rand(24, 16, 56, 56)\n",
        "x = x.flatten(2).transpose(1, 2) # [b,t,c]\n",
        "o = unroll(x) # [B, (H, W), C] -> [B, (Sy, Sx, H // Sy, W // Sx), C] ; stride of (Sy, Sx)\n",
        "tokens_spatial_shape = [i//s for i, s in zip(input_size, patch_stride)] # list [224/4, 224/4] = [56, 56]\n",
        "# x = Unroll(x, input_size, patch_stride, [q_stride]*len(stage_ends[:-1]))\n",
        "l = Unroll(x, tokens_spatial_shape, [q_stride]*len(stage_ends[:-1]))\n",
        "# print(x.shape)\n",
        "print(o)\n",
        "print(l)\n",
        "\n",
        "class Reroll(nn.Module):\n",
        "    \"\"\"Undos the \"unroll\" operation so that you can use intermediate features.\"\"\"\n",
        "    def __init__(self, input_size, patch_stride, unroll_schedule, stage_ends, q_pool):\n",
        "        super().__init__()\n",
        "        self.size = [i//s for i, s in zip(input_size, patch_stride)]\n",
        "\n",
        "        # The first stage has to reverse everything, The next stage has to reverse all but the first unroll, etc.\n",
        "        self.schedule = {}\n",
        "        size = self.size\n",
        "        for i in range(stage_ends[-1] + 1):\n",
        "            self.schedule[i] = unroll_schedule, size\n",
        "            # schedule unchanged if no pooling at a stage end\n",
        "            if i in stage_ends[:q_pool]:\n",
        "                if len(unroll_schedule) > 0:\n",
        "                    size = [n//s for n, s in zip(size, unroll_schedule[0])]\n",
        "                unroll_schedule = unroll_schedule[1:]\n",
        "\n",
        "    def forward(self, x, block_idx, mask=None):\n",
        "        \"\"\"Roll the given tensor back up to spatial order assuming it's from the given block.\"\"\"\n",
        "        schedule, size = self.schedule[block_idx]\n",
        "        B, N, C = x.shape\n",
        "\n",
        "        D = len(size)\n",
        "        cur_mu_shape = [1] * D\n",
        "\n",
        "        for strides in schedule:\n",
        "            x = x.view(B, *strides, N // math.prod(strides), *cur_mu_shape, C) # 2d: [B, Sy, Sx, N//(Sy*Sx), MUy, MUx, C]\n",
        "\n",
        "            L = len(x.shape)\n",
        "            permute = ([0, 1 + D] + sum([list(p) for p in zip(range(1, 1 + D), range(1 + D + 1, L - 1))], []) + [L - 1])\n",
        "            x = x.permute(permute) # 2d: [B, Sy, Sx, N//(Sy*Sx), MUy, MUx, C] -> [B, N//(Sy*Sx), Sy, MUy, Sx, MUx, C]\n",
        "\n",
        "            for i in range(D):\n",
        "                cur_mu_shape[i] *= strides[i]\n",
        "            x = x.reshape(B, -1, *cur_mu_shape, C) # [B, N//(Sy*Sx), *MU, C]\n",
        "            N = x.shape[1]\n",
        "\n",
        "        x = x.view(B, N, *cur_mu_shape, C) # 2d: [B, #MUy*#MUx, MUy, MUx, C]\n",
        "        if mask is not None: return x # 2d: [B, #MUs, MUy, MUx, C]\n",
        "        x = undo_windowing(x, size, cur_mu_shape) # [B, H, W, C] / [B, T, H, W, C]\n",
        "        return x\n",
        "\n",
        "\n",
        "def randpatch(length, B, gamma=0.9): # num patches of seq, mask patch size, masking ratio\n",
        "    g = torch.normal(gamma, std=.1, size=(1,)).clamp(.5,.9)\n",
        "    # g = gamma\n",
        "    noise = torch.rand(B, length, device=device)\n",
        "    ids = torch.argsort(noise, dim=1)  # ascend: small is keep, large is remove\n",
        "\n",
        "    mask = torch.zeros((B, length), dtype=bool, device=device)\n",
        "    mask[:, :int(length*(1-gamma))] = True\n",
        "    mask = torch.gather(mask, dim=1, index=ids)\n",
        "\n",
        "    # idx = torch.randperm(length)[:int(length*gamma)]\n",
        "    # mask = torch.zeros((B, length), dtype=bool)\n",
        "    # mask[:,idx] = True\n",
        "    return mask\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "77AZfrKgVURd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title facebookresearch hiera.py\n",
        "# https://github.com/facebookresearch/hiera/blob/main/hiera/hiera.py\n",
        "# Hiera: A Hierarchical Vision Transformer without the Bells-and-Whistles https://arxiv.org/abs/2306.00989/\n",
        "\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from timm.models.layers import DropPath, Mlp\n",
        "\n",
        "\n",
        "class MaskUnitAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Computes either Mask Unit or Global Attention. Also is able to perform q pooling.\n",
        "    Note: this assumes the tokens have already been flattened and unrolled into mask units.\n",
        "    See `Unroll` for more details.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_dim, d_model, n_heads, q_stride=1, window_size=0, use_mask_unit_attn=False):\n",
        "        super().__init__()\n",
        "        self.in_dim, self.d_model = in_dim, d_model\n",
        "        self.n_heads, self.d_head = n_heads, d_model // n_heads\n",
        "        self.scale = self.d_head**-.5\n",
        "        self.qkv = nn.Linear(in_dim, 3*d_model)\n",
        "        self.lin = nn.Linear(d_model, d_model)\n",
        "        self.q_stride = q_stride # If greater than 1, pool q with this stride. The stride should be flattened (e.g., 2x2 = 4).\n",
        "        self.window_size = window_size # The current (flattened) size of a mask unit *after* pooling (if any).\n",
        "        self.use_mask_unit_attn = use_mask_unit_attn # Use Mask Unit or Global Attention.\n",
        "        # within each window, pool exery q_stride\n",
        "\n",
        "    def forward(self, x): # [b,t,c]\n",
        "        B, N, _ = x.shape\n",
        "        num_windows = ((N // (self.q_stride * self.window_size)) if self.use_mask_unit_attn else 1)\n",
        "        q, k, v = self.qkv(x).reshape(B, -1, num_windows, 3, self.n_heads, self.d_head).permute(3, 0, 4, 2, 1, 5) # [b,t,3*d_model] -> [b, t/num_windows = q_stride*window_size, num_windows, 3, n_heads, d_head] -> [3, b, n_heads, num_windows, q_stride*window_size, d_head]\n",
        "        if self.q_stride > 1:\n",
        "            # Refer to Unroll to see how this performs a maxpool-Nd\n",
        "            q = q.view(B, self.n_heads, num_windows, self.q_stride, -1, self.d_head).max(dim=3).values # [b, n_heads, num_windows, q_stride,window_size, d_head]-> [b, n_heads, num_windows, window_size, d_head]\n",
        "\n",
        "        q, k = q.softmax(dim=-1)*self.scale, k.softmax(dim=-2)\n",
        "        context = k.transpose(-2,-1) @ v # [b, n_heads, num_windows, d_head, d_head]\n",
        "        x = q @ context # [b, n_heads, num_windows, window_size, d_head]\n",
        "\n",
        "        # # x = F.scaled_dot_product_attention(q, k, v)\n",
        "        # attn = (q * self.scale) @ k.transpose(-1, -2) # [b, n_heads, num_windows, window_size, d_head] @ [b, n_heads, num_windows, d_head, q_stride*window_size] = [b, n_heads, num_windows, window_size, q_stride*window_size]\n",
        "        # attn = attn.softmax(dim=-1)\n",
        "        # x = (attn @ v) # [b, n_heads, num_windows, window_size, d_head]\n",
        "\n",
        "        x = x.transpose(1, 3).reshape(B, -1, self.d_model)\n",
        "        x = self.lin(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "def do_pool(x, stride): # [b,t,c]\n",
        "    # Refer to `Unroll` to see how this performs a maxpool-Nd # [b, (Sy, Sx, h/Sy, w/Sx), c] -> [b, (Sy, Sx), (h/Sy, w/Sx), c] -> [b, (h/Sy, w/Sx), c]\n",
        "    # return x.view(x.shape[0], stride, -1, x.shape[-1]).max(dim=1).values # [b,s,t/s,c] -> [b,t/s,c]\n",
        "    return x.unflatten(1, (stride, -1)).max(dim=1).values # [b,s,t/s,c] -> [b,t/s,c]\n",
        "\n",
        "\n",
        "class HieraBlock(nn.Module):\n",
        "    def __init__(self, dim, dim_out, heads, drop_path = 0,\n",
        "        q_stride = 1, window_size = 0, use_mask_unit_attn = False,):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.dim_out = dim_out\n",
        "        self.norm1 = nn.LayerNorm(dim)\n",
        "        self.attn = MaskUnitAttention(dim, dim_out, heads, q_stride, window_size, use_mask_unit_attn)\n",
        "        self.norm2 = nn.LayerNorm(dim_out)\n",
        "        self.mlp = Mlp(dim_out, int(dim_out * 4), act_layer=nn.GELU)\n",
        "        self.drop_path = DropPath(drop_path) if drop_path > 0 else nn.Identity()\n",
        "        if dim != dim_out:\n",
        "            self.proj = nn.Linear(dim, dim_out)\n",
        "\n",
        "    def forward(self, x): # [b,t,c] # [b, (Sy, Sx, h/Sy, w/Sx), c]\n",
        "        # Attention + Q Pooling\n",
        "        x_norm = self.norm1(x)\n",
        "        if self.dim != self.dim_out:\n",
        "            x = do_pool(self.proj(x_norm), stride=self.attn.q_stride) # downsample res\n",
        "        x = x + self.drop_path(self.attn(x_norm))\n",
        "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
        "        return x\n",
        "\n",
        "\n",
        "class Head(nn.Module):\n",
        "    def __init__(self, dim, out_dim, drop=0):\n",
        "        super().__init__()\n",
        "        self.lin = nn.Sequential(\n",
        "            nn.LayerNorm(dim), nn.Dropout(drop), nn.Linear(dim, out_dim),\n",
        "        )\n",
        "        self.act_func = nn.Softmax(dim=-1) # act_fun for eval and testing only\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.lin(x)\n",
        "        if not self.training: x = self.act_func(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Hiera(nn.Module):\n",
        "    def __init__(self, input_size = (224, 224), in_chans = 3,\n",
        "        embed_dim = 96,  # initial embed dim\n",
        "        num_heads = 1,  # initial number of heads\n",
        "        num_classes = 1000,\n",
        "        stages = (2, 3, 16, 3),\n",
        "        q_pool = 3,  # number of q_pool stages\n",
        "        q_stride = (2, 2),\n",
        "        mask_unit_size = (8, 8),  # must divide q_stride ** (#stages-1)\n",
        "        # mask_unit_attn: which stages use mask unit attention?\n",
        "        mask_unit_attn = (True, True, False, False),\n",
        "        patch_kernel = (7, 7), patch_stride = (4, 4), patch_padding = (3, 3),\n",
        "        drop_path_rate = 0.0,\n",
        "        sep_pos_embed = False,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        depth = sum(stages)\n",
        "        self.patch_stride = patch_stride\n",
        "        self.tokens_spatial_shape = [i // s for i, s in zip(input_size, patch_stride)]\n",
        "        num_tokens = math.prod(self.tokens_spatial_shape)\n",
        "        flat_mu_size = math.prod(mask_unit_size)\n",
        "        flat_q_stride = math.prod(q_stride)\n",
        "\n",
        "        assert q_pool < len(stages)\n",
        "        self.q_pool, self.q_stride = q_pool, q_stride\n",
        "        self.mu_size, self.mask_unit_size = flat_mu_size, mask_unit_size\n",
        "        self.mask_spatial_shape = [i // s for i, s in zip(self.tokens_spatial_shape, self.mask_unit_size)]\n",
        "        self.stage_ends = [sum(stages[:i]) - 1 for i in range(1, len(stages) + 1)]\n",
        "        # self.patch_embed = PatchEmbed(in_chans, embed_dim, patch_kernel, patch_stride, patch_padding)\n",
        "        # self.patch_embed = nn.Conv2d(in_chans, embed_dim, patch_kernel, patch_stride, patch_padding)\n",
        "        self.proj = conv_nd(len(patch_kernel))(in_chans, embed_dim, patch_kernel, patch_stride, patch_padding)\n",
        "        # self.proj = conv_nd(len(kernel))(dim_in, dim_out, kernel, stride, padding)\n",
        "\n",
        "\n",
        "        self.sep_pos_embed = sep_pos_embed\n",
        "        if sep_pos_embed:\n",
        "            self.pos_embed_spatial = nn.Parameter(torch.randn(1, self.tokens_spatial_shape[1] * self.tokens_spatial_shape[2], embed_dim)*0.02)\n",
        "            self.pos_embed_temporal = nn.Parameter(torch.randn(1, self.tokens_spatial_shape[0], embed_dim)*0.02)\n",
        "        else:\n",
        "            self.pos_embed = nn.Parameter(torch.randn(1, num_tokens, embed_dim)*0.02)\n",
        "\n",
        "        # Setup roll and reroll modules\n",
        "        self.unroll = Unroll(input_size, patch_stride, [q_stride] * len(self.stage_ends[:-1]))\n",
        "        # self.reroll = Reroll(input_size, patch_stride, [q_stride] * len(self.stage_ends[:-1]), self.stage_ends, q_pool)\n",
        "        # q_pool locations\n",
        "        q_pool_blocks = [x + 1 for x in self.stage_ends[:q_pool]]\n",
        "        # stochastic depth decay rule\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]\n",
        "\n",
        "        # Transformer blocks\n",
        "        cur_stage = 0\n",
        "        self.blocks = nn.ModuleList()\n",
        "\n",
        "        for i in range(depth):\n",
        "            dim_out = embed_dim\n",
        "            # Mask unit or global attention.\n",
        "            # Lag by 1 block, so that global attention,\n",
        "            # applied post pooling on lower resolution\n",
        "            use_mask_unit_attn = mask_unit_attn[cur_stage]\n",
        "\n",
        "            if i - 1 in self.stage_ends:\n",
        "                dim_out = embed_dim * 2\n",
        "                num_heads = num_heads * 2\n",
        "                cur_stage += 1\n",
        "                if i in q_pool_blocks:\n",
        "                    flat_mu_size //= flat_q_stride\n",
        "\n",
        "            block = HieraBlock(dim=embed_dim, dim_out=dim_out, heads=num_heads,\n",
        "                drop_path=dpr[i],\n",
        "                q_stride=(flat_q_stride if i in q_pool_blocks else 1),\n",
        "                window_size=flat_mu_size,\n",
        "                use_mask_unit_attn=use_mask_unit_attn,\n",
        "            )\n",
        "            embed_dim = dim_out\n",
        "            self.blocks.append(block)\n",
        "        self.head = Head(embed_dim, num_classes)\n",
        "\n",
        "\n",
        "    def get_pos_embed(self):\n",
        "        if self.sep_pos_embed:\n",
        "            return self.pos_embed_spatial.repeat(1, self.tokens_spatial_shape[0], 1) + torch.repeat_interleave(self.pos_embed_temporal, self.tokens_spatial_shape[1] * self.tokens_spatial_shape[2], dim=1)\n",
        "        else:\n",
        "            return self.pos_embed\n",
        "\n",
        "    def forward(self, x, mask=None): # [b,c,h,w], [b,(f*)h*w]\n",
        "\n",
        "        if mask != None: x = x * F.interpolate(mask.unflatten(-1, (1, *self.mask_spatial_shape)).float(), size=x.shape[2:], mode='nearest-exact').bool()\n",
        "        x = self.proj(x)\n",
        "        x = x.flatten(2).transpose(1, 2) # [b,t,c]\n",
        "\n",
        "        x = x + self.get_pos_embed()\n",
        "        x = self.unroll(x) # [B, (H, W), C] -> [B, (Sy, Sx, H // Sy, W // Sx), C] ; stride of (Sy, Sx)\n",
        "\n",
        "        # print(x.shape, mask.shape)\n",
        "        # Discard masked tokens\n",
        "        if mask is not None:\n",
        "            x = x[mask[..., None].tile(1, self.mu_size, x.shape[2])].view(x.shape[0], -1, x.shape[-1])\n",
        "        # print(x.shape)\n",
        "\n",
        "        for i, blk in enumerate(self.blocks):\n",
        "            x = blk(x)\n",
        "\n",
        "        if mask is None: # mae decoder\n",
        "            x = self.head(x.mean(dim=1))\n",
        "\n",
        "        # x may not always be in spatial order here.\n",
        "        # e.g. if q_pool = 2, mask_unit_size = (8, 8), and\n",
        "        # q_stride = (2, 2), not all unrolls were consumed,\n",
        "        # intermediates[-1] is x in spatial order\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model = Hiera(embed_dim=16, num_heads=1, stages=(1, 1, 1, 1))\n",
        "batch=24\n",
        "# x = torch.randn(batch, 3, 224, 224)\n",
        "\n",
        "# mask should be a boolean tensor of shape [B, #MUt*#MUy*#MUx] where #MU are the number of mask units in that dim.\n",
        "# Note: 1 in mask is *keep*, 0 is *remove*; mask.sum(dim=-1) should be the same across the batch.\n",
        "# mask = (torch.randn(batch, (224//4)*(224//4))>0)\n",
        "print(model.mask_spatial_shape)\n",
        "gamma=0.8\n",
        "length = 7*7\n",
        "\n",
        "device=x.device\n",
        "\n",
        "def randpatch(length, B, gamma=0.9): # num patches of seq, mask patch size, masking ratio\n",
        "    g = torch.normal(gamma, std=.1, size=(1,)).clamp(.5,.9)\n",
        "    # g = gamma\n",
        "    noise = torch.rand(B, length, device=device)\n",
        "    ids = torch.argsort(noise, dim=1)  # ascend: small is keep, large is remove\n",
        "\n",
        "    mask = torch.zeros((B, length), dtype=bool, device=device)\n",
        "    mask[:, :int(length*(1-gamma))] = True\n",
        "    mask = torch.gather(mask, dim=1, index=ids)\n",
        "\n",
        "    # idx = torch.randperm(length)[:int(length*gamma)]\n",
        "    # mask = torch.zeros((B, length), dtype=bool)\n",
        "    # mask[:,idx] = True\n",
        "    return mask\n",
        "\n",
        "# mask = randpatch(length, batch, gamma=0.9)\n",
        "# print(mask)\n",
        "# print(mask.sum(-1))\n",
        "\n",
        "\n",
        "# print(mask)\n",
        "print(mask.shape)\n",
        "y = model(x, mask)\n",
        "print(y.shape)\n",
        "# print(y)\n",
        "\n",
        "\n",
        "# # Image models\n",
        "# def hiera_tiny_224( Hiera(embed_dim=96, num_heads=1, stages=(1, 2, 7, 2), **kwdargs)\n",
        "# def hiera_small_224( Hiera(embed_dim=96, num_heads=1, stages=(1, 2, 11, 2), **kwdargs)\n",
        "# def hiera_base_224( Hiera(embed_dim=96, num_heads=1, stages=(2, 3, 16, 3), **kwdargs)\n",
        "# def hiera_base_plus_224( Hiera(embed_dim=112, num_heads=2, stages=(2, 3, 16, 3), **kwdargs)\n",
        "# def hiera_large_224(Hiera(embed_dim=144, num_heads=2, stages=(2, 6, 36, 4), **kwdargs)\n",
        "# def hiera_huge_224( Hiera(embed_dim=256, num_heads=4, stages=(2, 6, 36, 4), **kwdargs)\n",
        "\n",
        "# # Video models\n",
        "# def hiera_base_16x224(num_classes = 400, **kwdargs):\n",
        "#     return Hiera(\n",
        "#         num_classes=num_classes,  # K400 has 400 classes\n",
        "#         input_size=(16, 224, 224),\n",
        "#         q_stride=(1, 2, 2),\n",
        "#         mask_unit_size=(1, 8, 8),\n",
        "#         patch_kernel=(3, 7, 7), patch_stride=(2, 4, 4), patch_padding=(1, 3, 3),\n",
        "#         sep_pos_embed=True,\n",
        "#     )\n",
        "\n",
        "# def hiera_base_plus_16x224(hiera_base_16x224(embed_dim=112, num_heads=2, stages=(2, 3, 16, 3), **kwdargs\n",
        "# def hiera_large_16x224( hiera_base_16x224(embed_dim=144, num_heads=2, stages=(2, 6, 36, 4), **kwdargs\n",
        "# def hiera_huge_16x224( hiera_base_16x224(embed_dim=256, num_heads=4, stages=(2, 6, 36, 4), **kwdargs\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bbfdeb8-5188-48cc-ae7c-37b383817535",
        "cellView": "form",
        "id": "EgfwkJh-wt3w"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[7, 7]\n",
            "torch.Size([24, 49])\n",
            "torch.Size([24, 4, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title hiera drawer\n",
        "def get_resized_mask(target_size, mask):\n",
        "    # target_size: [(T), (H), W]\n",
        "    # (spatial) mask: [B, C, (t), (h), w]\n",
        "    if mask is None:\n",
        "        return mask\n",
        "    assert len(mask.shape[2:]) == len(target_size)\n",
        "    if mask.shape[2:] != target_size:\n",
        "        print('get_resized_mask', mask.shape, target_size)\n",
        "        return F.interpolate(mask.float(), size=target_size)\n",
        "    return mask\n",
        "\n",
        "\n",
        "def do_masked_conv(x, conv, mask = None):\n",
        "    \"\"\"Zero-out the masked regions of the input before conv. Prevents leakage of masked regions when using overlapping kernels.\"\"\"\n",
        "    if conv is None: return x\n",
        "    if mask is None: return conv(x)\n",
        "    mask = get_resized_mask(target_size=x.shape[2:], mask=mask)\n",
        "    return conv(x * mask.bool())\n",
        "\n",
        "\n",
        "class PatchEmbed(nn.Module):\n",
        "    def __init__(self, dim_in, dim_out, kernel, stride, padding):\n",
        "        super().__init__()\n",
        "        self.proj = conv_nd(len(kernel))(dim_in, dim_out, kernel, stride, padding)\n",
        "\n",
        "    def forward(self, x, mask = None):\n",
        "        # x = do_masked_conv(x, self.proj, mask)\n",
        "        # mask = get_resized_mask(target_size=x.shape[2:], mask=mask)\n",
        "        if mask != None:\n",
        "            mask = mask.view(x.shape[0], 1, *self.mask_spatial_shape)\n",
        "            mask = F.interpolate(mask.float(), size=x.shape[2:])\n",
        "            x = x * mask.bool()\n",
        "        # x = self.proj(x * mask.bool())\n",
        "        x = self.proj(x)\n",
        "        x = x.reshape(x.shape[0], x.shape[1], -1).transpose(2, 1)\n",
        "        return x\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "MXld1eEOusW7"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "s3EO3PgMPH1x",
        "VeMRpak44QHm"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}